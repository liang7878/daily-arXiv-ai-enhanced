{"id": "2602.11443", "pdf": "https://arxiv.org/pdf/2602.11443", "abs": "https://arxiv.org/abs/2602.11443", "authors": ["Abylay Amanbayev", "Brian Tsan", "Tri Dang", "Florin Rusu"], "title": "Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis", "categories": ["cs.DB", "cs.IR"], "comment": "The artifacts are available at: https://github.com/aabylay/ANN-benchmark-HQ", "summary": "Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \\textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \\textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.\n  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \\textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \\textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \\textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.", "AI": {"tldr": "\u6587\u7ae0\u7cfb\u7edf\u5316\u8fc7\u6ee4\u7b56\u7565\u5206\u7c7b\uff0c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u63d0\u51faGLS\u6307\u6807\u8bc4\u4f30FANNS\u8fc7\u6ee4\u7b56\u7565\uff0c\u5b9e\u9a8c\u6709\u65b0\u53d1\u73b0\u5e76\u7ed9\u51fa\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u901a\u7528\u8fc7\u6ee4\u7b56\u7565\u8868\u73b0\u7684\u7406\u89e3\u3002", "method": "\u7cfb\u7edf\u5316\u8fc7\u6ee4\u7b56\u7565\u5206\u7c7b\uff0c\u5f15\u5165\u65b0\u6570\u636e\u96c6MoReVec\uff1b\u63d0\u51faGLS\u6307\u6807\uff1b\u6269\u5c55ANN - Benchmarks\u652f\u6301\u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u3002", "result": "\u5f15\u64ce\u5185\u7b97\u6cd5\u8c03\u6574\u5e38\u8986\u76d6\u539f\u59cb\u7d22\u5f15\u6027\u80fd\uff1bMilvus\u53ec\u56de\u7a33\u5b9a\u6027\u597d\uff1bpgvector\u67e5\u8be2\u4f18\u5316\u5668\u5e38\u9009\u6b21\u4f18\u65b9\u6848\uff1b\u4f4e\u9009\u62e9\u6027\u67e5\u8be2\u65f6IVFFlat\u4f18\u4e8eHNSW\u3002", "conclusion": "\u7ed9\u51fa\u6df7\u5408\u641c\u7d22\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u7d22\u5f15\u7c7b\u578b\u548c\u914d\u7f6e\u67e5\u8be2\u4f18\u5316\u5668\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2602.11573", "pdf": "https://arxiv.org/pdf/2602.11573", "abs": "https://arxiv.org/abs/2602.11573", "authors": ["Wenyang Zhou", "Jiadong Xie", "Yingfan Liu", "Zhihao Yin", "Jeffrey Xu Yu", "Hui Li", "Zhangqian Mu", "Xiaotian Qiao", "Jiangtao Cui"], "title": "Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases", "categories": ["cs.DB"], "comment": null, "summary": "k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ad8\u6548\u6846\u67b6FastPGT\u7528\u4e8e\u8c03\u6574\u90bb\u8fd1\u56fe\uff08PG\uff09\u6784\u5efa\u53c2\u6570\uff0c\u901a\u8fc7\u540c\u65f6\u6784\u5efa\u591a\u4e2aPG\u52a0\u901f\u53c2\u6570\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u663e\u793a\u6bd4SOTA\u65b9\u6cd5VDTuner\u63d0\u901f\u8fbe2.37\u500d\u4e14\u4e0d\u964d\u4f4e\u8c03\u4f18\u8d28\u91cf\u3002", "motivation": "\u90bb\u8fd1\u56fe\uff08PG\uff09\u662f\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4k-\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08k-ANNS\uff09\u7684SOTA\u65b9\u6cd5\uff0c\u4f46PG\u6784\u5efa\u53c2\u6570\u663e\u8457\u5f71\u54cd\u641c\u7d22\u6027\u80fd\uff0c\u8c03\u4f18\u65f6\u6784\u5efa\u548c\u8bc4\u4f30\u56fe\u7d22\u5f15\u6210\u672c\u9ad8\uff0c\u4e14\u76ee\u524d\u65e0\u65b9\u6cd5\u5bf9\u6b64\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u5f15\u5165FastPGT\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u6784\u5efa\u591a\u4e2aPG\u52a0\u901f\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u4fee\u6539SOTA\u8c03\u4f18\u6a21\u578b\u4ee5\u4e00\u6b21\u6027\u63a8\u8350\u591a\u4e2a\u53c2\u6570\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFastPGT\u6bd4SOTA\u65b9\u6cd5VDTuner\u63d0\u901f\u8fbe2.37\u500d\uff0c\u4e14\u4e0d\u964d\u4f4e\u8c03\u4f18\u8d28\u91cf\u3002", "conclusion": "FastPGT\u662f\u4e00\u79cd\u9ad8\u6548\u7684PG\u6784\u5efa\u53c2\u6570\u8c03\u4f18\u6846\u67b6\uff0c\u80fd\u5728\u4e0d\u635f\u5931\u8c03\u4f18\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u9ad8\u8c03\u4f18\u901f\u5ea6\u3002"}}
{"id": "2602.11756", "pdf": "https://arxiv.org/pdf/2602.11756", "abs": "https://arxiv.org/abs/2602.11756", "authors": ["Luigi Asprino", "Enrico Daga"], "title": "Towards a theory of Fa\u00e7ade-X data access: satisfiability of SPARQL basic graph patterns", "categories": ["cs.DB"], "comment": null, "summary": "Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Fa\u00e7ade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Fa\u00e7ade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Fa\u00e7ade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Fa\u00e7ade-X graph. In this article, we consolidate Fa\u00e7ade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Fa\u00e7ade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Fa\u00e7ade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.", "AI": {"tldr": "\u672c\u6587\u5de9\u56fa\u4e86Fa\u00e7ade - X\u65b9\u6cd5\uff0c\u7814\u7a76\u57fa\u672c\u56fe\u6a21\u5f0f\u7684\u53ef\u6ee1\u8db3\u6027\uff0c\u7ed9\u51fa\u5224\u5b9a\u7b97\u6cd5\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u4e3a\u7814\u7a76\u67e5\u8be2\u6267\u884c\u7b56\u7565\u548c\u6784\u5efa\u6570\u636e\u96c6\u6210\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "Fa\u00e7ade - X\u867d\u80fd\u8bbf\u95ee\u591a\u79cd\u6570\u636e\u683c\u5f0f\uff0c\u4f46\u5e76\u975e\u6240\u6709SPARQL\u67e5\u8be2\u5728\u5176\u56fe\u4e0a\u53ef\u6ee1\u8db3\uff0c\u9700\u7814\u7a76\u57fa\u672c\u56fe\u6a21\u5f0f\u7684\u53ef\u6ee1\u8db3\u6027\u3002", "method": "\u5bf9Fa\u00e7ade - X\u8fdb\u884c\u5de9\u56fa\u7814\u7a76\uff0c\u63d0\u51fa\u5224\u5b9a\u57fa\u672c\u56fe\u6a21\u5f0f\u53ef\u6ee1\u8db3\u6027\u7684\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u548c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u5b9e\u9645\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5305\u62ec\u5904\u7406\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76Fa\u00e7ade - X\u6570\u636e\u8bbf\u95ee\u7684\u67e5\u8be2\u6267\u884c\u7b56\u7565\u548c\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\u96c6\u6210\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.11890", "pdf": "https://arxiv.org/pdf/2602.11890", "abs": "https://arxiv.org/abs/2602.11890", "authors": ["Giannis Spiliopoulos", "Alexandros Troupiotis-Kapeliaris", "Kostas Patroumpas", "Nikolaos Liapis", "Dimitrios Skoutas", "Dimitris Zissis", "Nikos Bikakis"], "title": "Data-Driven Trajectory Imputation for Vessel Mobility Analysis", "categories": ["cs.DB", "cs.CG", "cs.RO", "eess.IV"], "comment": "International Conference on Extending Database Technology (EDBT 2026)", "summary": "Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528\u4e8e\u8239\u8236\u8f68\u8ff9\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u914d\u7f6e\u7684HABIT\u63d2\u8865\u6846\u67b6\uff0c\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4f7f\u7528AIS\u6570\u636e\u65f6\u5b58\u5728\u8239\u8236\u8f68\u8ff9\u6709\u5927\u7684\u7a7a\u767d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u63d2\u8865\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8f66\u8f86\u8f68\u8ff9\uff0c\u800c\u8239\u8236\u8fd0\u52a8\u6a21\u5f0f\u5dee\u5f02\u5927\uff0c\u9700\u8981\u65b0\u7684\u63d2\u8865\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHABIT\uff0c\u4e00\u4e2a\u57fa\u4e8eH3\u805a\u5408\u7684\u8239\u8236\u8f68\u8ff9\u63d2\u8865\u6846\u67b6\uff0c\u4ece\u5386\u53f2AIS\u6570\u636e\u4e2d\u63d0\u53d6\u3001\u5206\u6790\u548c\u7d22\u5f15\u8fd0\u52a8\u6a21\u5f0f\u4ee5\u63d2\u8865\u7f3a\u5931\u8f68\u8ff9\u6bb5\u3002", "result": "\u5728\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u3001\u5bc6\u5ea6\u548c\u8239\u8236\u7c7b\u578b\u7684AIS\u6570\u636e\u4e0a\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0cHABIT\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u8003\u8651\u4e86\u8239\u8236\u7279\u5f81\u548c\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "HABIT\u662f\u4e00\u79cd\u6709\u6548\u7684\u8239\u8236\u8f68\u8ff9\u63d2\u8865\u6846\u67b6\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u8239\u8236\u8f68\u8ff9\u63d2\u8865\u95ee\u9898\u3002"}}
{"id": "2602.11414", "pdf": "https://arxiv.org/pdf/2602.11414", "abs": "https://arxiv.org/abs/2602.11414", "authors": ["Kshitiz Upadhyay"], "title": "A physics-informed data-driven framework for modeling hyperelastic materials with progressive damage and failure", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cond-mat.soft"], "comment": null, "summary": "This work presents a two-stage physics-informed, data-driven constitutive modeling framework for hyperelastic soft materials undergoing progressive damage and failure. The framework is grounded in the concept of hyperelasticity with energy limiters and employs Gaussian Process Regression (GPR) to separately learn the intact (undamaged) elastic response and damage evolution directly from data. In Stage I, GPR models learn the intact hyperelastic response through volumetric and isochoric response functions (or only the isochoric response under incompressibility), ensuring energetic consistency of the intact response and satisfaction of fundamental principles such as material frame indifference and balance of angular momentum. In Stage II, damage is modeled via a separate GPR model that learns the mapping between the intact strain energy density predicted by Stage I models and a stress-reduction factor governing damage and failure, with monotonicity, non-negativity, and complete-failure constraints enforced through penalty-based optimization to ensure thermodynamic admissibility. Validation on synthetic datasets, including benchmarking against analytical constitutive models and competing data-driven approaches, demonstrates high in-distribution accuracy under uniaxial tension and robust generalization from limited training data to compression and shear modes not used during training. Application to experimental brain tissue data demonstrates the practical applicability of the framework and enables inference of damage evolution and critical failure energy. Overall, the proposed framework combines the physical consistency, interpretability, and generalizability of analytical models with the flexibility, predictive accuracy, and automation of machine learning, offering a powerful approach for modeling failure in soft materials under limited experimental data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u7269\u7406\u9a71\u52a8\u4e0e\u6570\u636e\u9a71\u52a8\u7ed3\u5408\u7684\u8d85\u5f39\u6027\u8f6f\u6750\u6599\u672c\u6784\u6a21\u578b\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u6709\u9650\u5b9e\u9a8c\u6570\u636e\u4e0b\u7684\u8f6f\u6750\u6599\u5931\u6548\u5efa\u6a21\u3002", "motivation": "\u4e3a\u8d85\u5f39\u6027\u8f6f\u6750\u6599\u5728\u6e10\u8fdb\u635f\u4f24\u548c\u5931\u6548\u60c5\u51b5\u4e0b\u5efa\u7acb\u672c\u6784\u6a21\u578b\uff0c\u7ed3\u5408\u5206\u6790\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u4f18\u52bf\u4ee5\u5e94\u5bf9\u6709\u9650\u5b9e\u9a8c\u6570\u636e\u7684\u5efa\u6a21\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u80fd\u91cf\u9650\u5236\u8d85\u5f39\u6027\u6982\u5ff5\uff0c\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u5206\u4e24\u9636\u6bb5\u5206\u522b\u5b66\u4e60\u5b8c\u6574\u5f39\u6027\u54cd\u5e94\u548c\u635f\u4f24\u6f14\u5316\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u60e9\u7f5a\u4f18\u5316\u65bd\u52a0\u7ea6\u675f\u4fdd\u8bc1\u70ed\u529b\u5b66\u5141\u8bb8\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u9ad8\u5185\u90e8\u5206\u5e03\u7cbe\u5ea6\uff0c\u4ece\u6709\u9650\u8bad\u7ec3\u6570\u636e\u5230\u672a\u8bad\u7ec3\u6a21\u5f0f\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e94\u7528\u4e8e\u5b9e\u9a8c\u8111\u7ec4\u7ec7\u6570\u636e\u53ef\u63a8\u65ad\u635f\u4f24\u6f14\u5316\u548c\u4e34\u754c\u5931\u6548\u80fd\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u5206\u6790\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u4f18\u70b9\uff0c\u662f\u6709\u9650\u5b9e\u9a8c\u6570\u636e\u4e0b\u8f6f\u6750\u6599\u5931\u6548\u5efa\u6a21\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.11315", "pdf": "https://arxiv.org/pdf/2602.11315", "abs": "https://arxiv.org/abs/2602.11315", "authors": ["Oliver Biggar", "Christos Papadimitriou"], "title": "Computing stable limit cycles of learning in games", "categories": ["cs.GT"], "comment": null, "summary": "Many well-studied learning dynamics, such as fictitious play and the replicator, are known to not converge in general $N$-player games. The simplest mode of non-convergence is cyclical or periodic behavior. Such cycles are fundamental objects, and have inspired a number of significant insights in the field, beginning with the pioneering work of Shapley (1964). However a central question remains unanswered: which cycles are stable under game dynamics? In this paper we give a complete and computational answer to this question for the two best-studied dynamics, fictitious play/best-response dynamics and the replicator dynamic. We show (1) that a periodic sequence of profiles is stable under one of these dynamics if and only it is stable under the other, and (2) we provide a polynomial-time spectral stability test to determine whether a given periodic sequence is stable under either dynamic. Finally, we give an entirely `structural' sufficient condition for stability: every cycle that is a sink equilibrium of the preference graph of the game is stable, and moreover it is an attractor of the replicator dynamic. This result generalizes the famous theorems of Shapley (1964) and Jordan (1993), and extends the frontier of recent work relating the preference graph to the replicator attractors.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u865a\u6784\u535a\u5f08/\u6700\u4f18\u54cd\u5e94\u52a8\u6001\u548c\u590d\u5236\u8005\u52a8\u6001\uff0c\u7ed9\u51fa\u4e86\u5468\u671f\u5e8f\u5217\u662f\u5426\u7a33\u5b9a\u7684\u5b8c\u6574\u8ba1\u7b97\u7b54\u6848\uff0c\u8bc1\u660e\u4e24\u79cd\u52a8\u6001\u4e0b\u7a33\u5b9a\u6027\u7684\u7b49\u4ef7\u6027\uff0c\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u8c31\u7a33\u5b9a\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u7ed9\u51fa\u7a33\u5b9a\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "motivation": "\u8bb8\u591a\u5b66\u4e60\u52a8\u6001\u5728N\u4eba\u535a\u5f08\u4e2d\u4e0d\u6536\u655b\uff0c\u5faa\u73af\u662f\u57fa\u672c\u73b0\u8c61\uff0c\u4f46\u54ea\u79cd\u5faa\u73af\u5728\u535a\u5f08\u52a8\u6001\u4e0b\u7a33\u5b9a\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u672c\u6587\u65e8\u5728\u56de\u7b54\u8be5\u95ee\u9898\u3002", "method": "\u5bf9\u865a\u6784\u535a\u5f08/\u6700\u4f18\u54cd\u5e94\u52a8\u6001\u548c\u590d\u5236\u8005\u52a8\u6001\u8fdb\u884c\u7814\u7a76\uff0c\u7ed9\u51fa\u591a\u9879\u5f0f\u65f6\u95f4\u8c31\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u5206\u6790\u504f\u597d\u56fe\u7684\u6c47\u5747\u8861\u3002", "result": "\uff081\uff09\u4e00\u4e2a\u5468\u671f\u5e8f\u5217\u5728\u4e00\u79cd\u52a8\u6001\u4e0b\u7a33\u5b9a\u5f53\u4e14\u4ec5\u5f53\u5728\u53e6\u4e00\u79cd\u52a8\u6001\u4e0b\u7a33\u5b9a\uff1b\uff082\uff09\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u8c31\u7a33\u5b9a\u6d4b\u8bd5\u5224\u65ad\u5468\u671f\u5e8f\u5217\u7a33\u5b9a\u6027\uff1b\uff083\uff09\u535a\u5f08\u504f\u597d\u56fe\u7684\u6c47\u5747\u8861\u5faa\u73af\u662f\u7a33\u5b9a\u7684\uff0c\u4e14\u662f\u590d\u5236\u8005\u52a8\u6001\u7684\u5438\u5f15\u5b50\u3002", "conclusion": "\u672c\u6587\u7ed3\u679c\u63a8\u5e7f\u4e86Shapley\uff081964\uff09\u548cJordan\uff081993\uff09\u7684\u8457\u540d\u5b9a\u7406\uff0c\u62d3\u5c55\u4e86\u504f\u597d\u56fe\u4e0e\u590d\u5236\u8005\u5438\u5f15\u5b50\u76f8\u5173\u7814\u7a76\u7684\u8fb9\u754c\u3002"}}
{"id": "2602.11235", "pdf": "https://arxiv.org/pdf/2602.11235", "abs": "https://arxiv.org/abs/2602.11235", "authors": ["Xin Song", "Zhilin Guan", "Ruidong Han", "Binghao Tang", "Tianwen Chen", "Bing Li", "Zihao Li", "Han Zhang", "Fei Jiang", "Chaolin Xie", "Chi Ma", "Chunyang Jiang", "Chunzhen Jing", "Dengxuan Li", "Fengyi Li", "Lei Yu", "Mengyao Sun", "Pu Wang", "Qing Wang", "Rui Fan", "Shangyu Chen", "Shifeng Du", "Siyuan Bai", "Wei Lin", "Wentao Zhu", "Zhou Han", "Zhuo Chen", "Zikang Xu"], "title": "MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan", "categories": ["cs.IR"], "comment": null, "summary": "Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u63a8\u8350\u6846\u67b6MTFM\uff0c\u901a\u8fc7\u65e0\u5bf9\u9f50\u65b9\u5f0f\u6355\u83b7\u591a\u573a\u666f\u77e5\u8bc6\uff0c\u8fdb\u884c\u7cfb\u7edf\u4f18\u5316\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u548c\u591a\u573a\u666f\u63a8\u8350\u65b9\u6cd5\u9700\u5927\u91cf\u8d44\u6e90\u548c\u4e25\u683c\u8f93\u5165\u5bf9\u9f50\uff0c\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u5c06\u8de8\u57df\u6570\u636e\u8f6c\u6362\u4e3a\u5f02\u6784\u4ee4\u724c\uff0c\u91c7\u7528\u591a\u573a\u666f\u7528\u6237\u7ea7\u6837\u672c\u805a\u5408\uff0c\u96c6\u6210Grouped - Query Attention\u548c\u5b9a\u5236\u7684Hybrid Target Attention\uff0c\u8fdb\u884c\u7cfb\u7edf\u7ea7\u4f18\u5316\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u5927\u6a21\u578b\u5bb9\u91cf\u548c\u591a\u573a\u666f\u8bad\u7ec3\u6570\u636e\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "MTFM\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u63a8\u8350\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2602.11324", "pdf": "https://arxiv.org/pdf/2602.11324", "abs": "https://arxiv.org/abs/2602.11324", "authors": ["Jonas Ellert", "Tomasz Kociumaka"], "title": "Time-Optimal Construction of String Synchronizing Sets", "categories": ["cs.DS"], "comment": "Full version of a work to appear in the proceedings of STACS 2026. The abstract has been abridged to comply with arXiv format requirements", "summary": "A key principle in string processing is local consistency: using short contexts to handle matching fragments of a string consistently. String synchronizing sets [Kempa, Kociumaka; STOC 2019] are an influential instantiation of this principle. A $\u03c4$-synchronizing set of a length-$n$ string is a set of $O(n/\u03c4)$ positions, chosen via their length-$2\u03c4$ contexts, such that (outside highly periodic regions) at least one position in every length-$\u03c4$ window is selected. Among their applications are faster algorithms for data compression, text indexing, and string similarity in the word RAM model.\n  We show how to preprocess any string $T \\in [0..\u03c3)^n$ in $O(n\\log\u03c3/\\log n)$ time so that, for any $\u03c4\\in[1..n]$, a $\u03c4$-synchronizing set of $T$ can be constructed in $O((n\\log\u03c4)/(\u03c4\\log n))$ time. Both bounds are optimal in the word RAM model with word size $w=\u0398(\\log n)$. Previously, the construction time was $O(n/\u03c4)$, either after an $O(n)$-time preprocessing [Kociumaka, Radoszewski, Rytter, Wale\u0144; SICOMP 2024], or without preprocessing if $\u03c4<0.2\\log_\u03c3n$ [Kempa, Kociumaka; STOC 2019].\n  A simple version of our method outputs the set as a sorted list in $O(n/\u03c4)$ time, or as a bitmask in $O(n/\\log n)$ time. Our optimal construction produces a compact fully indexable dictionary, supporting select queries in $O(1)$ time and rank queries in $O(\\log(\\tfrac{\\log\u03c4}{\\log\\log n}))$ time, matching unconditional cell-probe lower bounds for $\u03c4\\le n^{1-\u03a9(1)}$.\n  We achieve this via a new framework for processing sparse integer sequences in a custom variable-length encoding. For rank and select queries, we augment the optimal variant of van Emde Boas trees [P\u0103tra\u015fcu, Thorup; STOC 2006] with a deterministic linear-time construction. The above query-time guarantees hold after preprocessing time proportional to the encoding size (in words).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\uff0c\u53ef\u5728O(n log\u03c3 / log n)\u65f6\u95f4\u9884\u5904\u7406\u5b57\u7b26\u4e32\uff0c\u4ee5\u6700\u4f18\u65f6\u95f4\u6784\u9020\u03c4 - \u540c\u6b65\u96c6\uff0c\u8fd8\u80fd\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u3002", "motivation": "\u6539\u8fdb\u5b57\u7b26\u4e32\u03c4 - \u540c\u6b65\u96c6\u7684\u6784\u9020\u65f6\u95f4\uff0c\u63d0\u5347\u76f8\u5173\u7b97\u6cd5\u6548\u7387\u3002", "method": "\u91c7\u7528\u81ea\u5b9a\u4e49\u53d8\u957f\u7f16\u7801\u5904\u7406\u7a00\u758f\u6574\u6570\u5e8f\u5217\u7684\u65b0\u6846\u67b6\uff0c\u6539\u8fdbvan Emde Boas\u6811\u3002", "result": "\u5b9e\u73b0\u9884\u5904\u7406\u65f6\u95f4O(n log\u03c3 / log n)\uff0c\u6784\u9020\u65f6\u95f4O((n log\u03c4) / (\u03c4 log n))\uff0c\u652f\u6301\u9ad8\u6548\u67e5\u8be2\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b57RAM\u6a21\u578b\u4e0b\u65f6\u95f4\u590d\u6742\u5ea6\u6700\u4f18\uff0c\u80fd\u9ad8\u6548\u6784\u9020\u548c\u67e5\u8be2\u03c4 - \u540c\u6b65\u96c6\u3002"}}
{"id": "2602.11362", "pdf": "https://arxiv.org/pdf/2602.11362", "abs": "https://arxiv.org/abs/2602.11362", "authors": ["Reginald Frank", "Soujanya Ponnapalli", "Octavio Lomeli", "Neil Giridharan", "Marcos K Aguilera", "Natacha Crooks"], "title": "Real Life Is Uncertain. Consensus Should Be Too!", "categories": ["cs.DC", "cs.DB"], "comment": "HotOS '25: Proceedings of the 2025 Workshop on Hot Topics in Operating Systems", "summary": "Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \\textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.", "AI": {"tldr": "\u6307\u51fa\u4f20\u7edf\u5171\u8bc6\u534f\u8bae\u7684f\u9608\u503c\u6545\u969c\u6a21\u578b\u8fc7\u4e8e\u7b80\u5316\uff0c\u63d0\u51fa\u6982\u7387\u6545\u969c\u6a21\u578b\u53ef\u4f7f\u7cfb\u7edf\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u5171\u8bc6\u534f\u8bae\u7684f\u9608\u503c\u6545\u969c\u6a21\u578b\u7b80\u5316\u73b0\u5b9e\u4e16\u754c\uff0c\u9650\u5236\u6210\u672c\u548c\u6027\u80fd\u4f18\u5316\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u6982\u7387\u6545\u969c\u6a21\u578b\u7684\u6982\u7387\u5171\u8bc6\u534f\u8bae\uff0c\u5229\u7528\u673a\u5668\u6545\u969c\u66f2\u7ebf\u3002", "result": "\u53ef\u4f7f\u7cfb\u7edf\u907f\u514d\u4f20\u7edf\u74f6\u9888\uff0c\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u53ef\u6301\u7eed\u3002", "conclusion": "\u6982\u7387\u6545\u969c\u6a21\u578b\u66f4\u9002\u5408\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4f18\u5316\u3002"}}
{"id": "2602.11506", "pdf": "https://arxiv.org/pdf/2602.11506", "abs": "https://arxiv.org/abs/2602.11506", "authors": ["Zhen Bi", "Xueshu Chen", "Luoyang Sun", "Yuhang Yao", "Qing Shen", "Jungang Lou", "Cheng Deng"], "title": "RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.PF"], "comment": null, "summary": "The transition toward localized intelligence through Small Language Models (SLMs) has intensified the need for rigorous performance characterization on resource-constrained edge hardware. However, objectively measuring the theoretical performance ceilings of diverse architectures across heterogeneous platforms remains a formidable challenge. In this work, we propose a systematic framework based on the Roofline model that unifies architectural primitives and hardware constraints through the lens of operational intensity (OI). By defining an inference-potential region, we introduce the Relative Inference Potential as a novel metric to compare efficiency differences between Large Language Models (LLMs) on the same hardware substrate. Extensive empirical analysis across diverse compute tiers reveals that variations in performance and OI are significantly influenced by sequence length. We further identify a critical regression in OI as model depth increases. Additionally, our findings highlight an efficiency trap induced by hardware heterogeneity and demonstrate how structural refinements, such as Multi-head Latent Attention (M LA), can effectively unlock latent inference potential across various hardware substrates. These insights provide actionable directions for hardware-software co-design to align neural structures with physical constraints in on-device intelligence. The released code is available in the Appendix C.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRoofline\u6a21\u578b\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u64cd\u4f5c\u5f3a\u5ea6\u8861\u91cf\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\uff0c\u63ed\u793a\u6027\u80fd\u4e0e\u64cd\u4f5c\u5f3a\u5ea6\u53d7\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\uff0c\u8fd8\u63d0\u51fa\u5e94\u5bf9\u786c\u4ef6\u5f02\u6784\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5411\u672c\u5730\u667a\u80fd\u8fc7\u6e21\uff0c\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u786c\u4ef6\u4e0a\u4e25\u683c\u8868\u5f81\u6027\u80fd\uff0c\u4f46\u5ba2\u89c2\u8861\u91cf\u4e0d\u540c\u67b6\u6784\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u662f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eRoofline\u6a21\u578b\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u7edf\u4e00\u67b6\u6784\u539f\u8bed\u548c\u786c\u4ef6\u7ea6\u675f\uff1b\u5b9a\u4e49\u63a8\u7406\u6f5c\u529b\u533a\u57df\uff0c\u5f15\u5165\u76f8\u5bf9\u63a8\u7406\u6f5c\u529b\u6307\u6807\u3002", "result": "\u6027\u80fd\u548c\u64cd\u4f5c\u5f3a\u5ea6\u53d7\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\uff1b\u6a21\u578b\u6df1\u5ea6\u589e\u52a0\u64cd\u4f5c\u5f3a\u5ea6\u4f1a\u9000\u5316\uff1b\u786c\u4ef6\u5f02\u6784\u5b58\u5728\u6548\u7387\u9677\u9631\uff0c\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u53ef\u91ca\u653e\u63a8\u7406\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u786c\u4ef6 - \u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u65b9\u5411\uff0c\u4f7f\u795e\u7ecf\u7ed3\u6784\u4e0e\u7269\u7406\u7ea6\u675f\u76f8\u5339\u914d\u3002"}}
{"id": "2602.11398", "pdf": "https://arxiv.org/pdf/2602.11398", "abs": "https://arxiv.org/abs/2602.11398", "authors": ["Hormoz Shahrzad", "Niharika Gajawell", "Kaitlin Maile", "Manish Saggar", "Risto Miikkulainen"], "title": "Evolution With Purpose: Hierarchy-Informed Optimization of Whole-Brain Models", "categories": ["cs.NE"], "comment": null, "summary": "Evolutionary search is well suited for large-scale biophysical brain modeling, where many parameters with nonlinear interactions and no tractable gradients need to be optimized. Standard evolutionary approaches achieve an excellent fit to MRI data; however, among many possible such solutions, it finds ones that overfit to individual subjects and provide limited predictive power. This paper investigates whether guiding evolution with biological knowledge can help. Focusing on whole-brain Dynamic Mean Field (DMF) models, a baseline where 20 parameters were shared across the brain was compared against a heterogeneous formulation where different sets of 20 parameters were used for the seven canonical brain regions. The heterogeneous model was optimized using four strategies: optimizing all parameters at once, a curricular approach following the hierarchy of brain networks (HICO), a reversed curricular approach, and a randomly shuffled curricular approach. While all heterogeneous strategies fit the data well, only curricular approaches generalized to new subjects. Most importantly, only HICO made it possible to use the parameter sets to predict the subjects' behavioral abilities as well. Thus, by guiding evolution with biological knowledge about the hierarchy of brain regions, HICO demonstrated how domain knowledge can be harnessed to serve the purpose of optimization in real-world domains.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u7528\u751f\u7269\u77e5\u8bc6\u5f15\u5bfc\u8fdb\u5316\u641c\u7d22\u4f18\u5316\u5168\u8111\u52a8\u6001\u5e73\u5747\u573a\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e0d\u540c\u7b56\u7565\uff0c\u53d1\u73b0\u9075\u5faa\u8111\u7f51\u7edc\u5c42\u6b21\u7684\u8bfe\u7a0b\u5f0f\u65b9\u6cd5\uff08HICO\uff09\u80fd\u6cdb\u5316\u5230\u65b0\u5bf9\u8c61\u5e76\u53ef\u9884\u6d4b\u884c\u4e3a\u80fd\u529b\uff0c\u5c55\u793a\u9886\u57df\u77e5\u8bc6\u5728\u4f18\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u6807\u51c6\u8fdb\u5316\u65b9\u6cd5\u5728\u5927\u5c3a\u5ea6\u751f\u7269\u7269\u7406\u8111\u6a21\u578b\u4f18\u5316\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u4e2a\u4f53\u5bf9\u8c61\u3001\u9884\u6d4b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u7814\u7a76\u7528\u751f\u7269\u77e5\u8bc6\u5f15\u5bfc\u8fdb\u5316\u662f\u5426\u6709\u5e2e\u52a9\u3002", "method": "\u805a\u7126\u5168\u8111\u52a8\u6001\u5e73\u5747\u573a\u6a21\u578b\uff0c\u5bf9\u6bd4\u53c2\u6570\u5171\u4eab\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u4e0d\u540c\u8111\u533a\u7528\u4e0d\u540c\u53c2\u6570\u96c6\u7684\u5f02\u8d28\u6a21\u578b\uff0c\u7528\u56db\u79cd\u7b56\u7565\u4f18\u5316\u5f02\u8d28\u6a21\u578b\uff0c\u5305\u62ec\u4e00\u6b21\u6027\u4f18\u5316\u3001\u9075\u5faa\u8111\u7f51\u7edc\u5c42\u6b21\u7684\u8bfe\u7a0b\u5f0f\u65b9\u6cd5\uff08HICO\uff09\u3001\u53cd\u5411\u8bfe\u7a0b\u5f0f\u65b9\u6cd5\u548c\u968f\u673a\u6253\u4e71\u7684\u8bfe\u7a0b\u5f0f\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u5f02\u8d28\u7b56\u7565\u90fd\u80fd\u5f88\u597d\u62df\u5408\u6570\u636e\uff0c\u4f46\u53ea\u6709\u8bfe\u7a0b\u5f0f\u65b9\u6cd5\u80fd\u6cdb\u5316\u5230\u65b0\u5bf9\u8c61\uff0c\u4e14\u53ea\u6709 HICO \u80fd\u7528\u53c2\u6570\u96c6\u9884\u6d4b\u5bf9\u8c61\u7684\u884c\u4e3a\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7528\u5927\u8111\u533a\u57df\u5c42\u6b21\u7684\u751f\u7269\u77e5\u8bc6\u5f15\u5bfc\u8fdb\u5316\uff0cHICO \u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u9886\u57df\u77e5\u8bc6\u670d\u52a1\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u4f18\u5316\u3002"}}
{"id": "2602.11209", "pdf": "https://arxiv.org/pdf/2602.11209", "abs": "https://arxiv.org/abs/2602.11209", "authors": ["Ziyi Yang", "Kalit Inani", "Keshav Kabra", "Vima Gupta", "Anand Padmanabha Iyer"], "title": "SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code", "categories": ["cs.SE", "cs.CR"], "comment": "11 pages, 6 figures, 4 tables", "summary": "While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.", "AI": {"tldr": "\u73b0\u6709\u6d4b\u8bd5\u6846\u67b6\u96be\u9002\u914dAI\u751f\u6210\u4ee3\u7801\uff0c\u63d0\u51faSAFuzz\u6df7\u5408\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u7cbe\u5ea6\u3001\u964d\u4f4e\u65f6\u95f4\u6210\u672c\uff0c\u4e0e\u5355\u5143\u6d4b\u8bd5\u7ed3\u5408\u6709\u4e92\u8865\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6d4b\u8bd5\u6846\u67b6\u96be\u4ee5\u8ddf\u4e0aAI\u751f\u6210\u4ee3\u7801\u7684\u6570\u91cf\uff0c\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u8d44\u6e90\u5206\u914d\u5747\u5300\u3001\u7f3a\u4e4f\u8bed\u4e49\u611f\u77e5\uff0c\u5bfc\u81f4\u8d44\u6e90\u4f7f\u7528\u4f4e\u6548\u548c\u6f0f\u6d1e\u9057\u6f0f\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6d4b\u8bd5\u6846\u67b6SAFuzz\uff0c\u96c6\u6210\u57fa\u4e8e\u63d0\u793a\u7684\u884c\u4e3a\u591a\u6837\u5316\u3001\u5229\u7528\u7279\u5b9a\u95ee\u9898\u9884\u8a00\u673a\u7684\u7ebf\u675f\u751f\u6210\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u548c\u52a8\u6001\u65e9\u671f\u505c\u6b62\u3002", "result": "\u5728CSES\u7b97\u6cd5\u95ee\u9898\u4e0a\uff0c\u5c06\u6f0f\u6d1e\u5224\u522b\u7cbe\u5ea6\u4ece77.9%\u63d0\u9ad8\u523085.7%\uff0c\u65f6\u95f4\u6210\u672c\u6bd4SOTA GreenFuzz\u964d\u4f4e1.71\u500d\uff0c\u4e0e\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u7ed3\u5408\u4f7f\u6f0f\u6d1e\u68c0\u6d4b\u53ec\u56de\u7387\u4ece67.3%\u63d0\u9ad8\u523079.5%\u3002", "conclusion": "SAFuzz\u80fd\u6709\u6548\u68c0\u6d4b\u7b97\u6cd5\u6f0f\u6d1e\uff0c\u4e0e\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u7ed3\u5408\u6709\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2602.11334", "pdf": "https://arxiv.org/pdf/2602.11334", "abs": "https://arxiv.org/abs/2602.11334", "authors": ["Hashem Dezhbakhsh", "Daniel Levy"], "title": "Interpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results", "categories": ["econ.GN"], "comment": null, "summary": "It is well established that the US prewar output was more volatile and less shock persistent than the postwar output. This is often attributed to the data interpolation employed to construct the prewar series. Our analytical results, however, indicate that commonly used linear interpolation has the opposite effect on shock persistence and volatility of a series - it increases shock persistence and reduces volatility. The surprising implication of this finding is that the actual differences between the volatility and shock persistence of the prewar and postwar output series are likely greater than the existing literature recognizes, and interpolation has dampened rather than magnified this difference. Consequently, the view that postwar output was more stable than prewar output because of the effectiveness of the postwar stabilization policies and institutional changes has considerable merit. Our results hold for parsimonious stationary and nonstationary time series commonly used to model macroeconomic time series", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5e38\u7528\u7ebf\u6027\u5185\u63d2\u6cd5\u5bf9\u5e8f\u5217\u51b2\u51fb\u6301\u7eed\u6027\u548c\u6ce2\u52a8\u6027\u7684\u4f5c\u7528\u4e0e\u8ba4\u4e3a\u7684\u4e0d\u540c\uff0c\u5b9e\u9645\u6218\u524d\u548c\u6218\u540e\u4ea7\u51fa\u5dee\u5f02\u6216\u66f4\u5927\uff0c\u6218\u540e\u4ea7\u51fa\u66f4\u7a33\u5b9a\u7684\u89c2\u70b9\u6709\u4ef7\u503c\u3002", "motivation": "\u89e3\u91ca\u7f8e\u56fd\u6218\u524d\u4ea7\u51fa\u6bd4\u6218\u540e\u66f4\u5177\u6ce2\u52a8\u6027\u548c\u66f4\u4f4e\u51b2\u51fb\u6301\u7eed\u6027\u5e38\u88ab\u5f52\u56e0\u4e8e\u6570\u636e\u5185\u63d2\u6cd5\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u8fdb\u884c\u5206\u6790\u5f97\u51fa\u5e38\u7528\u7ebf\u6027\u5185\u63d2\u6cd5\u5bf9\u5e8f\u5217\u51b2\u51fb\u6301\u7eed\u6027\u548c\u6ce2\u52a8\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5e38\u7528\u7ebf\u6027\u5185\u63d2\u6cd5\u589e\u52a0\u51b2\u51fb\u6301\u7eed\u6027\u5e76\u964d\u4f4e\u6ce2\u52a8\u6027\uff0c\u5b9e\u9645\u6218\u524d\u548c\u6218\u540e\u4ea7\u51fa\u5dee\u5f02\u53ef\u80fd\u66f4\u5927\u3002", "conclusion": "\u6218\u540e\u4ea7\u51fa\u56e0\u653f\u7b56\u548c\u5236\u5ea6\u53d8\u5316\u66f4\u7a33\u5b9a\u7684\u89c2\u70b9\u6709\u76f8\u5f53\u4ef7\u503c\uff0c\u7ed3\u679c\u9002\u7528\u4e8e\u5e38\u7528\u5b8f\u89c2\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002"}}
{"id": "2602.11159", "pdf": "https://arxiv.org/pdf/2602.11159", "abs": "https://arxiv.org/abs/2602.11159", "authors": ["Natalia Abarca", "Andr\u00e9s Carvallo", "Claudia L\u00f3pez Moncada", "Felipe Bravo-Marquez"], "title": "Explaining AI Without Code: A User Study on Explainable AI", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "LatinX in AI Workshop @ NeurIPS-25", "summary": "The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\u03b1$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\u03b1$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.", "AI": {"tldr": "\u63d0\u51faDashAI\u65e0\u4ee3\u7801\u673a\u5668\u5b66\u4e60\u5e73\u53f0\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684XAI\u6a21\u5757\uff0c\u96c6\u6210\u4e09\u79cd\u6280\u672f\u7528\u4e8e\u8868\u683c\u5206\u7c7b\uff0c\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u53ef\u7528\u6027\u548c\u89e3\u91ca\u6548\u679c\uff0c\u6307\u51faXAI\u5728\u65e0\u4ee3\u7801ML\u4e2d\u9700\u517c\u987e\u65b0\u624b\u548c\u4e13\u5bb6\u9700\u6c42\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u654f\u611f\u9886\u57df\u5e94\u7528\u5f15\u53d1\u81ea\u52a8\u5316\u51b3\u7b56\u900f\u660e\u5ea6\u62c5\u5fe7\uff0c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u591a\u9700\u6280\u672f\u4e13\u957f\uff0c\u65e0\u4ee3\u7801ML\u5e73\u53f0\u7f3a\u5c11\u53ef\u89e3\u91ca\u6027\uff0c\u8981\u89e3\u51b3\u65b0\u624b\u548c\u4e13\u5bb6\u5bf9\u89e3\u91ca\u7684\u4e0d\u540c\u9700\u6c42\u95ee\u9898\u3002", "method": "\u5728DashAI\u65e0\u4ee3\u7801ML\u5e73\u53f0\u4e2d\u5f15\u5165XAI\u6a21\u5757\uff0c\u96c6\u6210PDP\u3001PFI\u548cKernelSHAP\u4e09\u79cd\u6280\u672f\u5230\u8868\u683c\u5206\u7c7b\u5de5\u4f5c\u6d41\uff0c\u5f00\u5c55\u670920\u540d\u65b0\u624b\u548c\u4e13\u5bb6\u53c2\u4e0e\u7684\u7528\u6237\u7814\u7a76\u3002", "result": "\u6240\u6709\u53ef\u89e3\u91ca\u6027\u4efb\u52a1\u4efb\u52a1\u6210\u529f\u7387\u226580%\uff1b\u65b0\u624b\u8ba4\u4e3a\u89e3\u91ca\u6709\u7528\u3001\u51c6\u786e\u548c\u53ef\u4fe1\uff0c\u4e13\u5bb6\u66f4\u5173\u6ce8\u5145\u5206\u6027\u548c\u5b8c\u6574\u6027\uff1b\u89e3\u91ca\u63d0\u5347\u4e86\u5bf9\u81ea\u52a8\u5316\u7684\u53ef\u9884\u6d4b\u6027\u548c\u4fe1\u5fc3\u611f\u77e5\uff0c\u65b0\u624b\u4fe1\u4efb\u5ea6\u9ad8\u4e8e\u4e13\u5bb6\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u5728\u65e0\u4ee3\u7801\u673a\u5668\u5b66\u4e60\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u9700\u4f7f\u89e3\u91ca\u5bf9\u65b0\u624b\u6613\u83b7\u53d6\u4e14\u5bf9\u4e13\u5bb6\u8db3\u591f\u8be6\u7ec6\u3002"}}
{"id": "2602.12030", "pdf": "https://arxiv.org/pdf/2602.12030", "abs": "https://arxiv.org/abs/2602.12030", "authors": ["Federico Cacciamani", "Roberto Daluiso", "Marco Pinciroli", "Michele Trapletti", "Edoardo Vittori"], "title": "Time-Inhomogeneous Volatility Aversion for Financial Applications of Reinforcement Learning", "categories": ["q-fin.CP", "q-fin.TR"], "comment": "18 pages, 6 figures", "summary": "In finance, sequential decision problems are often faced, for which reinforcement learning (RL) emerges as a promising tool for optimisation without the need of analytical tractability. However, the objective of classical RL is the expected cumulated reward, while financial applications typically require a trade-off between return and risk. In this work, we focus on settings where one cares about the time split of the total return, ruling out most risk-aware generalisations of RL which optimise a risk measure defined on the latter. We notice that a preference for homogeneous splits, which we found satisfactory for hedging, can be unfit for other problems, and therefore propose a new risk metric which still penalises uncertainty of the single rewards, but allows for an arbitrary planning of their target levels. We study the properties of the resulting objective and the generalisation of learning algorithms to optimise it. Finally, we show numerical results on toy examples.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u4e0d\u9002\u7528\u4e8e\u91d1\u878d\u5e8f\u8d2f\u51b3\u7b56\u4e2d\u6536\u76ca\u4e0e\u98ce\u9669\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u98ce\u9669\u5ea6\u91cf\uff0c\u7814\u7a76\u5bf9\u5e94\u76ee\u6807\u6027\u8d28\u53ca\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u7ed9\u51fa\u73a9\u5177\u793a\u4f8b\u6570\u503c\u7ed3\u679c\u3002", "motivation": "\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u662f\u671f\u671b\u7d2f\u79ef\u5956\u52b1\uff0c\u800c\u91d1\u878d\u5e94\u7528\u9700\u6536\u76ca\u4e0e\u98ce\u9669\u6743\u8861\uff0c\u4e14\u5173\u6ce8\u603b\u56de\u62a5\u7684\u65f6\u95f4\u62c6\u5206\uff0c\u73b0\u6709\u57fa\u4e8e\u98ce\u9669\u5ea6\u91cf\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e0d\u9002\u7528\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u98ce\u9669\u5ea6\u91cf\uff0c\u8be5\u5ea6\u91cf\u60e9\u7f5a\u5355\u5956\u52b1\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u5141\u8bb8\u4efb\u610f\u89c4\u5212\u76ee\u6807\u6c34\u5e73\uff0c\u7814\u7a76\u7531\u6b64\u4ea7\u751f\u7684\u76ee\u6807\u6027\u8d28\u548c\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u3002", "result": "\u7ed9\u51fa\u4e86\u73a9\u5177\u793a\u4f8b\u7684\u6570\u503c\u7ed3\u679c\u3002", "conclusion": "\u65b0\u7684\u98ce\u9669\u5ea6\u91cf\u9002\u7528\u4e8e\u5173\u6ce8\u603b\u56de\u62a5\u65f6\u95f4\u62c6\u5206\u7684\u91d1\u878d\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898\u3002"}}
{"id": "2602.11325", "pdf": "https://arxiv.org/pdf/2602.11325", "abs": "https://arxiv.org/abs/2602.11325", "authors": ["Ayush Bharti", "Charita Dellaporta", "Yuga Hikida", "Fran\u00e7ois-Xavier Briol"], "title": "Amortised and provably-robust simulation-based inference", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": null, "summary": "Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u52a0\u6743\u5f97\u5206\u5339\u914d\u635f\u5931\u795e\u7ecf\u8fd1\u4f3c\u7684\u6a21\u62df\u63a8\u7406\u65b0\u65b9\u6cd5\uff0c\u5bf9\u5f02\u5e38\u503c\u7a33\u5065\uff0c\u6709\u8ba1\u7b97\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u5e38\u65e0\u6cd5\u5904\u7406\u56e0\u6d4b\u91cf\u4eea\u5668\u6545\u969c\u6216\u4eba\u4e3a\u8bef\u5dee\u5bfc\u81f4\u7684\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u503c\u548c\u6781\u503c\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5e7f\u4e49\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u52a0\u6743\u5f97\u5206\u5339\u914d\u635f\u5931\u795e\u7ecf\u8fd1\u4f3c\u7684\u6a21\u62df\u63a8\u7406\u65b0\u65b9\u6cd5\uff0c\u9009\u7528\u6761\u4ef6\u5bc6\u5ea6\u6a21\u578b\u3002", "result": "\u5f97\u5230\u4e00\u79cd\u65e2\u644a\u9500\u53c8\u5bf9\u5f02\u5e38\u503c\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u7f57\u62bd\u6837\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u8fdc\u4f4e\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u91cd\u8981\u8ba1\u7b97\u4f18\u52bf\u3002"}}
{"id": "2602.11164", "pdf": "https://arxiv.org/pdf/2602.11164", "abs": "https://arxiv.org/abs/2602.11164", "authors": ["Weiting Liu", "Han Wu", "Yufei Kuang", "Xiongwei Han", "Tao Zhong", "Jianfeng Feng", "Wenlian Lu"], "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\\textbf{m}ated opt\\textbf{i}mization modeli\\textbf{n}g via a localizable error-\\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \\textbf{D}ynamic Supervised \\textbf{F}ine-Tuning \\textbf{P}olicy \\textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4f18\u5316\u5efa\u6a21\u4e2d\u540e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51faMIND\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4f18\u5316\u5efa\u6a21\u4e2d\u540e\u8bad\u7ec3\u56e0\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u5229\u7528\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u89e3\u51b3\u76f8\u5173\u95ee\u9898\u3002", "method": "\u63d0\u51faMIND\u6846\u67b6\uff0c\u57fa\u4e8e\u4f18\u5316\u5efa\u6a21\u8bef\u5dee\u4f20\u64ad\u7684\u5c40\u90e8\u5316\u6a21\u5f0f\uff0c\u6784\u5efa\u805a\u7126\u7684\u9ad8\u5bc6\u5ea6\u8bad\u7ec3\u8bed\u6599\uff0c\u63d0\u51faDFPO\u7b56\u7565\u5904\u7406\u96be\u9898\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMIND\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u81ea\u52a8\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u3002", "conclusion": "MIND\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u540e\u8bad\u7ec3\u7684\u6027\u80fd\u3002"}}
{"id": "2602.11949", "pdf": "https://arxiv.org/pdf/2602.11949", "abs": "https://arxiv.org/abs/2602.11949", "authors": ["Victor Marsault", "Antoine Meyer"], "title": "Designing and Comparing RPQ Semantics", "categories": ["cs.DB", "cs.FL"], "comment": "30 pages, 1 figure", "summary": "Modern property graph database query languages such as Cypher, PGQL, GSQL, and the standard GQL draw inspiration from the formalism of regular path queries (RPQs). In order to output walks explicitly, they depart from the classical and well-studied homomorphism semantics. However, it then becomes difficult to present results to users because RPQs may match infinitely many walks. The aforementioned languages use ad-hoc criteria to select a finite subset of those matches. For instance, Cypher uses trail semantics, discarding walks with repeated edges; PGQL and GSQL use shortest walk semantics, retaining only the walks of minimal length among all matched walks; and GQL allows users to choose from several semantics. Even though there is academic research on these semantics, it focuses almost exclusively on evaluation efficiency.\n  In an attempt to better understand, choose and design RPQ semantics, we present a framework to categorize and compare them according to other criteria. We formalize several possible properties, pertaining to the study of RPQ semantics seen as mathematical functions mapping a database and a query to a finite set of walks. We show that some properties are mutually exclusive, or cannot be met. We also give several new RPQ semantics as examples. Some of them may provide ideas for the design of new semantics for future graph database query languages.", "AI": {"tldr": "\u63d0\u51fa\u6846\u67b6\u5bf9\u6b63\u5219\u8def\u5f84\u67e5\u8be2\uff08RPQ\uff09\u8bed\u4e49\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u6307\u51fa\u4e00\u4e9b\u5c5e\u6027\u76f8\u4e92\u6392\u65a5\u6216\u65e0\u6cd5\u6ee1\u8db3\uff0c\u8fd8\u7ed9\u51fa\u65b0\u7684RPQ\u8bed\u4e49\u793a\u4f8b\u3002", "motivation": "\u4e3a\u66f4\u597d\u5730\u7406\u89e3\u3001\u9009\u62e9\u548c\u8bbe\u8ba1RPQ\u8bed\u4e49\uff0c\u73b0\u6709\u7814\u7a76\u51e0\u4e4e\u53ea\u5173\u6ce8\u8bc4\u4f30\u6548\u7387\u3002", "method": "\u63d0\u51fa\u6846\u67b6\uff0c\u6839\u636e\u5176\u4ed6\u6807\u51c6\u5bf9RPQ\u8bed\u4e49\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u5f62\u5f0f\u5316\u76f8\u5173\u5c5e\u6027\u3002", "result": "\u53d1\u73b0\u90e8\u5206\u5c5e\u6027\u76f8\u4e92\u6392\u65a5\u6216\u65e0\u6cd5\u6ee1\u8db3\uff0c\u7ed9\u51fa\u65b0\u7684RPQ\u8bed\u4e49\u793a\u4f8b\u3002", "conclusion": "\u65b0\u7684RPQ\u8bed\u4e49\u793a\u4f8b\u6216\u4e3a\u672a\u6765\u56fe\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\u8bed\u4e49\u8bbe\u8ba1\u63d0\u4f9b\u601d\u8def\u3002"}}
{"id": "2602.11578", "pdf": "https://arxiv.org/pdf/2602.11578", "abs": "https://arxiv.org/abs/2602.11578", "authors": ["Tien-Ching Hsieh", "Yun-Cheng Tsai", "Samuel Yen-Chi Chen"], "title": "Quantum-Enhanced Temporal Embeddings via a Hybrid Seq2Seq Architecture", "categories": ["cs.CE"], "comment": null, "summary": "This work investigates how shallow, NISQ-compatible quantum layers can improve temporal representation learning in real-world sequential data. We develop a QLSTM Seq2Seq autoencoder in which a depth-1 variational quantum circuit is embedded inside each recurrent gate, shaping the geometry of the learned latent manifold. Evaluated on fourteen rolling S and P 500 windows from 2022 to 2025, the quantum-enhanced encoder produces smoother trajectories, clearer regime transitions, and more stable, sector-coherent clusters than a classical LSTM baseline. These geometric properties support the use of a Radial Basis Function (RBF) kernel for downstream portfolio allocation, where both RBF-Graph and RBF-DivMom strategies consistently outperform their classical counterparts in risk-adjusted terms. Analysis across periods shows that compressed manifolds favor concentrated allocation, while dispersed manifolds favor diversification, demonstrating that latent geometry serves as a regime indicator. The results highlight a practical role for shallow hybrid quantum and classical layers in NISQ-era sequence modeling, offering a reproducible pathway for improving temporal embeddings in finance and other data-limited, noise-sensitive domains.", "AI": {"tldr": "\u7814\u7a76\u6d45\u91cf\u5b50\u5c42\u5bf9\u65f6\u5e8f\u6570\u636e\u8868\u793a\u5b66\u4e60\u7684\u4f5c\u7528\uff0c\u5f00\u53d1QLSTM Seq2Seq\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5728\u6807\u51c6\u666e\u5c14500\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178LSTM\uff0c\u5176\u6f5c\u5728\u51e0\u4f55\u7ed3\u6784\u53ef\u6307\u5bfc\u6295\u8d44\u7ec4\u5408\u5206\u914d\u7b56\u7565\u3002", "motivation": "\u63a2\u7a76\u6d45\u7684\u3001\u4e0eNISQ\u517c\u5bb9\u7684\u91cf\u5b50\u5c42\u5982\u4f55\u6539\u5584\u73b0\u5b9e\u4e16\u754c\u987a\u5e8f\u6570\u636e\u7684\u65f6\u95f4\u8868\u793a\u5b66\u4e60\u3002", "method": "\u5f00\u53d1QLSTM Seq2Seq\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5c06\u6df1\u5ea6\u4e3a1\u7684\u53d8\u5206\u91cf\u5b50\u7535\u8def\u5d4c\u5165\u6bcf\u4e2a\u5faa\u73af\u95e8\u3002", "result": "\u91cf\u5b50\u589e\u5f3a\u7f16\u7801\u5668\u5728\u6807\u51c6\u666e\u5c14500\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u66f4\u4f18\u7684\u8f68\u8ff9\u3001\u72b6\u6001\u8f6c\u6362\u548c\u805a\u7c7b\uff0c\u652f\u6301\u4f7f\u7528RBF\u6838\u8fdb\u884c\u6295\u8d44\u7ec4\u5408\u5206\u914d\u4e14\u7b56\u7565\u8868\u73b0\u66f4\u4f18\uff0c\u6f5c\u5728\u51e0\u4f55\u7ed3\u6784\u53ef\u4f5c\u4e3a\u72b6\u6001\u6307\u6807\u3002", "conclusion": "\u6d45\u7684\u6df7\u5408\u91cf\u5b50\u548c\u7ecf\u5178\u5c42\u5728NISQ\u65f6\u4ee3\u5e8f\u5217\u5efa\u6a21\u4e2d\u6709\u5b9e\u9645\u4f5c\u7528\uff0c\u4e3a\u91d1\u878d\u7b49\u9886\u57df\u6539\u5584\u65f6\u95f4\u5d4c\u5165\u63d0\u4f9b\u53ef\u590d\u5236\u9014\u5f84\u3002"}}
{"id": "2602.11330", "pdf": "https://arxiv.org/pdf/2602.11330", "abs": "https://arxiv.org/abs/2602.11330", "authors": ["Sushmita Gupta", "Pallavi Jain", "Sanjay Seetharaman", "Meirav Zehavi"], "title": "When agents choose bundles autonomously: guarantees beyond discrepancy", "categories": ["cs.GT", "cs.DS"], "comment": "40 pages; abstract shortened due to arXiv requirements", "summary": "We consider the fair division of indivisible items among $n$ agents with additive non-negative normalized valuations, with the goal of obtaining high value guarantees, that is, close to the proportional share for each agent.\n  We prove that partitions where \\emph{every} part yields high value for each agent are asymptotically limited by a discrepancy barrier of $\u0398(\\sqrt{n})$. Guided by this, our main objective is to overcome this barrier and achieve stronger individual guarantees for each agent in polynomial time.\n  Towards this, we are able to exhibit an exponential improvement over the discrepancy barrier. In particular, we can create partitions on-the-go such that when agents arrive sequentially (representing a previously-agreed priority order) and pick a part autonomously and rationally (i.e., one of highest value), then each is guaranteed a part of value at least $\\mathsf{PROP} - \\mathcal{O}{(\\log n)}$. Moreover, we show even better guarantees for three restricted valuation classes such as those defined by: a common ordering on items, a bound on the multiplicity of values, and a hypergraph with a bound on the \\emph{influence} of any agent. Specifically, we study instances where: (1) the agents are ``close'' to unanimity in their relative valuation of the items -- a generalization of the ordered additive setting; (2) the valuation functions do not assign the same positive value to more than $t$ items; and (3) the valuation functions respect a hypergraph, a setting introduced by Christodoulou et al. [EC'23], where agents are vertices and items are hyperedges. While the sizes of the hyperedges and neighborhoods can be arbitrary, the influence of any agent $a$, defined as the number of its neighbors who value at least one item positively that $a$ also values positively, is bounded.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u516c\u5e73\u5206\u914d\uff0c\u8bc1\u660e\u5206\u533a\u4ef7\u503c\u6709\u6e10\u8fd1\u9650\u5236\uff0c\u63d0\u51fa\u65b9\u6cd5\u514b\u670d\u969c\u788d\uff0c\u5b9e\u73b0\u6307\u6570\u7ea7\u6539\u8fdb\uff0c\u5bf9\u4e0d\u540c\u4f30\u503c\u7c7b\u522b\u6709\u66f4\u597d\u4fdd\u8bc1\u3002", "motivation": "\u5728\u5177\u6709\u52a0\u6027\u975e\u8d1f\u5f52\u4e00\u5316\u4f30\u503c\u7684n\u4e2a\u4ee3\u7406\u4eba\u4e4b\u95f4\u8fdb\u884c\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u516c\u5e73\u5206\u914d\uff0c\u76ee\u6807\u662f\u83b7\u5f97\u9ad8\u4ef7\u503c\u4fdd\u8bc1\uff0c\u514b\u670d\u73b0\u6709\u5206\u533a\u7684\u6e10\u8fd1\u9650\u5236\u3002", "method": "\u521b\u5efa\u52a8\u6001\u5206\u533a\uff0c\u8ba9\u4ee3\u7406\u4eba\u6309\u987a\u5e8f\u81ea\u4e3b\u7406\u6027\u9009\u62e9\u90e8\u5206\uff1b\u9488\u5bf9\u4e09\u79cd\u53d7\u9650\u4f30\u503c\u7c7b\u522b\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u5dee\u5f02\u969c\u788d\u7684\u6307\u6570\u7ea7\u6539\u8fdb\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u4ee3\u7406\u4eba\u83b7\u5f97\u4ef7\u503c\u81f3\u5c11\u4e3aPROP - O(log n)\u7684\u90e8\u5206\uff1b\u5bf9\u4e09\u79cd\u53d7\u9650\u4f30\u503c\u7c7b\u522b\u6709\u66f4\u597d\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u514b\u670d\u73b0\u6709\u5206\u533a\u7684\u6e10\u8fd1\u9650\u5236\uff0c\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u4eba\u5b9e\u73b0\u66f4\u5f3a\u7684\u4e2a\u4f53\u4fdd\u8bc1\u3002"}}
{"id": "2602.11453", "pdf": "https://arxiv.org/pdf/2602.11453", "abs": "https://arxiv.org/abs/2602.11453", "authors": ["Sajad Ebrahimi", "Bhaskar Mitra", "Negar Arabzadeh", "Ye Yuan", "Haolun Wu", "Fattane Zarrinkalam", "Ebrahim Bagheri"], "title": "From Noise to Order: Learning to Rank via Denoising Diffusion", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u7684\u6df1\u5ea6\u751f\u6210\u5f0f\u65b9\u6cd5DiffusionRank\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\u6392\u5e8f\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u5224\u522b\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6392\u5e8f\u7684\u5224\u522b\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6709\u5c40\u9650\uff0c\u8fc7\u53c2\u6570\u5316\u7684\u6392\u5e8f\u6a21\u578b\u5728\u5224\u522b\u5f0f\u8bbe\u7f6e\u4e0b\u53ef\u80fd\u6709\u4e0d\u540c\u62df\u5408\u65b9\u5f0f\uff0c\u5e0c\u671b\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u627e\u5230\u80fd\u89e3\u91ca\u5b8c\u6574\u6570\u636e\u5206\u5e03\u7684\u66f4\u9c81\u68d2\u6392\u5e8f\u6a21\u578b\u3002", "method": "\u5c06\u73b0\u6709\u7684\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u7684\u8868\u683c\u6570\u636e\u96c6\u751f\u6210\u6a21\u578bTabDiff\u6269\u5c55\uff0c\u521b\u5efa\u7ecf\u5178\u5224\u522b\u5f0f\u9010\u70b9\u548c\u9010\u70b9\u5b66\u4e60\u6392\u5e8f\u76ee\u6807\u7684\u751f\u6210\u5f0f\u7b49\u4ef7\u5f62\u5f0f\uff0c\u63d0\u51faDiffusionRank\u3002", "result": "DiffusionRank\u6a21\u578b\u76f8\u6bd4\u5224\u522b\u5f0f\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u4e3a\u5229\u7528\u6df1\u5ea6\u751f\u6210\u5f0f\u5efa\u6a21\u65b9\u6cd5\uff08\u5982\u6269\u6563\uff09\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u6392\u5e8f\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7a7a\u95f4\u3002"}}
{"id": "2602.11363", "pdf": "https://arxiv.org/pdf/2602.11363", "abs": "https://arxiv.org/abs/2602.11363", "authors": ["Yael Kirkpatrick", "John Kuszmaul", "Surya Mathialagan", "Virginia Vassilevska Williams"], "title": "Preprocessed 3SUM for Unknown Universes with Subquadratic Space", "categories": ["cs.DS"], "comment": "13 pages", "summary": "We consider the classic 3SUM problem: given sets of integers $A, B, C $, determine whether there is a tuple $(a, b, c) \\in A \\times B \\times C$ satisfying $a + b + c = 0$. The 3SUM Hypothesis, central in fine-grained complexity, states that there does not exist a truly subquadratic time 3SUM algorithm. Given this long-standing barrier, recent work over the past decade has explored 3SUM from a data structural perspective. Specifically, in the 3SUM in preprocessed universes regime, we are tasked with preprocessing sets $A, B$ of size $n$, to create a space-efficient data structure that can quickly answer queries, each of which is a 3SUM problem of the form $A', B', C'$, where $A' \\subseteq A$ and $B' \\subseteq B$. A series of results have achieved $\\tilde{O}(n^2)$ preprocessing time, $\\tilde{O}(n^2)$ space, and query time improving progressively from $\\tilde{O}(n^{1.9})$ [CL15] to $\\tilde{O}(n^{11/6})$ [CVX23] to $\\tilde{O}(n^{1.5})$ [KPS25]. Given these series of works improving query time, a natural open question has emerged: can one achieve both truly subquadratic space and truly subquadratic query time for 3SUM in preprocessed universes?\n  We resolve this question affirmatively, presenting a tradeoff curve between query and space complexity. Specifically, we present a simple randomized algorithm achieving $\\tilde{O}(n^{1.5 + \\varepsilon})$ query time and $\\tilde{O}(n^{2 - 2\\varepsilon/3})$ space complexity. Furthermore, our algorithm has $\\tilde{O}(n^2)$ preprocessing time, matching past work. Notably, quadratic preprocessing is likely necessary for our tradeoff as either the preprocessing or the query time must be at least $n^{2-o(1)}$ under the 3SUM Hypothesis.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11456", "pdf": "https://arxiv.org/pdf/2602.11456", "abs": "https://arxiv.org/abs/2602.11456", "authors": ["Chaoyi Ruan", "Geng Luo", "Xinyi Wan", "Long Zhao", "Qinghe Wang", "Jiaan Zhu", "Duling Xu", "Guanbin Xu", "Dehui Wei", "Xiang Liu", "Cheng Li", "Haifeng Sun", "Congcong Miao", "Jialin Li"], "title": "RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas", "categories": ["cs.DC"], "comment": null, "summary": "LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb. A natural alternative is to aggregate loosely-coupled GPUs over standard Ethernet and WAN links, but this commodity connectivity cannot sustain full-weight broadcasts: synchronizing an 8B model can take over 100~seconds on bandwidth-limited links, while rollout generation typically takes tens of seconds.\n  Toward making RL practical in this regime, we observe that RL fine-tuning yields highly sparse per-step updates, with only around 1\\% of parameter elements changing. Atop this insight, we present SparrowRL, a novel high-performance RL training system that preserves bit-exact updates without dropping or quantizing information, designed for commodity-networked, loosely-coupled GPU resources. SparrowRL represents each step as a sparse delta checkpoint, pipelines delta extraction with multi-stream transmission, overlaps transfer with rollout generation, and coordinates heterogeneous workers with throughput- and bandwidth-aware scheduling plus lease-based fault tolerance. On Qwen3 models from 4B to 14B deployed across up to four geographic regions, SparrowRL reduces per-step transfer payload by 79$\\times$ for Qwen3-8B and improves throughput by 2.4--9.5$\\times$ over full-weight broadcast across WAN, narrowing the throughput gap relative to an ideal RDMA single-datacenter baseline to within 8.91\\%. By leveraging on-demand, cross-cloud GPUs over commodity links, SparrowRL delivers 1.21--1.59$\\times$ higher tokens per dollar than reserved RDMA clusters at comparable throughput.", "AI": {"tldr": "\u63d0\u51faSparrowRL\u7cfb\u7edf\uff0c\u5229\u7528RL\u5fae\u8c03\u66f4\u65b0\u7a00\u758f\u6027\uff0c\u7528\u4e8e\u666e\u901a\u7f51\u7edcGPU\u8d44\u6e90\uff0c\u51cf\u5c11\u4f20\u8f93\u8d1f\u8f7d\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u6027\u4ef7\u6bd4\u3002", "motivation": "\u4f20\u7edfLLM\u7684RL\u540e\u8bad\u7ec3\u4f9d\u8d56\u4e13\u7528RDMA HPC\u96c6\u7fa4\uff0c\u6210\u672c\u9ad8\uff0c\u666e\u901a\u7f51\u7edc\u540c\u6b65\u5927\u6a21\u578b\u53c2\u6570\u6162\uff0c\u9700\u6539\u8fdb\u3002", "method": "SparrowRL\u5c06\u6bcf\u4e00\u6b65\u8868\u793a\u4e3a\u7a00\u758f\u589e\u91cf\u68c0\u67e5\u70b9\uff0c\u5c06\u589e\u91cf\u63d0\u53d6\u4e0e\u591a\u6d41\u4f20\u8f93\u8fdb\u884c\u6d41\u6c34\u7ebf\u5904\u7406\uff0c\u5c06\u4f20\u8f93\u4e0e\u6eda\u52a8\u751f\u6210\u91cd\u53e0\uff0c\u901a\u8fc7\u541e\u5410\u91cf\u548c\u5e26\u5bbd\u611f\u77e5\u8c03\u5ea6\u548c\u57fa\u4e8e\u79df\u7ea6\u7684\u5bb9\u9519\u534f\u8c03\u5f02\u6784\u5de5\u4f5c\u8282\u70b9\u3002", "result": "\u5728Qwen3\u6a21\u578b\u4e0a\uff0cSparrowRL\u51cf\u5c11\u4e86\u4f20\u8f93\u8d1f\u8f7d\uff0c\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u7f29\u5c0f\u4e86\u4e0e\u7406\u60f3RDMA\u5355\u6570\u636e\u4e2d\u5fc3\u57fa\u7ebf\u7684\u5dee\u8ddd\uff0c\u4e14\u6027\u4ef7\u6bd4\u66f4\u9ad8\u3002", "conclusion": "SparrowRL\u80fd\u6709\u6548\u5229\u7528\u666e\u901a\u7f51\u7edc\u7684GPU\u8d44\u6e90\u8fdb\u884c\u9ad8\u6027\u80fdRL\u8bad\u7ec3\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2602.11741", "pdf": "https://arxiv.org/pdf/2602.11741", "abs": "https://arxiv.org/abs/2602.11741", "authors": ["Bo Guan"], "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions", "categories": ["cs.DC", "cs.DB", "cs.PF", "cs.SE"], "comment": "27 pages, 8 figures, 2 tables", "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u751f\u4ea7\u7ea7\u5206\u5e03\u5f0f\u9650\u6d41\u7cfb\u7edf\u67b6\u6784\uff0c\u7528Redis\u53caSorted Set\uff0c\u91cf\u5316Rolling Window\u7b97\u6cd5\u6743\u8861\uff0c\u7528Lua\u811a\u672c\u6d88\u9664\u7ade\u6001\uff0c\u63d0\u51fa\u4e09\u5c42\u67b6\u6784\uff0c\u5206\u6790Redis Cluster\u90e8\u7f72\u5e76\u91c7\u7528AP\u7b56\u7565\u3002", "motivation": "\u8bbe\u8ba1\u540c\u65f6\u5177\u5907\u51c6\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u9650\u6d41\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5b58\u5728\u7b97\u6cd5\u7cbe\u5ea6\u3001\u53ef\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u9009\u62e9Redis\u53ca\u5176Sorted Set\u6570\u636e\u7ed3\u6784\uff1b\u91cf\u5316Rolling Window\u7b97\u6cd5\u4e0eToken Bucket\u548cFixed Window\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u548c\u5185\u5b58\u6210\u672c\u6743\u8861\uff1b\u4f7f\u7528\u670d\u52a1\u5668\u7aefLua\u811a\u672c\u5c06\u6e05\u7406\u3001\u8ba1\u6570\u548c\u63d2\u5165\u64cd\u4f5c\u6346\u7ed1\u4e3a\u539f\u5b50\u64cd\u4f5c\uff1b\u63d0\u51fa\u4e09\u5c42\u67b6\u6784\u7ba1\u7406\u9650\u6d41\u89c4\u5219\uff1b\u5206\u6790\u5728Redis Cluster\u4e0a\u7684\u90e8\u7f72\u5e76\u91c7\u7528AP\u7b56\u7565\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u5b9e\u9a8c\u6216\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\uff0c\u4f46\u67b6\u6784\u53ef\u5229\u7528Redis\u7279\u6027\u5b9e\u73b0\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u64cd\u4f5c\uff0c\u6d88\u9664\u5e76\u53d1\u73af\u5883\u7ade\u6001\u6761\u4ef6\uff0c\u89c4\u5219\u53ef\u7075\u6d3b\u66f4\u6539\uff0c\u901a\u8fc7Redis Cluster\u63d0\u4f9b\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u5206\u5e03\u5f0f\u9650\u6d41\u7cfb\u7edf\u67b6\u6784\u901a\u8fc7\u5408\u7406\u9009\u62e9\u6280\u672f\u548c\u7b56\u7565\uff0c\u80fd\u5728\u751f\u4ea7\u7ea7\u73af\u5883\u4e2d\u5e73\u8861\u51c6\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u63a5\u53d7AP\u4f5c\u4e3a\u5de5\u7a0b\u6743\u8861\u3002"}}
{"id": "2602.12236", "pdf": "https://arxiv.org/pdf/2602.12236", "abs": "https://arxiv.org/abs/2602.12236", "authors": ["Anika Tabassum Meem", "Muntasir Hossain Nadid", "Md Zesun Ahmed Mia"], "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": null, "summary": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11210", "pdf": "https://arxiv.org/pdf/2602.11210", "abs": "https://arxiv.org/abs/2602.11210", "authors": ["Danlong Yuan", "Wei Wu", "Zhengren Wang", "Xueliang Zhao", "Huishuai Zhang", "Dongyan Zhao"], "title": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "ICML under review", "summary": "Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\\% of that required by container-based pipelines and reduces environment preparation time to about 25\\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.", "AI": {"tldr": "\u63d0\u51faSWE - MiniSandbox\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u5bb9\u5668\u65b9\u6cd5\u7528\u4e8eSWE\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u964d\u4f4e\u5f00\u9500\u4e14\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u4f9d\u8d56\u5bb9\u5668\u9694\u79bb\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7ba1\u9053\u5b58\u5728\u5b58\u50a8\u5f00\u9500\u5927\u3001\u73af\u5883\u8bbe\u7f6e\u6162\u548c\u9700\u5bb9\u5668\u7ba1\u7406\u6743\u9650\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5185\u6838\u7ea7\u673a\u5236\u652f\u6301\u7684\u9694\u79bb\u5de5\u4f5c\u533a\u6267\u884c\u4efb\u52a1\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u73af\u5883\u9884\u7f13\u5b58\u6280\u672f\u3002", "result": "\u5c06\u78c1\u76d8\u4f7f\u7528\u91cf\u964d\u81f3\u57fa\u4e8e\u5bb9\u5668\u7ba1\u9053\u7684\u7ea65%\uff0c\u73af\u5883\u51c6\u5907\u65f6\u95f4\u964d\u81f3\u7ea625%\uff0c\u8bc4\u4f30\u6027\u80fd\u4e0e\u57fa\u4e8e\u5bb9\u5668\u7684\u7ba1\u9053\u76f8\u5f53\u3002", "conclusion": "SWE - MiniSandbox\u53bb\u9664\u5bf9\u91cd\u578b\u5bb9\u5668\u57fa\u7840\u8bbe\u65bd\u7684\u4f9d\u8d56\uff0c\u4e3a\u6269\u5c55\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684SWE\u4ee3\u7406\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u73af\u5883\u3002"}}
{"id": "2602.11442", "pdf": "https://arxiv.org/pdf/2602.11442", "abs": "https://arxiv.org/abs/2602.11442", "authors": ["Shuyao Wu", "Delong Li", "Zhonghao Zhang"], "title": "Ecosystem service demand relationship and trade-off patterns in urban parks across China", "categories": ["econ.GN"], "comment": null, "summary": "Urban parks play a vital role in delivering various essential ecosystem services that significantly contribute to the well-being of urban populations. However, there is quite a limited understanding of how people value these ecosystem services differently. Here, we investigated the relationships among nine ecosystem service demands in urban parks across China using a large-scale survey with 20,075 responses and a point-allotment experiment. We found particularly high preferences for air purification and recreation services at the expense of other services among urban residents in China. These preferences were further reflected in three distinct demand bundles: air purification-dominated, recreation-dominated, and balanced demands. Each bundle delineated a typical group of people with different representative characteristics. Socio-economic and environmental factors, such as environmental interest and vegetation coverage, were found to significantly influence the trade-off intensity among service demands. These results underscore the necessity for tailored urban park designs that address diverse service demands with the aim of enhancing the quality of urban life in China and beyond sustainably.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc72\u4e07\u591a\u4efd\u8c03\u67e5\u5206\u6790\u4e2d\u56fd\u57ce\u5e02\u516c\u56ed\u4e5d\u79cd\u751f\u6001\u670d\u52a1\u9700\u6c42\u5173\u7cfb\uff0c\u53d1\u73b0\u5bf9\u7a7a\u6c14\u51c0\u5316\u548c\u4f11\u95f2\u670d\u52a1\u504f\u597d\u9ad8\uff0c\u6709\u4e09\u79cd\u9700\u6c42\u7ec4\u5408\uff0c\u5f3a\u8c03\u8981\u8fdb\u884c\u5b9a\u5236\u5316\u57ce\u5e02\u516c\u56ed\u8bbe\u8ba1\u3002", "motivation": "\u76ee\u524d\u4eba\u4eec\u5bf9\u57ce\u5e02\u516c\u56ed\u751f\u6001\u7cfb\u7edf\u670d\u52a1\u7684\u4ef7\u503c\u8bc4\u4f30\u4e86\u89e3\u6709\u9650\uff0c\u9700\u63a2\u7a76\u4e0d\u540c\u751f\u6001\u670d\u52a1\u9700\u6c42\u5173\u7cfb\u3002", "method": "\u5f00\u5c55\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u6536\u96c620075\u4efd\u56de\u590d\uff0c\u5e76\u8fdb\u884c\u70b9\u5206\u914d\u5b9e\u9a8c\u3002", "result": "\u4e2d\u56fd\u57ce\u5e02\u5c45\u6c11\u5bf9\u7a7a\u6c14\u51c0\u5316\u548c\u4f11\u95f2\u670d\u52a1\u504f\u597d\u9ad8\uff0c\u5b58\u5728\u4e09\u79cd\u9700\u6c42\u7ec4\u5408\uff0c\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u5883\u56e0\u7d20\u5f71\u54cd\u670d\u52a1\u9700\u6c42\u95f4\u7684\u6743\u8861\u5f3a\u5ea6\u3002", "conclusion": "\u6709\u5fc5\u8981\u8fdb\u884c\u5b9a\u5236\u5316\u57ce\u5e02\u516c\u56ed\u8bbe\u8ba1\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u670d\u52a1\u9700\u6c42\uff0c\u53ef\u6301\u7eed\u5730\u63d0\u5347\u57ce\u5e02\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2602.11229", "pdf": "https://arxiv.org/pdf/2602.11229", "abs": "https://arxiv.org/abs/2602.11229", "authors": ["Zituo Chen", "Haixu Wu", "Sili Deng"], "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.", "AI": {"tldr": "\u7814\u7a76\u8de8\u5f02\u6784PDE\u7cfb\u7edf\u7684\u957f\u65f6\u66ff\u4ee3\u6a21\u62df\uff0c\u63d0\u51faLatent Generative Solvers (LGS)\u6846\u67b6\uff0c\u51cf\u5c11\u957f\u65f6\u9884\u6d4b\u6f02\u79fb\uff0cFLOPs\u4f4e\uff0c\u53ef\u9002\u5e94\u5206\u5e03\u5916\u6570\u636e\u3002", "motivation": "\u5b9e\u73b0\u8de8\u5f02\u6784PDE\u7cfb\u7edf\u7684\u957f\u65f6\u6a21\u62df\uff0c\u63d0\u5347\u795e\u7ecfPDE\u6c42\u89e3\u5668\u7684\u6cdb\u5316\u6027\u548c\u957f\u671f\u9884\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5LGS\u6846\u67b6\uff0c\u7528\u9884\u8bad\u7ec3VAE\u5c06PDE\u72b6\u6001\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7269\u7406\u7a7a\u95f4\uff0c\u7528Transformer\u5b66\u4e60\u6f5c\u5728\u52a8\u529b\u5b66\uff0c\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u65cb\u94ae\u548c\u6d41\u5f3a\u5236\u673a\u5236\u3002", "result": "LGS\u5728\u77ed\u65f6\u6a21\u62df\u4e0a\u4e0e\u786e\u5b9a\u6027\u795e\u7ecf\u7b97\u5b50\u57fa\u7ebf\u76f8\u5f53\uff0c\u957f\u65f6\u51cf\u5c11\u6f02\u79fb\uff0cFLOPs\u6bd4\u975e\u751f\u6210\u5f0f\u57fa\u7ebf\u4f4e70\u500d\uff0c\u80fd\u5728\u6709\u9650\u5fae\u8c03\u4e0b\u9002\u5e94\u5206\u5e03\u5916\u6570\u636e\u3002", "conclusion": "LGS\u4e3a\u6784\u5efa\u53ef\u6cdb\u5316\u3001\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u7684\u795e\u7ecfPDE\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2602.12104", "pdf": "https://arxiv.org/pdf/2602.12104", "abs": "https://arxiv.org/abs/2602.12104", "authors": ["Agathe Sadeghi", "Zachary Feinstein"], "title": "Liquidation Dynamics in DeFi and the Role of Transaction Fees", "categories": ["q-fin.MF", "math.DS", "q-fin.TR"], "comment": "28 pages, 9 figures", "summary": "Liquidation of collateral are the primary safeguard for solvency of lending protocols in decentralized finance. However, the mechanics of liquidations expose these protocols to predatory price manipulations and other forms of Maximal Extractable Value (MEV). In this paper, we characterize the optimal liquidation strategy, via a dynamic program, from the perspective of a profit-maximizing liquidator when the spot oracle is given by a Constant Product Market Maker (CPMM). We explicitly model Oracle Extractable Value (OEV) where liquidators manipulate the CPMM with sandwich attacks to trigger profitable liquidation events. We derive closed-form liquidation bounds and prove that CPMM transaction fees act as a critical security parameter. Crucially, we demonstrate that fees do not merely reduce attacker profits, but can make such manipulations unprofitable for an attacker. Our findings suggest that CPMM transaction fees serve a dual purpose: compensating liquidity providers and endogenously hardening CPMM oracles against manipulation without the latency of time-weighted averages or medianization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\u501f\u8d37\u534f\u8bae\u6e05\u7b97\u673a\u5236\uff0c\u523b\u753b\u6700\u4f18\u6e05\u7b97\u7b56\u7565\uff0c\u63ed\u793aCPMM\u4ea4\u6613\u8d39\u7528\u53ef\u62b5\u5fa1\u64cd\u7eb5\u3002", "motivation": "\u501f\u8d37\u534f\u8bae\u6e05\u7b97\u673a\u5236\u6613\u53d7\u4ef7\u683c\u64cd\u7eb5\u548cMEV\u5f71\u54cd\uff0c\u9700\u7814\u7a76\u6700\u4f18\u6e05\u7b97\u7b56\u7565\u53ca\u62b5\u5fa1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u523b\u753b\u6700\u4f18\u6e05\u7b97\u7b56\u7565\uff0c\u660e\u786e\u5efa\u6a21OEV\uff0c\u63a8\u5bfc\u6e05\u7b97\u8fb9\u754c\u3002", "result": "\u5f97\u51fa\u5c01\u95ed\u5f62\u5f0f\u7684\u6e05\u7b97\u8fb9\u754c\uff0c\u8bc1\u660eCPMM\u4ea4\u6613\u8d39\u7528\u662f\u5173\u952e\u5b89\u5168\u53c2\u6570\uff0c\u53ef\u4f7f\u64cd\u7eb5\u65e0\u5229\u53ef\u56fe\u3002", "conclusion": "CPMM\u4ea4\u6613\u8d39\u7528\u6709\u53cc\u91cd\u4f5c\u7528\uff0c\u8865\u507f\u6d41\u52a8\u6027\u63d0\u4f9b\u8005\u4e14\u53ef\u62b5\u5fa1\u64cd\u7eb5\u3002"}}
{"id": "2602.11406", "pdf": "https://arxiv.org/pdf/2602.11406", "abs": "https://arxiv.org/abs/2602.11406", "authors": ["Tomer Gafni", "Garud Iyengar", "Assaf Zeevi"], "title": "The Cost of Learning under Multiple Change Points", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We consider an online learning problem in environments with multiple change points. In contrast to the single change point problem that is widely studied using classical \"high confidence\" detection schemes, the multiple change point environment presents new learning-theoretic and algorithmic challenges. Specifically, we show that classical methods may exhibit catastrophic failure (high regret) due to a phenomenon we refer to as endogenous confounding. To overcome this, we propose a new class of learning algorithms dubbed Anytime Tracking CUSUM (ATC). These are horizon-free online algorithms that implement a selective detection principle, balancing the need to ignore \"small\" (hard-to-detect) shifts, while reacting \"quickly\" to significant ones. We prove that the performance of a properly tuned ATC algorithm is nearly minimax-optimal; its regret is guaranteed to closely match a novel information-theoretic lower bound on the achievable performance of any learning algorithm in the multiple change point problem. Experiments on synthetic as well as real-world data validate the aforementioned theoretical findings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u591a\u53d8\u5316\u70b9\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u5f0a\u7aef\uff0c\u63d0\u51faAnytime Tracking CUSUM\uff08ATC\uff09\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u8fd1\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u591a\u53d8\u5316\u70b9\u73af\u5883\u7ed9\u5728\u7ebf\u5b66\u4e60\u5e26\u6765\u65b0\u6311\u6218\uff0c\u7ecf\u5178\u65b9\u6cd5\u53ef\u80fd\u56e0\u5185\u751f\u6df7\u6dc6\u73b0\u8c61\u5bfc\u81f4\u707e\u96be\u6027\u5931\u8d25\uff0c\u9700\u65b0\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u63d0\u51faAnytime Tracking CUSUM\uff08ATC\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u662f\u65e0\u65f6\u95f4\u8303\u56f4\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u91c7\u7528\u9009\u62e9\u6027\u68c0\u6d4b\u539f\u5219\u3002", "result": "\u8bc1\u660e\u4e86\u9002\u5f53\u8c03\u4f18\u7684ATC\u7b97\u6cd5\u8fd1\u6781\u5c0f\u6781\u5927\u6700\u4f18\uff0c\u5176\u9057\u61be\u503c\u63a5\u8fd1\u4fe1\u606f\u8bba\u4e0b\u754c\uff1b\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "ATC\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u591a\u53d8\u5316\u70b9\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f73\u3002"}}
{"id": "2602.11184", "pdf": "https://arxiv.org/pdf/2602.11184", "abs": "https://arxiv.org/abs/2602.11184", "authors": ["Zukang Xu", "Zhixiong Zhao", "Xing Hu", "Zhixuan Chen", "Dawei Yang"], "title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICLR 2026", "summary": "Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9MoE\u6a21\u578b\u90e8\u7f72\u6311\u6218\uff0c\u63d0\u51faKBVQ - MoE\u6846\u67b6\u89e3\u51b3VQ\u5e94\u7528\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6bd4\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u80fd\u66f4\u597d\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "MoE\u6a21\u578b\u53c2\u6570\u5927\u3001\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u6709\u6311\u6218\uff0c\u76f4\u63a5\u5e94\u7528VQ\u5230MoE\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faKBVQ - MoE\u6846\u67b6\uff0c\u96c6\u6210\u8f93\u5165\u9a71\u52a8\u5197\u4f59\u6d88\u9664\uff08KLT\u5f15\u5bfcSVD\u63d0\u53d6\u4e3b\u5bfc\u6743\u91cd\u5206\u91cf\u5e76\u8de8\u4e13\u5bb6\u5171\u4eab\uff09\u548c\u504f\u5dee\u6821\u6b63\u8f93\u51fa\u7a33\u5b9a\u5316\uff08\u4ec5\u5bf9\u4e13\u5bb6\u7279\u5b9a\u8868\u793a\u8fdb\u884cVQ\u5e76\u901a\u8fc7\u901a\u9053\u4eff\u5c04\u8865\u507f\u6821\u6b63\u91cf\u5316\u8f93\u51fa\uff09\u3002", "result": "\u5728\u5404\u79cdMoE\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cKBVQ - MoE\u6bd4\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u66f4\u597d\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u59823\u4f4d\u91cf\u5316Qwen1.5 - MoE - A2.7B\u5e73\u5747\u51c6\u786e\u7387\u8fbe67.99\uff0c\u63a5\u8fd1FP16\u57fa\u7ebf\u768468.07\u3002", "conclusion": "KBVQ - MoE\u6709\u6f5c\u529b\u5728\u8fb9\u7f18\u8bbe\u5907\u7b49\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2602.12064", "pdf": "https://arxiv.org/pdf/2602.12064", "abs": "https://arxiv.org/abs/2602.12064", "authors": ["Yafeng Nan", "Haifeng Sun", "Zirui Zhuang", "Qi Qi", "Guojun Chu", "Jianxin Liao", "Dan Pei", "Jingyu Wang"], "title": "DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning", "categories": ["cs.DB"], "comment": "Accepted by SIGMOD 2026", "summary": "In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.", "AI": {"tldr": "\u73b0\u6709Text - to - SQL\u6a21\u578b\u5728\u65e0\u4e13\u5bb6\u5e2e\u52a9\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u63d0\u51faDIVER\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u8bc1\u636e\u63a8\u7406\u548c\u52a8\u6001\u4ea4\u4e92\u503c\u94fe\u63a5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709Text - to - SQL\u6a21\u578b\u6027\u80fd\u4f9d\u8d56\u4e13\u5bb6\u7f16\u5199\u7684\u8bc1\u636e\uff0c\u5728\u65e0\u4e13\u5bb6\u5e2e\u52a9\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faDIVER\u7cfb\u7edf\uff0c\u5229\u7528\u517c\u5bb9\u5de5\u5177\u7bb1\u63a2\u6d4b\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u533a\uff08CoTF\uff09\u6839\u636e\u63a2\u6d4b\u7ed3\u679c\u53cd\u601d\u5e76\u9009\u62e9\u65b0\u5de5\u5177\u8fdb\u884c\u4e0b\u4e00\u8f6e\u63a2\u6d4b\uff0c\u81ea\u52a8\u8fed\u4ee3\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\u9057\u6f0f\u7684\u6a21\u5f0f\u548c\u503c\u94fe\u63a5\u3002", "result": "DIVER\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cdText - to - SQL\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u6267\u884c\u51c6\u786e\u7387\uff08EX\uff09\u6700\u9ad8\u63d0\u534710.82%\uff0c\u6709\u6548\u6548\u7387\u5f97\u5206\uff08VES\uff09\u6700\u9ad8\u63d0\u534716.09%\uff1b\u52a8\u6001\u4ea4\u4e92\u503c\u94fe\u63a5\u63d0\u9ad8\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6a21\u5f0f\u4e0e\u503c\u94fe\u63a5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DIVER\u7cfb\u7edf\u80fd\u591f\u5728\u65e0\u4e13\u5bb6\u5e2e\u52a9\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684Text - to - SQL\u3002"}}
{"id": "2602.11708", "pdf": "https://arxiv.org/pdf/2602.11708", "abs": "https://arxiv.org/abs/2602.11708", "authors": ["Duc Bui", "Thanh Nguyen"], "title": "Systematic Trend-Following with Adaptive Portfolio Construction: Enhancing Risk-Adjusted Alpha in Cryptocurrency Markets", "categories": ["cs.CE"], "comment": null, "summary": "Cryptocurrency markets exhibit pronounced momentum effects and regime-dependent volatility, presenting both opportunities and challenges for systematic trading strategies. We propose AdaptiveTrend, a multi-component algorithmic trading framework that integrates high-frequency trend-following on 6-hour intervals with monthly adaptive portfolio construction and asymmetric long-short capital allocation. Our framework introduces three key innovations: (1) a dynamic trailing stop mechanism calibrated to intra-day volatility regimes, (2) a rolling Sharpe-ratio-based asset selection procedure with market-capitalization-aware filtering, and (3) a theoretically motivated asymmetric 70/30 long-short allocation scheme grounded in the empirical positive drift of crypto markets. Through extensive out-of-sample backtesting across 150+ cryptocurrency pairs over a 36-month evaluation window (2022-2024), AdaptiveTrend achieves an annualized Sharpe ratio of 2.41, a maximum drawdown of -12.7%, and a Calmar ratio of 3.18, significantly outperforming benchmark trend-following strategies (TSMOM, time-series momentum) and equal-weighted buy-and-hold portfolios. We further conduct rigorous robustness analyses including parameter sensitivity, transaction cost modeling, and regime-conditional performance decomposition, demonstrating the strategy's resilience across bull, bear, and sideways market conditions.", "AI": {"tldr": "\u63d0\u51faAdaptiveTrend\u4ea4\u6613\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u9891\u8d8b\u52bf\u8ddf\u8e2a\u3001\u81ea\u9002\u5e94\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u548c\u975e\u5bf9\u79f0\u591a\u7a7a\u8d44\u672c\u5206\u914d\uff0c\u56de\u6d4b\u8868\u73b0\u8d85\u57fa\u51c6\u7b56\u7565\uff0c\u4e14\u7ecf\u7a33\u5065\u6027\u5206\u6790\u8bc1\u660e\u7b56\u7565\u5728\u4e0d\u540c\u5e02\u573a\u6761\u4ef6\u4e0b\u6709\u97e7\u6027\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u6709\u52a8\u91cf\u6548\u5e94\u548c\u5236\u5ea6\u4f9d\u8d56\u6ce2\u52a8\u6027\uff0c\u4e3a\u7cfb\u7edf\u4ea4\u6613\u7b56\u7565\u5e26\u6765\u673a\u9047\u548c\u6311\u6218\uff0c\u9700\u5f00\u53d1\u6709\u6548\u4ea4\u6613\u6846\u67b6\u3002", "method": "\u63d0\u51faAdaptiveTrend\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u6b62\u635f\u673a\u5236\u3001\u57fa\u4e8e\u6eda\u52a8\u590f\u666e\u6bd4\u7387\u7684\u8d44\u4ea7\u9009\u62e9\u7a0b\u5e8f\u548c70/30\u975e\u5bf9\u79f0\u591a\u7a7a\u5206\u914d\u65b9\u6848\u3002", "result": "\u5728\u8d85150\u4e2a\u52a0\u5bc6\u8d27\u5e01\u5bf936\u4e2a\u6708\u56de\u6d4b\u4e2d\uff0c\u5e74\u5316\u590f\u666e\u6bd4\u73872.41\uff0c\u6700\u5927\u56de\u64a4-12.7%\uff0c\u5361\u5c14\u739b\u6bd4\u73873.18\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u7b56\u7565\u548c\u7b49\u6743\u91cd\u4e70\u5165\u6301\u6709\u7ec4\u5408\u3002", "conclusion": "AdaptiveTrend\u7b56\u7565\u6709\u6548\uff0c\u5728\u4e0d\u540c\u5e02\u573a\u6761\u4ef6\u4e0b\u6709\u97e7\u6027\u3002"}}
{"id": "2602.11400", "pdf": "https://arxiv.org/pdf/2602.11400", "abs": "https://arxiv.org/abs/2602.11400", "authors": ["Paula B\u00f6hm", "Robert Bredereck", "Till Fluschnik"], "title": "Maximizing Index Diversity in Committee Elections", "categories": ["cs.GT"], "comment": "A short version was published in the proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "We introduce two models of multiwinner elections with approval preferences and labelled candidates that take the committee's diversity into account. One model aims to find a committee with maximal diversity given a scoring function (e.g. of a scoring-based voting rule) and a lower bound for the score to be respected. The second model seeks to maximize the diversity given a minimal satisfaction for each agent to be respected. To measure the diversity of a committee, we use multiple diversity indices used in ecology and introduce one new index. We define (desirable) properties of diversity indices, test the indices considered against these properties, and characterize the new index. We analyze the computational complexity of computing a committee for both models and scoring functions of well-known voting rules, and investigate the influence of weakening the score or satisfaction constraints on the diversity empirically.", "AI": {"tldr": "\u5f15\u5165\u8003\u8651\u59d4\u5458\u4f1a\u591a\u6837\u6027\u7684\u591a\u83b7\u80dc\u8005\u9009\u4e3e\u6a21\u578b\uff0c\u7528\u591a\u79cd\u591a\u6837\u6027\u6307\u6570\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5b9e\u8bc1\u7814\u7a76\u7ea6\u675f\u5f31\u5316\u5bf9\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8003\u8651\u59d4\u5458\u4f1a\u591a\u6837\u6027\uff0c\u63d0\u51fa\u66f4\u5408\u7406\u7684\u591a\u83b7\u80dc\u8005\u9009\u4e3e\u6a21\u578b\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u8003\u8651\u591a\u6837\u6027\u7684\u591a\u83b7\u80dc\u8005\u9009\u4e3e\u6a21\u578b\uff0c\u4f7f\u7528\u751f\u6001\u5b66\u591a\u6837\u6027\u6307\u6570\u5e76\u5f15\u5165\u65b0\u6307\u6570\uff0c\u5b9a\u4e49\u6307\u6570\u5c5e\u6027\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u5b8c\u6210\u4e86\u6a21\u578b\u6784\u5efa\u3001\u6307\u6570\u5b9a\u4e49\u4e0e\u6d4b\u8bd5\u3001\u590d\u6742\u5ea6\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u53ef\u7528\u4e8e\u89e3\u51b3\u591a\u83b7\u80dc\u8005\u9009\u4e3e\u4e2d\u7684\u591a\u6837\u6027\u95ee\u9898\uff0c\u7ea6\u675f\u5f31\u5316\u4f1a\u5f71\u54cd\u591a\u6837\u6027\u3002"}}
{"id": "2602.11518", "pdf": "https://arxiv.org/pdf/2602.11518", "abs": "https://arxiv.org/abs/2602.11518", "authors": ["Yupeng Li", "Ben Chen", "Mingyue Cheng", "Zhiding Liu", "Xuxin Zhang", "Chenyi Lei", "Wenwu Ou"], "title": "KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance", "categories": ["cs.IR"], "comment": null, "summary": "E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u7535\u5546\u641c\u7d22\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u6709\u5c40\u9650\uff0c\u6784\u5efa\u5e76\u53d1\u5e03\u6700\u5927\u7535\u5546\u641c\u7d22\u6570\u636e\u96c6KuaiSearch\uff0c\u7ecf\u5b9e\u9a8c\u8bc1\u660e\u5176\u5bf9\u7535\u5546\u641c\u7d22\u7814\u7a76\u6709\u4ef7\u503c\u3002", "motivation": "\u89e3\u51b3\u7535\u5546\u641c\u7d22\u9762\u4e34\u9ad8\u5ea6\u6a21\u7cca\u67e5\u8be2\u3001\u5608\u6742\u4ea7\u54c1\u6587\u672c\u7b49\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u7535\u5546\u641c\u7d22\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5feb\u624b\u5e73\u53f0\u771f\u5b9e\u7528\u6237\u641c\u7d22\u4ea4\u4e92\u6784\u5efa\u5e76\u53d1\u5e03KuaiSearch\u6570\u636e\u96c6\uff0c\u5bf9\u5176\u4ece\u591a\u89c6\u89d2\u5206\u6790\u5e76\u5f00\u5c55\u57fa\u51c6\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eKuaiSearch\u4e3a\u73b0\u5b9e\u7535\u5546\u641c\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u57fa\u7840\u3002", "conclusion": "KuaiSearch\u6570\u636e\u96c6\u80fd\u63a8\u52a8\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7535\u5546\u641c\u7d22\u7814\u7a76\u3002"}}
{"id": "2602.11454", "pdf": "https://arxiv.org/pdf/2602.11454", "abs": "https://arxiv.org/abs/2602.11454", "authors": ["Ta Duy Nguyem", "Alina Ene", "Huy Le Nguyen"], "title": "Adaptive Power Iteration Method for Differentially Private PCA", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "We study $(\u03b5,\u03b4)$-differentially private algorithms for the problem of approximately computing the top singular vector of a matrix $A\\in\\mathbb{R}^{n\\times d}$ where each row of $A$ is a datapoint in $\\mathbb{R}^{d}$. In our privacy model, neighboring inputs differ by one single row/datapoint. We study the private variant of the power iteration method, which is widely adopted in practice. Our algorithm is based on a filtering technique which adapts to the coherence parameter of the input matrix. This technique provides a utility that goes beyond the worst-case guarantees for matrices with low coherence parameter. Our work departs from and complements the work by Hardt-Roth (STOC 2013) which designed a private power iteration method for the privacy model where neighboring inputs differ in one single entry by at most 1.", "AI": {"tldr": "\u7814\u7a76\u77e9\u9635\u8fd1\u4f3c\u8ba1\u7b97\u6700\u5927\u5947\u5f02\u5411\u91cf\u7684(\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8fc7\u6ee4\u6280\u672f\u6539\u8fdb\u79c1\u6709\u5e42\u8fed\u4ee3\u6cd5\u3002", "motivation": "\u4e3a\u77e9\u9635\u8fd1\u4f3c\u8ba1\u7b97\u6700\u5927\u5947\u5f02\u5411\u91cf\u95ee\u9898\u8bbe\u8ba1\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u4e14\u4e0e\u5df2\u6709\u6a21\u578b\u7684\u5de5\u4f5c\u5f62\u6210\u4e92\u8865\u3002", "method": "\u7814\u7a76\u79c1\u6709\u5e42\u8fed\u4ee3\u6cd5\uff0c\u91c7\u7528\u9002\u5e94\u8f93\u5165\u77e9\u9635\u76f8\u5e72\u53c2\u6570\u7684\u8fc7\u6ee4\u6280\u672f\u3002", "result": "\u7b97\u6cd5\u5728\u4f4e\u76f8\u5e72\u53c2\u6570\u77e9\u9635\u4e0a\u7684\u6548\u7528\u8d85\u8d8a\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u5de5\u4f5c\u4e0eHardt - Roth\u7684\u5de5\u4f5c\u4e0d\u540c\u4e14\u76f8\u4e92\u8865\u5145\u3002"}}
{"id": "2602.11544", "pdf": "https://arxiv.org/pdf/2602.11544", "abs": "https://arxiv.org/abs/2602.11544", "authors": ["Yiming Zhou", "Kaiping Xue", "Enhong Chen"], "title": "Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization", "categories": ["cs.DC"], "comment": null, "summary": "In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDPPS\u534f\u8bae\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u4fdd\u62a4\u9690\u79c1\uff0c\u8bbe\u8ba1PartPSP\u7b97\u6cd5\u7528\u4e8e\u975e\u51f8\u4f18\u5316\uff0c\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\uff0c\u7f3a\u4e4f\u901a\u7528\u534f\u8bae\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDPPS\u534f\u8bae\u5e76\u5f15\u5165\u65b0\u7684\u654f\u611f\u5ea6\u4f30\u8ba1\u673a\u5236\uff1b\u8bbe\u8ba1PartPSP\u7b97\u6cd5\uff0c\u5c06\u6a21\u578b\u53c2\u6570\u5206\u533a\uff0c\u5bf9\u5171\u4eab\u53c2\u6570\u7528DPPS\u3002", "result": "\u7406\u8bba\u8bc1\u660ePartPSP\u5728\u975e\u51f8\u76ee\u6807\u4e0b\u6536\u655b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1DPPS\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u548cPartPSP\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "DPPS\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0cPartPSP\u80fd\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u63d0\u5347\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2602.11322", "pdf": "https://arxiv.org/pdf/2602.11322", "abs": "https://arxiv.org/abs/2602.11322", "authors": ["Jason Dury"], "title": "Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "20 pages, 6 figures, for associated Git: https://github.com/EridosAI/PAM-Benchmark", "summary": "Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\\leq$ 0.012).", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u795e\u7ecf\u8bb0\u5fc6\u7cfb\u7edf\u4e0d\u8db3\u63d0\u51faPAM\u67b6\u6784\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u5728\u5173\u8054\u56de\u5fc6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8bb0\u5fc6\u7cfb\u7edf\u57fa\u4e8e\u76f8\u4f3c\u6027\u68c0\u7d22\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u7269\u8bb0\u5fc6\u7684\u65f6\u95f4\u5171\u73b0\u5173\u8054\u7279\u6027\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faPAM\u67b6\u6784\uff0c\u5f15\u5165Inward JEPA\uff0c\u5c06\u5176\u4f5c\u4e3a\u5173\u8054\u56de\u5fc6\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9884\u6d4b\u5668\u591a\u9879\u6307\u6807\u8868\u73b0\u597d\uff0c\u5982Association Precision@1 = 0.970\u7b49\uff0c\u6253\u4e71\u65f6\u95f4\u987a\u5e8f\u540e\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u7ed3\u679c\u7a33\u5b9a\u3002", "conclusion": "PAM\u67b6\u6784\u80fd\u6709\u6548\u5229\u7528\u65f6\u95f4\u5171\u73b0\u7ed3\u6784\u8fdb\u884c\u5173\u8054\u56de\u5fc6\uff0c\u4f18\u4e8e\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u65b9\u6cd5\u3002"}}
{"id": "2602.11223", "pdf": "https://arxiv.org/pdf/2602.11223", "abs": "https://arxiv.org/abs/2602.11223", "authors": ["Micheal P. Papazoglou", "Bernd J. Kr\u00e4mer", "Mira Raheem", "Amal Elgammal"], "title": "Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead", "categories": ["cs.SE", "cs.HC"], "comment": "Feature Article, Patient Medical Digital Twins, Under Review in IEEE SOftware", "summary": "Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.", "AI": {"tldr": "\u6162\u6027\u75c5\u8d1f\u62c5\u91cd\uff0c\u73b0\u6709\u533b\u7597\u7cfb\u7edf\u4e0d\u8db3\uff0c\u4ecb\u7ecd\u60a3\u8005\u533b\u7597\u6570\u5b57\u53cc\u80de\u80ce\uff08PMDTs\uff09\u65e9\u671f\u5b9e\u65bd\uff0c\u5206\u6790\u53ef\u884c\u6027\u3001\u6311\u6218\u4e0e\u6280\u672f\u6536\u83b7\uff0c\u7ed9\u51fa\u884c\u52a8\u89c1\u89e3\u548c\u53d1\u5c55\u673a\u4f1a\u3002", "motivation": "\u6162\u6027\u75c5\u662f\u5168\u7403\u53d1\u75c5\u3001\u6b7b\u4ea1\u548c\u533b\u7597\u6210\u672c\u7684\u4e3b\u8981\u8d1f\u62c5\uff0c\u73b0\u6709\u5065\u5eb7\u7cfb\u7edf\u788e\u7247\u5316\u4e14\u88ab\u52a8\uff0cPMDTs \u53ef\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u672c\u4f53\u9a71\u52a8\u5efa\u6a21\u548c\u8054\u5408\u5206\u6790\u8bd5\u70b9\u8fdb\u884c PMDTs \u65e9\u671f\u5b9e\u65bd\u3002", "result": "QUALITOP \u80bf\u7624\u5b66\u7814\u7a76\u548c\u5206\u5e03\u5f0f AI \u5e73\u53f0\u8bc1\u5b9e\u4e86 PMDTs \u7684\u53ef\u884c\u6027\u4e0e\u6311\u6218\uff0c\u4e5f\u6709\u6280\u672f\u6536\u83b7\u3002", "conclusion": "\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u63d0\u4f9b\u884c\u52a8\u89c1\u89e3\uff0c\u6307\u51fa\u5982 DSLs \u548c\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u7b49\u63a8\u8fdb PMDTs \u53d1\u5c55\u7684\u673a\u4f1a\u3002"}}
{"id": "2602.11992", "pdf": "https://arxiv.org/pdf/2602.11992", "abs": "https://arxiv.org/abs/2602.11992", "authors": ["Mats Ekman", "Niklas Jakobsson", "Andreas Kotsadam"], "title": "Labor Supply under Temporary Wage Increases: Evidence from a Randomized Field Experiment", "categories": ["econ.GN"], "comment": null, "summary": "We conduct a pre-registered randomized controlled trial to test for income targeting in labor supply decisions among sellers of a Swedish street paper. These workers face liquidity constraints, high income volatility, and discretion over hours. Treated individuals received a 25 percent bonus per copy sold for the duration of an issue, simulating an increase in earnings potential. Treated sellers sold more papers, worked longer hours, and took fewer days off. These findings contrast with studies on intertemporal labor supply that find small substitution effects. Notably, when we apply strategies similar to observational studies, we recover patterns consistent with income targeting.", "AI": {"tldr": "\u5bf9\u745e\u5178\u8857\u5934\u62a5\u7eb8\u5356\u5bb6\u8fdb\u884c\u9884\u6ce8\u518c\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u6d4b\u8bd5\u52b3\u52a8\u4f9b\u7ed9\u51b3\u7b56\u4e2d\u7684\u6536\u5165\u76ee\u6807\u8bbe\u5b9a\uff0c\u53d1\u73b0\u5904\u7406\u7ec4\u5356\u5bb6\u8868\u73b0\u4e0d\u540c\uff0c\u7528\u7c7b\u4f3c\u89c2\u5bdf\u6027\u7814\u7a76\u7b56\u7565\u6709\u4e00\u81f4\u6a21\u5f0f\u3002", "motivation": "\u6d4b\u8bd5\u745e\u5178\u8857\u5934\u62a5\u7eb8\u5356\u5bb6\u52b3\u52a8\u4f9b\u7ed9\u51b3\u7b56\u4e2d\u7684\u6536\u5165\u76ee\u6807\u8bbe\u5b9a\u3002", "method": "\u8fdb\u884c\u9884\u6ce8\u518c\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u7ed9\u5904\u7406\u7ec4\u4e2a\u4f53\u6bcf\u5356\u51fa\u4e00\u4efd\u62a5\u7eb8\u63d0\u4f9b25%\u7684\u5956\u91d1\u3002", "result": "\u5904\u7406\u7ec4\u5356\u5bb6\u5356\u51fa\u66f4\u591a\u62a5\u7eb8\u3001\u5de5\u4f5c\u65f6\u95f4\u66f4\u957f\u3001\u4f11\u5047\u5929\u6570\u66f4\u5c11\uff0c\u4e0e\u8de8\u671f\u52b3\u52a8\u4f9b\u7ed9\u7814\u7a76\u7ed3\u679c\u4e0d\u540c\uff0c\u7528\u7c7b\u4f3c\u89c2\u5bdf\u6027\u7814\u7a76\u7b56\u7565\u6709\u4e00\u81f4\u6a21\u5f0f\u3002", "conclusion": "\u5728\u8be5\u7814\u7a76\u60c5\u5883\u4e0b\u5b58\u5728\u4e0e\u4ee5\u5f80\u7814\u7a76\u4e0d\u540c\u7684\u52b3\u52a8\u4f9b\u7ed9\u8868\u73b0\u53ca\u7279\u5b9a\u6a21\u5f0f\u3002"}}
{"id": "2602.11295", "pdf": "https://arxiv.org/pdf/2602.11295", "abs": "https://arxiv.org/abs/2602.11295", "authors": ["Gil Raitses"], "title": "On Decision-Valued Maps and Representational Dependence", "categories": ["cs.AI", "cs.DB"], "comment": "10 pages, 3 figures, 5 tables", "summary": "A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.", "AI": {"tldr": "\u672c\u6587\u5f62\u5f0f\u5316\u51b3\u7b56\u503c\u6620\u5c04\uff0c\u4ecb\u7ecdDecisionDB\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u8bb0\u5f55\u3001\u56de\u653e\u548c\u5ba1\u8ba1\u76f8\u5173\u5173\u7cfb\uff0c\u5c06\u8868\u793a\u7a7a\u95f4\u5206\u533a\u5e76\u5c06\u51b3\u7b56\u91cd\u7528\u4f5c\u4e3a\u53ef\u673a\u68b0\u68c0\u67e5\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u5f15\u64ce\u5bf9\u540c\u4e00\u6570\u636e\u4e0d\u540c\u8868\u793a\u4ea7\u751f\u4e0d\u540c\u79bb\u6563\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u5206\u6790\u54ea\u4e9b\u8868\u793a\u4fdd\u7559\u7ed3\u679c\u3001\u54ea\u4e9b\u6539\u53d8\u7ed3\u679c\u3002", "method": "\u5f62\u5f0f\u5316\u51b3\u7b56\u503c\u6620\u5c04\uff0c\u6784\u5efaDecisionDB\u57fa\u7840\u8bbe\u65bd\uff0c\u5229\u7528\u5185\u5bb9\u8ba1\u7b97\u7684\u6807\u8bc6\u7b26\u548c\u4e00\u6b21\u5199\u5165\u5f62\u5f0f\u5b58\u50a8\u7684\u5de5\u4ef6\u6765\u8bb0\u5f55\u3001\u56de\u653e\u548c\u5ba1\u8ba1\u5173\u7cfb\u3002", "result": "\u786e\u5b9a\u6027\u56de\u653e\u80fd\u4ece\u5b58\u50a8\u5de5\u4ef6\u4e2d\u7cbe\u786e\u6062\u590d\u6bcf\u4e2a\u8bb0\u5f55\u7684\u51b3\u7b56\u6807\u8bc6\u7b26\uff0c\u4e09\u4e2a\u8bc6\u522b\u5b57\u6bb5\u90fd\u4e0e\u6301\u4e45\u503c\u5339\u914d\u3002", "conclusion": "\u5c06\u8868\u793a\u7a7a\u95f4\u5212\u5206\u4e3a\u6301\u4e45\u533a\u57df\u548c\u8fb9\u754c\uff0c\u628a\u51b3\u7b56\u91cd\u7528\u4f5c\u4e3a\u53ef\u673a\u68b0\u68c0\u67e5\u7684\u6761\u4ef6\u3002"}}
{"id": "2602.11679", "pdf": "https://arxiv.org/pdf/2602.11679", "abs": "https://arxiv.org/abs/2602.11679", "authors": ["Kyungbok Lee", "Angelica Cristello Sarteau", "Michael R. Kosorok"], "title": "Provable Offline Reinforcement Learning for Structured Cyclic MDPs", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.OC", "stat.ME"], "comment": "65 pages, 4 figures. Submitted to JMLR", "summary": "We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state distributions of subsequent stages, propagating mismatch across the cycle. To address this, we propose a modular structural framework that decomposes the cyclic process into stage-wise sub-problems. While generally applicable, we instantiate this principle as CycleFQI, an extension of fitted Q-iteration enabling theoretical analysis and interpretation. It uses a vector of stage-specific Q-functions, tailored to each stage, to capture within-stage sequences and transitions between stages. This modular design enables partial control, allowing some stages to be optimized while others follow predefined policies. We establish finite-sample suboptimality error bounds and derive global convergence rates under Besov regularity, demonstrating that CycleFQI mitigates the curse of dimensionality compared to monolithic baselines. Additionally, we propose a sieve-based method for asymptotic inference of optimal policy values under a margin condition. Experiments on simulated and real-world Type 1 Diabetes data sets demonstrate CycleFQI's effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u5faa\u73afMDP\u6846\u67b6\u89e3\u51b3\u591a\u6b65\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51faCycleFQI\u65b9\u6cd5\uff0c\u5efa\u7acb\u8bef\u5dee\u754c\u548c\u6536\u655b\u7387\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u5f02\u6784\u9636\u6bb5\u7279\u5b9a\u52a8\u6001\u3001\u8f6c\u79fb\u548c\u6298\u6263\u56e0\u5b50\u7684\u591a\u6b65\u51b3\u7b56\u95ee\u9898\u4e2d\uff0c\u79bb\u7ebf\u5b66\u4e60\u6709\u6311\u6218\uff0c\u4f18\u5316\u4e00\u4e2a\u9636\u6bb5\u7b56\u7565\u4f1a\u5f71\u54cd\u540e\u7eed\u9636\u6bb5\u72b6\u6001\u5206\u5e03", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u7ed3\u6784\u6846\u67b6\u5c06\u5faa\u73af\u8fc7\u7a0b\u5206\u89e3\u4e3a\u9636\u6bb5\u5b50\u95ee\u9898\uff0c\u5b9e\u4f8b\u5316CycleFQI\u65b9\u6cd5\uff0c\u4f7f\u7528\u7279\u5b9a\u9636\u6bb5Q\u51fd\u6570\u5411\u91cf\uff0c\u8fd8\u63d0\u51fa\u57fa\u4e8e\u7b5b\u5b50\u7684\u6e10\u8fd1\u63a8\u65ad\u65b9\u6cd5", "result": "\u5efa\u7acb\u6709\u9650\u6837\u672c\u6b21\u4f18\u6027\u8bef\u5dee\u754c\uff0c\u63a8\u5bfc\u4e86\u5168\u5c40\u6536\u655b\u7387\uff0cCycleFQI\u51cf\u8f7b\u4e86\u7ef4\u5ea6\u707e\u96be\uff0c\u5b9e\u9a8c\u8bc1\u660eCycleFQI\u6709\u6548", "conclusion": "CycleFQI\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5f02\u6784\u9636\u6bb5\u591a\u6b65\u51b3\u7b56\u95ee\u9898"}}
{"id": "2602.11185", "pdf": "https://arxiv.org/pdf/2602.11185", "abs": "https://arxiv.org/abs/2602.11185", "authors": ["Zhendong Huang", "Hengjie Cao", "Fang Dong", "Ruijun Huang", "Mengyi Chen", "Yifeng Yang", "Xin Zhang", "Anrui Chen", "Mingzhi Dong", "Yujiang Wang", "Jinlong Hou", "Qin Lv", "Robert P. Dick", "Yuan Cheng", "Fan Yang", "Tun Lu", "Li Shang"], "title": "Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faLLM\u8bad\u7ec3\u4e2d\u68af\u5ea6\u4fe1\u53f7\u5404\u5411\u5f02\u6027\uff0c\u63d0\u51faSpectra\u4f18\u5316\u5668\uff0c\u5728LLaMA3 8B\u4e0a\u6548\u679c\u4f18\u4e8eAdamW\u548cMuon\u3002", "motivation": "LLM\u8bad\u7ec3\u4e2d\u68af\u5ea6\u4fe1\u53f7\u7684\u5404\u5411\u5f02\u6027\u5bfc\u81f4\u4e3b\u5bfc\u65b9\u5411\u6291\u5236\u5c3e\u90e8\u5b66\u4e60\uff0c\u9700\u6539\u8fdb\u4f18\u5316\u5668\u3002", "method": "\u63d0\u51faSpectra\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u7f13\u5b58\u3001\u70ed\u542f\u52a8\u5e42\u8fed\u4ee3\u8ddf\u8e2a\u5c16\u5cf0\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u4f4e\u79e9\u8c31\u6574\u5f62\u3002", "result": "\u5728LLaMA3 8B\u4e0a\uff0cSpectra\u6bd4AdamW\u8bad\u7ec3\u5feb30%\uff0c\u51cf\u5c11\u5f00\u9500\u548c\u5185\u5b58\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\uff1b\u6bd4Muon\u5904\u7406\u901f\u5ea6\u5feb5.1\u500d\uff0c\u635f\u5931\u66f4\u4f4e\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u3002", "conclusion": "Spectra\u4f18\u5316\u5668\u6709\u6548\u6539\u5584\u4e86LLM\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2602.12178", "pdf": "https://arxiv.org/pdf/2602.12178", "abs": "https://arxiv.org/abs/2602.12178", "authors": ["Nicole Pellizzon", "Richard Huber", "Jon Spangenberg", "Jakob Sauer J\u00f8rgensen"], "title": "Systematic Analysis of Penalty-Optimised Illumination Design for Tomographic Volumetric Additive Manufacturing via the Extendable Framework TVAM AID Using the Core Imaging Library", "categories": ["cs.CE", "cs.MS", "eess.SP"], "comment": "22 Pages, 19 Figures", "summary": "Tomographic Volumetric Additive Manufacturing(TVAM) is a novel manufacturing method that allows for the fast creation of objects of complex geometry in layerless fashion. The process is based on the solidification of photopolymer that occurs when a sufficient threshold dose of light-energy is absorbed. In order to create complex shapes, an illumination plan must be designed to force solidification in some desired areas while leaving other regions liquid. Determining an illumination plan can be considered as an optimisation problem where a variety of objective functionals (penalties) can be used. This work considers a selection of penalty functions and their impact on selected printing metrics; linking the shape of penalty functions to ranges of light-energy dose levels in in-part regions that should be printed and out-of-part regions that should remain liquid. Further, the threshold parameters that are typically used to demarcate minimum light-energy for in-part regions and maximum light-energy for out-of-part regions are investigated systematically as design parameters on both existing and new methods. This enables the characterisation of their effects on some selected printing metrics as well as informed selection for default values. This work is underpinned by a reproducible and extensible framework, TVAM Adaptive Illumination Design(TVAM AID), which makes use of the open-source Core Imaging Library(CIL) that is designed for tomographic imaging with an emphasis on reconstruction. The foundation of TVAM AID which is presented here can hence be easily enhanced by existing functionality in CIL thus lowering the barrier to entry and encouraging use of strategies that already exist for reconstruction optimisation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65ad\u5c42\u4f53\u79ef\u589e\u6750\u5236\u9020\uff08TVAM\uff09\u4e2d\u60e9\u7f5a\u51fd\u6570\u5bf9\u6253\u5370\u6307\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u9608\u503c\u53c2\u6570\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u53ef\u6269\u5c55\u6846\u67b6TVAM AID\u3002", "motivation": "\u4e3aTVAM\u8bbe\u8ba1\u7167\u660e\u65b9\u6848\uff0c\u4f18\u5316\u6253\u5370\u6548\u679c\u3002", "method": "\u9009\u62e9\u60e9\u7f5a\u51fd\u6570\uff0c\u7cfb\u7edf\u7814\u7a76\u9608\u503c\u53c2\u6570\uff0c\u4f7f\u7528\u5f00\u6e90Core Imaging Library\u6784\u5efaTVAM AID\u6846\u67b6\u3002", "result": "\u660e\u786e\u60e9\u7f5a\u51fd\u6570\u5f62\u72b6\u4e0e\u5149\u80fd\u5242\u91cf\u6c34\u5e73\u8303\u56f4\u7684\u8054\u7cfb\uff0c\u53ef\u8868\u5f81\u9608\u503c\u53c2\u6570\u5bf9\u6253\u5370\u6307\u6807\u7684\u5f71\u54cd\u5e76\u9009\u62e9\u9ed8\u8ba4\u503c\u3002", "conclusion": "TVAM AID\u6846\u67b6\u53ef\u5229\u7528CIL\u73b0\u6709\u529f\u80fd\uff0c\u964d\u4f4e\u5165\u95e8\u95e8\u69db\uff0c\u9f13\u52b1\u4f7f\u7528\u91cd\u5efa\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2602.11404", "pdf": "https://arxiv.org/pdf/2602.11404", "abs": "https://arxiv.org/abs/2602.11404", "authors": ["Ioannis Caragiannis", "Vasilis Gkatzelis", "Sebastian Homrighausen"], "title": "The Distortion of Prior-Independent b-Matching Mechanisms", "categories": ["cs.GT", "cs.DS"], "comment": null, "summary": "In a setting where $m$ items need to be partitioned among $n$ agents, we evaluate the performance of mechanisms that take as input each agent's \\emph{ordinal preferences}, i.e., their ranking of the items from most- to least-preferred. The standard measure for evaluating ordinal mechanisms is the \\emph{distortion}, and the vast majority of the literature on distortion has focused on worst-case analysis, leading to some overly pessimistic results. We instead evaluate the distortion of mechanisms with respect to their expected performance when the agents' preferences are generated stochastically. We first show that no ordinal mechanism can achieve a distortion better than $e/(e-1)\\approx 1.582$, even if each agent needs to receive exactly one item (i.e., $m=n$) and every agent's values for different items are drawn i.i.d.\\ from the same known distribution. We then complement this negative result by proposing an ordinal mechanism that achieves the optimal distortion of $e/(e-1)$ even if each agent's values are drawn from an agent-specific distribution that is unknown to the mechanism. To further refine our analysis, we also optimize the \\emph{distortion gap}, i.e., the extent to which an ordinal mechanism approximates the optimal distortion possible for the instance at hand, and we propose a mechanism with a near-optimal distortion gap of $1.076$. Finally, we also evaluate the distortion and distortion gap of simple mechanisms that have a one-pass structure.", "AI": {"tldr": "\u6587\u7ae0\u8bc4\u4f30\u5206\u914d\u7269\u54c1\u673a\u5236\u5728\u968f\u673a\u504f\u597d\u4e0b\u7684\u5931\u771f\uff08distortion\uff09\uff0c\u7ed9\u51fa\u4e0b\u754c\uff0c\u63d0\u51fa\u8fbe\u5230\u6700\u4f18\u5931\u771f\u4e0e\u8fd1\u4e4e\u6700\u4f18\u5931\u771f\u5dee\u8ddd\u7684\u673a\u5236\uff0c\u8fd8\u8bc4\u4f30\u5355\u6b65\u7ed3\u6784\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u5bf9\u5e8f\u6570\u673a\u5236\u7684\u5931\u771f\u8bc4\u4f30\u591a\u4e3a\u6700\u574f\u60c5\u51b5\u5206\u6790\uff0c\u7ed3\u679c\u8fc7\u4e8e\u60b2\u89c2\uff0c\u672c\u6587\u4ece\u968f\u673a\u751f\u6210\u504f\u597d\u7684\u9884\u671f\u6027\u80fd\u89d2\u5ea6\u8bc4\u4f30\u5931\u771f\u3002", "method": "\u5148\u8bc1\u660e\u5373\u4f7f\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5e8f\u6570\u673a\u5236\u5931\u771f\u65e0\u6cd5\u4f18\u4e8ee/(e - 1)\uff1b\u518d\u63d0\u51fa\u80fd\u8fbe\u5230\u8be5\u6700\u4f18\u5931\u771f\u7684\u673a\u5236\uff1b\u4f18\u5316\u5931\u771f\u5dee\u8ddd\u5e76\u63d0\u51fa\u76f8\u5e94\u673a\u5236\uff1b\u8bc4\u4f30\u5355\u6b65\u7ed3\u6784\u673a\u5236\u7684\u5931\u771f\u548c\u5931\u771f\u5dee\u8ddd\u3002", "result": "\u8bc1\u660e\u5e8f\u6570\u673a\u5236\u5931\u771f\u4e0b\u754c\u4e3ae/(e - 1)\uff1b\u63d0\u51fa\u8fbe\u5230\u6700\u4f18\u5931\u771fe/(e - 1)\u548c\u8fd1\u6700\u4f18\u5931\u771f\u5dee\u8ddd1.076\u7684\u673a\u5236\uff1b\u8bc4\u4f30\u4e86\u5355\u6b65\u7ed3\u6784\u673a\u5236\u7684\u5931\u771f\u548c\u5931\u771f\u5dee\u8ddd\u3002", "conclusion": "\u4ece\u968f\u673a\u504f\u597d\u89d2\u5ea6\u8bc4\u4f30\u673a\u5236\u5931\u771f\uff0c\u5f97\u5230\u7406\u8bba\u4e0b\u754c\uff0c\u63d0\u51fa\u6709\u6548\u673a\u5236\uff0c\u8fd8\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u5355\u6b65\u7ed3\u6784\u673a\u5236\u3002"}}
{"id": "2602.11562", "pdf": "https://arxiv.org/pdf/2602.11562", "abs": "https://arxiv.org/abs/2602.11562", "authors": ["Tianhe Lin", "Ziwei Xiong", "Baoyuan Ou", "Yingjie Qin", "Lai Xu", "Xiaocheng Zhong", "Yao Hu", "Zhiyong Wang", "Tao Zhou", "Yubin Xu", "Di Wu"], "title": "LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling", "categories": ["cs.IR"], "comment": "9 pages", "summary": "Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict \"Latency Wall\", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.", "AI": {"tldr": "\u63d0\u51faLASER\u6846\u67b6\u6253\u7834\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u7ed3\u5408\u7cfb\u7edf\u548c\u7b97\u6cd5\u4f18\u5316\uff0c\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebf\u6d4b\u8bd5\u6548\u679c\u597d\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u957f\u5e8f\u5217\u5efa\u6a21\u5728\u5b9e\u65f6\u5de5\u4e1a\u73af\u5883\u9762\u4e34\u9ad8I/O\u5ef6\u8fdf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u74f6\u9888\u3002", "method": "\u63d0\u51faLASER\u6846\u67b6\uff0c\u5305\u542bSeqVault\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u964d\u4f4e\u68c0\u7d22\u5ef6\u8fdf\u548cCPU\u4f7f\u7528\uff0c\u63d0\u51faSTA\u673a\u5236\u548cGSTA\u6a21\u5757\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u4f7fADVV\u63d0\u53472.36%\uff0c\u6536\u5165\u63d0\u53472.08%\u3002", "conclusion": "LASER\u6846\u67b6\u6709\u6548\u6253\u7834\u5ef6\u8fdf\u74f6\u9888\uff0c\u6709\u826f\u597d\u53ef\u6269\u5c55\u6027\u548c\u5546\u4e1a\u5f71\u54cd\u3002"}}
{"id": "2602.11791", "pdf": "https://arxiv.org/pdf/2602.11791", "abs": "https://arxiv.org/abs/2602.11791", "authors": ["Antoine Amarilli", "Claire David", "Nadime Francis", "Victor Marsault", "Mika\u00ebl Monet", "Yann Strozecki"], "title": "Gray Codes With Constant Delay and Constant Auxiliary Space", "categories": ["cs.DS", "cs.CC"], "comment": "29 pages, 8 figures", "summary": "We give the first two algorithms to enumerate all binary words of $\\{0,1\\}^\\ell$ (like Gray codes) while ensuring that the delay and the auxiliary space is independent from $\\ell$, i.e., constant time for each word, and constant memory in addition to the $\\ell$ bits storing the current word. Our algorithms are given in two new computational models: tape machines and deque machines. We also study more restricted models, queue machines and stack machines, and show that they cannot enumerate all binary words with constant auxiliary space, even with unrestricted delay.\n  A tape machine is a Turing machine that stores the current binary word on a single working tape of length $\\ell$. The machine has a single head and must edit its tape to reach all possible words of $\\{0,1\\}^{\\ell}$ , and output them (in unit time, by entering special output states), with no duplicates. We construct a tape machine that achieves this task with constant delay between consecutive outputs, which implies that the machine implements a so-called skew-tolerant quasi-Gray code. We then construct a more involved tape machine that implements a Gray code.\n  A deque machine stores the current binary word on a double-ended queue of length $\\ell$, and stores a constant-size internal state. It works as a tape machine, except that it modifies the content of the deque by performing push and pop operations on the endpoints. We construct deque machines that enumerate all words of $\\{0,1\\}^\\ell$ with constant-delay. The main technical challenge in this model is to correctly detect when enumeration has finished.\n  Our work on deque machine is also motivated by other contexts in which endpoint modifications occur naturally. In particular, our result is a first step towards enumerating walks in directed graphs with constant delay and constant auxiliary space, addressing a core task in modern graph database query processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7b97\u6cd5\u5728\u78c1\u5e26\u673a\u548c\u53cc\u7aef\u961f\u5217\u673a\u6a21\u578b\u4e0b\u679a\u4e3e\u6240\u6709\u4e8c\u8fdb\u5236\u5b57\uff0c\u4fdd\u8bc1\u5ef6\u8fdf\u548c\u8f85\u52a9\u7a7a\u95f4\u4e3a\u5e38\u6570\uff0c\u8fd8\u7814\u7a76\u53d7\u9650\u6a21\u578b\u5e76\u8bc1\u660e\u5176\u4e0d\u80fd\u4ee5\u5e38\u6570\u8f85\u52a9\u7a7a\u95f4\u679a\u4e3e\u3002\u53cc\u7aef\u961f\u5217\u673a\u7684\u7814\u7a76\u4e3a\u6709\u5411\u56fe\u4e2d\u679a\u4e3e\u8def\u5f84\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5bfb\u627e\u80fd\u4ee5\u5e38\u6570\u5ef6\u8fdf\u548c\u5e38\u6570\u8f85\u52a9\u7a7a\u95f4\u679a\u4e3e\u6240\u6709\u4e8c\u8fdb\u5236\u5b57\u7684\u7b97\u6cd5\uff0c\u4e14\u53cc\u7aef\u961f\u5217\u673a\u7814\u7a76\u4e0e\u56fe\u6570\u636e\u5e93\u67e5\u8be2\u5904\u7406\u76f8\u5173\u3002", "method": "\u5728\u78c1\u5e26\u673a\u548c\u53cc\u7aef\u961f\u5217\u673a\u4e24\u79cd\u8ba1\u7b97\u6a21\u578b\u4e0b\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u78c1\u5e26\u673a\u901a\u8fc7\u7f16\u8f91\u5de5\u4f5c\u5e26\uff0c\u53cc\u7aef\u961f\u5217\u673a\u901a\u8fc7\u7aef\u70b9\u7684\u63a8\u548c\u5f39\u51fa\u64cd\u4f5c\u3002", "result": "\u5728\u78c1\u5e26\u673a\u6a21\u578b\u4e0b\u6784\u9020\u4e86\u5b9e\u73b0\u659c\u5bb9\u9519\u51c6\u683c\u96f7\u7801\u548c\u683c\u96f7\u7801\u7684\u673a\u5668\uff1b\u5728\u53cc\u7aef\u961f\u5217\u673a\u6a21\u578b\u4e0b\u6784\u9020\u4e86\u80fd\u4ee5\u5e38\u6570\u5ef6\u8fdf\u679a\u4e3e\u6240\u6709\u4e8c\u8fdb\u5236\u5b57\u7684\u673a\u5668\u3002", "conclusion": "\u78c1\u5e26\u673a\u548c\u53cc\u7aef\u961f\u5217\u673a\u80fd\u4ee5\u5e38\u6570\u5ef6\u8fdf\u548c\u5e38\u6570\u8f85\u52a9\u7a7a\u95f4\u679a\u4e3e\u6240\u6709\u4e8c\u8fdb\u5236\u5b57\uff0c\u961f\u5217\u673a\u548c\u6808\u673a\u4e0d\u80fd\uff1b\u53cc\u7aef\u961f\u5217\u673a\u7684\u7814\u7a76\u662f\u6709\u5411\u56fe\u5e38\u6570\u5ef6\u8fdf\u548c\u5e38\u6570\u8f85\u52a9\u7a7a\u95f4\u679a\u4e3e\u8def\u5f84\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2602.11686", "pdf": "https://arxiv.org/pdf/2602.11686", "abs": "https://arxiv.org/abs/2602.11686", "authors": ["Xinyi Liu", "Yujie Wang", "Fangcheng Fu", "Xuefeng Xiao", "Huixia Li", "Jiashi Li", "Bin Cui"], "title": "LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training", "categories": ["cs.DC", "cs.LG"], "comment": "19 pages, 12 figures, the paper will be presented at ASPLOS 2026", "summary": "Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.\n  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.", "AI": {"tldr": "\u63d0\u51faLAER - MoE\u6846\u67b6\u89e3\u51b3MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6709\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u4e2d\u52a8\u6001\u8def\u7531\u5bfc\u81f4\u4e13\u5bb6\u95f4\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888\u3002", "method": "\u5f15\u5165LAER - MoE\u6846\u67b6\uff0c\u91c7\u7528FSEP\u5e76\u884c\u8303\u5f0f\uff0c\u5bf9\u901a\u4fe1\u64cd\u4f5c\u7ec6\u7c92\u5ea6\u8c03\u5ea6\uff0c\u5f00\u53d1\u8d1f\u8f7d\u5747\u8861\u89c4\u5212\u5668\u3002", "result": "\u5728A100\u96c6\u7fa4\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u8bad\u7ec3\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad81.69\u500d\u52a0\u901f\u3002", "conclusion": "LAER - MoE\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.11632", "pdf": "https://arxiv.org/pdf/2602.11632", "abs": "https://arxiv.org/abs/2602.11632", "authors": ["David Hogan", "Andrew Doherty", "Boon Kien Khoo", "Johnson Zhou", "Richard Salib", "James Stewart", "Kiaran Lawson", "Alon Loeffler", "Brett Kagan"], "title": "CL API: Real-Time Closed-Loop Interactions with Biological Neural Networks", "categories": ["q-bio.NC", "cs.ET", "cs.NE", "eess.SY"], "comment": null, "summary": "Biological neural networks (BNNs) are increasingly explored for their rich dynamics, parallelism, and adaptive behavior. Beyond understanding their function as a scientific endeavour, a key focus has been using these biological systems as a novel computing substrate. However, BNNs can only function as reliable information-processing systems if inputs are delivered in a temporally and structurally consistent manner. In practice, this requires stimulation with precisely controlled structure, microsecond-scale timing, multi-channel synchronization, and the ability to observe and respond to neural activity in real-time. Existing approaches to interacting with BNNs face a fundamental trade-off: they either depend on low-level hardware mechanisms, imposing prohibitive complexity for rapid iteration, or they sacrifice temporal and structural control, undermining consistency and reproducibility - particularly in closed-loop experiments. The Cortical Labs Application Programming Interface (CL API) enables real-time, sub-millisecond closed-loop interactions with BNNs. Taking a contract-based API design approach, the CL API provides users with precise stimulation semantics, transactional admission, deterministic ordering, and explicit synchronization guarantees. This contract is presented through a declarative Python interface, enabling non-expert programmers to express complex stimulation and closed-loop behavior without managing low-level scheduling or hardware details. Ultimately, the CL API provides an accessible and reproducible foundation for real-time experimentation with BNNs, supporting both fundamental biological research and emerging neurocomputing applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u73b0\u6709\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\u4ea4\u4e92\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51faCortical Labs\u5e94\u7528\u7a0b\u5e8f\u7f16\u7a0b\u63a5\u53e3\uff08CL API\uff09\uff0c\u5b83\u80fd\u5b9e\u73b0\u4e0eBNNs\u7684\u5b9e\u65f6\u4e9a\u6beb\u79d2\u7ea7\u95ed\u73af\u4ea4\u4e92\uff0c\u4e3a\u5176\u5b9e\u9a8c\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u4e0eBNNs\u4ea4\u4e92\u7684\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u5e95\u5c42\u786c\u4ef6\u673a\u5236\u5bfc\u81f4\u8fed\u4ee3\u590d\u6742\uff0c\u6216\u727a\u7272\u65f6\u95f4\u548c\u7ed3\u6784\u63a7\u5236\u5f71\u54cd\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5951\u7ea6\u7684API\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fPython\u63a5\u53e3\u5448\u73b0\u5951\u7ea6\uff0c\u8ba9\u975e\u4e13\u4e1a\u7a0b\u5e8f\u5458\u80fd\u8868\u8fbe\u590d\u6742\u884c\u4e3a\u800c\u65e0\u9700\u7ba1\u7406\u5e95\u5c42\u7ec6\u8282\u3002", "result": "CL API\u80fd\u5b9e\u73b0\u4e0eBNNs\u7684\u5b9e\u65f6\u4e9a\u6beb\u79d2\u7ea7\u95ed\u73af\u4ea4\u4e92\uff0c\u63d0\u4f9b\u7cbe\u786e\u523a\u6fc0\u8bed\u4e49\u3001\u4e8b\u52a1\u6027\u51c6\u5165\u3001\u786e\u5b9a\u6027\u6392\u5e8f\u548c\u663e\u5f0f\u540c\u6b65\u4fdd\u8bc1\u3002", "conclusion": "CL API\u4e3aBNNs\u7684\u5b9e\u65f6\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u548c\u53ef\u91cd\u73b0\u7684\u57fa\u7840\uff0c\u652f\u6301\u57fa\u7840\u751f\u7269\u5b66\u7814\u7a76\u548c\u65b0\u5174\u795e\u7ecf\u8ba1\u7b97\u5e94\u7528\u3002"}}
{"id": "2602.11224", "pdf": "https://arxiv.org/pdf/2602.11224", "abs": "https://arxiv.org/abs/2602.11224", "authors": ["Hubert M. Pysklo", "Artem Zhuravel", "Patrick D. Watson"], "title": "Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation", "categories": ["cs.SE", "cs.CL"], "comment": "Pre-Print. Under review for KDD 2026", "summary": "We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access, prompt structures, and agentic frameworks. Benchmarks must make fundamental trade-offs between a sandboxed approach that controls for variation in software environments and more ecologically valid approaches employing real services. Agent-Diff attempts to capture the desirable features of both of these approaches by including access to the real API interfaces for software services while sandboxing the environment in which calls are made, processed, and evaluated. This approach relies on two key innovations. The first is a novel state-diff contract, which separates process from outcome - rather than fuzzy trace or parameter matching, we define task success as whether the expected change in environment state was achieved. The second is a novel sandbox that provides a standardized scripting layer that all models use to execute code against external APIs (Slack, Box, Linear, Google Calendar). Thus, we can evaluate different agentic LLMs against a standardized set of contracts using a unified sandbox while still evaluating their performance on real-world service interfaces. Using the Agent-Diff framework, we provide benchmarks for nine LLMs across 224 tasks utilizing enterprise software workflows. In addition, we evaluate the robustness of the framework with ablation experiments to assess the contribution of access to API documentation on benchmark performance. Code and data: https://github.com/agent-diff-bench/agent-diff.", "AI": {"tldr": "\u63d0\u51faAgent - Diff\u57fa\u51c6\u6846\u67b6\u8bc4\u4f30\u80fd\u901a\u8fc7\u5916\u90e8API\u6267\u884c\u4ee3\u7801\u7684\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ecb\u7ecd\u6846\u67b6\u521b\u65b0\u70b9\u5e76\u8fdb\u884c\u591a\u6a21\u578b\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u53ca\u6d88\u878d\u5b9e\u9a8c\u3002", "motivation": "\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u6c99\u76d2\u65b9\u6cd5\u548c\u751f\u6001\u6709\u6548\u65b9\u6cd5\u95f4\u9700\u6743\u8861\uff0c\u9700\u65b0\u6846\u67b6\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u5dee\u5f02\u5408\u7ea6\u5206\u79bb\u8fc7\u7a0b\u4e0e\u7ed3\u679c\uff0c\u5b9a\u4e49\u4efb\u52a1\u6210\u529f\u6807\u51c6\uff1b\u521b\u5efa\u6807\u51c6\u5316\u811a\u672c\u5c42\u6c99\u76d2\u4f9b\u6a21\u578b\u8c03\u7528\u5916\u90e8API\uff1b\u7528\u6846\u67b6\u5bf99\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728224\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u5c55\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u5b8c\u6210\u5bf99\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728224\u4e2a\u4f7f\u7528\u4f01\u4e1a\u8f6f\u4ef6\u5de5\u4f5c\u6d41\u4efb\u52a1\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fdb\u884c\u4e86\u6846\u67b6\u9c81\u68d2\u6027\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "Agent - Diff\u6846\u67b6\u53ef\u5728\u7edf\u4e00\u6c99\u76d2\u4e0b\u6309\u6807\u51c6\u5316\u5408\u7ea6\u8bc4\u4f30\u4e0d\u540c\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u670d\u52a1\u63a5\u53e3\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.12066", "pdf": "https://arxiv.org/pdf/2602.12066", "abs": "https://arxiv.org/abs/2602.12066", "authors": ["Brian C. Albrecht", "Alex Tabarrok", "Mark Whitmeyer"], "title": "Chaos and Misallocation under Price Controls", "categories": ["econ.GN"], "comment": null, "summary": "Price controls kill the incentive for arbitrage. We prove a Chaos Theorem: under a binding price ceiling, suppliers are indifferent across destinations, so arbitrarily small cost differences can determine the entire allocation. The economy tips to corner outcomes in which some markets are fully served while others are starved; small parameter changes flip the identity of the corners, generating discontinuous welfare jumps. These corner allocations create a distinct source of cross-market misallocation, separate from the aggregate quantity loss (the Harberger triangle) and from within-market misallocation emphasized in prior work. They also create an identification problem: welfare depends on demand far from the observed equilibrium. We derive sharp bounds on misallocation that require no parametric assumptions. In an efficient allocation, shadow prices are equalized across markets; combined with the adding-up constraint, this collapses the infinite-dimensional welfare problem to a one-dimensional search over a common shadow price, with extremal losses achieved by piecewise-linear demand schedules. Calibrating the bounds to station-level AAA survey data from the 1973-74 U.S. gasoline crisis, misallocation losses range from roughly 1 to 9 times the Harberger triangle.", "AI": {"tldr": "\u4ef7\u683c\u7ba1\u5236\u4f1a\u6d88\u9664\u5957\u5229\u52a8\u673a\uff0c\u5728\u4ef7\u683c\u4e0a\u9650\u7ea6\u675f\u4e0b\u4f1a\u51fa\u73b0\u6781\u7aef\u5e02\u573a\u5206\u914d\uff0c\u4ea7\u751f\u8de8\u5e02\u573a\u9519\u914d\uff0c\u8bba\u6587\u63a8\u5bfc\u9519\u914d\u754c\u9650\u5e76\u6821\u51c6\u6570\u636e\u5f97\u51fa\u635f\u5931\u8303\u56f4\u3002", "motivation": "\u7814\u7a76\u4ef7\u683c\u7ba1\u5236\u4e0b\u7684\u5e02\u573a\u5206\u914d\u60c5\u51b5\u4ee5\u53ca\u7531\u6b64\u4ea7\u751f\u7684\u798f\u5229\u53d8\u5316\u548c\u9519\u914d\u95ee\u9898\u3002", "method": "\u8bc1\u660e\u6df7\u6c8c\u5b9a\u7406\uff0c\u63a8\u5bfc\u4e0d\u9700\u8981\u53c2\u6570\u5047\u8bbe\u7684\u9519\u914d\u754c\u9650\uff0c\u5c06\u798f\u5229\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u7ef4\u641c\u7d22\u3002", "result": "\u4ef7\u683c\u7ba1\u5236\u4e0b\u7ecf\u6d4e\u4f1a\u51fa\u73b0\u6781\u7aef\u5e02\u573a\u5206\u914d\uff0c\u4ea7\u751f\u8de8\u5e02\u573a\u9519\u914d\uff0c\u6821\u51c6\u6570\u636e\u663e\u793a\u9519\u914d\u635f\u5931\u662f\u54c8\u4f2f\u683c\u4e09\u89d2\u76841 - 9\u500d\u3002", "conclusion": "\u4ef7\u683c\u7ba1\u5236\u4f1a\u5bfc\u81f4\u5e02\u573a\u5206\u914d\u51fa\u73b0\u6781\u7aef\u60c5\u51b5\u548c\u8de8\u5e02\u573a\u9519\u914d\uff0c\u798f\u5229\u4f9d\u8d56\u4e8e\u8ddd\u89c2\u5bdf\u5230\u7684\u5747\u8861\u8f83\u8fdc\u7684\u9700\u6c42\u3002"}}
{"id": "2602.11298", "pdf": "https://arxiv.org/pdf/2602.11298", "abs": "https://arxiv.org/abs/2602.11298", "authors": ["Alexander H. Liu", "Andy Ehrenberg", "Andy Lo", "Chen-Yo Sun", "Guillaume Lample", "Jean-Malo Delignon", "Khyathi Raghavi Chandu", "Patrick von Platen", "Pavankumar Reddy Muddireddy", "Rohin Arora", "Sanchit Gandhi", "Sandeep Subramanian", "Soham Ghosh", "Srijan Mishra", "Abhinav Rastogi", "Alan Jeffares", "Albert Jiang", "Alexandre Sablayrolles", "Am\u00e9lie H\u00e9liou", "Andrew Bai", "Angele Lenglemetz", "Anmol Agarwal", "Anton Eliseev", "Antonia Calvi", "Arjun Majumdar", "Baptiste Bout", "Baptiste Rozi\u00e8re", "Baudouin De Monicault", "Benjamin Tibi", "Cl\u00e9mence Lanfranchi", "Connor Chen", "Corentin Barreau", "Corentin Sautier", "Cyprien Courtot", "Darius Dabert", "Diego de las Casas", "Elliot Chane-Sane", "Enguerrand Paquin", "Faruk Ahmed", "Federico Baldassarre", "Gabrielle Berrada", "Ga\u00ebtan Ecrepont", "Gauthier Guinet", "Genevieve Hayes", "Georgii Novikov", "Giada Pistilli", "Guillaume Martin", "Gunjan Dhanuka", "Gunshi Gupta", "Han Zhou", "Indraneel Mukherjee", "Irene Zhang", "Jaeyoung Kim", "Jan Ludziejewski", "Jason Rute", "Joachim Studnia", "John Harvill", "Jonas Amar", "Josselin Somerville Roberts", "Julien Tauran", "Karmesh Yadav", "Kartik Khandelwal", "Kush Jain", "Laurence Aitchison", "L\u00e9onard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Maarten Buyl", "Manan Sharma", "Margaret Jennings", "Marie Pellat", "Mark Prins", "Mathieu Poir\u00e9e", "Mathilde Guillaumin", "Matthieu Dinot", "Matthieu Futeral", "Maxime Darrin", "Maximilian Augustin", "Mert Unsal", "Mia Chiquier", "Nathan Grinsztajn", "Neha Gupta", "Olivier Bousquet", "Olivier Duchenne", "Patricia Wang", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philom\u00e8ne Chagniot", "Pierre Stock", "Piotr Mi\u0142o\u015b", "Prateek Gupta", "Pravesh Agrawal", "Quentin Torroba", "Ram Ramrakhya", "Rishi Shah", "Romain Sauvestre", "Roman Soletskyi", "Rosalie Millner", "Sagar Vaze", "Samuel Humeau", "Siddharth Gandhi", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Th\u00e9o Cachet", "Theo Simon Sorg", "Thibaut Lavril", "Thomas Chabal", "Thomas Foubert", "Thomas Robert", "Thomas Wang", "Tim Lawson", "Tom Bewley", "Tom Edwards", "Tyler Wang", "Valeriia Nemychnikova", "Van Phung", "Vedant Nanda", "Victor Jouault", "Virgile Richard", "Vladislav Bataev", "Wassim Bouaziz", "Wen-Ding Li", "William Marshall", "Xinghui Li", "Xingran Guo", "Xinyu Yang", "Yannic Neuhaus", "Yihan Wang", "Zaccharie Ramzi", "Zhenlin Xu"], "title": "Voxtral Realtime", "categories": ["cs.AI"], "comment": null, "summary": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.", "AI": {"tldr": "\u4ecb\u7ecdVoxtral Realtime\uff0c\u4e00\u79cd\u539f\u751f\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u5728\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u4e0b\u8fbe\u5230\u79bb\u7ebf\u8f6c\u5f55\u8d28\u91cf\uff0c\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u5f00\u53d1\u80fd\u5728\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u4e0b\u8fbe\u5230\u79bb\u7ebf\u8f6c\u5f55\u8d28\u91cf\u7684\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eDelayed Streams Modeling\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u7684\u56e0\u679c\u97f3\u9891\u7f16\u7801\u5668\u548cAda RMS - Norm\uff0c\u572813\u79cd\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728480ms\u5ef6\u8fdf\u4e0b\uff0cVoxtral Realtime\u6027\u80fd\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u79bb\u7ebf\u8f6c\u5f55\u7cfb\u7edfWhisper\u76f8\u5f53\u3002", "conclusion": "Voxtral Realtime\u80fd\u6709\u6548\u5b9e\u73b0\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\u4e14\u8fbe\u5230\u8f83\u597d\u6548\u679c\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2602.11711", "pdf": "https://arxiv.org/pdf/2602.11711", "abs": "https://arxiv.org/abs/2602.11711", "authors": ["Jean-Fran\u00e7ois Giovannelli"], "title": "Estimation of instrument and noise parameters for inverse problem based on prior diffusion model", "categories": ["stat.ML", "cs.LG", "math.NA", "stat.AP"], "comment": null, "summary": "This article addresses the issue of estimating observation parameters (response and error parameters) in inverse problems. The focus is on cases where regularization is introduced in a Bayesian framework and the prior is modeled by a diffusion process. In this context, the issue of posterior sampling is well known to be thorny, and a recent paper proposes a notably simple and effective solution. Consequently, it offers an remarkable additional flexibility when it comes to estimating observation parameters. The proposed strategy enables us to define an optimal estimator for both the observation parameters and the image of interest. Furthermore, the strategy provides a means of quantifying uncertainty. In addition, MCMC algorithms allow for the efficient computation of estimates and properties of posteriors, while offering some guarantees. The paper presents several numerical experiments that clearly confirm the computational efficiency and the quality of both estimates and uncertainties quantification.", "AI": {"tldr": "\u6587\u7ae0\u805a\u7126\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u6b63\u5219\u5316\u4e14\u5148\u9a8c\u7531\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u65f6\u9006\u95ee\u9898\u4e2d\u89c2\u6d4b\u53c2\u6570\uff08\u54cd\u5e94\u548c\u8bef\u5dee\u53c2\u6570\uff09\u7684\u4f30\u8ba1\uff0c\u63d0\u51fa\u7b56\u7565\u4f30\u8ba1\u53c2\u6570\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u6b63\u5219\u5316\u4e14\u5148\u9a8c\u7531\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u65f6\u9006\u95ee\u9898\u4e2d\u89c2\u6d4b\u53c2\u6570\u4f30\u8ba1\u7684\u96be\u9898\uff0c\u5c24\u5176\u662f\u540e\u9a8c\u91c7\u6837\u7684\u68d8\u624b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b56\u7565\u6765\u5b9a\u4e49\u89c2\u6d4b\u53c2\u6570\u548c\u611f\u5174\u8da3\u56fe\u50cf\u7684\u6700\u4f18\u4f30\u8ba1\u5668\uff0c\u4f7f\u7528MCMC\u7b97\u6cd5\u8ba1\u7b97\u540e\u9a8c\u4f30\u8ba1\u548c\u6027\u8d28\u3002", "result": "\u6240\u63d0\u7b56\u7565\u80fd\u6709\u6548\u4f30\u8ba1\u89c2\u6d4b\u53c2\u6570\uff0c\u53ef\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8ba1\u7b97\u6548\u7387\u3001\u4f30\u8ba1\u8d28\u91cf\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u5728\u9006\u95ee\u9898\u89c2\u6d4b\u53c2\u6570\u4f30\u8ba1\u4e2d\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u4f30\u8ba1\u8d28\u91cf\u597d\u3001\u80fd\u6709\u6548\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u4f18\u70b9\u3002"}}
{"id": "2602.11186", "pdf": "https://arxiv.org/pdf/2602.11186", "abs": "https://arxiv.org/abs/2602.11186", "authors": ["Zhihan Zeng", "Kaihe Wang", "Zhongpei Zhang", "Yue Xiu"], "title": "GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0\\%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter--approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal \"always-on\" security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGAC - KAN\u6846\u67b6\u89e3\u51b3GenAI\u65f6\u4ee3\u6570\u636e\u7a00\u7f3a\u548c\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u540c\u65f6\u5177\u6709\u6781\u8f7b\u91cf\u7ea7\u7279\u70b9\u3002", "motivation": "GenAI\u5e94\u7528\u7ed9\u8fb9\u7f18\u786c\u4ef6\u5e26\u6765\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4e14\u771f\u5b9e\u5e72\u6270\u6570\u636e\u7a00\u7f3a\uff0c\u5f71\u54cd\u5b89\u5168\u4efb\u52a1\u5982GNSS\u4fe1\u53f7\u4fdd\u62a4\u548c\u5206\u7c7b\u5668\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u6a21\u62df\u65b9\u6cd5\u5408\u6210\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1MS - GAC\u9aa8\u5e72\u7f51\u7edc\uff1b\u5f15\u5165KAN\u51b3\u7b56\u5934\u3002", "result": "GAC - KAN\u6574\u4f53\u51c6\u786e\u7387\u8fbe98.0%\uff0c\u6a21\u578b\u4ec5\u542b0.13\u4e07\u4e2a\u53c2\u6570\uff0c\u6bd4ViT\u57fa\u7ebf\u5c11\u7ea6660\u500d\u3002", "conclusion": "GAC - KAN\u662f\u7406\u60f3\u7684\u201c\u59cb\u7ec8\u5728\u7ebf\u201d\u5b89\u5168\u4f34\u4fa3\uff0c\u80fd\u786e\u4fddGNSS\u53ef\u9760\u6027\u4e14\u4e0d\u4e0eGenAI\u4e3b\u8981\u4efb\u52a1\u4e89\u593a\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2602.12242", "pdf": "https://arxiv.org/pdf/2602.12242", "abs": "https://arxiv.org/abs/2602.12242", "authors": ["Andy Nonaka", "Yingheng Tang", "Julian C. LePelch", "Prabhat Kumar", "Weiqun Zhang", "Jorge A. Munoz", "Christian Fernandez-Soria", "Cesar Diaz", "David J. Gardner", "Zhi Jackie Yao"], "title": "MagneX: A High-Performance, GPU-Enabled, Data-Driven Micromagnetics Solver for Spintronics", "categories": ["cs.CE", "cond-mat.other"], "comment": null, "summary": "In order to comprehensively investigate the multiphysics coupling in spintronic devices, it is essential to parallelize and utilize GPU-acceleration to address the spatial and temporal disparities inherent in the relevant physics. Additionally, the use of cutting-edge time integration libraries as well as machine learning (ML) approaches to replace and potentially accelerate expensive computational routines are attractive capabilities to enhance modeling capabilities moving forward. Leveraging the Exascale Computing Project software framework AMReX, as well as SUNDIALS time-integration libraries and python-based ML workflows, we have developed an open-source micromagnetics modeling tool called MagneX. This tool incorporates various crucial magnetic coupling mechanisms, including Zeeman coupling, demagnetization coupling, crystalline anisotropy interaction, exchange coupling, and Dzyaloshinskii-Moriya interaction (DMI) coupling. We demonstrate the GPU performance and scalability of the code and rigorously validate MagneX's functionality using the mumag standard problems and widely-accepted DMI benchmarks. Furthermore, we demonstrate the data-driven capability of MagneX by replacing the computationally-expensive demagnetization physics with neural network libraries trained from our simulation data. With the capacity to explore complete physical interactions, this innovative approach offers a promising pathway to better understand and develop fully integrated spintronic and electronic systems.", "AI": {"tldr": "\u5f00\u53d1\u5f00\u6e90\u5fae\u78c1\u5b66\u5efa\u6a21\u5de5\u5177MagneX\uff0c\u5229\u7528\u591a\u79cd\u6280\u672f\uff0c\u9a8c\u8bc1\u6027\u80fd\u548c\u529f\u80fd\uff0c\u5c55\u793a\u6570\u636e\u9a71\u52a8\u80fd\u529b\uff0c\u52a9\u529b\u81ea\u65cb\u7535\u5b50\u7cfb\u7edf\u7814\u7a76\u3002", "motivation": "\u5168\u9762\u7814\u7a76\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u591a\u7269\u7406\u573a\u8026\u5408\uff0c\u9700\u5e76\u884c\u5316\u548cGPU\u52a0\u901f\uff0c\u5229\u7528\u5148\u8fdb\u65f6\u95f4\u79ef\u5206\u5e93\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u5229\u7528Exascale Computing Project\u8f6f\u4ef6\u6846\u67b6AMReX\u3001SUNDIALS\u65f6\u95f4\u79ef\u5206\u5e93\u548c\u57fa\u4e8ePython\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u5f00\u53d1MagneX\uff0c\u5305\u542b\u591a\u79cd\u78c1\u8026\u5408\u673a\u5236\u3002", "result": "\u5c55\u793a\u4ee3\u7801\u7684GPU\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7528\u6807\u51c6\u95ee\u9898\u548c\u57fa\u51c6\u9a8c\u8bc1\u529f\u80fd\uff0c\u7528\u795e\u7ecf\u7f51\u7edc\u5e93\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u7684\u9000\u78c1\u7269\u7406\u5c55\u793a\u6570\u636e\u9a71\u52a8\u80fd\u529b\u3002", "conclusion": "\u8be5\u521b\u65b0\u65b9\u6cd5\u80fd\u63a2\u7d22\u5b8c\u6574\u7269\u7406\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u548c\u5f00\u53d1\u81ea\u65cb\u7535\u5b50\u4e0e\u7535\u5b50\u96c6\u6210\u7cfb\u7edf\u63d0\u4f9b\u9014\u5f84\u3002"}}
{"id": "2602.11417", "pdf": "https://arxiv.org/pdf/2602.11417", "abs": "https://arxiv.org/abs/2602.11417", "authors": ["Rashida Hakim", "Christos Papadimitriou", "Mihalis Yannakakis"], "title": "Fair Data-Exchange Mechanisms", "categories": ["cs.GT"], "comment": "29 pages, 2 figures", "summary": "We study data exchange among strategic agents without monetary transfers, motivated by domains such as research consortia and healthcare collaborations where payments are infeasible or restricted. The central challenge is to reap the benefits of data-sharing while preventing free-riding that would otherwise lead agents to under invest in data collection. We introduce a simple fair-exchange contract in which, for every pair of agents, each agent receives exactly as many data points as it provides, equal to the minimum of their two collection levels. We show that the game induced by this contract is supermodular under a transformation of the strategy space. This results in a clean structure: pure Nash equilibria exist, they form a lattice, and can be computed in time quadratic in the number of agents. In addition, the maximal equilibrium is truthfully implementable under natural enforcement assumptions and is globally Pareto-optimal across all strategy profiles. In a graph-restricted variant of the model supermodularity fails, but an adaptation of the construction still yields efficiently computable pure Nash equilibria and Pareto-optimal outcomes. Overall, fair exchange provides a tractable and incentive-aligned mechanism for data exchange in the absence of payments.", "AI": {"tldr": "\u7814\u7a76\u65e0\u91d1\u94b1\u8f6c\u79fb\u4e0b\u6218\u7565\u4e3b\u4f53\u95f4\u7684\u6570\u636e\u4ea4\u6362\uff0c\u63d0\u51fa\u516c\u5e73\u4ea4\u6362\u5408\u540c\uff0c\u5206\u6790\u5176\u535a\u5f08\u7279\u6027\u53ca\u5747\u8861\u60c5\u51b5\uff0c\u8bc1\u660e\u516c\u5e73\u4ea4\u6362\u662f\u53ef\u884c\u4e14\u6fc0\u52b1\u76f8\u5bb9\u7684\u673a\u5236\u3002", "motivation": "\u5728\u7814\u7a76\u8054\u76df\u548c\u533b\u7597\u5408\u4f5c\u7b49\u65e0\u6cd5\u6216\u9650\u5236\u652f\u4ed8\u7684\u9886\u57df\uff0c\u89e3\u51b3\u6570\u636e\u5171\u4eab\u4e2d\u642d\u4fbf\u8f66\u95ee\u9898\uff0c\u5b9e\u73b0\u6570\u636e\u5171\u4eab\u7684\u5229\u76ca\u3002", "method": "\u5f15\u5165\u516c\u5e73\u4ea4\u6362\u5408\u540c\uff0c\u5bf9\u7b56\u7565\u7a7a\u95f4\u8fdb\u884c\u53d8\u6362\uff0c\u5206\u6790\u5408\u540c\u8bf1\u5bfc\u7684\u535a\u5f08\u3002", "result": "\u5408\u540c\u8bf1\u5bfc\u7684\u535a\u5f08\u5728\u7b56\u7565\u7a7a\u95f4\u53d8\u6362\u4e0b\u662f\u8d85\u6a21\u7684\uff0c\u5b58\u5728\u7eaf\u7eb3\u4ec0\u5747\u8861\u4e14\u5f62\u6210\u683c\uff0c\u53ef\u5728\u4e8c\u6b21\u65f6\u95f4\u5185\u8ba1\u7b97\uff1b\u5728\u56fe\u9650\u5236\u6a21\u578b\u4e2d\u7ecf\u8c03\u6574\u4ecd\u6709\u53ef\u6709\u6548\u8ba1\u7b97\u7684\u7eaf\u7eb3\u4ec0\u5747\u8861\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u516c\u5e73\u4ea4\u6362\u4e3a\u65e0\u652f\u4ed8\u60c5\u51b5\u4e0b\u7684\u6570\u636e\u4ea4\u6362\u63d0\u4f9b\u4e86\u6613\u5904\u7406\u4e14\u6fc0\u52b1\u76f8\u5bb9\u7684\u673a\u5236\u3002"}}
{"id": "2602.11581", "pdf": "https://arxiv.org/pdf/2602.11581", "abs": "https://arxiv.org/abs/2602.11581", "authors": ["Yiteng Tu", "Shuo Miao", "Weihang Su", "Yiqun Liu", "Qingyao Ai"], "title": "Analytical Search", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.\n  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.", "AI": {"tldr": "\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u8303\u5f0f\u96be\u4ee5\u6ee1\u8db3\u5206\u6790\u6027\u4fe1\u606f\u9700\u6c42\uff0c\u672c\u6587\u63d0\u51fa\u5206\u6790\u6027\u641c\u7d22\u8303\u5f0f\uff0c\u4ecb\u7ecd\u6846\u67b6\u5e76\u63a2\u8ba8\u7814\u7a76\u65b9\u5411\uff0c\u547c\u5401\u5f00\u53d1\u652f\u6301\u5206\u6790\u9700\u6c42\u7684\u4e0b\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u8303\u5f0f\u96be\u4ee5\u6ee1\u8db3\u5404\u9886\u57df\u5206\u6790\u4fe1\u606f\u9700\u6c42\uff0c\u5982\u8d8b\u52bf\u5206\u6790\u548c\u56e0\u679c\u5f71\u54cd\u8bc4\u4f30\u7b49\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u5206\u6790\u6027\u641c\u7d22\u8303\u5f0f\uff0c\u5c06\u641c\u7d22\u91cd\u6784\u4e3a\u8bc1\u636e\u9a71\u52a8\u3001\u9762\u5411\u8fc7\u7a0b\u7684\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u6784\u5efa\u5305\u542b\u67e5\u8be2\u7406\u89e3\u3001\u53ec\u56de\u5f0f\u68c0\u7d22\u3001\u63a8\u7406\u611f\u77e5\u878d\u5408\u548c\u81ea\u9002\u5e94\u9a8c\u8bc1\u7684\u7edf\u4e00\u7cfb\u7edf\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u5206\u6790\u6027\u641c\u7d22\u8303\u5f0f\u53ca\u7edf\u4e00\u7cfb\u7edf\u6846\u67b6\uff0c\u63a2\u8ba8\u6f5c\u5728\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u5f3a\u8c03\u5206\u6790\u6027\u641c\u7d22\u7684\u6982\u5ff5\u548c\u5b9e\u8df5\u91cd\u8981\u6027\uff0c\u547c\u5401\u5f00\u53d1\u652f\u6301\u5206\u6790\u4fe1\u606f\u9700\u6c42\u7684\u4e0b\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u3002"}}
{"id": "2602.11826", "pdf": "https://arxiv.org/pdf/2602.11826", "abs": "https://arxiv.org/abs/2602.11826", "authors": ["Mirabel Mendoza-Cadena", "Arturo Merino", "Mads Anker Nielsen", "Kevin Schewior"], "title": "Combinatorial Perpetual Scheduling", "categories": ["cs.DS"], "comment": null, "summary": "This paper introduces a framework for combinatorial variants of perpetual-scheduling problems. Given a set system $(E,\\mathcal{I})$, a schedule consists of an independent set $I_t \\in \\mathcal{I}$ for every time step $t \\in \\mathbb{N}$, with the objective of fulfilling frequency requirements on the occurrence of elements in $E$. We focus specifically on combinatorial bamboo garden trimming, where elements accumulate height at growth rates $g(e)$ for $e \\in E$ given as a convex combination of incidence vectors of $\\mathcal{I}$ and are reset to zero when scheduled, with the goal of minimizing the maximum height attained by any element.\n  Using the integrality of the matroid-intersection polytope, we prove that, when $(E,\\mathcal{I})$ is a matroid, it is possible to guarantee a maximum height of at most 2, which is optimal. We complement this existential result with efficient algorithms for specific matroid classes, achieving a maximum height of 2 for uniform and partition matroids, and 4 for graphic and laminar matroids. In contrast, we show that for general set systems, the optimal guaranteed height is $\u0398(\\log |E|)$ and can be achieved by an efficient algorithm. For combinatorial pinwheel scheduling, where each element $e\\in E$ needs to occur in the schedule at least every $a_e \\in \\mathbb{N}$ time steps, our results imply bounds on the density sufficient for schedulability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ec4\u5408\u6c38\u4e45\u8c03\u5ea6\u95ee\u9898\u6846\u67b6\uff0c\u9488\u5bf9\u7ec4\u5408\u7af9\u82b1\u56ed\u4fee\u526a\u548c\u7ec4\u5408\u98ce\u8f66\u8c03\u5ea6\u95ee\u9898\u7ed9\u51fa\u9ad8\u5ea6\u754c\u9650\u53ca\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7ec4\u5408\u6c38\u4e45\u8c03\u5ea6\u95ee\u9898\uff0c\u6ee1\u8db3\u5143\u7d20\u51fa\u73b0\u9891\u7387\u8981\u6c42\uff0c\u6700\u5c0f\u5316\u5143\u7d20\u6700\u5927\u9ad8\u5ea6\u3002", "method": "\u5229\u7528\u62df\u9635\u4ea4\u591a\u9762\u4f53\u7684\u6574\u6027\u8bc1\u660e\u7ed3\u679c\uff0c\u4e3a\u7279\u5b9a\u62df\u9635\u7c7b\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u5f53(E,\u2110)\u4e3a\u62df\u9635\u65f6\u53ef\u4fdd\u8bc1\u6700\u5927\u9ad8\u5ea6\u81f3\u591a\u4e3a2\uff1b\u5bf9\u7279\u5b9a\u62df\u9635\u7c7b\u7ed9\u51fa\u4e0d\u540c\u6700\u5927\u9ad8\u5ea6\uff1b\u4e00\u822c\u96c6\u7cfb\u6700\u4f18\u4fdd\u8bc1\u9ad8\u5ea6\u4e3a\u0398(log |E|)\uff1b\u5bf9\u7ec4\u5408\u98ce\u8f66\u8c03\u5ea6\u7ed9\u51fa\u53ef\u8c03\u5ea6\u6027\u7684\u5bc6\u5ea6\u754c\u9650\u3002", "conclusion": "\u4e3a\u7ec4\u5408\u6c38\u4e45\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u6846\u67b6\u3001\u7b97\u6cd5\u53ca\u9ad8\u5ea6\u754c\u9650\uff0c\u4e0d\u540c\u60c5\u51b5\u6709\u4e0d\u540c\u7ed3\u679c\u3002"}}
{"id": "2602.11411", "pdf": "https://arxiv.org/pdf/2602.11411", "abs": "https://arxiv.org/abs/2602.11411", "authors": ["Yang Liu", "Armstrong Foundjem", "Xingfang Wu", "Heng Li", "Foutse Khomh"], "title": "Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data", "categories": ["cs.SE"], "comment": null, "summary": "Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.\n  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.\n  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.\n  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\\% - 6\\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\\% - 3\\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.\n  Conclusion \\& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u63d0\u9ad8\u7f16\u7801\u4efb\u52a1\u4e2d\u6a21\u578b\u5bf9\u6f5c\u5728\u5bf9\u6297\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u591a\u6837\u8f93\u5165\u65f6\u53ef\u80fd\u5b58\u5728\u8106\u5f31\u6027\uff0c\u786e\u4fdd\u5176\u9c81\u68d2\u6027\u5f88\u5173\u952e\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30LLM\u9c81\u68d2\u6027\uff0c\u5206\u522b\u7528\u5b57\u7b26\u7ea7\u3001\u5355\u8bcd\u7ea7\u548c\u53e5\u5b50\u7ea7\u6270\u52a8\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u672a\u6270\u52a8\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u4f7f\u7528\u6270\u52a8\u6570\u636e\u96c6\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u6027\u80fd\u7565\u6709\u4e0b\u964d\uff0c\u4e0d\u8fc7\u4e5f\u6709\u65f6\u4f1a\u6709\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7528\u6270\u52a8\u6570\u636e\u5fae\u8c03LLM\u80fd\u6709\u6548\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u4f1a\u6709\u8f7b\u5fae\u6027\u80fd\u635f\u5931\uff0c\u9700\u5e73\u8861\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.11379", "pdf": "https://arxiv.org/pdf/2602.11379", "abs": "https://arxiv.org/abs/2602.11379", "authors": ["Han Su", "Xiaojia Guo", "Xiaoke Zhang"], "title": "Regularized Ensemble Forecasting for Learning Weights from Historical and Current Forecasts", "categories": ["stat.AP", "econ.GN", "stat.ME"], "comment": null, "summary": "Combining forecasts from multiple experts often yields more accurate results than relying on a single expert. In this paper, we introduce a novel regularized ensemble method that extends the traditional linear opinion pool by leveraging both current forecasts and historical performances to set the weights. Unlike existing approaches that rely only on either the current forecasts or past accuracy, our method accounts for both sources simultaneously. It learns weights by minimizing the variance of the combined forecast (or its transformed version) while incorporating a regularization term informed by historical performances. We also show that this approach has a Bayesian interpretation. Different distributional assumptions within this Bayesian framework yield different functional forms for the variance component and the regularization term, adapting the method to various scenarios. In empirical studies on Walmart sales and macroeconomic forecasting, our ensemble outperforms leading benchmark models both when experts' full forecasting histories are available and when experts enter and exit over time, resulting in incomplete historical records. Throughout, we provide illustrative examples that show how the optimal weights are determined and, based on the empirical results, we discuss where the framework's strengths lie and when experts' past versus current forecasts are more informative.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f53\u524d\u9884\u6d4b\u548c\u5386\u53f2\u8868\u73b0\u8bbe\u7f6e\u6743\u91cd\uff0c\u5728\u6c83\u5c14\u739b\u9500\u552e\u548c\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u65b9\u6cd5\u4f18\u52bf\u53ca\u4fe1\u606f\u6765\u6e90\u3002", "motivation": "\u7ed3\u5408\u591a\u4e13\u5bb6\u9884\u6d4b\u901a\u5e38\u66f4\u51c6\u786e\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5f53\u524d\u9884\u6d4b\u6216\u8fc7\u53bb\u51c6\u786e\u6027\uff0c\u672c\u6587\u5e0c\u671b\u540c\u65f6\u8003\u8651\u4e24\u8005\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7ec4\u5408\u9884\u6d4b\u7684\u65b9\u5dee\u5e76\u7ed3\u5408\u5386\u53f2\u8868\u73b0\u7684\u6b63\u5219\u5316\u9879\u6765\u5b66\u4e60\u6743\u91cd\uff0c\u4e14\u8be5\u65b9\u6cd5\u6709\u8d1d\u53f6\u65af\u89e3\u91ca\u3002", "result": "\u5728\u6c83\u5c14\u739b\u9500\u552e\u548c\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u7684\u5b9e\u8bc1\u7814\u7a76\u4e2d\uff0c\u8be5\u96c6\u6210\u65b9\u6cd5\u5728\u4e13\u5bb6\u5b8c\u6574\u5386\u53f2\u9884\u6d4b\u8bb0\u5f55\u548c\u4e0d\u5b8c\u6574\u5386\u53f2\u8bb0\u5f55\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u9886\u5148\u7684\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u7ed9\u51fa\u4e86\u786e\u5b9a\u6700\u4f18\u6743\u91cd\u7684\u793a\u4f8b\uff0c\u8ba8\u8bba\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u4e13\u5bb6\u8fc7\u53bb\u548c\u5f53\u524d\u9884\u6d4b\u5206\u522b\u4f55\u65f6\u66f4\u5177\u4fe1\u606f\u6027\u3002"}}
{"id": "2602.11301", "pdf": "https://arxiv.org/pdf/2602.11301", "abs": "https://arxiv.org/abs/2602.11301", "authors": ["John M. Willis"], "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates", "categories": ["cs.AI", "cs.CR"], "comment": "43 pages, plus 12 pages of appendices. One Figure", "summary": "Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.\n  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.", "AI": {"tldr": "\u4f01\u4e1a\u5feb\u901f\u90e8\u7f72AI\u7cfb\u7edf\u5f62\u6210AI\u5730\u4ea7\uff0c\u73b0\u6709\u6846\u67b6\u7f3a\u53ef\u5b9e\u65bd\u67b6\u6784\uff0c\u672c\u6587\u63d0\u51faPBSAI\u6cbb\u7406\u751f\u6001\u7cfb\u7edf\u67b6\u6784\u5e76\u8bf4\u660e\u5176\u7279\u70b9\u4e0e\u5e94\u7528\u3002", "motivation": "\u4f01\u4e1a\u90e8\u7f72AI\u7cfb\u7edf\u5f62\u6210AI\u5730\u4ea7\uff0c\u4f46\u73b0\u6709\u7684\u6cbb\u7406\u548c\u5b89\u5168\u6846\u67b6\u7f3a\u4e4f\u53ef\u5b9e\u65bd\u7684\u591a\u667a\u80fd\u4f53\u3001\u652f\u6301AI\u7684\u7f51\u7edc\u9632\u5fa1\u67b6\u6784\u3002", "method": "\u5f15\u5165PBSAI\u6cbb\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u5c06\u804c\u8d23\u5206\u4e3a\u5341\u4e8c\u4e2a\u9886\u57df\u5206\u7c7b\u6cd5\uff0c\u5b9a\u4e49\u6709\u754c\u667a\u80fd\u4f53\u5bb6\u65cf\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5f62\u5f0f\u5316\u6a21\u578b\u3002", "result": "PBSAI\u67b6\u6784\u5c55\u793a\u4e86\u4e0eNIST AI RMF\u529f\u80fd\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u8bf4\u660e\u4e86\u5728\u4f01\u4e1aSOC\u548c\u8d85\u5927\u89c4\u6a21\u9632\u5fa1\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "PBSAI\u53ef\u4f5c\u4e3a\u5f00\u653e\u751f\u6001\u7cfb\u7edf\u53d1\u5c55\u548c\u672a\u6765\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u3001\u4ee5\u8bc1\u636e\u4e3a\u4e2d\u5fc3\u7684\u57fa\u7840\u3002"}}
{"id": "2602.11722", "pdf": "https://arxiv.org/pdf/2602.11722", "abs": "https://arxiv.org/abs/2602.11722", "authors": ["Julien Bastian", "Benjamin Leblanc", "Pascal Germain", "Amaury Habrard", "Christine Largeron", "Guillaume Metzler", "Emilie Morvant", "Paul Viallard"], "title": "PAC-Bayesian Generalization Guarantees for Fairness on Stochastic and Deterministic Classifiers", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Classical PAC generalization bounds on the prediction risk of a classifier are insufficient to provide theoretical guarantees on fairness when the goal is to learn models balancing predictive risk and fairness constraints. We propose a PAC-Bayesian framework for deriving generalization bounds for fairness, covering both stochastic and deterministic classifiers. For stochastic classifiers, we derive a fairness bound using standard PAC-Bayes techniques. Whereas for deterministic classifiers, as usual PAC-Bayes arguments do not apply directly, we leverage a recent advance in PAC-Bayes to extend the fairness bound beyond the stochastic setting. Our framework has two advantages: (i) It applies to a broad class of fairness measures that can be expressed as a risk discrepancy, and (ii) it leads to a self-bounding algorithm in which the learning procedure directly optimizes a trade-off between generalization bounds on the prediction risk and on the fairness. We empirically evaluate our framework with three classical fairness measures, demonstrating not only its usefulness but also the tightness of our bounds.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u63a8\u5bfc\u516c\u5e73\u6027\u6cdb\u5316\u754c\u9650\u7684PAC - \u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u968f\u673a\u548c\u786e\u5b9a\u6027\u5206\u7c7b\u5668\uff0c\u6709\u4e24\u5927\u4f18\u52bf\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u7ecf\u5178PAC\u6cdb\u5316\u754c\u9650\u65e0\u6cd5\u4e3a\u5b66\u4e60\u5e73\u8861\u9884\u6d4b\u98ce\u9669\u548c\u516c\u5e73\u6027\u7ea6\u675f\u7684\u6a21\u578b\u63d0\u4f9b\u516c\u5e73\u6027\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5bf9\u968f\u673a\u5206\u7c7b\u5668\uff0c\u7528\u6807\u51c6PAC - \u8d1d\u53f6\u65af\u6280\u672f\u63a8\u5bfc\u516c\u5e73\u6027\u754c\u9650\uff1b\u5bf9\u786e\u5b9a\u6027\u5206\u7c7b\u5668\uff0c\u5229\u7528PAC - \u8d1d\u53f6\u65af\u65b0\u8fdb\u5c55\u6269\u5c55\u516c\u5e73\u6027\u754c\u9650\u3002", "result": "\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd\u53ef\u8868\u793a\u4e3a\u98ce\u9669\u5dee\u5f02\u7684\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u80fd\u4ea7\u751f\u81ea\u754c\u7b97\u6cd5\u3002\u901a\u8fc7\u4e09\u79cd\u7ecf\u5178\u516c\u5e73\u6027\u5ea6\u91cf\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "conclusion": "\u6846\u67b6\u6709\u7528\u4e14\u754c\u9650\u7d27\u5bc6\u3002"}}
{"id": "2602.11187", "pdf": "https://arxiv.org/pdf/2602.11187", "abs": "https://arxiv.org/abs/2602.11187", "authors": ["Yubo Hou", "Furen Zhuang", "Partha Pratim Kundu", "Sezin Ata Kircali", "Jie Wang", "Mihai Dragos Rotaru", "Dutta Rahul", "Ashish James"], "title": "TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.", "AI": {"tldr": "\u63d0\u51faTDPNavigator - Placer\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u53162.5D\u96c6\u6210\u7535\u8def\u82af\u7247\u5c0f\u6838\u5e03\u5c40\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u82af\u7247\u5c0f\u6838\u5e03\u5c40\u65b9\u6cd5\u5728\u5904\u7406\u7ebf\u957f\u548c\u70ed\u7ba1\u7406\u7b49\u51b2\u7a81\u8bbe\u8ba1\u9700\u6c42\u65f6\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faTDPNavigator - Placer\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u51b2\u7a81\u76ee\u6807\u5206\u914d\u7ed9\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u5728\u7edf\u4e00\u5e03\u5c40\u8303\u5f0f\u4e0b\u6309\u4e0d\u540c\u5956\u52b1\u673a\u5236\u548c\u73af\u5883\u7ea6\u675f\u8fd0\u884c\u3002", "result": "TDPNavigator - Placer\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u80fd\u5728\u7ebf\u957f\u548c\u70ed\u6027\u80fd\u95f4\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u6743\u8861\u3002", "conclusion": "TDPNavigator - Placer\u57282.5D\u96c6\u6210\u7535\u8def\u82af\u7247\u5c0f\u6838\u5e03\u5c40\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u53ef\u5e94\u5bf9\u8bbe\u8ba1\u9700\u6c42\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2602.11176", "pdf": "https://arxiv.org/pdf/2602.11176", "abs": "https://arxiv.org/abs/2602.11176", "authors": ["Maral Doctorarastoo", "Katherine A. Flanigan", "Mario Berg\u00e9s", "Christopher McComb"], "title": "Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u63a8\u7406\u65e5\u5e38\u6d3b\u52a8\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u5b9e\u9a8c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u6709\u8f83\u5f3a\u7684\u4eba\u7c7b\u884c\u4e3a\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u53ef\u4f5c\u4e3a\u6709\u524d\u666f\u7684\u65f6\u95f4\u63a8\u7406\u5668\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u9700\u63a2\u7d22\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u7b56\u7565\uff0c\u6574\u5408\u65f6\u95f4\u3001\u7a7a\u95f4\u3001\u884c\u4e3a\u5386\u53f2\u548c\u4eba\u7269\u89d2\u8272\u56db\u79cd\u4e0a\u4e0b\u6587\u6765\u6e90\uff0c\u5e76\u5728CASAS Aruba\u667a\u80fd\u5bb6\u5c45\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6570\u91cf\u7684\u5c11\u6837\u672c\u793a\u4f8b\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u4ea7\u751f\u8fde\u8d2f\u7684\u65e5\u5e38\u6d3b\u52a8\u9884\u6d4b\uff0c\u6dfb\u52a0\u4e00\u4e24\u4e2a\u793a\u4f8b\u53ef\u8fdb\u4e00\u6b65\u5b8c\u5584\u6301\u7eed\u65f6\u95f4\u6821\u51c6\u548c\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u8d85\u8fc7\u51e0\u4e2a\u793a\u4f8b\u540e\u6027\u80fd\u9971\u548c\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u6709\u524d\u666f\u7684\u65f6\u95f4\u63a8\u7406\u5668\uff0c\u80fd\u6355\u6349\u91cd\u590d\u4f8b\u7a0b\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u52a0\u5f3a\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5757\u3002"}}
{"id": "2602.11486", "pdf": "https://arxiv.org/pdf/2602.11486", "abs": "https://arxiv.org/abs/2602.11486", "authors": ["Simina Br\u00e2nzei", "Reed Phillips"], "title": "Dueling over Multiple Pieces of Dessert", "categories": ["cs.GT"], "comment": "52 pages, 6 figures", "summary": "We study the dynamics of repeated fair division between two players, Alice and Bob, where Alice partitions a cake into two subsets and Bob chooses his preferred one over $T$ rounds. Alice aims to minimize her regret relative to the Stackelberg value -- the maximum utility she could achieve if she knew Bob's private valuation.\n  We show that if Alice uses arbitrary measurable partitions, achieving strongly sublinear regret is impossible; she suffers a regret of $\u03a9\\Bigl(\\frac{T}{\\log^2 T}\\Bigr)$ regret even against a myopic Bob. However, when Alice uses at most $k$ cuts, the learning landscape becomes tractable. We analyze Alice's performance based on her knowledge of Bob's strategic sophistication (his regret budget). When Bob's learning rate is public, we establish a hierarchy of polynomial regret bounds determined by $k$ and Bob's regret budget. In contrast, when this learning rate is private, Alice can universally guarantee $O\\Bigl(\\frac{T}{\\log T}\\Bigr)$ regret, but any attempt to secure a polynomial rate $O(T^\u03b2)$ (for $\u03b2< 1$) leaves her vulnerable to incurring strictly linear regret against some Bob.\n  Finally, as a corollary of our online learning dynamics, we characterize the randomized query complexity of finding approximate Stackelberg allocations with a constant number of cuts in the Robertson-Webb model.", "AI": {"tldr": "\u7814\u7a76\u4e24\u4eba\u91cd\u590d\u516c\u5e73\u5206\u914d\u86cb\u7cd5\u52a8\u6001\uff0c\u5206\u6790\u4e0d\u540c\u5206\u533a\u60c5\u51b5\u4e0b Alice \u76f8\u5bf9 Stackelberg \u503c\u7684\u540e\u6094\u503c\uff0c\u8fd8\u5f97\u51fa Robertson - Webb \u6a21\u578b\u4e2d\u8fd1\u4f3c Stackelberg \u5206\u914d\u7684\u968f\u673a\u67e5\u8be2\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76\u91cd\u590d\u516c\u5e73\u5206\u914d\u4e2d\uff0cAlice \u5982\u4f55\u6700\u5c0f\u5316\u5176\u76f8\u5bf9\u4e8e Stackelberg \u503c\u7684\u540e\u6094\u503c\u3002", "method": "\u5206\u6790 Alice \u4f7f\u7528\u4efb\u610f\u53ef\u6d4b\u5206\u533a\u4ee5\u53ca\u6700\u591a k \u6b21\u5207\u5272\u7684\u60c5\u51b5\uff0c\u6839\u636e Bob \u5b66\u4e60\u7387\u662f\u5426\u516c\u5f00\u8fdb\u884c\u4e0d\u540c\u5206\u6790\u3002", "result": "\u4efb\u610f\u53ef\u6d4b\u5206\u533a\u65f6\u8fbe\u5230\u5f3a\u6b21\u7ebf\u6027\u540e\u6094\u503c\u4e0d\u53ef\u80fd\uff1b\u6700\u591a k \u6b21\u5207\u5272\u65f6\uff0cBob \u5b66\u4e60\u7387\u516c\u5f00\u6709\u591a\u9879\u5f0f\u540e\u6094\u754c\u5c42\u7ea7\uff0c\u79c1\u6709\u5219\u80fd\u4fdd\u8bc1\u4e00\u5b9a\u540e\u6094\u503c\u4f46\u96be\u83b7\u591a\u9879\u5f0f\u901f\u7387\u3002", "conclusion": "\u523b\u753b Robertson - Webb \u6a21\u578b\u4e2d\u7528\u56fa\u5b9a\u6b21\u6570\u5207\u5272\u627e\u5230\u8fd1\u4f3c Stackelberg \u5206\u914d\u7684\u968f\u673a\u67e5\u8be2\u590d\u6742\u6027\u3002"}}
{"id": "2602.11605", "pdf": "https://arxiv.org/pdf/2602.11605", "abs": "https://arxiv.org/abs/2602.11605", "authors": ["Yixiao Chen", "Yuan Wang", "Yue Liu", "Qiyao Wang", "Ke Cheng", "Xin Xu", "Juntong Yan", "Shuojin Yang", "Menghao Guo", "Jun Zhang", "Huan Yu", "Jie Jiang"], "title": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation", "categories": ["cs.IR"], "comment": "12 pages, 6figures", "summary": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.", "AI": {"tldr": "\u63d0\u51fa\u6846\u67b6Rec2PM\uff0c\u538b\u7f29\u7528\u6237\u4ea4\u4e92\u5386\u53f2\uff0c\u91c7\u7528\u65b0\u7b56\u7565\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u5b58\u50a8\u6548\u7387\uff0c\u5b9e\u9a8c\u6548\u679c\u597d\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u63a8\u8350\u6a21\u578b\u5728\u7ec8\u8eab\u5e8f\u5217\u6269\u5c55\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u566a\u58f0\u7d2f\u79ef\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Rec2PM\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u5f15\u7528\u6559\u5e08\u5f3a\u5236\u7b56\u7565\u751f\u6210\u53c2\u8003\u8bb0\u5fc6\uff0c\u4ee5\u6807\u8bb0\u5d4c\u5165\u8868\u793a\u8bb0\u5fc6\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRec2PM\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\uff0c\u4e14\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u504f\u597d\u8bb0\u5fc6\u53ef\u4f5c\u4e3a\u53bb\u566a\u4fe1\u606f\u74f6\u9888\uff0c\u6709\u6548\u8fc7\u6ee4\u4ea4\u4e92\u566a\u58f0\uff0c\u6355\u6349\u957f\u671f\u5174\u8da3\u3002"}}
{"id": "2602.11953", "pdf": "https://arxiv.org/pdf/2602.11953", "abs": "https://arxiv.org/abs/2602.11953", "authors": ["Michael A. Bender", "William Kuszmaul", "Elaine Shi", "Rose Silver"], "title": "History-Independent Load Balancing", "categories": ["cs.DS"], "comment": "Appeared in the Proceedings of SODA 2026", "summary": "We give a (strongly) history-independent two-choice balls-and-bins algorithm on $n$ bins that supports both insertions and deletions on a set of up to $m$ balls, while guaranteeing a maximum load of $m / n + O(1)$ with high probability, and achieving an expected recourse of $O(\\log \\log (m/n))$ per operation. To the best of our knowledge, this is the first history-independent solution to achieve nontrivial guarantees of any sort for $m/n \\ge \u03c9(1)$ and is the first fully dynamic solution (history independent or not) to achieve $O(1)$ overload with $o(m/n)$ expected recourse.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\uff08\u5f3a\uff09\u5386\u53f2\u65e0\u5173\u7684\u53cc\u9009\u62e9\u7403\u4e0e\u7bb1\u5b50\u7b97\u6cd5\uff0c\u652f\u6301\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\uff0c\u6709\u8d1f\u8f7d\u548c\u9884\u671f\u8ffd\u7d22\u4fdd\u8bc1\u3002", "motivation": "\u5bfb\u627e\u80fd\u652f\u6301\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\uff0c\u5728\u7403\u4e0e\u7bb1\u5b50\u95ee\u9898\u4e0a\u6709\u8f83\u597d\u8d1f\u8f7d\u548c\u8ffd\u7d22\u6027\u80fd\u7684\u7b97\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\uff08\u5f3a\uff09\u5386\u53f2\u65e0\u5173\u7684\u53cc\u9009\u62e9\u7403\u4e0e\u7bb1\u5b50\u7b97\u6cd5\u3002", "result": "\u4fdd\u8bc1\u6700\u5927\u8d1f\u8f7d\u4e3a $m / n + O(1)$ \u7684\u9ad8\u6982\u7387\uff0c\u6bcf\u6b21\u64cd\u4f5c\u7684\u9884\u671f\u8ffd\u7d22\u4e3a $O(\\log \\log (m/n))$\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728 $m/n \\ge \\omega(1)$ \u65f6\u5b9e\u73b0\u975e\u5e73\u51e1\u4fdd\u8bc1\uff0c\u4e14\u9996\u6b21\u5b9e\u73b0 $O(1)$ \u8fc7\u8f7d\u548c $o(m/n)$ \u9884\u671f\u8ffd\u7d22\u7684\u5168\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11998", "pdf": "https://arxiv.org/pdf/2602.11998", "abs": "https://arxiv.org/abs/2602.11998", "authors": ["Ramakant kumar"], "title": "An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization", "categories": ["cs.DC", "cs.NI"], "comment": null, "summary": "Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAUC - RAC\u673a\u5236\u7528\u4e8e\u7269\u8054\u7f51\u8bbe\u5907\u591a\u672c\u5730\u670d\u52a1\u5668\u95f4\u8ba1\u7b97\u4efb\u52a1\u9ad8\u6548\u5378\u8f7d\uff0c\u5229\u7528Docker swarm\u548c\u5bb9\u5668\u5316\uff0c\u901a\u8fc7\u62cd\u5356\u673a\u5236\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u6539\u5584\u5378\u8f7d\u548c\u8ba1\u7b97\u670d\u52a1\u3002", "motivation": "\u5728\u57fa\u4e8e\u4e91\u7684\u7269\u8054\u7f51\u5bb9\u5668\u5316\u4e2d\uff0c\u8d44\u6e90\u7ba1\u7406\u548c\u6210\u672c\u4f18\u5316\u5bf9\u4efb\u52a1\u6210\u529f\u6267\u884c\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAUC - RAC\u673a\u5236\uff0c\u5229\u7528Docker swarm\u8fde\u63a5\u672c\u5730\u670d\u52a1\u5668\uff0c\u91c7\u7528Manager Node\uff08MN\uff09\u548cWorker Nodes\uff08WNs\uff09\u5f62\u5f0f\uff0c\u901a\u8fc7Docker\u5bb9\u5668\u5316\u5e76\u884c\u6267\u884c\u4efb\u52a1\uff0cMN\u63a5\u6536\u4efb\u52a1\u540e\u8ba9WNs\u53c2\u4e0e\u62cd\u5356\u5f0f\u7ade\u4ef7\u8fc7\u7a0b\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u73b0\u672c\u5730\u670d\u52a1\u5668\u95f4\u7684\u5408\u4f5c\uff0c\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5378\u8f7d\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u3002", "conclusion": "AUC - RAC\u673a\u5236\u80fd\u9ad8\u6548\u5378\u8f7d\u8ba1\u7b97\u4efb\u52a1\uff0c\u4f18\u5316\u4efb\u52a1\u5728\u591a\u7cfb\u7edf\u4e2d\u7684\u5206\u914d\uff0c\u5bf9\u7269\u8054\u7f51\u8bbe\u5907\u8ba1\u7b97\u670d\u52a1\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2602.11435", "pdf": "https://arxiv.org/pdf/2602.11435", "abs": "https://arxiv.org/abs/2602.11435", "authors": ["Haolin Li", "Michael Coblenz"], "title": "A Grounded Theory of Debugging in Professional Software Engineering Practice", "categories": ["cs.SE"], "comment": "Accepted by FSE'26", "summary": "Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.", "AI": {"tldr": "\u672c\u6587\u91c7\u7528\u624e\u6839\u7406\u8bba\u65b9\u6cd5\u7814\u7a76\u4e13\u4e1a\u5f00\u53d1\u8005\u8c03\u8bd5\u7b56\u7565\uff0c\u63d0\u51fa\u8c03\u8bd5\u662f\u7ed3\u6784\u5316\u8fed\u4ee3\u8bca\u65ad\u8fc7\u7a0b\u7684\u7406\u8bba\uff0c\u5bf9\u5de5\u5177\u8bbe\u8ba1\u548c\u6559\u80b2\u6709\u542f\u793a\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e13\u4e1a\u5f00\u53d1\u8005\u5728\u5927\u578b\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u8c03\u8bd5\u63a8\u7406\u65b9\u5f0f\u7684\u7406\u8bba\u89e3\u91ca\uff0c\u65e8\u5728\u586b\u8865\u6b64\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u624e\u6839\u7406\u8bba\u65b9\u6cd5\uff0c\u89c2\u5bdf7\u540d\u4e13\u4e1a\u5f00\u53d1\u8005\u548c5\u540d\u4e13\u4e1a\u76f4\u64ad\u7f16\u7801\u8005\u5728\u81ea\u6709\u4ee3\u7801\u5e93\u4e2d\u768417\u4e2a\u8c03\u8bd5\u4efb\u52a1\u3002", "result": "\u8c03\u8bd5\u662f\u7ed3\u6784\u5316\u8fed\u4ee3\u8bca\u65ad\u8fc7\u7a0b\uff0c\u5f00\u53d1\u8005\u901a\u8fc7\u5bfc\u822a\u548c\u6267\u884c\u7b56\u7565\u4ea4\u66ff\u6536\u96c6\u4fe1\u606f\uff0c\u8fd0\u7528\u524d\u540e\u8ffd\u8e2a\u63a8\u7406\u6a21\u5f0f\u5e76\u4f9d\u60c5\u51b5\u8c03\u6574\uff0c\u8fd8\u6536\u96c6\u5916\u90e8\u8d44\u6e90\uff0c\u7ecf\u9a8c\u52a9\u5176\u6784\u5efa\u5fc3\u667a\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e13\u4e1a\u8c03\u8bd5\u7684\u624e\u6839\u7406\u8bba\uff0c\u4f53\u73b0\u8be5\u5b9e\u8df5\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u7ef4\u5ea6\uff0c\u5bf9\u5de5\u5177\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u6709\u610f\u4e49\u3002"}}
{"id": "2602.11687", "pdf": "https://arxiv.org/pdf/2602.11687", "abs": "https://arxiv.org/abs/2602.11687", "authors": ["Atilla Aras"], "title": "Exact Value Solution to the Equity Premium Puzzle", "categories": ["q-fin.GN", "econ.GN"], "comment": "16 pages, 2 tables and appendix", "summary": "The aim of this article is to provide the solution to the equity premium puzzle without using calibrated values. Calibrated values of subjective time discount factor were used in the prior derived models because 4 variables were determined from 3 different equations. Furthermore, calculated values and risk behavior determination of prior models were compatible with empirical literature. 4 unknown variables are now calculated from 4 different equations in the new derived model in this article. Subjective time discount factor and coefficient of relative risk aversion are found 0.9581 and 1.0319, respectively from the system of equations which are compatible with empirical studies. Micro and macro studies about CRRA value affirm each other for the first time in the literature. Furthermore, equity and risk-free asset investors are pinned down to be insufficient risk-loving, which can be considered a type of risk-averse behavior. Hence it can be said that calculated values and risk attitude determination align with empirical literature. This shows that derived model is valid and make CCAPM work under the same assumptions with those of prior derived models.", "AI": {"tldr": "\u672c\u6587\u5728\u4e0d\u4f7f\u7528\u6821\u51c6\u503c\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u80a1\u6743\u6ea2\u4ef7\u4e4b\u8c1c\uff0c\u65b0\u6a21\u578b\u4ece4\u4e2a\u65b9\u7a0b\u8ba1\u7b974\u4e2a\u672a\u77e5\u53d8\u91cf\uff0c\u6240\u5f97\u7ed3\u679c\u4e0e\u5b9e\u8bc1\u7814\u7a76\u76f8\u7b26\uff0c\u8bc1\u660e\u6a21\u578b\u6709\u6548\u3002", "motivation": "\u5728\u4e0d\u4f7f\u7528\u6821\u51c6\u503c\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u80a1\u6743\u6ea2\u4ef7\u4e4b\u8c1c\uff0c\u6539\u8fdb\u5148\u524d\u4f7f\u7528\u6821\u51c6\u503c\u7684\u6a21\u578b\u3002", "method": "\u6784\u5efa\u65b0\u6a21\u578b\uff0c\u4ece4\u4e2a\u4e0d\u540c\u65b9\u7a0b\u8ba1\u7b974\u4e2a\u672a\u77e5\u53d8\u91cf\uff0c\u6c42\u89e3\u65b9\u7a0b\u7ec4\u5f97\u5230\u4e3b\u89c2\u65f6\u95f4\u8d34\u73b0\u56e0\u5b50\u548c\u76f8\u5bf9\u98ce\u9669\u538c\u6076\u7cfb\u6570\u3002", "result": "\u4e3b\u89c2\u65f6\u95f4\u8d34\u73b0\u56e0\u5b50\u4e3a0.9581\uff0c\u76f8\u5bf9\u98ce\u9669\u538c\u6076\u7cfb\u6570\u4e3a1.0319\uff0c\u4e0e\u5b9e\u8bc1\u7814\u7a76\u517c\u5bb9\uff0c\u5fae\u89c2\u548c\u5b8f\u89c2\u5173\u4e8eCRRA\u503c\u7684\u7814\u7a76\u9996\u6b21\u76f8\u4e92\u5370\u8bc1\uff0c\u6295\u8d44\u8005\u88ab\u786e\u5b9a\u4e3a\u98ce\u9669\u89c4\u907f\u884c\u4e3a\u3002", "conclusion": "\u65b0\u63a8\u5bfc\u7684\u6a21\u578b\u6709\u6548\uff0c\u80fd\u4f7fCCAPM\u5728\u4e0e\u5148\u524d\u6a21\u578b\u76f8\u540c\u5047\u8bbe\u4e0b\u8fd0\u884c\u3002"}}
{"id": "2602.11318", "pdf": "https://arxiv.org/pdf/2602.11318", "abs": "https://arxiv.org/abs/2602.11318", "authors": ["Sheza Munir", "Benjamin Mah", "Krisha Kalsi", "Shivani Kapania", "Julian Posada", "Edith Law", "Ding Wang", "Syed Ishtiaque Ahmed"], "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the \"noisy sensor\" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular \"right\" answer to mapping the diversity of human experience.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u673a\u5668\u5b66\u4e60\u2018\u5730\u9762\u771f\u503c\u2019\u8303\u5f0f\u5b58\u5728\u5b9e\u8bc1\u4e3b\u4e49\u8c2c\u8bef\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5206\u6790\u2018\u5171\u8bc6\u9677\u9631\u2019\u673a\u5236\uff0c\u63ed\u793a\u7cfb\u7edf\u95ee\u9898\uff0c\u6279\u5224\u2018\u566a\u58f0\u4f20\u611f\u5668\u2019\u8c2c\u8bef\uff0c\u63d0\u51fa\u591a\u5143\u6ce8\u91ca\u57fa\u7840\u8bbe\u65bd\u8def\u7ebf\u56fe\u3002", "motivation": "\u6307\u51fa\u673a\u5668\u5b66\u4e60\u4e2d\u2018\u5730\u9762\u771f\u503c\u2019\u8303\u5f0f\u5c06\u4eba\u7c7b\u5206\u6b67\u89c6\u4e3a\u6280\u672f\u566a\u58f0\u7684\u5b9e\u8bc1\u4e3b\u4e49\u8c2c\u8bef\uff0c\u5206\u6790\u6570\u636e\u6807\u6ce8\u5b9e\u8df5\u4e2d\u5bfc\u81f4\u2018\u5171\u8bc6\u9677\u9631\u2019\u7684\u673a\u5236\u3002", "method": "\u8fdb\u884c\u7cfb\u7edf\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u5bf92020 - 2025\u5e74\u4e03\u4e2a\u9876\u7ea7\u4f1a\u8bae\u7684\u7814\u7a76\u8fdb\u884c\u5206\u6790\uff0c\u91c7\u7528\u5206\u5c42\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001\u4eba\u5de5\u7b5b\u9009\u548c\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u7b49\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4f4d\u7f6e\u53ef\u8bfb\u6027\u7684\u7cfb\u7edf\u5931\u8d25\u4ee5\u53ca\u5411\u4eba\u7c7b\u9a8c\u8bc1\u8005\u6a21\u578b\u7684\u67b6\u6784\u8f6c\u53d8\u5e26\u6765\u951a\u5b9a\u504f\u5dee\u548c\u53bb\u9664\u4eba\u7c7b\u58f0\u97f3\uff0c\u5730\u7406\u9738\u6743\u5f3a\u52a0\u897f\u65b9\u89c4\u8303\uff0c\u7edf\u8ba1\u6a21\u578b\u8bef\u5224\u6587\u5316\u591a\u5143\u4e3b\u4e49\u4e3a\u968f\u673a\u8bef\u5dee\u3002", "conclusion": "\u6279\u5224\u2018\u566a\u58f0\u4f20\u611f\u5668\u2019\u8c2c\u8bef\uff0c\u4e3b\u5f20\u5c06\u5206\u6b67\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u4fe1\u53f7\uff0c\u63d0\u51fa\u6784\u5efa\u591a\u5143\u6ce8\u91ca\u57fa\u7840\u8bbe\u65bd\u7684\u8def\u7ebf\u56fe\uff0c\u4ece\u5bfb\u627e\u5355\u4e00\u6b63\u786e\u7b54\u6848\u8f6c\u5411\u6620\u5c04\u4eba\u7c7b\u7ecf\u9a8c\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2602.11760", "pdf": "https://arxiv.org/pdf/2602.11760", "abs": "https://arxiv.org/abs/2602.11760", "authors": ["Joseph Paillard", "Angel Reyero Lobo", "Denis A. Engemann", "Bertrand Thirion"], "title": "Aggregate Models, Not Explanations: Improving Feature Importance Estimation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Feature-importance methods show promise in transforming machine learning models from predictive engines into tools for scientific discovery. However, due to data sampling and algorithmic stochasticity, expressive models can be unstable, leading to inaccurate variable importance estimates and undermining their utility in critical biomedical applications. Although ensembling offers a solution, deciding whether to explain a single ensemble model or aggregate individual model explanations is difficult due to the nonlinearity of importance measures and remains largely understudied. Our theoretical analysis, developed under assumptions accommodating complex state-of-the-art ML models, reveals that this choice is primarily driven by the model's excess risk. In contrast to prior literature, we show that ensembling at the model level provides more accurate variable-importance estimates, particularly for expressive models, by reducing this leading error term. We validate these findings on classical benchmarks and a large-scale proteomic study from the UK Biobank.", "AI": {"tldr": "\u7279\u5f81\u91cd\u8981\u6027\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u79d1\u5b66\u53d1\u73b0\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u6a21\u578b\u4e0d\u7a33\u5b9a\u5f71\u54cd\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u7814\u7a76\u8868\u660e\u6a21\u578b\u5c42\u9762\u96c6\u6210\u5bf9\u8868\u8fbe\u6027\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u53d8\u91cf\u91cd\u8981\u6027\u4f30\u8ba1\u5e76\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u8868\u8fbe\u6027\u6a21\u578b\u4e0d\u7a33\u5b9a\u5bfc\u81f4\u53d8\u91cf\u91cd\u8981\u6027\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u4ee5\u53ca\u96c6\u6210\u65f6\u9009\u62e9\u89e3\u91ca\u5355\u4e2a\u96c6\u6210\u6a21\u578b\u8fd8\u662f\u805a\u5408\u4e2a\u4f53\u6a21\u578b\u89e3\u91ca\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5728\u8003\u8651\u590d\u6742\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5047\u8bbe\u4e0b\u5c55\u5f00\uff0c\u7814\u7a76\u9009\u62e9\u4e3b\u8981\u53d7\u6a21\u578b\u8d85\u989d\u98ce\u9669\u7684\u9a71\u52a8\u60c5\u51b5\u3002", "result": "\u6a21\u578b\u5c42\u9762\u96c6\u6210\u80fd\u51cf\u5c11\u4e3b\u8981\u8bef\u5dee\u9879\uff0c\u4e3a\u8868\u8fbe\u6027\u6a21\u578b\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u53d8\u91cf\u91cd\u8981\u6027\u4f30\u8ba1\u3002", "conclusion": "\u6a21\u578b\u5c42\u9762\u96c6\u6210\u5bf9\u8868\u8fbe\u6027\u6a21\u578b\u7684\u53d8\u91cf\u91cd\u8981\u6027\u4f30\u8ba1\u66f4\u51c6\u786e\uff0c\u5728\u7ecf\u5178\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u86cb\u767d\u8d28\u7ec4\u5b66\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2602.11190", "pdf": "https://arxiv.org/pdf/2602.11190", "abs": "https://arxiv.org/abs/2602.11190", "authors": ["Fan Zhang", "Shiming Fan", "Hua Wang"], "title": "Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have found that existing methods typically employ a strategy of embedding each time step as an independent token. This paradigm introduces a fundamental information bottleneck when processing long sequences, the root cause of which is that independent token embedding destroys a crucial structure within the sequence - what we term as multi-offset temporal correlation. This refers to the fine-grained dependencies embedded within the sequence that span across different time steps, which is especially prevalent in regular Web data. To fundamentally address this issue, we propose a new perspective on time series embedding. We provide an upper bound on the approximate reconstruction performance of token embedding, which guides our design of a concise yet effective Multi-Offset Time Embedding method to mitigate the performance degradation caused by standard token embedding. Furthermore, our MOTE can be integrated into various existing models and serve as a universal building block. Based on this paradigm, we further design a novel forecasting architecture named Time-TK. This architecture first utilizes a Multi-Offset Interactive KAN to learn and represent specific temporal patterns among multiple offset sub-sequences. Subsequently, it employs an efficient Multi-Offset Temporal Interaction mechanism to effectively capture the complex dependencies between these sub-sequences, achieving global information integration. Extensive experiments on 14 real-world benchmark datasets, covering domains such as traffic flow and BTC/USDT throughput, demonstrate that Time-TK significantly outperforms all baseline models, achieving state-of-the-art forecasting accuracy.", "AI": {"tldr": "\u6587\u7ae0\u6307\u51fa\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u74f6\u9888\uff0c\u63d0\u51fa\u65b0\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u89c6\u89d2\uff0c\u8bbe\u8ba1MOTE\u65b9\u6cd5\u548cTime - TK\u67b6\u6784\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u9884\u6d4b\u7cbe\u5ea6\u8fbe\u6700\u4f18\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\uff0c\u72ec\u7acb\u4ee4\u724c\u5d4c\u5165\u7834\u574f\u591a\u504f\u79fb\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u4ea7\u751f\u4fe1\u606f\u74f6\u9888\uff0c\u9700\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u89c6\u89d2\uff0c\u7ed9\u51fa\u4ee4\u724c\u5d4c\u5165\u8fd1\u4f3c\u91cd\u5efa\u6027\u80fd\u4e0a\u9650\uff0c\u8bbe\u8ba1MOTE\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1Time - TK\u67b6\u6784\uff0c\u5305\u62ec\u591a\u504f\u79fb\u4ea4\u4e92\u5f0fKAN\u5b66\u4e60\u65f6\u95f4\u6a21\u5f0f\u548c\u591a\u504f\u79fb\u65f6\u95f4\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u4fe1\u606f\u96c6\u6210\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cTime - TK\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Time - TK\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2602.11378", "pdf": "https://arxiv.org/pdf/2602.11378", "abs": "https://arxiv.org/abs/2602.11378", "authors": ["Amirpasha Hedayat", "Alberto Padovan", "Karthik Duraisamy"], "title": "Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges", "categories": ["cs.LG", "cs.CE", "math.NA", "physics.comp-ph"], "comment": null, "summary": "Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u81ea\u9002\u5e94\u975e\u4fb5\u5165\u5f0f\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u516c\u5f0f\uff0c\u5206\u6790\u5176\u5728\u77ac\u6001\u6270\u52a8\u9a71\u52a8\u8154\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f3a\u8c03ROM\u9884\u6d4b\u5e94\u8003\u8651\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6295\u5f71\u7684\u9759\u6001ROM\u5728\u7cfb\u7edf\u79bb\u5f00\u8bad\u7ec3\u6d41\u5f62\u65f6\u5b9e\u7528\u6027\u53d7\u9650\uff0c\u9700\u7814\u7a76\u81ea\u9002\u5e94\u975e\u4fb5\u5165\u5f0fROM\u3002", "method": "\u57fa\u4e8e\u9759\u6001\u975e\u4fb5\u5165\u5f0fROM\u7684\u601d\u60f3\uff0c\u63d0\u51faAdaptive OpInf\u3001Adaptive NiTROM\u548c\u6df7\u5408\u516c\u5f0f\u4e09\u79cd\u81ea\u9002\u5e94ROM\u516c\u5f0f\uff0c\u5e76\u63cf\u8ff0\u76f8\u5173\u7a97\u53e3\u548c\u8ba1\u7b97\u9884\u7b97\uff0c\u5206\u6790\u6210\u672c\u7f29\u653e\u3002", "result": "\u5728\u77ac\u6001\u6270\u52a8\u9a71\u52a8\u8154\u6d41\u4e2d\uff0c\u9759\u6001ROM\u9884\u6d4b\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\u4f1a\u6f02\u79fb\u6216\u5931\u7a33\uff0c\u800c\u81ea\u9002\u5e94ROM\u5404\u6709\u4f18\u52bf\uff0c\u5982Adaptive OpInf\u80fd\u6291\u5236\u5e45\u5ea6\u6f02\u79fb\uff0cAdaptive NiTROM\u80fd\u5b9e\u73b0\u8fd1\u7cbe\u786e\u80fd\u91cf\u8ddf\u8e2a\uff0c\u6df7\u5408\u516c\u5f0f\u5728\u5de5\u51b5\u53d8\u5316\u548c\u79bb\u7ebf\u6570\u636e\u5c11\u7684\u60c5\u51b5\u4e0b\u66f4\u53ef\u9760\u3002", "conclusion": "ROM\u7684\u9884\u6d4b\u58f0\u660e\u5e94\u8003\u8651\u6210\u672c\uff0c\u672c\u6587\u4e3a\u6784\u5efa\u81ea\u6821\u6b63\u3001\u975e\u4fb5\u5165\u5f0fROM\u63d0\u4f9b\u4e86\u5b9e\u7528\u6a21\u677f\u3002"}}
{"id": "2602.11691", "pdf": "https://arxiv.org/pdf/2602.11691", "abs": "https://arxiv.org/abs/2602.11691", "authors": ["Yiding Feng", "Mengfan Ma", "Bo Peng", "Zongqi Wan"], "title": "Searching for Optimal Prices in Two-Sided Markets", "categories": ["cs.GT"], "comment": null, "summary": "We investigate online pricing in two-sided markets where a platform repeatedly posts prices based on binary accept/reject feedback to maximize gains-from-trade (GFT) or profit. We characterize the regret achievable across three mechanism classes: Single-Price, Two-Price, and Segmented-Price.\n  For profit maximization, we design an algorithm using Two-Price Mechanisms that achieves $O(n^2 \\log\\log T)$ regret, where $n$ is the number of traders.\n  For GFT maximization, the optimal regret depends critically on both market size and mechanism expressiveness. Constant regret is achievable in bilateral trade, but this guarantee breaks down as the market grows: even in a one-seller, two-buyer market, any algorithm using Single-Price Mechanisms suffers regret at least $\u03a9\\!\\big(\\frac{\\log\\log T}{\\log\\log\\log\\log T}\\big)$, and we provide a nearly matching $O(\\log\\log T)$ upper bound for general one-to-many markets. In full many-to-many markets, we prove that Two-Price Mechanisms inevitably incur linear regret $\u03a9(T)$ due to a \\emph{mismatch phenomenon}, wherein inefficient pairings prevent near-optimal trade. To overcome this barrier, we introduce \\emph{Segmented-Price Mechanisms}, which partition traders into groups and assign distinct prices per group. Using this richer mechanism, we design an algorithm achieving $O(n^2 \\log\\log T + n^3)$ regret for GFT maximization.\n  Finally, we extend our results to the contextual setting, where traders' costs and values depend linearly on observed $d$-dimensional features that vary across rounds, obtaining regret bounds of $O(n^2 d \\log\\log T + n^2 d \\log d)$ for profit and $O(n^2 d^2 \\log T)$ for GFT. Our work delineates sharp boundaries between learnable and unlearnable regimes in two-sided dynamic pricing and demonstrates how modest increases in pricing expressiveness can circumvent fundamental hardness barriers.", "AI": {"tldr": "\u7814\u7a76\u53cc\u8fb9\u5e02\u573a\u5728\u7ebf\u5b9a\u4ef7\uff0c\u5206\u6790\u4e0d\u540c\u673a\u5236\u4e0b\u6536\u76ca\u548c\u8d38\u6613\u589e\u76ca\u6700\u5927\u5316\u7684\u9057\u61be\u503c\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u5e76\u7ed9\u51fa\u754c\u9650\uff0c\u6269\u5c55\u5230\u4e0a\u4e0b\u6587\u573a\u666f\u3002", "motivation": "\u5728\u53cc\u8fb9\u5e02\u573a\u4e2d\uff0c\u5e73\u53f0\u4f9d\u636e\u4e8c\u5143\u53cd\u9988\u53cd\u590d\u5b9a\u4ef7\u4ee5\u6700\u5927\u5316\u8d38\u6613\u589e\u76ca\u6216\u5229\u6da6\uff0c\u9700\u7814\u7a76\u4e0d\u540c\u673a\u5236\u4e0b\u53ef\u5b9e\u73b0\u7684\u9057\u61be\u503c\u3002", "method": "\u9488\u5bf9\u6536\u76ca\u548c\u8d38\u6613\u589e\u76ca\u6700\u5927\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u4f7f\u7528\u53cc\u4ef7\u683c\u673a\u5236\u7684\u7b97\u6cd5\uff1b\u5f15\u5165\u5206\u6bb5\u4ef7\u683c\u673a\u5236\uff1b\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u4e0a\u4e0b\u6587\u573a\u666f\u3002", "result": "\u8bbe\u8ba1\u51fa\u6536\u76ca\u6700\u5927\u5316\u7684\u53cc\u4ef7\u683c\u673a\u5236\u7b97\u6cd5\uff0c\u5f97\u5230\u4e0d\u540c\u5e02\u573a\u4e0b\u8d38\u6613\u589e\u76ca\u6700\u5927\u5316\u7684\u9057\u61be\u503c\u754c\u9650\uff0c\u6269\u5c55\u573a\u666f\u4e2d\u4e5f\u5f97\u5230\u76f8\u5e94\u9057\u61be\u503c\u754c\u9650\u3002", "conclusion": "\u660e\u786e\u4e86\u53cc\u8fb9\u52a8\u6001\u5b9a\u4ef7\u4e2d\u53ef\u5b66\u4e60\u548c\u4e0d\u53ef\u5b66\u4e60\u72b6\u6001\u7684\u754c\u9650\uff0c\u9002\u5ea6\u589e\u52a0\u5b9a\u4ef7\u8868\u8fbe\u80fd\u529b\u53ef\u7a81\u7834\u57fa\u672c\u56f0\u96be\u969c\u788d\u3002"}}
{"id": "2602.11622", "pdf": "https://arxiv.org/pdf/2602.11622", "abs": "https://arxiv.org/abs/2602.11622", "authors": ["Haiyang Jiang", "Tong Chen", "Xinyi Gao", "Guansong Pang", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts", "categories": ["cs.IR"], "comment": null, "summary": "Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684MoE\u6846\u67b6EvoFG\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5355GNN\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u56e0\u56fe\u7684\u5f02\u8d28\u6027\u8868\u8fbe\u4e0d\u8db3\uff0cMoE\u67b6\u6784\u53d7\u5206\u5e03\u504f\u79fb\u9650\u5236\uff0c\u8def\u7531\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8fdb\u5316\u7279\u5f81\u751f\u6210\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5668\u548cShapley\u5f15\u5bfc\u8bc4\u4f30\u6784\u5efa\u548c\u9009\u62e9\u7279\u5f81\uff1b\u8bbe\u8ba1\u5177\u6709\u4e0d\u53d8\u5b66\u4e60\u76ee\u6807\u7684\u8bb0\u5fc6\u589e\u5f3a\u8def\u7531\u5668\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cEvoFG\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "EvoFG\u5728\u89e3\u51b3\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u8def\u7531\u6311\u6218\u65b9\u9762\u6709\u6548\uff0c\u80fd\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.12126", "pdf": "https://arxiv.org/pdf/2602.12126", "abs": "https://arxiv.org/abs/2602.12126", "authors": ["Daniele Carnevale", "Gianlorenzo D'Angelo"], "title": "Optimizing Distances for Multi-Broadcast in Temporal Graphs", "categories": ["cs.DS"], "comment": null, "summary": "Temporal graphs represent networks in which connections change over time, with edges available only at specific moments. Motivated by applications in logistics, multi-agent information spreading, and wireless networks, we introduce the D-Temporal Multi-Broadcast (D-TMB) problem, which asks for scheduling the availability of edges so that a predetermined subset of sources reach all other vertices while optimizing the worst-case temporal distance D from any source. We show that D-TMB generalizes ReachFast (arXiv:2112.08797). We then characterize the computational complexity and approximability of D-TMB under six definitions of temporal distance D, namely Earliest-Arrival (EA), Latest-Departure (LD), Fastest-Time (FT), Shortest-Traveling (ST), Minimum-Hop (MH), and Minimum-Waiting (MW). For a single source, we show that D-TMB can be solved in polynomial time for EA and LD, while for the other temporal distances it is NP-hard and hard to approximate within a factor that depends on the adopted distance function. We give approximation algorithms for FT and MW. For multiple sources, if feasibility is not assumed a priori, the problem is inapproximable within any factor unless P = NP, even with just two sources. We complement this negative result by identifying structural conditions that guarantee tractability for EA and LD for any number of sources.", "AI": {"tldr": "\u5f15\u5165D - Temporal Multi - Broadcast (D - TMB) \u95ee\u9898\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u65f6\u95f4\u8ddd\u79bb\u5b9a\u4e49\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53ef\u8fd1\u4f3c\u6027\u3002", "motivation": "\u53d7\u7269\u6d41\u3001\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u4f20\u64ad\u548c\u65e0\u7ebf\u7f51\u7edc\u5e94\u7528\u7684\u542f\u53d1\uff0c\u5f15\u5165D - TMB\u95ee\u9898\u3002", "method": "\u5206\u6790D - TMB\u5728\u516d\u79cd\u65f6\u95f4\u8ddd\u79bb\u5b9a\u4e49\uff08EA\u3001LD\u3001FT\u3001ST\u3001MH\u548cMW\uff09\u4e0b\u7684\u60c5\u51b5\uff0c\u7ed9\u51fa\u5355\u6e90\u548c\u591a\u6e90\u7684\u590d\u6742\u5ea6\u5206\u6790\u53ca\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u5355\u6e90\u4e0b\uff0cEA\u548cLD\u53ef\u591a\u9879\u5f0f\u65f6\u95f4\u6c42\u89e3\uff0c\u5176\u4ed6\u662fNP\u96be\u4e14\u8fd1\u4f3c\u56f0\u96be\uff1b\u591a\u6e90\u4e0b\uff0c\u82e5\u65e0\u5148\u9a8c\u53ef\u884c\u6027\u5047\u8bbe\uff0c\u95ee\u9898\u4e0d\u53ef\u8fd1\u4f3c\uff0c\u8fd8\u627e\u51fa\u4e86EA\u548cLD\u53ef\u5904\u7406\u7684\u7ed3\u6784\u6761\u4ef6\u3002", "conclusion": "\u5168\u9762\u523b\u753b\u4e86D - TMB\u95ee\u9898\u5728\u4e0d\u540c\u573a\u666f\u548c\u8ddd\u79bb\u5b9a\u4e49\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53ef\u8fd1\u4f3c\u6027\u3002"}}
{"id": "2602.12070", "pdf": "https://arxiv.org/pdf/2602.12070", "abs": "https://arxiv.org/abs/2602.12070", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Tsvi Kopelowitz", "Seth Pettie", "Ben Plosk"], "title": "Contention Resolution, With and Without a Global Clock", "categories": ["cs.DC", "math.PR"], "comment": null, "summary": "In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\\left(\\left(n\\log\\log n\\log^{(3)} n\\log^{(4)} n\\cdots \\log^{(\\log^* n)} n\\right)\\cdot 2^{\\log^* n}\\right) \\le n(\\log\\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $\u0398(n \\log n/\\log\\log n)$ whereas the With-High-Probability latency is $\u0398(n\\log^2 n/\\log\\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\\log^2 n/(\\log\\log n)^2)$ and With-High-Probability latency $n\\log^{O(1)} n$ simultaneously.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e89\u7528\u89e3\u51b3\u95ee\u9898\uff0c\u8003\u8651\u5168\u5c40\u65f6\u949f\u5047\u8bbe\uff0c\u7ed9\u51fa\u65b0\u534f\u8bae\uff0c\u5206\u6790\u590d\u6742\u5ea6\u5dee\u8ddd\u53ca\u8bc1\u660e\u540c\u65f6\u4f18\u5316\u4e24\u79cd\u6307\u6807\u7684\u4e0d\u53ef\u80fd\u6027\u3002", "motivation": "\u4ee5\u5f80\u4e89\u7528\u89e3\u51b3\u95ee\u9898\u7814\u7a76\u591a\u5047\u8bbe\u65e0\u5168\u5c40\u65f6\u949f\uff0c\u800c\u5168\u5c40\u65f6\u949f\u5047\u8bbe\u5728\u6280\u672f\u4e0a\u73b0\u5b9e\u4e14\u7b97\u6cd5\u4e0a\u6709\u8da3\uff0c\u80fd\u4e30\u5bcc\u95ee\u9898\u548c\u5f15\u5165\u65b0\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u4e89\u7528\u89e3\u51b3\u534f\u8bae\uff0c\u5bf9\u4e0d\u540c\u5047\u8bbe\u4e0b\u7684\u968f\u673a\u4e89\u7528\u89e3\u51b3\u534f\u8bae\u8fdb\u884c\u5206\u6790\u3002", "result": "1. \u65b0\u534f\u8bae\u4fdd\u8bc1\u7279\u5b9a\u671f\u671b\u548c\u9ad8\u6982\u7387\u4e0b\u7684\u5ef6\u8fdf\uff1b2. \u8bc1\u660e\u65e0\u8bb0\u5fc6\u534f\u8bae\u4e24\u79cd\u76ee\u6807\u95f4\u5b58\u5728log n\u590d\u6742\u5ea6\u5dee\u8ddd\uff1b3. \u8bc1\u660e\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u4e24\u79cd\u6307\u6807\u3002", "conclusion": "\u5168\u5c40\u65f6\u949f\u5047\u8bbe\u4e0b\u4e89\u7528\u89e3\u51b3\u95ee\u9898\u6709\u65b0\u7684\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u4e14\u65e0\u6cd5\u540c\u65f6\u5728\u4e24\u79cd\u6307\u6807\u4e0b\u8fbe\u5230\u6700\u4f18\u3002"}}
{"id": "2602.11447", "pdf": "https://arxiv.org/pdf/2602.11447", "abs": "https://arxiv.org/abs/2602.11447", "authors": ["Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Marco Gerosa", "Anita Sarma"], "title": "Addressing OSS Community Managers' Challenges in Contributor Retention", "categories": ["cs.SE"], "comment": null, "summary": "Open-source software (OSS) community managers face significant challenges in retaining contributors, as they must monitor activity and engagement while navigating complex dynamics of collaboration. Current tools designed for managing contributor retention (e.g., dashboards) fall short by providing retrospective rather than predictive insights to identify potential disengagement early. Without understanding how to anticipate and prevent disengagement, new solutions risk burdening community managers rather than supporting retention management. Following the Design Science Research paradigm, we employed a mixed-methods approach for problem identification and solution design to address contributor retention. To identify the challenges hindering retention management in OSS, we conducted semi-structured interviews, a multi-vocal literature review, and community surveys. Then through an iterative build-evaluate cycle, we developed and refined strategies for diagnosing retention risks and informing engagement efforts. We operationalized these strategies into a web-based prototype, incorporating feedback from 100+ OSS practitioners, and conducted an in situ evaluation across two OSS communities. Our study offers (1) empirical insights into the challenges of contributor retention management in OSS, (2) actionable strategies that support OSS community managers' retention efforts, and (3) a practical framework for future research in developing or validating theories about OSS sustainability.", "AI": {"tldr": "\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u7ba1\u7406\u8005\u5728\u4fdd\u7559\u8d21\u732e\u8005\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u5de5\u5177\u4e0d\u8db3\uff0c\u672c\u6587\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3001\u5f00\u53d1\u539f\u578b\u5e76\u8bc4\u4f30\uff0c\u63d0\u4f9b\u89c1\u89e3\u3001\u7b56\u7565\u548c\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u7ba1\u7406\u8d21\u732e\u8005\u4fdd\u7559\u7684\u5de5\u5177\u53ea\u80fd\u63d0\u4f9b\u56de\u987e\u6027\u89c1\u89e3\uff0c\u65e0\u6cd5\u63d0\u524d\u8bc6\u522b\u6f5c\u5728\u7684\u9000\u51fa\u60c5\u51b5\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u4fdd\u7559\u7ba1\u7406\u3002", "method": "\u9075\u5faa\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u8303\u5f0f\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3001\u591a\u89c6\u89d2\u6587\u732e\u7efc\u8ff0\u3001\u793e\u533a\u8c03\u67e5\uff0c\u901a\u8fc7\u8fed\u4ee3\u6784\u5efa - \u8bc4\u4f30\u5468\u671f\u5f00\u53d1\u548c\u5b8c\u5584\u7b56\u7565\uff0c\u5f00\u53d1\u57fa\u4e8e\u7f51\u7edc\u7684\u539f\u578b\u5e76\u7eb3\u5165\u53cd\u9988\uff0c\u8fdb\u884c\u5b9e\u5730\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f00\u6e90\u8f6f\u4ef6\u8d21\u732e\u8005\u4fdd\u7559\u7ba1\u7406\u6311\u6218\u7684\u5b9e\u8bc1\u89c1\u89e3\u3001\u652f\u6301\u7ba1\u7406\u8005\u4fdd\u7559\u5de5\u4f5c\u7684\u53ef\u884c\u7b56\u7565\uff0c\u4ee5\u53ca\u7528\u4e8e\u672a\u6765\u7814\u7a76\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u7406\u8bba\u7684\u5b9e\u7528\u6846\u67b6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6210\u679c\u5bf9\u5f00\u6e90\u8f6f\u4ef6\u8d21\u732e\u8005\u4fdd\u7559\u7ba1\u7406\u53ca\u76f8\u5173\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.11340", "pdf": "https://arxiv.org/pdf/2602.11340", "abs": "https://arxiv.org/abs/2602.11340", "authors": ["Bo Pan", "Xuan Kan", "Kaitai Zhang", "Yan Yan", "Shunwen Tan", "Zihao He", "Zixin Ding", "Junjie Wu", "Liang Zhao"], "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.", "AI": {"tldr": "\u5f53\u524d\u5c06\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u5bf9\u9f50\u6709\u6311\u6218\uff0c\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u573a\u666f\u63a2\u7d22\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51faBLPO\u6846\u67b6\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7eaf\u6587\u672c\u8bc4\u4f30\uff0c\u5728\u591a\u6a21\u6001\u573a\u666f\u7814\u7a76\u4e0d\u8db3\uff0c\u591a\u6a21\u6001\u6a21\u578b\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u96be\u4ee5\u9ad8\u6548\u4f18\u5316\u63d0\u793a\u3002", "method": "\u63d0\u51faBLPO\u53cc\u7ea7\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\u5e76\u8054\u5408\u4f18\u5316\u8bc4\u4f30\u63d0\u793a\u548c\u56fe\u50cf\u5230\u6587\u672c\u7684\u63d0\u793a\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5668\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684BLPO\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\u65f6\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u5bfc\u81f4\u7684\u63d0\u793a\u4f18\u5316\u96be\u9898\u3002"}}
{"id": "2602.12039", "pdf": "https://arxiv.org/pdf/2602.12039", "abs": "https://arxiv.org/abs/2602.12039", "authors": ["Alon Beck", "Yohai Bar Sinai", "Noam Levi"], "title": "The Implicit Bias of Logit Regularization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Logit regularization, the addition a convex penalty directly in logit space, is widely used in modern classifiers, with label smoothing as a prominent example. While such methods often improve calibration and generalization, their mechanism remains under-explored. In this work, we analyze a general class of such logit regularizers in the context of linear classification, and demonstrate that they induce an implicit bias of logit clustering around finite per-sample targets. For Gaussian data, or whenever logits are sufficiently clustered, we prove that logit clustering drives the weight vector to align exactly with Fisher's Linear Discriminant. To demonstrate the consequences, we study a simple signal-plus-noise model in which this transition has dramatic effects: Logit regularization halves the critical sample complexity and induces grokking in the small-noise limit, while making generalization robust to noise. Our results extend the theoretical understanding of label smoothing and highlight the efficacy of a broader class of logit-regularization methods.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u7ebf\u6027\u5206\u7c7b\u4e2dlogit\u6b63\u5219\u5316\u5668\uff0c\u63ed\u793a\u5176\u8bf1\u5bfclogit\u805a\u7c7b\u9690\u5f0f\u504f\u5dee\uff0c\u8bc1\u660e\u7279\u5b9a\u6761\u4ef6\u4e0b\u6743\u91cd\u5411\u91cf\u4e0eFisher\u7ebf\u6027\u5224\u522b\u5bf9\u9f50\uff0c\u7814\u7a76\u4fe1\u53f7\u52a0\u566a\u58f0\u6a21\u578b\u4f53\u73b0\u5176\u6548\u679c\uff0c\u62d3\u5c55\u7406\u8bba\u7406\u89e3\u3002", "motivation": "\u73b0\u4ee3\u5206\u7c7b\u5668\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684logit\u6b63\u5219\u5316\u65b9\u6cd5\u867d\u80fd\u6539\u5584\u6821\u51c6\u548c\u6cdb\u5316\uff0c\u4f46\u673a\u5236\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u5206\u6790\u7ebf\u6027\u5206\u7c7b\u4e2d\u4e00\u822c\u7c7b\u522b\u7684logit\u6b63\u5219\u5316\u5668\uff0c\u7814\u7a76\u9ad8\u65af\u6570\u636e\u53calogit\u5145\u5206\u805a\u7c7b\u60c5\u51b5\uff0c\u63a2\u8ba8\u4fe1\u53f7\u52a0\u566a\u58f0\u6a21\u578b\u3002", "result": "logit\u6b63\u5219\u5316\u8bf1\u5bfclogit\u805a\u7c7b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f7f\u6743\u91cd\u5411\u91cf\u4e0eFisher\u7ebf\u6027\u5224\u522b\u5bf9\u9f50\uff0c\u5728\u4fe1\u53f7\u52a0\u566a\u58f0\u6a21\u578b\u4e2d\u51cf\u534a\u4e34\u754c\u6837\u672c\u590d\u6742\u5ea6\u3001\u8bf1\u5bfc\u5c0f\u566a\u58f0\u6781\u9650\u4e0b\u7684\u5b66\u4e60\u5e76\u4f7f\u6cdb\u5316\u5bf9\u566a\u58f0\u9c81\u68d2\u3002", "conclusion": "\u7814\u7a76\u62d3\u5c55\u4e86\u5bf9\u6807\u7b7e\u5e73\u6ed1\u7684\u7406\u8bba\u7406\u89e3\uff0c\u51f8\u663e\u66f4\u5e7f\u6cdblogit\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.11192", "pdf": "https://arxiv.org/pdf/2602.11192", "abs": "https://arxiv.org/abs/2602.11192", "authors": ["Arian Raje", "Anupam Nayak", "Gauri Joshi"], "title": "MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\\times$ over efficient baselines and up to $14.7\\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.", "AI": {"tldr": "\u63d0\u51faMELINOE\u65b9\u6cd5\u4f18\u5316MoE\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u63d0\u5347\u541e\u5410\u91cf\uff0c\u4e0d\u635f\u5931\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "MoE\u6a21\u578b\u867d\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u56e0\u53c2\u6570\u591a\u96be\u4ee5\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u73b0\u6709\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u65b9\u6cd5\u6709I/O\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u5fae\u8c03MoE\u6a21\u578b\uff0c\u4f7f\u5176\u503e\u5411\u6fc0\u6d3b\u5c11\u91cf\u4e13\u5bb6\uff0c\u5c06\u8fd9\u4e9b\u4e13\u5bb6\u7f13\u5b58\u5230GPU\u5185\u5b58\u3002", "result": "\u76f8\u6bd4\u9ad8\u6548\u57fa\u7ebf\u541e\u5410\u91cf\u63d0\u53471.2 - 3\u500d\uff0c\u76f8\u6bd4\u9ad8\u4f20\u8f93\u57fa\u7ebf\u63d0\u5347\u8fbe14.7\u500d\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0d\u53d8\u6216\u63d0\u5347\u3002", "conclusion": "MELINOE\u662f\u63d0\u5347MoE\u63a8\u7406\u6548\u7387\u7684\u53ef\u9760\u65b9\u6cd5\u3002"}}
{"id": "2602.11477", "pdf": "https://arxiv.org/pdf/2602.11477", "abs": "https://arxiv.org/abs/2602.11477", "authors": ["Yifan Liang", "Andong Li", "Kang Yang", "Guochen Yu", "Fangkun Liu", "Lingling Dai", "Xiaodong Li", "Chengshi Zheng"], "title": "SLD-L2S: Hierarchical Subspace Latent Diffusion for High-Fidelity Lip to Speech Synthesis", "categories": ["eess.AS", "cs.CE"], "comment": null, "summary": "Although lip-to-speech synthesis (L2S) has achieved significant progress in recent years, current state-of-the-art methods typically rely on intermediate representations such as mel-spectrograms or discrete self-supervised learning (SSL) tokens. The potential of latent diffusion models (LDMs) in this task remains largely unexplored. In this paper, we introduce SLD-L2S, a novel L2S framework built upon a hierarchical subspace latent diffusion model. Our method aims to directly map visual lip movements to the continuous latent space of a pre-trained neural audio codec, thereby avoiding the information loss inherent in traditional intermediate representations. The core of our method is a hierarchical architecture that processes visual representations through multiple parallel subspaces, initiated by a subspace decomposition module. To efficiently enhance interactions within and between these subspaces, we design the diffusion convolution block (DiCB) as our network backbone. Furthermore, we employ a reparameterized flow matching technique to directly generate the target latent vectors. This enables a principled inclusion of speech language model (SLM) and semantic losses during training, moving beyond conventional flow matching objectives and improving synthesized speech quality. Our experiments show that SLD-L2S achieves state-of-the-art generation quality on multiple benchmark datasets, surpassing existing methods in both objective and subjective evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u5b50\u7a7a\u95f4\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684SLD - L2S\u5507\u8bed\u5408\u6210\u6846\u67b6\uff0c\u907f\u514d\u4f20\u7edf\u4e2d\u95f4\u8868\u5f81\u4fe1\u606f\u635f\u5931\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u6700\u4f18\u3002", "motivation": "\u5f53\u524d\u5507\u8bed\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e2d\u95f4\u8868\u5f81\uff0c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u6f5c\u529b\u672a\u5145\u5206\u6316\u6398\uff0c\u9700\u907f\u514d\u4f20\u7edf\u4e2d\u95f4\u8868\u5f81\u4fe1\u606f\u635f\u5931\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5206\u5c42\u5b50\u7a7a\u95f4\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684SLD - L2S\u6846\u67b6\uff0c\u7528\u5206\u5c42\u67b6\u6784\u548c\u6269\u6563\u5377\u79ef\u5757\uff08DiCB\uff09\u5904\u7406\u89c6\u89c9\u8868\u5f81\uff0c\u91c7\u7528\u91cd\u53c2\u6570\u5316\u6d41\u5339\u914d\u6280\u672f\u751f\u6210\u76ee\u6807\u6f5c\u5728\u5411\u91cf\u3002", "result": "SLD - L2S\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u751f\u6210\u8d28\u91cf\uff0c\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SLD - L2S\u6846\u67b6\u6709\u6548\uff0c\u80fd\u63d0\u5347\u5507\u8bed\u5408\u6210\u7684\u8bed\u97f3\u8d28\u91cf\u3002"}}
{"id": "2602.11732", "pdf": "https://arxiv.org/pdf/2602.11732", "abs": "https://arxiv.org/abs/2602.11732", "authors": ["Hannaneh Akrami", "Ryoga Mahara", "Kurt Mehlhorn", "Nidhi Rathi"], "title": "Achieving EF1 and Epistemic EFX Guarantees Simultaneously", "categories": ["cs.GT"], "comment": null, "summary": "We study the fundamental problem of fairly dividing a set of indivisible goods among agents with additive valuations. Here, envy-freeness up to any good (EFX) is a central fairness notion and resolving its existence is regarded as one of the most important open problems in this area of research. Two prominent relaxations of EFX are envy-freeness up to one good (EF1) and epistemic EFX (EEFX). While allocations satisfying each of these notions individually are known to exist even for general monotone valuations, whether both can be satisfied simultaneously remains open for all instances in which the EFX problem is itself unresolved.\n  In this work, we show that there always exists an allocation that is both EF1 (in fact, the stronger notion EFL) and EEFX for additive valuations, thereby resolving the primary open question raised by Akrami and Rathi (2025) and bringing us one step closer to resolving the elusive EFX problem. We introduce a new share-based fairness notion, termed strong EEFX share, which may be of independent interest and which implies EEFX feasibility of bundles. We show that this notion is compatible with EF1, leading to the desired existence result.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u8bc1\u660e\u5b58\u5728\u5bf9\u53ef\u52a0\u4f30\u503c\u540c\u65f6\u6ee1\u8db3EF1\u548cEEFX\u7684\u5206\u914d\uff0c\u5f15\u5165\u5f3aEEFX\u4efd\u989d\u6982\u5ff5\u89e3\u51b3\u5f00\u653e\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u516c\u5e73\u5206\u914d\u4e2dEFX\u5b58\u5728\u6027\u8fd9\u4e00\u91cd\u8981\u5f00\u653e\u95ee\u9898\uff0c\u63a2\u8ba8EF1\u548cEEFX\u80fd\u5426\u540c\u65f6\u6ee1\u8db3\u3002", "method": "\u5f15\u5165\u65b0\u7684\u57fa\u4e8e\u4efd\u989d\u7684\u516c\u5e73\u6982\u5ff5\u2014\u2014\u5f3aEEFX\u4efd\u989d\uff0c\u8bc1\u660e\u5176\u4e0eEF1\u517c\u5bb9\u3002", "result": "\u8bc1\u660e\u5b58\u5728\u5bf9\u53ef\u52a0\u4f30\u503c\u540c\u65f6\u6ee1\u8db3EF1\uff08\u66f4\u5f3a\u7684EFL\uff09\u548cEEFX\u7684\u5206\u914d\u3002", "conclusion": "\u89e3\u51b3\u4e86Akrami\u548cRathi (2025)\u63d0\u51fa\u7684\u4e3b\u8981\u5f00\u653e\u95ee\u9898\uff0c\u5411\u89e3\u51b3EFX\u95ee\u9898\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2602.11664", "pdf": "https://arxiv.org/pdf/2602.11664", "abs": "https://arxiv.org/abs/2602.11664", "authors": ["Huimin Yan", "Longfei Xu", "Junjie Sun", "Zheng Liu", "Wei Luo", "Kaikui Liu", "Xiangxiang Chu"], "title": "IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation", "categories": ["cs.IR"], "comment": null, "summary": "Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: \"when to depart\", \"how to travel\", \"where to go\", and \"what needs arise via the route\". However, current research is limited by fragmented datasets that focus merely on next POI recommendation (\"where to go\"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.", "AI": {"tldr": "\u63d0\u51fa\u5927\u89c4\u6a21\u516c\u5171\u6570\u636e\u96c6IntTravel\u7528\u4e8e\u7efc\u5408\u65c5\u884c\u63a8\u8350\uff0c\u5e76\u6784\u5efa\u7aef\u5230\u7aef\u751f\u6210\u6846\u67b6\uff0c\u5728\u591a\u6570\u636e\u96c6\u8868\u73b0\u51fa\u8272\u4e14\u5df2\u90e8\u7f72\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d7\u788e\u7247\u5316\u6570\u636e\u96c6\u9650\u5236\uff0c\u4ec5\u5173\u6ce8\u4e0b\u4e00\u5174\u8da3\u70b9\u63a8\u8350\uff0c\u5ffd\u7565\u51fa\u53d1\u65f6\u95f4\u3001\u51fa\u884c\u65b9\u5f0f\u548c\u60c5\u5883\u9700\u6c42\uff0c\u4e14\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u5f71\u54cd\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5f15\u5165IntTravel\u6570\u636e\u96c6\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u3001\u4ec5\u89e3\u7801\u5668\u7684\u751f\u6210\u6846\u67b6\u7528\u4e8e\u591a\u4efb\u52a1\u63a8\u8350\uff0c\u878d\u5165\u4fe1\u606f\u4fdd\u5b58\u3001\u9009\u62e9\u548c\u5206\u89e3\u3002", "result": "\u6846\u67b6\u5728IntTravel\u6570\u636e\u96c6\u548c\u975e\u65c5\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u8fbe\u5230\u6700\u4f18\uff0cIntTravel\u5728\u9ad8\u5fb7\u5730\u56fe\u90e8\u7f72\u4f7fCTR\u63d0\u53471.09%\u3002", "conclusion": "IntTravel\u6570\u636e\u96c6\u548c\u751f\u6210\u6846\u67b6\u6709\u6548\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u6027\u80fd\u548c\u53ef\u63a8\u5e7f\u6027\u3002"}}
{"id": "2602.12175", "pdf": "https://arxiv.org/pdf/2602.12175", "abs": "https://arxiv.org/abs/2602.12175", "authors": ["David Shmoys", "Varun Suriyanarayana", "Seeun William Umboh"], "title": "Improved Online Algorithms for Inventory Management Problems with Holding and Delay Costs: Riding the Wave Makes Things Simpler, Stronger, & More General", "categories": ["cs.DS"], "comment": "19 pages, 1 figure, appeared at SODA 2026", "summary": "The Joint Replenishment Problem (JRP) is a classical inventory management problem, that aims to model the trade-off between coordinating orders for multiple commodities (and their cost) with holding costs incurred by meeting demand in advance. Moseley, Niaparast and Ravi introduced a natural online generalization of the JRP in which inventory corresponding to demands may be replenished late, for a delay cost, or early, for a holding cost. They established that when the holding and delay costs are monotone and uniform across demands, there is a 30-competitive algorithm that employs a greedy strategy and a dual-fitting based analysis.\n  We develop a 5-competitive algorithm that handles arbitrary monotone demand-specific holding and delay cost functions, thus simultaneously improving upon the competitive ratio and relaxing the uniformity assumption. Our primal-dual algorithm is in the spirit of the work Buchbinder, Kimbrel, Levi, Makarychev, and Sviridenko, which maintains a wavefront dual solution to decide when to place an order and which items to order. The main twist is in deciding which requests to serve early. In contrast to the work of Moseley et al., which ranks early requests in ascending order of desired service time and serves them until their total holding cost matches the ordering cost incurred for that item, we extend to the non-uniform case by instead ranking in ascending order of when the delay cost of a demand would reach its current holding cost. An important special case of the JRP is the single-item lot-sizing problem. Here, Moseley et al. gave a 3-competitive algorithm when the holding and delay costs are uniform across demands. We provide a new algorithm for which the competitive ratio is $\u03c6+1 \\approx 2.681$, where $\u03c6$ is the golden ratio, which again holds for arbitrary monotone holding-delay costs.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8054\u5408\u8865\u8d27\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u5728\u7ebf\u8865\u8d27\u7b97\u6cd5\u7684\u7ade\u4e89\u6bd4\uff0c\u5904\u7406\u4e86\u4efb\u610f\u5355\u8c03\u7684\u7279\u5b9a\u9700\u6c42\u6301\u6709\u548c\u5ef6\u8fdf\u6210\u672c\u51fd\u6570\uff0c\u8fd8\u4e3a\u5355\u7269\u54c1\u6279\u91cf\u89c4\u6a21\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7b97\u6cd5\u3002", "motivation": "Moseley\u7b49\u4eba\u7684\u7b97\u6cd5\u6709\u7ade\u4e89\u6bd4\u548c\u6210\u672c\u51fd\u6570\u4e00\u81f4\u6027\u5047\u8bbe\u7684\u5c40\u9650\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u3002", "method": "\u63d0\u51fa5 - \u7ade\u4e89\u539f\u5bf9\u5076\u7b97\u6cd5\uff0c\u501f\u9274\u524d\u4eba\u4fdd\u6301\u6ce2\u524d\u5bf9\u5076\u89e3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u9700\u6c42\u5ef6\u8fdf\u6210\u672c\u8fbe\u5230\u5f53\u524d\u6301\u6709\u6210\u672c\u7684\u65f6\u95f4\u6392\u5e8f\u51b3\u5b9a\u63d0\u524d\u670d\u52a1\u7684\u8bf7\u6c42\uff1b\u4e3a\u5355\u7269\u54c1\u6279\u91cf\u89c4\u6a21\u95ee\u9898\u63d0\u4f9b\u65b0\u7b97\u6cd5\u3002", "result": "5 - \u7ade\u4e89\u7b97\u6cd5\u80fd\u5904\u7406\u4efb\u610f\u5355\u8c03\u9700\u6c42\u7279\u5b9a\u6210\u672c\u51fd\u6570\uff0c\u5355\u7269\u54c1\u6279\u91cf\u89c4\u6a21\u95ee\u9898\u65b0\u7b97\u6cd5\u7ade\u4e89\u6bd4\u7ea6\u4e3a2.681\u3002", "conclusion": "\u672c\u6587\u7b97\u6cd5\u5728\u8054\u5408\u8865\u8d27\u95ee\u9898\u548c\u5355\u7269\u54c1\u6279\u91cf\u89c4\u6a21\u95ee\u9898\u4e0a\u90fd\u6709\u4f18\u52bf\uff0c\u6539\u8fdb\u4e86\u7ade\u4e89\u6bd4\u5e76\u653e\u677e\u4e86\u4e00\u81f4\u6027\u5047\u8bbe\u3002"}}
{"id": "2602.12151", "pdf": "https://arxiv.org/pdf/2602.12151", "abs": "https://arxiv.org/abs/2602.12151", "authors": ["Youhe Jiang", "Fangcheng Fu", "Taiyi Wang", "Guoliang He", "Eiko Yoneki"], "title": "OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration", "categories": ["cs.DC"], "comment": null, "summary": "Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\\times$ (average: 1.5$\\times$) compared to state-of-the-art serving systems.", "AI": {"tldr": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u5047\u8bbe\u5de5\u4f5c\u8d1f\u8f7d\u5747\u5300\u7a33\u5b9a\uff0c\u4e0e\u5b9e\u9645\u4e0d\u7b26\uff0cOServe \u7cfb\u7edf\u901a\u8fc7\u65b0\u8c03\u5ea6\u7b97\u6cd5\u548c\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5207\u6362\u65b9\u6cd5\u89e3\u51b3\u65f6\u7a7a\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u5047\u8bbe\u5de5\u4f5c\u8d1f\u8f7d\u5747\u5300\u7a33\u5b9a\uff0c\u4e0e\u5b9e\u9645\u65f6\u7a7a\u5f02\u8d28\u6027\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\uff0c\u6839\u636e\u5b9e\u65f6\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u4f18\u5316\u5f02\u6784\u6a21\u578b\u90e8\u7f72\uff1b\u63d0\u51fa\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5207\u6362\u65b9\u6cd5\uff0c\u6839\u636e\u9884\u6d4b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u8fc1\u79fb\u6a21\u578b\u90e8\u7f72\u3002", "result": "\u5728\u771f\u5b9e\u8f68\u8ff9\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u670d\u52a1\u7cfb\u7edf\u76f8\u6bd4\uff0cOServe \u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe 2 \u500d\uff0c\u5e73\u5747 1.5 \u500d\u3002", "conclusion": "OServe \u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4e2d\u7684\u65f6\u7a7a\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.11487", "pdf": "https://arxiv.org/pdf/2602.11487", "abs": "https://arxiv.org/abs/2602.11487", "authors": ["Asmar Muqeet", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-Based Quantum Program Testing via Commuting Pauli String", "categories": ["cs.SE"], "comment": null, "summary": "Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.", "AI": {"tldr": "\u63d0\u51faSB - QOPS\u641c\u7d22\u5f0f\u91cf\u5b50\u7a0b\u5e8f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8eQOPS\u4e14\u6709\u8de8\u5e73\u53f0\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u8f93\u5165\u3001\u7edf\u8ba1\u9884\u8a00\u673a\u3001\u6602\u8d35\u7a0b\u5e8f\u89c4\u8303\uff0c\u5728\u771f\u5b9e\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u6709\u9650\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faSB - QOPS\u65b9\u6cd5\uff0c\u4ee5\u6ce1\u5229\u5b57\u7b26\u4e32\u91cd\u65b0\u5b9a\u4e49\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5f15\u5165\u4ee5\u6d4b\u91cf\u4e3a\u4e2d\u5fc3\u7684\u9884\u8a00\u673a\uff0c\u7528\u57fa\u4e8e\u671f\u671b\u503c\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728\u6700\u591a29\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u7535\u8def\u4e0a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e09\u79cd\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u679c\u663e\u793aSB - QOPS\u663e\u8457\u4f18\u4e8eQOPS\uff0c\u5bf9\u6700\u591a29\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7535\u8def\u6545\u969c\u68c0\u6d4b\u5f97\u5206\u8fbe100%\u3002", "conclusion": "SB - QOPS\u80fd\u6709\u6548\u6d4b\u8bd5\u91cf\u5b50\u7a0b\u5e8f\uff0c\u51cf\u5c11\u5bf9\u5b8c\u6574\u7a0b\u5e8f\u89c4\u8303\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u9884\u7b97\u5229\u7528\u7387\uff0c\u4e14\u5177\u6709\u8de8\u91cf\u5b50\u5e73\u53f0\u7684\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2602.11348", "pdf": "https://arxiv.org/pdf/2602.11348", "abs": "https://arxiv.org/abs/2602.11348", "authors": ["Ruipeng Wang", "Yuxin Chen", "Yukai Wang", "Chang Wu", "Junfeng Fang", "Xiaodong Cai", "Qi Gu", "Hui Su", "An Zhang", "Xiang Wang", "Xunliang Cai", "Tat-Seng Chua"], "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.", "AI": {"tldr": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f73\uff0c\u4f46\u73b0\u5b9e\u90e8\u7f72\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u63d0\u51fa AgentNoiseBench \u6846\u67b6\u8bc4\u4f30\u667a\u80fd\u4f53\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u5e76\u8fdb\u884c\u8bc4\u4f30\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u73af\u5883\u6270\u52a8\u7684\u654f\u611f\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u73b0\u5b9e\u90e8\u7f72\u4e2d\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u4e3b\u8981\u56e0\u73b0\u6709\u8bad\u7ec3\u548c\u8bc4\u4f30\u8303\u5f0f\u57fa\u4e8e\u7406\u60f3\u5316\u5047\u8bbe\uff0c\u5ffd\u7565\u73b0\u5b9e\u4ea4\u4e92\u7684\u968f\u673a\u6027\u548c\u566a\u58f0\uff0c\u9700\u8bc4\u4f30\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5148\u5206\u6790\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u504f\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u73af\u5883\u566a\u58f0\u5206\u4e3a\u7528\u6237\u566a\u58f0\u548c\u5de5\u5177\u566a\u58f0\uff1b\u5f00\u53d1\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5728\u73b0\u6709\u4ee5\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6ce8\u5165\u53ef\u63a7\u566a\u58f0\uff1b\u5bf9\u591a\u79cd\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0d\u540c\u566a\u58f0\u6761\u4ef6\u4e0b\u6a21\u578b\u6027\u80fd\u6709\u4e00\u81f4\u53d8\u5316\uff0c\u8868\u660e\u5f53\u524d\u667a\u80fd\u4f53\u6a21\u578b\u5bf9\u73b0\u5b9e\u73af\u5883\u6270\u52a8\u654f\u611f\u3002", "conclusion": "AgentNoiseBench \u6846\u67b6\u53ef\u6709\u6548\u8bc4\u4f30\u667a\u80fd\u4f53\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5f53\u524d\u6a21\u578b\u5bf9\u73b0\u5b9e\u73af\u5883\u6270\u52a8\u8f83\u654f\u611f\u3002"}}
{"id": "2602.11333", "pdf": "https://arxiv.org/pdf/2602.11333", "abs": "https://arxiv.org/abs/2602.11333", "authors": ["Kaicheng Chen", "Harold D. Chiang"], "title": "Cross-Fitting-Free Debiased Machine Learning with Multiway Dependence", "categories": ["econ.EM", "stat.ML"], "comment": "This paper supersedes the earlier manuscript \"Maximal inequalities for separately exchangeable empirical processes\" (arXiv:2502.11432) by Harold D. Chiang", "summary": "This paper develops an asymptotic theory for two-step debiased machine learning (DML) estimators in generalised method of moments (GMM) models with general multiway clustered dependence, without relying on cross-fitting. While cross-fitting is commonly employed, it can be statistically inefficient and computationally burdensome when first-stage learners are complex and the effective sample size is governed by the number of independent clusters. We show that valid inference can be achieved without sample splitting by combining Neyman-orthogonal moment conditions with a localisation-based empirical process approach, allowing for an arbitrary number of clustering dimensions. The resulting DML-GMM estimators are shown to be asymptotically linear and asymptotically normal under multiway clustered dependence. A central technical contribution of the paper is the derivation of novel global and local maximal inequalities for general classes of functions of sums of separately exchangeable arrays, which underpin our theoretical arguments and are of independent interest.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5e7f\u4e49\u77e9\u4f30\u8ba1\uff08GMM\uff09\u6a21\u578b\u4e2d\u4e24\u6b65\u53bb\u504f\u673a\u5668\u5b66\u4e60\uff08DML\uff09\u4f30\u8ba1\u91cf\u5f00\u53d1\u4e86\u6e10\u8fd1\u7406\u8bba\uff0c\u65e0\u9700\u4ea4\u53c9\u62df\u5408\u3002", "motivation": "\u4ea4\u53c9\u62df\u5408\u5728\u590d\u6742\u4e00\u9636\u5b66\u4e60\u5668\u548c\u6709\u6548\u6837\u672c\u91cf\u53d7\u72ec\u7acb\u805a\u7c7b\u6570\u5f71\u54cd\u65f6\uff0c\u7edf\u8ba1\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Neyman\u6b63\u4ea4\u77e9\u6761\u4ef6\u4e0e\u57fa\u4e8e\u5c40\u90e8\u5316\u7684\u7ecf\u9a8c\u8fc7\u7a0b\u65b9\u6cd5\uff0c\u4e0d\u8fdb\u884c\u6837\u672c\u5206\u5272\u3002", "result": "\u5f97\u5230\u7684DML - GMM\u4f30\u8ba1\u91cf\u5728\u591a\u5411\u805a\u7c7b\u4f9d\u8d56\u4e0b\u6e10\u8fd1\u7ebf\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u3002", "conclusion": "\u63a8\u5bfc\u51fa\u65b0\u7684\u5168\u5c40\u548c\u5c40\u90e8\u6700\u5927\u4e0d\u7b49\u5f0f\uff0c\u4e3a\u7406\u8bba\u8bba\u8bc1\u63d0\u4f9b\u652f\u6491\u3002"}}
{"id": "2602.11194", "pdf": "https://arxiv.org/pdf/2602.11194", "abs": "https://arxiv.org/abs/2602.11194", "authors": ["Mahta Movasat", "Ingrid Tomac"], "title": "Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data", "categories": ["cs.LG", "cond-mat.soft"], "comment": null, "summary": "Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.", "AI": {"tldr": "\u7814\u7a76\u8fd0\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9884\u6d4b\u548c\u5206\u7c7b\u6a21\u62df\u5b9e\u9a8c\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u5bf9\u91ce\u706b\u540e\u707e\u5bb3\u8bc4\u4f30\u6709\u6f5c\u529b", "motivation": "\u91ce\u706b\u540e\u6ce5\u77f3\u6d41\u5371\u5bb3\u589e\u5927\uff0c\u9700\u7406\u89e3\u6ce5\u77f3\u6d41\u53d1\u751f\u65f6\u95f4\u548c\u6761\u4ef6\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u5efa\u6a21\u590d\u6742\u7cfb\u7edf", "method": "\u5e94\u7528\u591a\u5143\u7ebf\u6027\u56de\u5f52\u3001\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u5206\u7c7b\u5668\u3001K - \u5747\u503c\u805a\u7c7b\u548c\u4e3b\u6210\u5206\u5206\u6790\u7b49\u7b97\u6cd5\uff0c\u5bf9\u6a21\u62df\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u9884\u6d4b\u548c\u5206\u7c7b", "result": "\u591a\u5143\u7ebf\u6027\u56de\u5f52\u9884\u6d4b\u603b\u6d41\u91cf\u8f83\u6709\u6548\uff0c\u903b\u8f91\u56de\u5f52\u548c\u652f\u6301\u5411\u91cf\u5206\u7c7b\u5668\u5206\u7c7b\u5931\u6548\u7ed3\u679c\u51c6\u786e\u6027\u9ad8\uff0c\u7ec6\u7802\u6613\u4fb5\u8680\uff0c\u9ad8\u5f3a\u5ea6\u964d\u96e8\u524d10\u5206\u949f\u5173\u952e", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u91ce\u706b\u540e\u707e\u5bb3\u8bc4\u4f30\u548c\u5e94\u6025\u54cd\u5e94\u89c4\u5212\u4e2d\u6709\u6f5c\u529b"}}
{"id": "2602.11712", "pdf": "https://arxiv.org/pdf/2602.11712", "abs": "https://arxiv.org/abs/2602.11712", "authors": ["Luigi Simeone"], "title": "Potential-energy gating for robust state estimation in bistable stochastic systems", "categories": ["cs.LG", "cs.CE", "nlin.CD", "physics.data-an", "stat.ME"], "comment": "20 pages, 8 figures", "summary": "We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.", "AI": {"tldr": "\u63d0\u51fa\u52bf\u80fd\u95e8\u63a7\u65b9\u6cd5\u7528\u4e8e\u53cc\u9631\u968f\u673a\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\uff0c\u5728\u591a\u79cd\u6ee4\u6ce2\u5668\u5b9e\u73b0\uff0c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6709\u663e\u8457RMSE\u6539\u5584\uff0c\u4e14\u5bf9\u6a21\u578b\u8bef\u8bbe\u6709\u9c81\u68d2\u6027\uff0c\u8fd8\u5e94\u7528\u4e8e\u51b0\u82af\u8bb0\u5f55\u5206\u6790\u3002", "motivation": "\u4e3a\u53cc\u9631\u968f\u673a\u52a8\u529b\u5b66\u7cfb\u7edf\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7edf\u8ba1\u9c81\u68d2\u6ee4\u6ce2\u5668\u548c\u7ea6\u675f\u6ee4\u6ce2\u5668\u3002", "method": "\u901a\u8fc7\u5df2\u77e5\u6216\u5047\u8bbe\u7684\u52bf\u80fd\u51fd\u6570\u7684\u5c40\u90e8\u503c\u8c03\u5236\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u7684\u89c2\u6d4b\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u5728\u591a\u79cd\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e2d\u5b9e\u73b0\u95e8\u63a7\uff0c\u4ec5\u9700\u4e24\u4e2a\u989d\u5916\u8d85\u53c2\u6570\u3002", "result": "\u5728\u542b10%\u5f02\u5e38\u503c\u6c61\u67d3\u7684Ginzburg - Landau\u53cc\u9631\u8fc7\u7a0b\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u6807\u51c6\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u670957 - 80%\u7684RMSE\u6539\u5584\uff1b\u5bf9\u6a21\u578b\u8bef\u8bbe\u9c81\u68d2\uff0c\u504f\u5dee50%\u65f6\u6539\u5584\u4ecd\u4e0d\u4f4e\u4e8e47%\uff1b\u5e94\u7528\u4e8e\u51b0\u82af\u8bb0\u5f55\u5206\u6790\uff0c\u4f30\u8ba1\u51fa\u4e0d\u5bf9\u79f0\u53c2\u6570\u5e76\u8868\u660e\u5f02\u5e38\u503c\u6bd4\u4f8b\u89e3\u91ca91%\u7684\u6ee4\u6ce2\u5668\u6539\u5584\u65b9\u5dee\u3002", "conclusion": "\u52bf\u80fd\u95e8\u63a7\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u53cc\u9631\u968f\u673a\u52a8\u529b\u5b66\u7cfb\u7edf\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5bf9\u566a\u58f0\u548c\u6a21\u578b\u8bef\u8bbe\u5177\u6709\u8f83\u597d\u9c81\u68d2\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u6548\u679c\u3002"}}
{"id": "2602.11835", "pdf": "https://arxiv.org/pdf/2602.11835", "abs": "https://arxiv.org/abs/2602.11835", "authors": ["Yutong Chao", "Jalal Etesami"], "title": "Global Convergence to Nash Equilibrium in Nonconvex General-Sum Games under the $n$-Sided PL Condition", "categories": ["cs.GT", "cs.MA", "math.NA"], "comment": "24 pages", "summary": "We consider the problem of finding a Nash equilibrium (NE) in a general-sum game, where player $i$'s objective is $f_i(x)=f_i(x_1,...,x_n)$, with $x_j\\in\\mathbb{R}^{d_j}$ denoting the strategy variables of player $j$. Our focus is on investigating first-order gradient-based algorithms and their variations, such as the block coordinate descent (BCD) algorithm, for tackling this problem. We introduce a set of conditions, called the $n$-sided PL condition, which extends the well-established gradient dominance condition a.k.a Polyak-\u0141ojasiewicz (PL) condition and the concept of multi-convexity. This condition, satisfied by various classes of non-convex functions, allows us to analyze the convergence of various gradient descent (GD) algorithms. Moreover, our study delves into scenarios where the standard gradient descent methods fail to converge to NE. In such cases, we propose adapted variants of GD that converge towards NE and analyze their convergence rates. Finally, we evaluate the performance of the proposed algorithms through several experiments.", "AI": {"tldr": "\u7814\u7a76\u4e00\u822c\u548c\u535a\u5f08\u4e2d\u5bfb\u627e\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u5f15\u5165n\u8fb9PL\u6761\u4ef6\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6536\u655b\u6027\uff0c\u63d0\u51fa\u9002\u5e94\u53d8\u4f53\u5e76\u5b9e\u9a8c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u4e00\u822c\u548c\u535a\u5f08\u4e2d\u5bfb\u627e\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u5206\u6790\u73b0\u6709\u7b97\u6cd5\u6536\u655b\u6027\u53ca\u5904\u7406\u4e0d\u6536\u655b\u60c5\u51b5\u3002", "method": "\u5f15\u5165n\u8fb9PL\u6761\u4ef6\uff0c\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u53ca\u5176\u53d8\u4f53\u7b97\u6cd5\uff0c\u63d0\u51fa\u9002\u5e94\u53d8\u4f53\u7b97\u6cd5\u3002", "result": "\u53ef\u5206\u6790\u5404\u79cd\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6536\u655b\u6027\uff0c\u63d0\u51fa\u7684\u9002\u5e94\u53d8\u4f53\u7b97\u6cd5\u671dNE\u6536\u655b\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u6c42\u89e3\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\u4e0a\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2602.11680", "pdf": "https://arxiv.org/pdf/2602.11680", "abs": "https://arxiv.org/abs/2602.11680", "authors": ["Yihang Li", "Zhuo Liu", "Wei Wei"], "title": "EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation", "categories": ["cs.IR"], "comment": "10 pages, 3 figures, 5 tables, accepted by WSDM 2026", "summary": "Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.", "AI": {"tldr": "\u63d0\u51faEpicCBR\u6846\u67b6\u89e3\u51b3\u51b7\u542f\u52a8\u6346\u7ed1\u63a8\u8350\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6346\u7ed1\u63a8\u8350\u6a21\u578b\u4f9d\u8d56\u89c2\u5bdf\u5230\u7684\u7528\u6237 - \u6346\u7ed1\u4ea4\u4e92\uff0c\u96be\u4ee5\u63a2\u7d22\u65b0\u6346\u7ed1\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u7528\u6237 - \u7269\u54c1\u548c\u6346\u7ed1 - \u7269\u54c1\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6EpicCBR\uff0c\u7cbe\u786e\u6316\u6398\u548c\u5229\u7528\u7269\u54c1\u5173\u7cfb\u6784\u5efa\u7528\u6237\u753b\u50cf\uff0c\u63d0\u51fa\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u65b9\u6cd5\u523b\u753b\u65b0\u6346\u7ed1\u7279\u5f81\uff0c\u5f15\u5165\u591a\u89c6\u56fe\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u786e\u4fdd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u4e0a\u5b9e\u9a8c\uff0cEpicCBR\u6bd4\u73b0\u6709\u6280\u672f\u5927\u5e45\u9886\u5148\uff08\u6700\u9ad8\u8fbe387%\uff09\u3002", "conclusion": "EpicCBR\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.10559", "pdf": "https://arxiv.org/pdf/2602.10559", "abs": "https://arxiv.org/abs/2602.10559", "authors": ["Guangyan Zhou"], "title": "Self-referential instances of the dominating set problem are irreducible", "categories": ["cs.CC", "cs.DS"], "comment": "12 pages, 1 figure", "summary": "We study the algorithmic decidability of the domination number in the Erdos-Renyi random graph model $G(n,p)$. We show that for a carefully chosen edge probability $p=p(n)$, the domination problem exhibits a strong irreducible property. Specifically, for any constant $0<c<1$, no algorithm that inspects only an induced subgraph of order at most $n^c$ can determine whether $G(n,p)$ contains a dominating set of size $k=\\ln n$. We demonstrate that the existence of such a dominating set can be flipped by a local symmetry mapping that alters only a constant number of edges, thereby producing indistinguishable random graph instances which require exhaustive search. These results demonstrate that the extreme hardness of the dominating set problem in random graphs cannot be attributed to local structure, but instead arises from the self-referential nature and near-independence structure of the entire solution space.", "AI": {"tldr": "\u7814\u7a76Erdos - Renyi\u968f\u673a\u56fe\u6a21\u578b\u4e2d\u652f\u914d\u6570\u7684\u7b97\u6cd5\u53ef\u5224\u5b9a\u6027\uff0c\u8868\u660e\u652f\u914d\u95ee\u9898\u6709\u5f3a\u4e0d\u53ef\u7ea6\u6027\uff0c\u6781\u7aef\u56f0\u96be\u6e90\u4e8e\u89e3\u7a7a\u95f4\u6027\u8d28\u3002", "motivation": "\u7814\u7a76Erdos - Renyi\u968f\u673a\u56fe\u6a21\u578b\u4e2d\u652f\u914d\u6570\u7684\u7b97\u6cd5\u53ef\u5224\u5b9a\u6027\u3002", "method": "\u9009\u53d6\u5408\u9002\u7684\u8fb9\u6982\u7387\uff0c\u8bc1\u660e\u68c0\u67e5\u5c0f\u9636\u8bf1\u5bfc\u5b50\u56fe\u7684\u7b97\u6cd5\u65e0\u6cd5\u786e\u5b9a\u56fe\u662f\u5426\u542b\u7279\u5b9a\u5927\u5c0f\u652f\u914d\u96c6\uff0c\u901a\u8fc7\u5c40\u90e8\u5bf9\u79f0\u6620\u5c04\u5c55\u793a\u4ea7\u751f\u96be\u4ee5\u533a\u5206\u7684\u968f\u673a\u56fe\u5b9e\u4f8b\u3002", "result": "\u5bf9\u4e8e\u7cbe\u5fc3\u9009\u62e9\u7684\u8fb9\u6982\u7387\uff0c\u68c0\u67e5\u81f3\u591an^c\u9636\u8bf1\u5bfc\u5b50\u56fe\u7684\u7b97\u6cd5\u65e0\u6cd5\u786e\u5b9aG(n,p)\u662f\u5426\u542b\u5927\u5c0f\u4e3aln n\u7684\u652f\u914d\u96c6\uff1b\u5b58\u5728\u5c40\u90e8\u5bf9\u79f0\u6620\u5c04\u53ef\u7ffb\u8f6c\u652f\u914d\u96c6\u5b58\u5728\u6027\u3002", "conclusion": "\u968f\u673a\u56fe\u4e2d\u652f\u914d\u96c6\u95ee\u9898\u7684\u6781\u7aef\u56f0\u96be\u6027\u5e76\u975e\u6e90\u4e8e\u5c40\u90e8\u7ed3\u6784\uff0c\u800c\u662f\u89e3\u7a7a\u95f4\u7684\u81ea\u6307\u6027\u548c\u8fd1\u72ec\u7acb\u6027\u3002"}}
{"id": "2602.11472", "pdf": "https://arxiv.org/pdf/2602.11472", "abs": "https://arxiv.org/abs/2602.11472", "authors": ["Md Sazedur Rahman", "Mizanur Rahman Jewel", "Sanjay Madria"], "title": "Future Mining: Learning for Safety and Security", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Mining is rapidly evolving into an AI driven cyber physical ecosystem where safety and operational reliability depend on robust perception, trustworthy distributed intelligence, and continuous monitoring of miners and equipment. However, real world mining environments impose severe constraints, including poor illumination, GPS denied conditions, irregular underground topologies and intermittent connectivity. These factors degrade perception accuracy, disrupt situational awareness and weaken distributed learning systems. At the same time, emerging cyber physical threats such as backdoor triggers, sensor spoofing, label flipping attacks, and poisoned model updates further jeopardize operational safety as mines adopt autonomous vehicles, humanoid assistance, and federated learning for collaborative intelligence. Energy constrained sensors also experience uneven battery depletion, creating blind spots in safety coverage and disrupting hazard detection pipelines. This paper presents a vision for a Unified Smart Safety and Security Architecture that integrates multimodal perception, secure federated learning, reinforcement learning, DTN enabled communication, and energy aware sensing into a cohesive safety framework. We introduce five core modules: Miner Finder, Multimodal Situational Awareness, Backdoor Attack Monitor, TrustFed LFD, and IoT driven Equipment Health Monitoring. These modules collectively address miner localization, hazard understanding, federated robustness, and predictive maintenance. Together, they form an end to end framework capable of guiding miners through obstructed pathways, identifying compromised models or sensors, and ensuring mission critical equipment reliability. This work outlines a comprehensive research vision for building a resilient and trustworthy intelligent mining system capable of maintaining operational continuity under adversarial conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u667a\u80fd\u5b89\u5168\u67b6\u6784\u5e94\u5bf9\u77ff\u4e1a\u73af\u5883\u6311\u6218\uff0c\u4ecb\u7ecd\u4e94\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u6784\u5efa\u6709\u5f39\u6027\u3001\u53ef\u4fe1\u7684\u667a\u80fd\u77ff\u4e1a\u7cfb\u7edf\u3002", "motivation": "\u73b0\u5b9e\u77ff\u4e1a\u73af\u5883\u6709\u8bf8\u591a\u7ea6\u675f\uff0c\u65b0\u5174\u7f51\u7edc\u7269\u7406\u5a01\u80c1\u548c\u80fd\u6e90\u53d7\u9650\u95ee\u9898\u5f71\u54cd\u77ff\u4e1a\u5b89\u5168\u4e0e\u53ef\u9760\u6027\uff0c\u9700\u6784\u5efa\u65b0\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u667a\u80fd\u5b89\u5168\u67b6\u6784\uff0c\u6574\u5408\u591a\u6a21\u6001\u611f\u77e5\u3001\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5f15\u5165\u4e94\u4e2a\u6838\u5fc3\u6a21\u5757\u3002", "result": "\u5f62\u6210\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u53ef\u5f15\u5bfc\u77ff\u5de5\u3001\u8bc6\u522b\u53d7\u635f\u6a21\u578b\u6216\u4f20\u611f\u5668\u3001\u786e\u4fdd\u5173\u952e\u8bbe\u5907\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u6784\u5efa\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u80fd\u4fdd\u6301\u8fd0\u8425\u8fde\u7eed\u6027\u7684\u667a\u80fd\u77ff\u4e1a\u7cfb\u7edf\u63d0\u4f9b\u5168\u9762\u7814\u7a76\u613f\u666f\u3002"}}
{"id": "2602.11514", "pdf": "https://arxiv.org/pdf/2602.11514", "abs": "https://arxiv.org/abs/2602.11514", "authors": ["Sidong Feng", "Chunyang Chen"], "title": "How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction", "categories": ["cs.SE", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual clarity through GUI Agent Autonomy Levels (GAL), a six-level framework that makes autonomy explicit and helps benchmark progress toward trustworthy software interaction.", "AI": {"tldr": "\u63d0\u51faGUI Agent Autonomy Levels (GAL)\u6846\u67b6\u4ee5\u660e\u786eGUI\u4ee3\u7406\u81ea\u4e3b\u6027\u7a0b\u5ea6", "motivation": "\u5f53\u524d\u2018\u4ee3\u7406\u2019\u81ea\u4e3b\u6027\u63cf\u8ff0\u5dee\u5f02\u5927\uff0c\u63a9\u76d6\u80fd\u529b\u3001\u8d23\u4efb\u548c\u98ce\u9669\uff0c\u7f3a\u4e4f\u6982\u5ff5\u6e05\u6670\u5ea6", "method": "\u6784\u5efa\u4e00\u4e2a\u516d\u7ea7\u6846\u67b6GAL", "result": "\u521b\u5efa\u4e86GAL\u6846\u67b6", "conclusion": "GAL\u6846\u67b6\u80fd\u4f7f\u81ea\u4e3b\u6027\u660e\u786e\uff0c\u6709\u52a9\u4e8e\u8861\u91cf\u5411\u53ef\u4fe1\u8f6f\u4ef6\u4ea4\u4e92\u7684\u8fdb\u5c55"}}
{"id": "2602.11351", "pdf": "https://arxiv.org/pdf/2602.11351", "abs": "https://arxiv.org/abs/2602.11351", "authors": ["Yihang Yao", "Zhepeng Cen", "Haohong Lin", "Shiqi Liu", "Zuxin Liu", "Jiacheng Zhu", "Zhang-Wei Hong", "Laixi Shi", "Ding Zhao"], "title": "Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.", "AI": {"tldr": "\u63d0\u51faBAO\u6846\u67b6\u89e3\u51b3\u4e3b\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u7684\u4efb\u52a1\u6027\u80fd\u4e0e\u7528\u6237\u53c2\u4e0e\u5ea6\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u8bad\u7ec3\u7ba1\u9053\u96be\u4ee5\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u63d0\u51faBAO\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u589e\u5f3a\u548c\u884c\u4e3a\u6b63\u5219\u5316\u3002", "result": "BAO\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5927\u5e45\u8d85\u8d8a\u4e3b\u52a8\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u6027\u80fd\u4e0e\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "BAO\u5728\u590d\u6742\u591a\u8f6e\u573a\u666f\u4e2d\u8bad\u7ec3\u4e3b\u52a8\u3001\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u65b9\u9762\u5f88\u6709\u6548\u3002"}}
{"id": "2602.11360", "pdf": "https://arxiv.org/pdf/2602.11360", "abs": "https://arxiv.org/abs/2602.11360", "authors": ["Sara Matijevic", "Christopher Yau"], "title": "Bootstrapping-based Regularisation for Reducing Individual Prediction Instability in Clinical Risk Prediction Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Clinical prediction models are increasingly used to support patient care, yet many deep learning-based approaches remain unstable, as their predictions can vary substantially when trained on different samples from the same population. Such instability undermines reliability and limits clinical adoption. In this study, we propose a novel bootstrapping-based regularisation framework that embeds the bootstrapping process directly into the training of deep neural networks. This approach constrains prediction variability across resampled datasets, producing a single model with inherent stability properties. We evaluated models constructed using the proposed regularisation approach against conventional and ensemble models using simulated data and three clinical datasets: GUSTO-I, Framingham, and SUPPORT. Across all datasets, our model exhibited improved prediction stability, with lower mean absolute differences (e.g., 0.019 vs. 0.059 in GUSTO-I; 0.057 vs. 0.088 in Framingham) and markedly fewer significantly deviating predictions. Importantly, discriminative performance and feature importance consistency were maintained, with high SHAP correlations between models (e.g., 0.894 for GUSTO-I; 0.965 for Framingham). While ensemble models achieved greater stability, we show that this came at the expense of interpretability, as each constituent model used predictors in different ways. By regularising predictions to align with bootstrapped distributions, our approach allows prediction models to be developed that achieve greater robustness and reproducibility without sacrificing interpretability. This method provides a practical route toward more reliable and clinically trustworthy deep learning models, particularly valuable in data-limited healthcare settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u81ea\u52a9\u6cd5\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u80fd\u63d0\u9ad8\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u4e14\u4e0d\u727a\u7272\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u533b\u7597\u573a\u666f\u63d0\u4f9b\u53ef\u9760\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5e94\u7528\uff0c\u9700\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u81ea\u52a9\u6cd5\u8fc7\u7a0b\u76f4\u63a5\u5d4c\u5165\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7ea6\u675f\u91cd\u91c7\u6837\u6570\u636e\u96c6\u7684\u9884\u6d4b\u53d8\u5f02\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u9884\u6d4b\u7a33\u5b9a\u6027\u63d0\u9ad8\uff0c\u5e73\u5747\u7edd\u5bf9\u5dee\u5f02\u964d\u4f4e\uff0c\u663e\u8457\u504f\u5dee\u9884\u6d4b\u51cf\u5c11\uff0c\u4fdd\u6301\u4e86\u5224\u522b\u6027\u80fd\u548c\u7279\u5f81\u91cd\u8981\u6027\u4e00\u81f4\u6027\u3002\u96c6\u6210\u6a21\u578b\u867d\u66f4\u7a33\u5b9a\u4f46\u727a\u7272\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5f00\u53d1\u51fa\u66f4\u9c81\u68d2\u3001\u53ef\u91cd\u590d\u4e14\u4e0d\u727a\u7272\u53ef\u89e3\u91ca\u6027\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5bf9\u6570\u636e\u6709\u9650\u7684\u533b\u7597\u573a\u666f\u6709\u4ef7\u503c\u3002"}}
{"id": "2602.11200", "pdf": "https://arxiv.org/pdf/2602.11200", "abs": "https://arxiv.org/abs/2602.11200", "authors": ["Guozhen Zhu", "Yuqian Hu", "Sakila Jayaweera", "Weihang Gao", "Wei-Hsiang Wang", "Jiaxuan Zhang", "Beibei Wang", "Chenshu Wu", "K. J. Ray Liu"], "title": "AM-FM: A Foundation Model for Ambient Intelligence Through WiFi", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u901a\u8fc7WiFi\u8fdb\u884c\u73af\u5883\u667a\u80fd\u611f\u77e5\u7684\u57fa\u7840\u6a21\u578bAM - FM\uff0c\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u611f\u77e5\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\uff0c\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u9650\u5236\u5b9e\u9645\u90e8\u7f72\uff0cWiFi\u6f5c\u529b\u672a\u5145\u5206\u6316\u6398\u3002", "method": "\u5728920\u4e07\u4e2a\u672a\u6807\u6ce8\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u6837\u672c\u4e0a\u7528\u5bf9\u6bd4\u5b66\u4e60\u3001\u63a9\u7801\u91cd\u5efa\u548c\u7269\u7406\u4fe1\u606f\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4e5d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAM - FM\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u6027\u80fd\u548c\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u53ef\u5229\u7528\u73b0\u6709\u65e0\u7ebf\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u73af\u5883\u667a\u80fd\u3002"}}
{"id": "2602.11857", "pdf": "https://arxiv.org/pdf/2602.11857", "abs": "https://arxiv.org/abs/2602.11857", "authors": ["Taira Tsuchiya", "Haipeng Luo", "Shinji Ito"], "title": "Scale-Invariant Fast Convergence in Games", "categories": ["cs.GT", "cs.LG", "stat.ML"], "comment": "44 pages", "summary": "Scale-invariance in games has recently emerged as a widely valued desirable property. Yet, almost all fast convergence guarantees in learning in games require prior knowledge of the utility scale. To address this, we develop learning dynamics that achieve fast convergence while being both scale-free, requiring no prior information about utilities, and scale-invariant, remaining unchanged under positive rescaling of utilities. For two-player zero-sum games, we obtain scale-free and scale-invariant dynamics with external regret bounded by $\\tilde{O}(A_{\\mathrm{diff}})$, where $A_{\\mathrm{diff}}$ is the payoff range, which implies an $\\tilde{O}(A_{\\mathrm{diff}} / T)$ convergence rate to Nash equilibrium after $T$ rounds. For multiplayer general-sum games with $n$ players and $m$ actions, we obtain scale-free and scale-invariant dynamics with swap regret bounded by $O(U_{\\mathrm{max}} \\log T)$, where $U_{\\mathrm{max}}$ is the range of the utilities, ignoring the dependence on the number of players and actions. This yields an $O(U_{\\mathrm{max}} \\log T / T)$ convergence rate to correlated equilibrium. Our learning dynamics are based on optimistic follow-the-regularized-leader with an adaptive learning rate that incorporates the squared path length of the opponents' gradient vectors, together with a new stopping-time analysis that exploits negative terms in regret bounds without scale-dependent tuning. For general-sum games, scale-free learning is enabled also by a technique called doubling clipping, which clips observed gradients based on past observations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u5c3a\u5ea6\u4e14\u5c3a\u5ea6\u4e0d\u53d8\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u5728\u4e0d\u540c\u7c7b\u578b\u535a\u5f08\u4e2d\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u7ed9\u51fa\u6536\u655b\u7387\uff0c\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u4e50\u89c2\u6b63\u5219\u5316\u9886\u5bfc\u8005\u548c\u65b0\u5206\u6790\u65b9\u6cd5\u53ca\u52a0\u500d\u88c1\u526a\u6280\u672f\u3002", "motivation": "\u591a\u6570\u535a\u5f08\u5b66\u4e60\u7684\u5feb\u901f\u6536\u655b\u4fdd\u8bc1\u9700\u6548\u7528\u5c3a\u5ea6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u5f00\u5c55\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u4e50\u89c2\u6b63\u5219\u5316\u9886\u5bfc\u8005\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u7ed3\u5408\u5bf9\u624b\u68af\u5ea6\u5411\u91cf\u5e73\u65b9\u8def\u5f84\u957f\u5ea6\uff0c\u8fd0\u7528\u65b0\u7684\u505c\u6b62\u65f6\u95f4\u5206\u6790\uff0c\u5728\u4e00\u822c\u548c\u535a\u5f08\u4e2d\u4f7f\u7528\u52a0\u500d\u88c1\u526a\u6280\u672f\u3002", "result": "\u5728\u4e24\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u5f97\u5230\u5916\u90e8\u9057\u61be\u6709\u754c\u7684\u65e0\u5c3a\u5ea6\u53ca\u5c3a\u5ea6\u4e0d\u53d8\u52a8\u6001\uff0c\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\uff1b\u5728\u591a\u4eba\u4e00\u822c\u548c\u535a\u5f08\u4e2d\u5f97\u5230\u4ea4\u6362\u9057\u61be\u6709\u754c\u7684\u52a8\u6001\uff0c\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\uff0c\u5e76\u7ed9\u51fa\u6536\u655b\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u52a8\u6001\u80fd\u5728\u65e0\u6548\u7528\u5148\u9a8c\u4fe1\u606f\u4e14\u5c3a\u5ea6\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u3002"}}
{"id": "2602.11719", "pdf": "https://arxiv.org/pdf/2602.11719", "abs": "https://arxiv.org/abs/2602.11719", "authors": ["Chenxiao Fan", "Chongming Gao", "Yaxin Gong", "Haoyan Liu", "Fuli Feng", "Xiangnan He"], "title": "Uncertainty-aware Generative Recommendation", "categories": ["cs.IR"], "comment": null, "summary": "Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.", "AI": {"tldr": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u751f\u6210\u63a8\u8350\u6846\u67b6UGR\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u786e\u5b9a\u6027\u76f2\u89c6\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660eUGR\u6027\u80fd\u4f18\u4e14\u7a33\u5b9a\uff0c\u8fd8\u80fd\u652f\u6301\u4e0b\u6e38\u98ce\u9669\u611f\u77e5\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u76f2\u89c6\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u52a8\u6001\u4e0d\u7a33\u5b9a\u548c\u51b3\u7b56\u98ce\u9669\u65e0\u6cd5\u91cf\u5316\u3002", "method": "\u63d0\u51faUGR\u6846\u67b6\uff0c\u534f\u540c\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5956\u52b1\u3001\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u52a8\u6001\u548c\u663e\u5f0f\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u4e09\u79cd\u673a\u5236\u3002", "result": "UGR\u63a8\u8350\u6027\u80fd\u4f18\u8d8a\uff0c\u7a33\u5b9a\u8bad\u7ec3\uff0c\u907f\u514d\u6807\u51c6\u65b9\u6cd5\u5e38\u89c1\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5b66\u4e60\u5230\u7684\u7f6e\u4fe1\u5ea6\u652f\u6301\u4e0b\u6e38\u98ce\u9669\u611f\u77e5\u5e94\u7528\u3002", "conclusion": "UGR\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u751f\u6210\u63a8\u8350\u65b9\u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u53ef\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2602.11250", "pdf": "https://arxiv.org/pdf/2602.11250", "abs": "https://arxiv.org/abs/2602.11250", "authors": ["Julia Gaudio", "Charlie K. Guan"], "title": "An Improved Upper Bound for the Euclidean TSP Constant Using Band Crossovers", "categories": ["cs.CG", "cs.DS", "math.CO", "math.PR"], "comment": null, "summary": "Consider $n$ points generated uniformly at random in the unit square, and let $L_n$ be the length of their optimal traveling salesman tour. Beardwood, Halton, and Hammersley (1959) showed $L_n / \\sqrt n \\to \u03b2$ almost surely as $n\\to \\infty$ for some constant $\u03b2$. The exact value of $\u03b2$ is unknown but estimated to be approximately $0.71$ (Applegate, Bixby, Chv\u00e1tal, Cook 2011). Beardwood et al. further showed that $0.625 \\leq \u03b2\\leq 0.92116.$ Currently, the best known bounds are $0.6277 \\leq \u03b2\\leq 0.90380$, due to Gaudio and Jaillet (2019) and Carlsson and Yu (2023), respectively. The upper bound was derived using a computer-aided approach that is amenable to lower bounds with improved computation speed. In this paper, we show via simulation and concentration analysis that future improvement of the $0.90380$ is limited to $\\sim0.88$. Moreover, we provide an alternative tour-constructing heuristic that, via simulation, could potentially improve the upper bound to $\\sim0.85$. Our approach builds on a prior \\emph{band-traversal} strategy, initially proposed by Beardwood et al. (1959) and subsequently refined by Carlsson and Yu (2023): divide the unit square into bands of height $\u0398(1/\\sqrt{n})$, construct paths within each band, and then connect the paths to create a TSP tour. Our approach allows paths to cross bands, and takes advantage of pairs of points in adjacent bands which are close to each other. A rigorous numerical analysis improves the upper bound to $0.90367$.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5355\u4f4d\u6b63\u65b9\u5f62\u5185\u968f\u673a\u70b9\u6700\u4f18\u65c5\u884c\u5546\u8def\u5f84\u957f\u5ea6\u5e38\u6570\u03b2\u7684\u4e0a\u4e0b\u754c\uff0c\u6307\u51fa\u5f53\u524d\u4e0a\u754c0.90380\u7684\u6539\u8fdb\u6709\u9650\u81f3\u7ea60.88\uff0c\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u53ef\u5c06\u4e0a\u754c\u63d0\u5347\u81f3\u7ea60.85\uff0c\u4e25\u683c\u6570\u503c\u5206\u6790\u5c06\u4e0a\u754c\u6539\u8fdb\u52300.90367\u3002", "motivation": "\u8fdb\u4e00\u6b65\u6539\u8fdb\u5355\u4f4d\u6b63\u65b9\u5f62\u4e2d\u968f\u673a\u70b9\u6700\u4f18\u65c5\u884c\u5546\u8def\u5f84\u957f\u5ea6\u5e38\u6570\u03b2\u7684\u4e0a\u754c\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u548c\u96c6\u4e2d\u5206\u6790\uff0c\u57fa\u4e8e\u4e4b\u524d\u7684\u5e26\u904d\u5386\u7b56\u7565\uff0c\u5141\u8bb8\u8def\u5f84\u8de8\u5e26\u5e76\u5229\u7528\u76f8\u90bb\u5e26\u4e2d\u76f8\u8fd1\u70b9\u5bf9\uff0c\u8fdb\u884c\u4e25\u683c\u6570\u503c\u5206\u6790\u3002", "result": "\u6307\u51fa\u5f53\u524d\u4e0a\u754c0.90380\u672a\u6765\u6539\u8fdb\u6709\u9650\u81f3\u7ea60.88\uff1b\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u53ef\u5c06\u4e0a\u754c\u63d0\u5347\u81f3\u7ea60.85\uff1b\u4e25\u683c\u5206\u6790\u5c06\u4e0a\u754c\u6539\u8fdb\u52300.90367\u3002", "conclusion": "\u5f53\u524d\u4e0a\u754c\u6539\u8fdb\u7a7a\u95f4\u6709\u9650\uff0c\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u4e25\u683c\u6570\u503c\u5206\u6790\u53ef\u6539\u8fdb\u4e0a\u754c\u3002"}}
{"id": "2602.11521", "pdf": "https://arxiv.org/pdf/2602.11521", "abs": "https://arxiv.org/abs/2602.11521", "authors": ["Lian Liu", "Shixin Zhao", "Yutian Zhou", "Yintao He", "Mengdi Wang", "Yinhe Han", "Ying Wang"], "title": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System", "categories": ["cs.AR", "cs.DC"], "comment": "15 pages, 13 figures", "summary": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.", "AI": {"tldr": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u4e2dKV\u64cd\u4f5c\u74f6\u9888\uff0c\u63d0\u51faPAM\u7cfb\u7edf\uff0c\u53ef\u63d0\u5347\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9ad8\u6548\u670d\u52a1\u7cfb\u7edf\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u5904\u7406KV\u76f8\u5173\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u3002", "method": "\u63d0\u51faProcessing Across Memory (PAM)\u7cfb\u7edf\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5c40\u90e8\u6027\u5206\u5e03KV\u4ee4\u724c\uff0c\u5f15\u5165PAMattention\u7b97\u6cd5\uff0c\u7ed3\u5408KV\u6620\u5c04\u3001\u8fc1\u79fb\u63a5\u53e3\u548c\u8c03\u5ea6\u7b97\u6cd5\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "PAM\u80fd\u540c\u65f6\u6ee1\u8db3\u5e26\u5bbd\u548c\u5bb9\u91cf\u9700\u6c42\u3002", "conclusion": "PAM\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5927\u89c4\u6a21AI\u65f6\u4ee3\u63d0\u4f9b\u4e86\u9ad8\u6027\u4ef7\u6bd4\u3001\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11671", "pdf": "https://arxiv.org/pdf/2602.11671", "abs": "https://arxiv.org/abs/2602.11671", "authors": ["Minh Le-Anh", "Huyen Nguyen", "Khanh An Tran", "Nam Le Hai", "Linh Ngo Van", "Nghi D. Q. Bui", "Bach Le"], "title": "Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond", "categories": ["cs.SE"], "comment": "Accepted to FSE 2026", "summary": "Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.", "AI": {"tldr": "\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u6548\u679c\u4e0d\u4f73\uff0c\u672c\u6587\u4ecb\u7ecdHydra\u6846\u67b6\uff0c\u80fd\u5904\u7406\u4ee3\u7801\u7ed3\u6784\u4e0e\u4f9d\u8d56\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u8fbe\u6700\u4f18\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u4efb\u52a1\u4e2d\uff0c\u56e0\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u6548\u679c\u53d8\u5dee\u3002", "method": "\u63d0\u51faHydra\u6846\u67b6\uff0c\u6709\u7ed3\u6784\u611f\u77e5\u7d22\u5f15\u7b56\u7565\u3001\u8f7b\u91cf\u7ea7\u4f9d\u8d56\u611f\u77e5\u68c0\u7d22\u5668\u53ca\u6df7\u5408\u68c0\u7d22\u673a\u5236\u3002", "result": "\u5728DevEval\u548cRepoExec\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHydra\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0cPass@1\u8d85\u6700\u5f3a\u57fa\u7ebf5%\u4ee5\u4e0a\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u8868\u73b0\u5f88\u51fa\u8272\u3002", "conclusion": "Hydra\u5728\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u4f18\u6c34\u5e73\u3002"}}
{"id": "2602.11354", "pdf": "https://arxiv.org/pdf/2602.11354", "abs": "https://arxiv.org/abs/2602.11354", "authors": ["Bang Nguyen", "Dominik So\u00f3s", "Qian Ma", "Rochana R. Obadage", "Zack Ranjan", "Sai Koneru", "Timothy M. Errington", "Shakhlo Nematova", "Sarah Rajtmajer", "Jian Wu", "Meng Jiang"], "title": "ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.", "AI": {"tldr": "\u63d0\u51faReplicatorBench\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30AI\u4ee3\u7406\u7814\u7a76\u590d\u5236\u80fd\u529b\uff0c\u5f00\u53d1ReplicatorAgent\u6846\u67b6\u5e76\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524dLLM\u4ee3\u7406\u68c0\u7d22\u8d44\u6e90\u6709\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8ba1\u7b97\u65b9\u9762\uff0c\u672a\u8003\u8651\u65b0\u6570\u636e\u53ef\u7528\u6027\u3001\u7f3a\u4e4f\u771f\u5b9e\u591a\u6837\u6027\u4e14\u53ea\u8bc4\u4f30\u7ed3\u679c\uff0c\u4e0d\u80fd\u8bc4\u4f30\u8bc6\u522b\u4e0d\u53ef\u590d\u5236\u7814\u7a76\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165ReplicatorBench\uff0c\u6db5\u76d6\u793e\u4f1a\u548c\u884c\u4e3a\u79d1\u5b66\u4e2d\u53ef\u590d\u5236\u548c\u4e0d\u53ef\u590d\u5236\u7814\u7a76\u58f0\u660e\uff1b\u5f00\u53d1ReplicatorAgent\u6846\u67b6\uff1b\u5728\u56db\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u8003\u8651\u7f16\u7a0b\u8bed\u8a00\u8bbe\u8ba1\u9009\u62e9\u548c\u4ee3\u7801\u8bbf\u95ee\u7ea7\u522b\u3002", "result": "\u5f53\u524dLLM\u4ee3\u7406\u80fd\u6709\u6548\u8bbe\u8ba1\u548c\u6267\u884c\u8ba1\u7b97\u5b9e\u9a8c\uff0c\u4f46\u5728\u68c0\u7d22\u590d\u5236\u6240\u9700\u8d44\u6e90\uff08\u5982\u65b0\u6570\u636e\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u57fa\u51c6\u53ef\u66f4\u597d\u8bc4\u4f30AI\u4ee3\u7406\u7814\u7a76\u590d\u5236\u80fd\u529b\uff0c\u6307\u51fa\u5f53\u524dLLM\u4ee3\u7406\u5728\u8d44\u6e90\u68c0\u7d22\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.11520", "pdf": "https://arxiv.org/pdf/2602.11520", "abs": "https://arxiv.org/abs/2602.11520", "authors": ["Yasin Khadem Charvadeh", "Katherine S. Panageas", "Yuan Chen"], "title": "Locally Interpretable Individualized Treatment Rules for Black-Box Decision Models", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Individualized treatment rules (ITRs) aim to optimize healthcare by tailoring treatment decisions to patient-specific characteristics. Existing methods typically rely on either interpretable but inflexible models or highly flexible black-box approaches that sacrifice interpretability; moreover, most impose a single global decision rule across patients. We introduce the Locally Interpretable Individualized Treatment Rule (LI-ITR) method, which combines flexible machine learning models to accurately learn complex treatment outcomes with locally interpretable approximations to construct subject-specific treatment rules. LI-ITR employs variational autoencoders to generate realistic local synthetic samples and learns individualized decision rules through a mixture of interpretable experts. Simulation studies show that LI-ITR accurately recovers true subject-specific local coefficients and optimal treatment strategies. An application to precision side-effect management in breast cancer illustrates the necessity of flexible predictive modeling and highlights the practical utility of LI-ITR in estimating optimal treatment rules while providing transparent, clinically interpretable explanations.", "AI": {"tldr": "\u63d0\u51faLocally Interpretable Individualized Treatment Rule (LI - ITR)\u65b9\u6cd5\u4f18\u5316\u533b\u7597\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5219\uff0c\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5219\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u53ef\u89e3\u91ca\u4f46\u4e0d\u7075\u6d3b\u6a21\u578b\u6216\u727a\u7272\u53ef\u89e3\u91ca\u6027\u7684\u9ed1\u76d2\u65b9\u6cd5\uff0c\u4e14\u591a\u91c7\u7528\u5355\u4e00\u5168\u5c40\u51b3\u7b56\u89c4\u5219\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7075\u6d3b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5c40\u90e8\u53ef\u89e3\u91ca\u8fd1\u4f3c\uff0c\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u5c40\u90e8\u5408\u6210\u6837\u672c\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u4e13\u5bb6\u6df7\u5408\u5b66\u4e60\u4e2a\u4f53\u51b3\u7b56\u89c4\u5219\u3002", "result": "\u6a21\u62df\u7814\u7a76\u663e\u793aLI - ITR\u80fd\u51c6\u786e\u6062\u590d\u771f\u5b9e\u4e3b\u4f53\u5c40\u90e8\u7cfb\u6570\u548c\u6700\u4f73\u6cbb\u7597\u7b56\u7565\uff1b\u4e73\u817a\u764c\u526f\u4f5c\u7528\u7cbe\u51c6\u7ba1\u7406\u5e94\u7528\u4f53\u73b0\u5176\u7075\u6d3b\u9884\u6d4b\u5efa\u6a21\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "LI - ITR\u5728\u4f30\u8ba1\u6700\u4f73\u6cbb\u7597\u89c4\u5219\u65f6\u80fd\u63d0\u4f9b\u900f\u660e\u3001\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.11204", "pdf": "https://arxiv.org/pdf/2602.11204", "abs": "https://arxiv.org/abs/2602.11204", "authors": ["Zhuxin Lei", "Ziyuan Yang", "Yi Zhang"], "title": "Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.", "AI": {"tldr": "\u63d0\u51faZero - Sacrifice Persistent - Robustness Adversarial Defense (ZePAD)\u9632\u5fa1\u65b9\u6cd5\u62b5\u5fa1\u4e0b\u6e38\u65e0\u5173\u5bf9\u6297\u6837\u672c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u5bf9\u6297\u5fae\u8c03\uff0c\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u826f\u6027\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9700\u66f4\u4e25\u683c\u9632\u5fa1\u76ee\u6807\u3002", "method": "\u63d0\u51faZePAD\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u5305\u62ecMPAE - Branch\u589e\u5f3a\u5bf9\u6297\u62b5\u6297\uff0cBMP - Branch\u4fdd\u8bc1\u826f\u6027\u6027\u80fd\uff0c\u901a\u8fc7\u8bc4\u4f30\u5206\u652f\u7f6e\u4fe1\u5ea6\u68c0\u6d4bDAEs\u3002", "result": "\u572811\u79cdSSL\u65b9\u6cd5\u548c6\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u826f\u6027\u6027\u80fd\u63d0\u534729.20%\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u534773.86%\u3002", "conclusion": "ZePAD\u80fd\u901a\u8fc7\u4e00\u6b21\u5bf9\u6297\u5fae\u8c03\u62b5\u5fa1\u4e0b\u6e38\u4efb\u52a1DAEs\uff0c\u5b9e\u73b0\u6301\u4e45\u9c81\u68d2\u6027\uff0c\u5177\u6709\u96f6\u727a\u7272\u7279\u6027\u3002"}}
{"id": "2602.11914", "pdf": "https://arxiv.org/pdf/2602.11914", "abs": "https://arxiv.org/abs/2602.11914", "authors": ["Hanbing Liu", "Ningyuan Li", "Weian Li", "Qi Qi", "Changyuan Yu"], "title": "Incentive Effects of a Cut-Off Score: Optimal Contest Design with Transparent Pre-Selection", "categories": ["cs.GT"], "comment": null, "summary": "Shortlisting is a common and effective method for pre-selecting participants in competitive settings. To ensure fairness, a cut-off score is typically announced, allowing only contestants who exceed it to enter the contest, while others are eliminated. In this paper, we study rank-order contests with shortlisting and cut-off score disclosure. We fully characterize the equilibrium behavior of shortlisted contestants for any given prize structure and shortlist size. We examine two objective functions: the highest individual performance and total performance. For both objectives, the optimal contest is in a winner-take-all format. For the highest individual performance, the optimal shortlist size is exactly two contestants, but, in contrast, for total performance, the shortlist size does not affect the outcome, i.e., any size yields the same total performance. Furthermore, we compare the highest individual performance achieved with and without shortlisting, and show that the former is 4/3 times greater than the latter.", "AI": {"tldr": "\u7814\u7a76\u5e26\u5165\u56f4\u7b5b\u9009\u548c\u5206\u6570\u7ebf\u516c\u5e03\u7684\u6392\u540d\u8d5b\uff0c\u5206\u6790\u5165\u56f4\u9009\u624b\u5747\u8861\u884c\u4e3a\uff0c\u5f97\u51fa\u6700\u4f18\u7ade\u8d5b\u683c\u5f0f\u3001\u5165\u56f4\u89c4\u6a21\u53ca\u6709\u65e0\u5165\u56f4\u7b5b\u9009\u4e0b\u6700\u9ad8\u4e2a\u4eba\u8868\u73b0\u5bf9\u6bd4\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u5e26\u5165\u56f4\u7b5b\u9009\u548c\u5206\u6570\u7ebf\u516c\u5e03\u7684\u6392\u540d\u8d5b\uff0c\u4ee5\u660e\u786e\u4e0d\u540c\u76ee\u6807\u4e0b\u7684\u6700\u4f18\u7ade\u8d5b\u8bbe\u7f6e\u3002", "method": "\u5bf9\u7ed9\u5b9a\u5956\u54c1\u7ed3\u6784\u548c\u5165\u56f4\u89c4\u6a21\uff0c\u5168\u9762\u523b\u753b\u5165\u56f4\u9009\u624b\u7684\u5747\u8861\u884c\u4e3a\uff0c\u8003\u5bdf\u6700\u9ad8\u4e2a\u4eba\u8868\u73b0\u548c\u603b\u8868\u73b0\u4e24\u4e2a\u76ee\u6807\u51fd\u6570\u3002", "result": "\u4e24\u4e2a\u76ee\u6807\u4e0b\u6700\u4f18\u7ade\u8d5b\u90fd\u662f\u8d62\u5bb6\u901a\u5403\u683c\u5f0f\uff1b\u6700\u9ad8\u4e2a\u4eba\u8868\u73b0\u76ee\u6807\u4e0b\u6700\u4f18\u5165\u56f4\u89c4\u6a21\u4e3a 2 \u4eba\uff0c\u603b\u8868\u73b0\u76ee\u6807\u4e0b\u5165\u56f4\u89c4\u6a21\u4e0d\u5f71\u54cd\u7ed3\u679c\uff1b\u6709\u5165\u56f4\u7b5b\u9009\u7684\u6700\u9ad8\u4e2a\u4eba\u8868\u73b0\u662f\u65e0\u5165\u56f4\u7b5b\u9009\u7684 4/3 \u500d\u3002", "conclusion": "\u786e\u5b9a\u4e86\u4e0d\u540c\u76ee\u6807\u4e0b\u6392\u540d\u8d5b\u7684\u6700\u4f18\u7ed3\u6784\u548c\u5165\u56f4\u89c4\u6a21\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u5165\u56f4\u7b5b\u9009\u5bf9\u63d0\u9ad8\u6700\u9ad8\u4e2a\u4eba\u8868\u73b0\u6709\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2602.11836", "pdf": "https://arxiv.org/pdf/2602.11836", "abs": "https://arxiv.org/abs/2602.11836", "authors": ["Alishbah Bashir", "Fatima Qaiser", "Ijaz Hussain"], "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.", "AI": {"tldr": "\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6709\u6548\u8bed\u4e49\u5185\u5bb9\u63a8\u8350\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u63d0\u51faULTRA\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u63d0\u5347\u63a8\u8350\u76f8\u5173\u6027\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u73b0\u6709\u7684\u5185\u5bb9\u63a8\u8350\u65b9\u6cd5\u4f9d\u8d56\u8bcd\u6c47\u5339\u914d\u6216\u901a\u7528\u6280\u672f\uff0c\u96be\u4ee5\u6355\u6349\u8bed\u4e49\u610f\u56fe\uff0c\u5bfc\u81f4\u63a8\u8350\u7684\u76f8\u5173\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faULTRA\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5d4c\u5165\u67b6\u6784\u548c\u67e5\u8be2\u957f\u5ea6\u611f\u77e5\u8def\u7531\u673a\u5236\uff0c\u6839\u636e\u9608\u503c\u5c06\u7528\u6237\u67e5\u8be2\u8def\u7531\u5230\u4e13\u95e8\u7684\u8bed\u4e49\u7ba1\u9053\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u5d4c\u5165\u548c\u4f18\u5316\u6c60\u5316\u7b56\u7565\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u76f8\u4f3c\u6027\u641c\u7d22\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4e4c\u5c14\u90fd\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u4e0d\u540c\u67e5\u8be2\u7c7b\u578b\u4e0b\u6301\u7eed\u63d0\u9ad8\u63a8\u8350\u76f8\u5173\u6027\uff0c\u4e0e\u5355\u7ba1\u9053\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u523090%\u4ee5\u4e0a\u3002", "conclusion": "ULTRA\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u63a8\u5e7f\u7684\u5185\u5bb9\u63a8\u8350\u67b6\u6784\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u89c1\u89e3\u3002"}}
{"id": "2602.11655", "pdf": "https://arxiv.org/pdf/2602.11655", "abs": "https://arxiv.org/abs/2602.11655", "authors": ["Christian Rondanini", "Barbara Carminati", "Elena Ferrari", "Niccol\u00f2 Lardo", "Ashish Kundu"], "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection", "categories": ["cs.CR", "cs.AI", "cs.DC"], "comment": null, "summary": "The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u8fb9\u7f18\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u8fde\u7eed\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u672c\u5730\u9002\u5e94\u4e0e\u5168\u5c40\u77e5\u8bc6\u5171\u4eab\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u51c6\u786e\u7387\u63d0\u5347\u4e14\u9002\u914d\u8fb9\u7f18\u8bbe\u5907", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u5b9e\u65f6\u68c0\u6d4b\u6076\u610f\u8f6f\u4ef6\uff0c\u73b0\u6709\u9759\u6001\u3001\u96c6\u4e2d\u91cd\u8bad\u7ec3\u53ca\u672c\u5730\u8bad\u7ec3\u6a21\u578b\u6709\u5c40\u9650", "method": "\u63d0\u51fa\u7ed3\u5408\u672c\u5730\u9002\u5e94\u4e0e\u5168\u5c40\u77e5\u8bc6\u5171\u4eab\u7684\u8fde\u7eed\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u8f7b\u91cf\u7ea7\u53d8\u538b\u5668\u6a21\u578b\u5728\u8fb9\u7f18\u8282\u70b9\u5fae\u8c03\uff0c\u805a\u5408\u5e76\u91cd\u65b0\u5206\u53d1LoRA\u6a21\u5757", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u57fa\u4e8eLoRA\u4ea4\u6362\u5728\u9047\u5230\u672a\u77e5\u653b\u51fb\u65f6\u51c6\u786e\u7387\u63d0\u534720 - 25%\uff0c\u635f\u5931\u548cF1\u7a33\u5b9a\uff0cLoRA\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u4e0d\u52301%", "conclusion": "\u8be5\u67b6\u6784\u80fd\u5b9e\u73b0\u8de8\u8bbe\u5907\u6cdb\u5316\u4e14\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u786c\u4ef6"}}
{"id": "2602.11692", "pdf": "https://arxiv.org/pdf/2602.11692", "abs": "https://arxiv.org/abs/2602.11692", "authors": ["Shashiwadana Nirmani", "Hourieh Khalajzadeh", "Mojtaba Shahin", "Xiao Liu"], "title": "Beyond Code: Empirical Insights into How Team Dynamics Influence OSS Project Selection", "categories": ["cs.SE"], "comment": null, "summary": "Open-source software (OSS) development relies on effective collaboration among distributed contributors. Yet, current OSS project recommendation systems primarily emphasize technical attributes, overlooking the collaboration and community aspects that influence contributors' decisions to join and remain in projects. This study investigates how team dynamics within OSS communities influence project selection and how these preferences vary across contributors' motivations. We conducted an online survey with 198 OSS practitioners, combining quantitative and qualitative analyses to capture contributors' perceptions of team dynamics. The results reveal that communication-related team dynamics such as responsiveness, tone, and clarity of replies are consistently prioritized across practitioners. However, the relative importance of these team dynamics differs according to contributors' motivations. For instance, practitioners motivated by gaining reputation or networking preferred inclusive project communities that encouraged diverse participation. These findings highlight that understanding how team dynamics align with contributors' motivations provides valuable insights into practitioners' project selection behaviour. Those insights can inform the design of future human-aware project recommendation systems that better account for social collaboration quality and motivational fit.", "AI": {"tldr": "\u73b0\u6709OSS\u9879\u76ee\u63a8\u8350\u7cfb\u7edf\u5ffd\u89c6\u534f\u4f5c\u4e0e\u793e\u533a\u56e0\u7d20\uff0c\u7814\u7a76\u8c03\u67e5\u56e2\u961f\u52a8\u6001\u5bf9\u9879\u76ee\u9009\u62e9\u7684\u5f71\u54cd\u53ca\u4e0d\u540c\u52a8\u673a\u8d21\u732e\u8005\u7684\u504f\u597d\u5dee\u5f02\uff0c\u7ed3\u679c\u8868\u660e\u6c9f\u901a\u76f8\u5173\u56e2\u961f\u52a8\u6001\u53d7\u91cd\u89c6\u4e14\u91cd\u8981\u6027\u56e0\u52a8\u673a\u800c\u5f02\uff0c\u7814\u7a76\u6210\u679c\u53ef\u7528\u4e8e\u8bbe\u8ba1\u66f4\u4f18\u63a8\u8350\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dOSS\u9879\u76ee\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u5c5e\u6027\uff0c\u5ffd\u7565\u534f\u4f5c\u548c\u793e\u533a\u65b9\u9762\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76OSS\u793e\u533a\u56e2\u961f\u52a8\u6001\u5bf9\u9879\u76ee\u9009\u62e9\u7684\u5f71\u54cd\u53ca\u4e0d\u540c\u52a8\u673a\u8d21\u732e\u8005\u7684\u504f\u597d\u5dee\u5f02\u3002", "method": "\u5bf9198\u540dOSS\u4ece\u4e1a\u8005\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u6765\u6355\u6349\u8d21\u732e\u8005\u5bf9\u56e2\u961f\u52a8\u6001\u7684\u770b\u6cd5\u3002", "result": "\u6c9f\u901a\u76f8\u5173\u56e2\u961f\u52a8\u6001\u53d7\u4ece\u4e1a\u8005\u4e00\u81f4\u91cd\u89c6\uff0c\u4f46\u91cd\u8981\u6027\u56e0\u8d21\u732e\u8005\u52a8\u673a\u800c\u5f02\uff0c\u5982\u8ffd\u6c42\u58f0\u8a89\u6216\u793e\u4ea4\u7684\u4ece\u4e1a\u8005\u504f\u597d\u9f13\u52b1\u591a\u5143\u53c2\u4e0e\u7684\u9879\u76ee\u793e\u533a\u3002", "conclusion": "\u4e86\u89e3\u56e2\u961f\u52a8\u6001\u4e0e\u8d21\u732e\u8005\u52a8\u673a\u7684\u5951\u5408\u5ea6\u80fd\u4e3a\u4ece\u4e1a\u8005\u9879\u76ee\u9009\u62e9\u884c\u4e3a\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u53ef\u7528\u4e8e\u8bbe\u8ba1\u66f4\u8003\u8651\u793e\u4f1a\u534f\u4f5c\u8d28\u91cf\u548c\u52a8\u673a\u5339\u914d\u7684\u9879\u76ee\u63a8\u8350\u7cfb\u7edf\u3002"}}
{"id": "2602.11389", "pdf": "https://arxiv.org/pdf/2602.11389", "abs": "https://arxiv.org/abs/2602.11389", "authors": ["Heejeong Nam", "Quentin Le Lidec", "Lucas Maes", "Yann LeCun", "Randall Balestriero"], "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions", "categories": ["cs.AI"], "comment": "Project Page: https://hazel-heejeong-nam.github.io/cjepa/", "summary": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.", "AI": {"tldr": "\u63d0\u51faC - JEPA\u5bf9\u8c61\u4e2d\u5fc3\u4e16\u754c\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u63a9\u7801\u8054\u5408\u5d4c\u5165\u9884\u6d4b\uff0c\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u4ee3\u7406\u63a7\u5236\u4efb\u52a1\u53d6\u5f97\u6210\u679c\uff0c\u5e76\u8bc1\u660e\u5176\u6709\u56e0\u679c\u5f52\u7eb3\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u8868\u793a\u4e0d\u8db3\u4ee5\u6355\u6349\u4f9d\u8d56\u4ea4\u4e92\u7684\u52a8\u6001\uff0c\u4e16\u754c\u6a21\u578b\u9700\u5f3a\u5927\u5173\u7cfb\u7406\u89e3\u3002", "method": "\u63d0\u51faC - JEPA\uff0c\u5e94\u7528\u5bf9\u8c61\u7ea7\u63a9\u7801\uff0c\u4ece\u5176\u4ed6\u5bf9\u8c61\u63a8\u65ad\u5bf9\u8c61\u72b6\u6001\uff0c\u8bf1\u5bfc\u6f5c\u5728\u5e72\u9884\u3002", "result": "\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u6301\u7eed\u63d0\u5347\uff0c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7edd\u5bf9\u63d0\u5347\u7ea620%\uff1b\u5728\u4ee3\u7406\u63a7\u5236\u4efb\u52a1\u4e2d\u4ec5\u7528\u57fa\u4e8e\u8865\u4e01\u4e16\u754c\u6a21\u578b\u6240\u97001%\u6f5c\u5728\u8f93\u5165\u7279\u5f81\u5c31\u6709\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "\u5bf9\u8c61\u7ea7\u63a9\u7801\u901a\u8fc7\u6f5c\u5728\u5e72\u9884\u8bf1\u5bfc\u56e0\u679c\u5f52\u7eb3\u504f\u5dee\u3002"}}
{"id": "2602.11747", "pdf": "https://arxiv.org/pdf/2602.11747", "abs": "https://arxiv.org/abs/2602.11747", "authors": ["Paul Liautaud", "Pierre Gaillard", "Olivier Wintenberger"], "title": "High-Probability Minimax Adaptive Estimation in Besov Spaces via Online-to-Batch", "categories": ["math.ST", "stat.ML"], "comment": null, "summary": "We study nonparametric regression over Besov spaces from noisy observations under sub-exponential noise, aiming to achieve minimax-optimal guarantees on the integrated squared error that hold with high probability and adapt to the unknown noise level. To this end, we propose a wavelet-based online learning algorithm that dynamically adjusts to the observed gradient noise by adaptively clipping it at an appropriate level, eliminating the need to tune parameters such as the noise variance or gradient bounds. As a by-product of our analysis, we derive high-probability adaptive regret bounds that scale with the $\\ell_1$-norm of the competitor. Finally, in the batch statistical setting, we obtain adaptive and minimax-optimal estimation rates for Besov spaces via a refined online-to-batch conversion. This approach carefully exploits the structure of the squared loss in combination with self-normalized concentration inequalities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11206", "pdf": "https://arxiv.org/pdf/2602.11206", "abs": "https://arxiv.org/abs/2602.11206", "authors": ["Jose Marie Antonio Mi\u00f1oza"], "title": "UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.RA", "q-bio.NC"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\\eps \\to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUltraLIF\u6846\u67b6\uff0c\u7528\u8d85\u79bb\u6563\u5316\u66ff\u4ee3\u4ee3\u7406\u68af\u5ea6\uff0c\u63a8\u5bfc\u4e24\u79cd\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7406\u8bba\u5206\u6790\u6709\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u4ee3\u7406\u68af\u5ea6\u57fa\u7ebf\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u56e0\u8109\u51b2\u751f\u6210\u4e0d\u53ef\u5fae\u4f9d\u8d56\u542f\u53d1\u5f0f\u4ee3\u7406\u68af\u5ea6\uff0c\u9700\u66f4\u597d\u65b9\u6cd5\u3002", "method": "\u5f15\u5165UltraLIF\u6846\u67b6\uff0c\u7528\u8d85\u79bb\u6563\u5316\u66ff\u4ee3\u4ee3\u7406\u68af\u5ea6\uff0c\u4ece\u4e0d\u540c\u52a8\u529b\u5b66\u7cfb\u7edf\u63a8\u5bfc\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u5206\u6790\u6709\u6536\u655b\u6027\u548c\u6709\u754c\u975e\u96f6\u68af\u5ea6\uff0c\u5b9e\u9a8c\u5728\u516d\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4ee3\u7406\u68af\u5ea6\u57fa\u7ebf\uff0c\u53ef\u9009\u7a00\u758f\u60e9\u7f5a\u53ef\u964d\u4f4e\u80fd\u8017\u3002", "conclusion": "UltraLIF\u6846\u67b6\u6709\u6548\uff0c\u80fd\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\uff0c\u5728\u5355\u65f6\u95f4\u6b65\u8bbe\u7f6e\u548c\u964d\u4f4e\u80fd\u8017\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2602.11959", "pdf": "https://arxiv.org/pdf/2602.11959", "abs": "https://arxiv.org/abs/2602.11959", "authors": ["Moshe Babaioff", "Yiding Feng", "Zihan Luo"], "title": "Strengthening Bulow-Klemperer-Style Results for Multi-Unit Auctions", "categories": ["cs.GT"], "comment": null, "summary": "The classic result of Bulow and Klemperer (1996) shows that in multi-unit auctions with $m$ units and $n\\geq m$ buyers whose values are sampled i.i.d. from a regular distribution, the revenue of the VCG auction with $m$ additional buyers is at least as large as the optimal revenue. Unfortunately, for regular distributions, adding $m$ additional buyers is sometimes indeed necessary, so the \"competition complexity\" of the VCG auction is $m$. We seek proving better competition complexity results in two dimensions.\n  First, under stronger distributional assumptions, the competition complexity of VCG auction drops dramatically. In balanced markets (where $m=n$) with MHR distributions, it is sufficient to only add $(e^{1/e} - 1 + o(1))n \\approx 0.4447n$ additional buyers to match the optimal revenue -- less than half the number that is necessary under regularity -- and this bound is asymptotically tight. We provide both exact finite-market results for small value of $n$, and closed-form asymptotic formulas for general market with any $m\\leq n$, and any target fraction of the optimal revenue.\n  Second, we analyze a supply-limiting variant of VCG auction that caps the number of units sold in a prior-independent way. Whenever the goal is to achieve almost the optimal revenue, this mechanism strictly improves upon standard VCG auction, requiring significantly fewer additional buyers.\n  Together, our results show that both stronger distributional assumptions, as well as a simple prior-independent refinement to the VCG auction, can each substantially reduce the number of additional buyers that is sufficient to achieve (near-)optimal revenue. Our analysis hinges on a unified worst-case reduction to truncated generalized Pareto distributions, enabling both numerical computation and analytical tractability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76VCG\u62cd\u5356\u7ade\u4e89\u590d\u6742\u5ea6\uff0c\u5728\u66f4\u5f3a\u5206\u5e03\u5047\u8bbe\u4e0b\u5176\u7ade\u4e89\u590d\u6742\u5ea6\u5927\u5e45\u4e0b\u964d\uff0c\u4e14\u63d0\u51fa\u4f9b\u5e94\u9650\u5236\u53d8\u4f53\u673a\u5236\u80fd\u51cf\u5c11\u989d\u5916\u4e70\u5bb6\u6570\u91cf\u4ee5\u5b9e\u73b0\u8fd1\u6700\u4f18\u6536\u76ca\u3002", "motivation": "\u6539\u8fdb\u7ecf\u5178\u7ed3\u679c\u4e2dVCG\u62cd\u5356\u7ade\u4e89\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u66f4\u597d\u7684\u7ade\u4e89\u590d\u6742\u5ea6\u7ed3\u679c\u3002", "method": "\u5728\u66f4\u5f3a\u5206\u5e03\u5047\u8bbe\uff08\u5982MHR\u5206\u5e03\uff09\u4e0b\u5206\u6790\uff0c\u63d0\u51fa\u4f9b\u5e94\u9650\u5236\u53d8\u4f53\u7684VCG\u62cd\u5356\u673a\u5236\uff0c\u7edf\u4e00\u5f52\u7ed3\u5230\u622a\u65ad\u5e7f\u4e49\u5e15\u7d2f\u6258\u5206\u5e03\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728MHR\u5206\u5e03\u5e73\u8861\u5e02\u573a\uff0c\u53ea\u9700\u6dfb\u52a0\u7ea60.4447n\u4e2a\u989d\u5916\u4e70\u5bb6\uff1b\u4f9b\u5e94\u9650\u5236\u53d8\u4f53\u673a\u5236\u4e25\u683c\u4f18\u4e8e\u6807\u51c6VCG\u62cd\u5356\u3002", "conclusion": "\u66f4\u5f3a\u5206\u5e03\u5047\u8bbe\u548c\u5bf9VCG\u62cd\u5356\u7684\u7b80\u5355\u6539\u8fdb\u53ef\u5927\u5e45\u51cf\u5c11\u5b9e\u73b0\u8fd1\u6700\u4f18\u6536\u76ca\u6240\u9700\u989d\u5916\u4e70\u5bb6\u6570\u91cf\u3002"}}
{"id": "2602.11841", "pdf": "https://arxiv.org/pdf/2602.11841", "abs": "https://arxiv.org/abs/2602.11841", "authors": ["Moncef Garouani", "Josiane Mothe"], "title": "Improving Neural Retrieval with Attribution-Guided Query Rewriting", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.", "AI": {"tldr": "\u63d0\u51fa\u5f52\u56e0\u5f15\u5bfc\u7684\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\uff0c\u5728BEIR\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u90e8\u5206\u89e3\u51b3\u795e\u7ecf\u68c0\u7d22\u5668\u7684\u8106\u6027\u95ee\u9898\uff0c\u5982LLM\u91cd\u5199\u67e5\u8be2\u65e0\u68c0\u7d22\u5668\u53cd\u9988\uff0c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7528\u4e8e\u4e8b\u540e\u5206\u6790\u3002", "method": "\u8ba1\u7b97\u57fa\u4e8e\u68af\u5ea6\u7684\u6807\u8bb0\u5f52\u56e0\uff0c\u5c06\u5206\u6570\u4f5c\u4e3a\u8f6f\u6307\u5bfc\u7528\u4e8e\u7ed3\u6784\u5316\u63d0\u793a\u6765\u5f15\u5bfcLLM\u91cd\u5199\u67e5\u8be2\u3002", "result": "\u5728BEIR\u96c6\u5408\u4e0a\uff0c\u91cd\u5199\u67e5\u8be2\u5728\u5f3a\u57fa\u7ebf\u57fa\u7840\u4e0a\u6301\u7eed\u63d0\u9ad8\u68c0\u7d22\u6548\u679c\uff0c\u5bf9\u9690\u5f0f\u6216\u6a21\u7cca\u4fe1\u606f\u9700\u6c42\u63d0\u5347\u66f4\u5927\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f52\u56e0\u5f15\u5bfc\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u6539\u5584\u795e\u7ecf\u68c0\u7d22\u5668\u7684\u8106\u6027\u95ee\u9898\u3002"}}
{"id": "2602.11382", "pdf": "https://arxiv.org/pdf/2602.11382", "abs": "https://arxiv.org/abs/2602.11382", "authors": ["M. Szusterman"], "title": "Markovian protocols and an upper bound on the extension complexity of the matching polytope", "categories": ["cs.DM", "cs.DS"], "comment": "21 pages (of which 10 page appendix), 2 figures", "summary": "This paper investigates the extension complexity of polytopes by exploiting the correspondence between non-negative factorizations of slack matrices and randomized communication protocols. We introduce a geometric characterization of extension complexity based on the width of Markovian protocols, as a variant of the framework introduced by Faenza et al. This enables us to derive a new upper bound of $\\tilde{O}(n^3\\cdot 1.5^n)$ for the extension complexity of the matching polytope $P_{\\text{match}}(n)$, improving upon the standard $2^n$-bound given by Edmonds' description. Additionally, we recover Goemans' compact formulation for the permutahedron using a one-round protocol based on sorting networks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11688", "pdf": "https://arxiv.org/pdf/2602.11688", "abs": "https://arxiv.org/abs/2602.11688", "authors": ["Alessio Ricci Toniolo", "Abinaya Dinesh", "Rome Thorstenson"], "title": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing", "categories": ["cs.NI", "cs.DC"], "comment": "12 pages, 4 figures. Code: https://github.com/atoniolo76/gotoni/tree/benchmark-load-balancing", "summary": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.", "AI": {"tldr": "\u63d0\u51faGORGO\u65b9\u6cd5\u4f18\u5316\u591a\u533a\u57dfLLM\u63a8\u7406\u7684TTFT\uff0c\u7ecf\u4e0e\u4e09\u4e2a\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u8bc1\u660eGORGO\u53ef\u964d\u4f4eP99 TTFT\uff0cGORGO - proxy\u5728TTFT\u4e2d\u4f4d\u6570\u4e0a\u5feb2.5\u500d\u3002", "motivation": "\u73b0\u6709\u591a\u533a\u57df\u8d1f\u8f7d\u5747\u8861\u5668\u5728\u8def\u7531\u51b3\u7b56\u65f6\u5ffd\u7565\u96c6\u7fa4\u7f51\u7edc\u5ef6\u8fdf\uff0c\u800c\u5206\u5e03\u5f0fLLM\u63a8\u7406\u9700\u4f18\u5316TTFT\u3002", "method": "\u5f15\u5165GORGO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u603b\u670d\u52a1\u6210\u672c\uff08\u8003\u8651\u53ef\u7528\u8ba1\u7b97\u3001\u7f51\u7edc\u5ef6\u8fdf\u548c\u524d\u7f00\u7f13\u5b58\uff09\u6765\u6700\u5c0f\u5316TTFT\uff0c\u540c\u65f6\u4e0e\u4e09\u4e2a\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "GORGO\u901a\u8fc7\u7f51\u7edc\u611f\u77e5\u8def\u7531\u964d\u4f4eP99 TTFT\uff0c\u9632\u6b62\u8de8\u533a\u57df\u8f6c\u53d1\u95ee\u9898\u63d0\u9ad8\u5e73\u5747TTFT\uff0cGORGO - proxy\u514b\u670d\u540c\u6b65\u5f00\u9500\uff0cTTFT\u4e2d\u4f4d\u6570\u5feb2.5\u500d\u3002", "conclusion": "GORGO\u65b9\u6cd5\u6709\u6548\uff0c\u96c6\u4e2d\u5f0f\u8def\u7531\u5668GORGO - proxy\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.11724", "pdf": "https://arxiv.org/pdf/2602.11724", "abs": "https://arxiv.org/abs/2602.11724", "authors": ["Xiwen Teoh", "Yun Lin", "Duc-Minh Nguyen", "Ruofei Ren", "Wenjie Zhang", "Jin Song Dong"], "title": "WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements", "categories": ["cs.SE"], "comment": null, "summary": "Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.\n  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.", "AI": {"tldr": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u7aef\u5230\u7aef\u7f51\u9875\u6d4b\u8bd5\u4e2d\u5b58\u5728\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faWebTestPilot\u89e3\u51b3\u6311\u6218\uff0c\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7528\u4e8e\u7aef\u5230\u7aef\u7f51\u9875\u6d4b\u8bd5\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u96be\u4ee5\u533a\u5206\u662f\u5e7b\u89c9\u8fd8\u662f\u771f\u5b9e\u5e94\u7528\u7a0b\u5e8f\u9519\u8bef\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u9690\u5f0f\u9884\u8a00\u673a\u3002", "method": "\u5f15\u5165WebTestPilot\uff0c\u4f7f\u7528\u7b26\u53f7\u5316\u5c42\u5c06\u5173\u952eGUI\u5143\u7d20\u7b26\u53f7\u5316\uff0c\u5e76\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u6362\u4e3a\u5e26\u9884\u6761\u4ef6\u548c\u540e\u6761\u4ef6\u7684\u6b65\u9aa4\u5e8f\u5217\u4f5c\u4e3a\u9884\u8a00\u673a\uff0c\u540c\u65f6\u6784\u5efa\u542b\u6f0f\u6d1e\u7f51\u9875\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "WebTestPilot\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe99%\uff0c\u6f0f\u6d1e\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5747\u4e3a96%\uff0c\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\u3002", "conclusion": "WebTestPilot\u89e3\u51b3\u4e86\u7f51\u9875\u6d4b\u8bd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u80fd\u5904\u7406\u591a\u6837\u5316\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u548c\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2602.11408", "pdf": "https://arxiv.org/pdf/2602.11408", "abs": "https://arxiv.org/abs/2602.11408", "authors": ["Michael Menezes", "Anastasios Kyrillidis"], "title": "GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation", "categories": ["cs.AI", "cs.LG", "eess.SY"], "comment": "16 pages, 7 figures", "summary": "While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.", "AI": {"tldr": "\u63d0\u51faGHOST\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\u89e3\u51b3Mamba2\u63a8\u7406\u5f00\u9500\u5927\u95ee\u9898\uff0c\u5728\u591a\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u964d\u7ef4\u4e14\u589e\u52a0\u5c11\u91cf\u56f0\u60d1\u5ea6\u3002", "motivation": "Mamba2\u6269\u5c55\u72b6\u6001\u7ef4\u5ea6\u63d0\u5347\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u81ea\u56de\u5f52\u751f\u6210\u65f6\u63a8\u7406\u5f00\u9500\u5927\uff0c\u6807\u51c6\u526a\u679d\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u6b64\u74f6\u9888\u3002", "method": "\u5f15\u5165GHOST\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u4ec5\u7528\u524d\u5411\u4f20\u64ad\u7edf\u8ba1\u8fd1\u4f3c\u63a7\u5236\u7406\u8bba\u5e73\u8861\u622a\u65ad\uff0c\u8054\u5408\u6d4b\u91cf\u53ef\u63a7\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u3002", "result": "\u5728130M\u52302.7B\u53c2\u6570\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b050%\u72b6\u6001\u7ef4\u5ea6\u964d\u4f4e\uff0cWikiText - 2\u4e0a\u56f0\u60d1\u5ea6\u7ea6\u589e\u52a01\u70b9\u3002", "conclusion": "GHOST\u6846\u67b6\u5728\u4e0d\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u8fbe\u5230\u63a5\u8fd1\u57fa\u4e8e\u68af\u5ea6\u65b9\u6cd5\u7684\u4fdd\u771f\u5ea6\uff0c\u6709\u6548\u89e3\u51b3Mamba2\u63a8\u7406\u5f00\u9500\u95ee\u9898\u3002"}}
{"id": "2602.11208", "pdf": "https://arxiv.org/pdf/2602.11208", "abs": "https://arxiv.org/abs/2602.11208", "authors": ["Xin Ju", "Nok Hei", "Fung", "Yuyan Zhang", "Carl Jacquemyn", "Matthew Jackson", "Randolph Settgast", "Sally M. Benson", "Gege Wen"], "title": "Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems", "categories": ["cs.LG"], "comment": null, "summary": "The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \\textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.", "AI": {"tldr": "\u63d0\u51faAdaptive Physics Transformer (APT)\u89e3\u51b3\u5730\u7403\u5730\u4e0b\u7cfb\u7edf\u5168\u7269\u7406\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u67b6\u6784\uff0c\u53ef\u7528\u4e8e\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u5730\u4e0b\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u3002", "motivation": "\u5730\u7403\u5730\u4e0b\u7cfb\u7edf\u5168\u7269\u7406\u6570\u503c\u6a21\u62df\u56e0\u5730\u8d28\u5f02\u8d28\u6027\u3001\u9ad8\u5206\u8fa8\u7387\u8981\u6c42\u548c\u7269\u7406\u8fc7\u7a0b\u8026\u5408\u7b49\u95ee\u9898\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u3001\u7f51\u683c\u548c\u7269\u7406\u65e0\u5173\u7684\u795e\u7ecf\u7b97\u5b50APT\uff0c\u878d\u5408\u57fa\u4e8e\u56fe\u7684\u7f16\u7801\u5668\u548c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "APT\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7f51\u683c\u7684\u5730\u4e0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u8d85\u5206\u8fa8\u7387\u80fd\u529b\uff0c\u53ef\u4ece\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u6a21\u62df\u4e2d\u76f4\u63a5\u5b66\u4e60\uff0c\u80fd\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u3002", "conclusion": "APT\u662f\u5f00\u53d1\u5927\u89c4\u6a21\u5730\u4e0b\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u9aa8\u5e72\u67b6\u6784\u3002"}}
{"id": "2602.11967", "pdf": "https://arxiv.org/pdf/2602.11967", "abs": "https://arxiv.org/abs/2602.11967", "authors": ["Moshe Babaioff", "Sijin Chen", "Zhaohua Chen", "Yiding Feng"], "title": "Pareto-Efficient Multi-Buyer Mechanisms: Characterization, Fairness and Welfare", "categories": ["cs.GT"], "comment": null, "summary": "A truthful mechanism for a Bayesian single-item auction results with some ex-ante revenue for the seller, and some ex-ante total surplus for the buyers. We study the Pareto frontier of the set of seller-buyers ex-ante utilities, generated by all truthful mechanisms when buyers values are sampled independently and identically (i.i.d.). We first provide a complete structural characterization of the Pareto frontier under natural distributional assumptions. For example, when valuations are drawn i.i.d. from a distribution that is both regular and anti-MHR, every Pareto-optimal mechanism is a second-price auction with a reserve no larger than the monopoly reserve.\n  Building on this, we interpret the problem of picking a mechanism as a two-sided bargaining game, and analyze two canonical Pareto-optimal solutions from cooperative bargaining theory: the Kalai-Smorodinsky (KS) solution, and the Nash solution. We prove that when values are drawn i.i.d. from a distribution that is both regular and anti-MHR, in large markets both solutions yield near-optimal welfare. In contrast, under worst-case MHR distributions, their performance diverges sharply: the KS solution guarantees one-half of the optimal welfare, while the Nash solution might only achieve an arbitrarily small fraction of it. These results highlight the sensitivity of fairness-efficiency tradeoffs to distributional structure, and affirm the KS solution as the more robust notion of fairness for asymmetric two-sided markets.", "AI": {"tldr": "\u7814\u7a76\u8d1d\u53f6\u65af\u5355\u7269\u54c1\u62cd\u5356\u4e2d\u4e70\u5356\u53cc\u65b9\u4e8b\u524d\u6548\u7528\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5206\u6790\u4e24\u79cd\u5408\u4f5c\u8bae\u4ef7\u7406\u8bba\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\uff0c\u63ed\u793a\u516c\u5e73 - \u6548\u7387\u6743\u8861\u5bf9\u5206\u5e03\u7ed3\u6784\u7684\u654f\u611f\u6027\u3002", "motivation": "\u7814\u7a76\u6240\u6709\u771f\u5b9e\u673a\u5236\u4e0b\uff0c\u4e70\u5bb6\u4ef7\u503c\u72ec\u7acb\u540c\u5206\u5e03\u65f6\u4e70\u5356\u53cc\u65b9\u4e8b\u524d\u6548\u7528\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "method": "\u5148\u5bf9\u5e15\u7d2f\u6258\u524d\u6cbf\u8fdb\u884c\u5b8c\u6574\u7ed3\u6784\u523b\u753b\uff0c\u518d\u5c06\u673a\u5236\u9009\u62e9\u95ee\u9898\u89c6\u4e3a\u53cc\u8fb9\u8bae\u4ef7\u535a\u5f08\uff0c\u5206\u6790\u4e24\u79cd\u5408\u4f5c\u8bae\u4ef7\u7406\u8bba\u7684\u89e3\u3002", "result": "\u6b63\u5219\u4e14\u53cdMHR\u5206\u5e03\u4e0b\uff0c\u5927\u5e02\u573a\u4e2d\u4e24\u79cd\u89e3\u63a5\u8fd1\u6700\u4f18\u798f\u5229\uff1b\u6700\u574f\u60c5\u51b5MHR\u5206\u5e03\u4e0b\uff0cKS\u89e3\u4fdd\u8bc1\u4e00\u534a\u6700\u4f18\u798f\u5229\uff0cNash\u89e3\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u516c\u5e73 - \u6548\u7387\u6743\u8861\u5bf9\u5206\u5e03\u7ed3\u6784\u654f\u611f\uff0cKS\u89e3\u662f\u4e0d\u5bf9\u79f0\u53cc\u8fb9\u5e02\u573a\u66f4\u7a33\u5065\u7684\u516c\u5e73\u6982\u5ff5\u3002"}}
{"id": "2602.11874", "pdf": "https://arxiv.org/pdf/2602.11874", "abs": "https://arxiv.org/abs/2602.11874", "authors": ["Antoine Gauquier", "Ioana Manolescu", "Pierre Senellart"], "title": "Efficient Crawling for Scalable Web Data Acquisition (Extended Version)", "categories": ["cs.IR"], "comment": "Extended version of a paper published at the EDBT 2026 conference", "summary": "Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.", "AI": {"tldr": "\u4e3a\u63d0\u9ad8\u5f00\u653e\u7edf\u8ba1\u6570\u636e\u53ef\u8bbf\u95ee\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u805a\u7126\u7f51\u7edc\u722c\u866b\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u3002", "motivation": "\u65b0\u95fb\u4e8b\u5b9e\u6838\u67e5\u548c\u793e\u4f1a\u7ecf\u6d4e\u7814\u7a76\u9700\u5206\u6790\u9ad8\u8d28\u91cf\u7edf\u8ba1\u6570\u636e\u96c6\uff0c\u4f46\u5927\u89c4\u6a21\u68c0\u7d22\u56f0\u96be\uff0c\u4e3a\u63d0\u9ad8\u5f00\u653e\u7edf\u8ba1\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u5f00\u5c55\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08\u7761\u7720\u591a\u81c2\u8001\u864e\u673a\uff09\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1SB - CLASSIFIER\u722c\u866b\uff0c\u6839\u636e\u7f51\u9875\u4e2d\u94fe\u63a5\u8def\u5f84\u5b66\u4e60\u54ea\u4e9b\u8d85\u94fe\u63a5\u6307\u5411\u542b\u591a\u76ee\u6807\u7684\u9875\u9762\u3002", "result": "\u5728\u6570\u767e\u4e07\u7f51\u9875\u7684\u7f51\u7ad9\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u722c\u866b\u9ad8\u6548\uff0c\u53ea\u722c\u53d6\u5c0f\u90e8\u5206\u7f51\u7ad9\u5c31\u80fd\u83b7\u53d6\u9ad8\u6bd4\u4f8b\u76ee\u6807\u3002", "conclusion": "\u6240\u63d0\u805a\u7126\u7f51\u7edc\u722c\u866b\u7b97\u6cd5\u80fd\u9ad8\u6548\u83b7\u53d6\u76ee\u6807\uff0c\u63d0\u9ad8\u5f00\u653e\u7edf\u8ba1\u6570\u636e\u7684\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2602.11776", "pdf": "https://arxiv.org/pdf/2602.11776", "abs": "https://arxiv.org/abs/2602.11776", "authors": ["Cl\u00e1udio Correia", "Alberto E. A. Ferreira", "Lucas Martins", "Miguel P. Bento", "Sofia Guerreiro", "Ricardo Ribeiro Pereira", "Ana Sofia Gomes", "Jacopo Bono", "Hugo Ferreira", "Pedro Bizarro"], "title": "MUSE: Multi-Tenant Model Serving With Seamless Model Updates", "categories": ["cs.LG", "cs.DC"], "comment": "Currently under review for KDD 2026 (Applied Data Science)", "summary": "In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86MUSE\u6a21\u578b\u670d\u52a1\u6846\u67b6\uff0c\u53ef\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u5b9e\u73b0\u65e0\u7f1d\u6a21\u578b\u66f4\u65b0\uff0c\u964d\u4f4e\u6a21\u578b\u66f4\u65b0\u65f6\u95f4\uff0c\u8282\u7701\u6210\u672c\u3002", "motivation": "\u5728\u4e8c\u8fdb\u5236\u5206\u7c7b\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u91cd\u8bad\u7ec3\u4f1a\u4f7f\u5206\u6570\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u73b0\u6709\u9608\u503c\u5931\u6548\uff0c\u591a\u79df\u6237\u73af\u5883\u4e2d\u91cd\u65b0\u6821\u51c6\u9608\u503c\u5b58\u5728\u4e25\u91cd\u74f6\u9888\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u5f15\u5165MUSE\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u5206\u6570\u4e0e\u5ba2\u6237\u51b3\u7b56\u8fb9\u754c\u89e3\u8026\u5b9e\u73b0\u65e0\u7f1d\u66f4\u65b0\uff0c\u91c7\u7528\u52a8\u6001\u57fa\u4e8e\u610f\u56fe\u7684\u8def\u7531\u5171\u4eab\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u7ea7\u5206\u6570\u8f6c\u6362\u5c06\u6a21\u578b\u8f93\u51fa\u6620\u5c04\u5230\u7a33\u5b9a\u53c2\u8003\u5206\u5e03\u3002", "result": "Feedzai\u5927\u89c4\u6a21\u90e8\u7f72\u540e\uff0c\u6bcf\u79d2\u5904\u7406\u8d85\u5343\u4e2a\u4e8b\u4ef6\uff0c\u8fc7\u53bb12\u4e2a\u6708\u5904\u7406\u8d85550\u4ebf\u4e2a\u4e8b\u4ef6\uff0c\u8de8\u6570\u5341\u4e2a\u79df\u6237\uff0c\u4fdd\u6301\u9ad8\u53ef\u7528\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "MUSE\u5c06\u6a21\u578b\u524d\u7f6e\u65f6\u95f4\u4ece\u6570\u5468\u7f29\u77ed\u81f3\u6570\u5206\u949f\uff0c\u589e\u5f3a\u6a21\u578b\u6297\u653b\u51fb\u80fd\u529b\uff0c\u8282\u7701\u6570\u767e\u4e07\u7f8e\u5143\u6b3a\u8bc8\u635f\u5931\u548c\u8fd0\u8425\u6210\u672c\u3002"}}
{"id": "2602.11746", "pdf": "https://arxiv.org/pdf/2602.11746", "abs": "https://arxiv.org/abs/2602.11746", "authors": ["Nafiz Imtiaz Khan", "Vladimir Filkov"], "title": "Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability", "categories": ["cs.SE"], "comment": null, "summary": "When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.\n  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.\n  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.", "AI": {"tldr": "\u5229\u7528LLMs\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u6587\u732e\u4e2d\u6316\u6398\u53ef\u64cd\u4f5c\u5efa\u8bae\uff0c\u5c06\u5206\u6563\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u6307\u5bfc\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u53ef\u6301\u7eed\u6027\u7684\u7ed3\u6784\u5316\u884c\u52a8\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9884\u6d4b\u6a21\u578b\u65e0\u6cd5\u4e3a\u7ef4\u62a4\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5efa\u8bae\uff0c\u800c\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u79ef\u7d2f\u4e86\u5927\u91cf\u6539\u5584\u9879\u76ee\u5065\u5eb7\u7684\u5177\u4f53\u5b9e\u8df5\u8bc1\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u8bbe\u8ba1RAG - pipeline\u548c\u4e24\u5c42\u63d0\u793a\u7b56\u7565\uff0c\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u6587\u732e\u4e2d\u63d0\u53d6\u53ef\u64cd\u4f5c\u5efa\u8bae\uff08ReACTs\uff09\uff0c\u7b5b\u9009\u5e76\u7ec4\u7ec7\u8fd9\u4e9b\u5efa\u8bae\u3002", "result": "\u5f97\u52301922\u6761ReACTs\uff0c\u5176\u4e2d1312\u6761\u901a\u8fc7\u4e25\u683c\u8d28\u91cf\u6807\u51c6\uff0c\u53ef\u4e0e\u5de5\u5177\u4e2d\u7684\u9879\u76ee\u4fe1\u53f7\u5173\u8054\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u73b0\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5c06\u5206\u6563\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u7ed3\u6784\u5316\u884c\u52a8\uff0c\u6307\u5bfc\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u5b9e\u73b0\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2602.11409", "pdf": "https://arxiv.org/pdf/2602.11409", "abs": "https://arxiv.org/abs/2602.11409", "authors": ["Sina Tayebati", "Divake Kumar", "Nastaran Darabi", "Davide Ettori", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "title": "TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $\u03c4^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.", "AI": {"tldr": "\u5f15\u5165\u8f68\u8ff9\u7ea7\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cfTRACER\u7528\u4e8e\u4eba\u673a\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\uff0c\u80fd\u66f4\u597d\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee3\u7801\u548c\u57fa\u51c6\u5df2\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u4ee3\u7406\u805a\u7126\u5355\u8f6e\u6587\u672c\u751f\u6210\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8f68\u8ff9\u7ea7\u6545\u969c\u4fe1\u53f7\uff0c\u96be\u4ee5\u4f30\u8ba1\u4eba\u673a\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u4e2dAI\u667a\u80fd\u4f53\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165TRACER\uff0c\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u610f\u5916\u6027\u3001\u60c5\u5883\u611f\u77e5\u4fe1\u53f7\u7b49\uff0c\u5e76\u4f7f\u7528\u5c3e\u98ce\u9669\u51fd\u6570\u805a\u5408\u3002", "result": "\u5728$\u03c4^2$-bench\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7ebf\uff0cTRACER\u4f7fAUROC\u6700\u591a\u63d0\u9ad837.1%\uff0cAUARC\u6700\u591a\u63d0\u9ad855%\u3002", "conclusion": "TRACER\u80fd\u5728\u590d\u6742\u5bf9\u8bdd\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u66f4\u65e9\u3001\u66f4\u51c6\u786e\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2602.11920", "pdf": "https://arxiv.org/pdf/2602.11920", "abs": "https://arxiv.org/abs/2602.11920", "authors": ["Marco Bressan", "Nataly Brukhim", "Nicolo Cesa-Bianchi", "Emmanuel Esposito", "Yishay Mansour", "Shay Moran", "Maximilian Thiessen"], "title": "Learning Conditional Averages", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.", "AI": {"tldr": "\u672c\u6587\u5728PAC\u6846\u67b6\u4e0b\u5f15\u5165\u5b66\u4e60\u6761\u4ef6\u5747\u503c\u7684\u95ee\u9898\uff0c\u5c06PAC\u5b66\u4e60\u6269\u5c55\u5230\u591a\u4e2a\u9886\u57df\uff0c\u7ed9\u51fa\u6761\u4ef6\u5747\u503c\u53ef\u5b66\u4e60\u6027\u7684\u5b8c\u6574\u523b\u753b\u53ca\u6837\u672c\u590d\u6742\u5ea6\u754c\u3002", "motivation": "\u5c06\u6807\u51c6PAC\u5b66\u4e60\u6269\u5c55\u5230\u80fd\u6db5\u76d6\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b66\u4e60\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u5b66\u4e60\u6761\u4ef6\u5747\u503c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u4e24\u4e2a\u4e0e\u6982\u5ff5\u7c7b\u548c\u90bb\u57df\u7cfb\u7edf\u76f8\u5173\u7684\u7ec4\u5408\u53c2\u6570\u7684\u8054\u5408\u6709\u9650\u6027\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u5f97\u5230\u6761\u4ef6\u5747\u503c\u53ef\u5b66\u4e60\u6027\u7684\u5b8c\u6574\u523b\u753b\uff0c\u4ee5\u53ca\u5728\u5bf9\u6570\u56e0\u5b50\u8303\u56f4\u5185\u7684\u7d27\u6837\u672c\u590d\u6742\u5ea6\u754c\u3002", "conclusion": "\u5b66\u4e60\u6761\u4ef6\u5747\u503c\u95ee\u9898\u6269\u5c55\u4e86PAC\u5b66\u4e60\uff0c\u5176\u53ef\u5b66\u4e60\u6027\u53d6\u51b3\u4e8e\u4e24\u4e2a\u7ec4\u5408\u53c2\u6570\u7684\u8054\u5408\u6709\u9650\u6027\u3002"}}
{"id": "2602.11212", "pdf": "https://arxiv.org/pdf/2602.11212", "abs": "https://arxiv.org/abs/2602.11212", "authors": ["Yunchong Song", "Jushi Kai", "Liming Lu", "Kaixi Qiu", "Zhouhan Lin"], "title": "Towards Compressive and Scalable Recurrent Memory", "categories": ["cs.LG"], "comment": null, "summary": "Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \\textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.", "AI": {"tldr": "\u63d0\u51faElastic Memory\u5185\u5b58\u67b6\u6784\u7528\u4e8eTransformer\u5904\u7406\u957f\u6587\u672c\uff0c\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u4e14\u6027\u80fd\u66f4\u4f73\uff0c\u8bbe\u8ba1\u7075\u6d3b\u3002", "motivation": "\u89e3\u51b3Transformers\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u74f6\u9888\u4ee5\u53ca\u73b0\u6709\u5f15\u5165\u5faa\u73af\u5185\u5b58\u65b9\u6cd5\u5728\u7406\u8bba\u539f\u5219\u548c\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8eHiPPO\u6846\u67b6\u7684Elastic Memory\uff0c\u5c06\u5386\u53f2\u5e8f\u5217\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\u6837\u672c\uff0c\u7528\u6700\u4f18\u5728\u7ebf\u538b\u7f29\u7f16\u7801\u4e3a\u56fa\u5b9a\u5927\u5c0f\u5185\u5b58\u72b6\u6001\uff0c\u7528\u591a\u9879\u5f0f\u91c7\u6837\u673a\u5236\u68c0\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u957f\u4e0a\u4e0b\u6587(32k+)\u6570\u636e\u96c6\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u53c2\u6570\u76f8\u540c\u6216\u6a21\u578b\u6269\u5c55\u65f6\u8868\u73b0\u4f18\u79c0\uff0c\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "Elastic Memory\u6709\u6548\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u95ee\u9898\uff0c\u89e3\u8026\u8bbe\u8ba1\u53ef\u5728\u6d4b\u8bd5\u65f6\u6ce8\u5165\u5f52\u7eb3\u504f\u7f6e\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.12089", "pdf": "https://arxiv.org/pdf/2602.12089", "abs": "https://arxiv.org/abs/2602.12089", "authors": ["Kehang Zhu", "Lithium Thain", "Vivian Tsai", "James Wexler", "Crystal Qian"], "title": "Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation", "categories": ["cs.GT", "cs.AI", "cs.HC"], "comment": null, "summary": "As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \\textit{access to} a single LLM assistance modality: proactive recommendations from an \\textit{Advisor}, reactive feedback from a \\textit{Coach}, or autonomous execution by a \\textit{Delegate}; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the \\textit{Advisor} modality, participants achieve the highest mean individual gains with the \\textit{Delegate}, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in \\textit{access-to-delegate} treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the \\textit{Delegate} agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.", "AI": {"tldr": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u7814\u7a76AI\u8f85\u52a9\u6a21\u5f0f\u5728\u591a\u4eba\u8bae\u4ef7\u6e38\u620f\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0Delegate\u6a21\u5f0f\u6536\u76ca\u6700\u9ad8\u4f46\u7528\u6237\u504f\u597dAdvisor\u6a21\u5f0f\uff0c\u63ed\u793a\u4ee3\u7406\u80fd\u529b\u4e0e\u7fa4\u4f53\u798f\u5229\u95f4\u5dee\u8ddd\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u4e0e\u91c7\u7528\u517c\u5bb9\u7684\u4ea4\u4e92\u89c4\u5219\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u793e\u4ea4\u573a\u666f\u4f7f\u7528\u66f4\u666e\u904d\uff0c\u7406\u89e3\u4ee3\u7406 - \u7528\u6237\u4ea4\u4e92\u5bf9\u8bbe\u8ba1\u6539\u5584\u4e2a\u4eba\u548c\u7fa4\u4f53\u7ed3\u679c\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8fdb\u884c\u5728\u7ebf\u884c\u4e3a\u5b9e\u9a8c\uff08N = 243\uff09\uff0c\u53c2\u4e0e\u8005\u8fdb\u884c\u4e09\u8f6e\u4e09\u4eba\u4e00\u7ec4\u7684\u591a\u8f6e\u8bae\u4ef7\u6e38\u620f\uff0c\u968f\u673a\u5206\u914dAdvisor\u3001Coach\u3001Delegate\u4e09\u79cdAI\u8f85\u52a9\u6a21\u5f0f\uff0c\u53c2\u4e0e\u8005\u5728\u6bcf\u8f6e\u51b3\u5b9a\u624b\u52a8\u884c\u52a8\u6216\u4f7f\u7528AI\u3002", "result": "\u53c2\u4e0e\u8005\u867d\u504f\u597dAdvisor\u6a21\u5f0f\uff0c\u4f46Delegate\u6a21\u5f0f\u4e0b\u4e2a\u4eba\u5e73\u5747\u6536\u76ca\u6700\u9ad8\uff0c\u51fa\u73b0\u504f\u597d - \u7ee9\u6548\u4e0d\u4e00\u81f4\uff1bDelegate\u4ea7\u751f\u6b63\u5916\u90e8\u6027\uff0c\u975e\u91c7\u7528\u7528\u6237\u4e5f\u53d7\u76ca\uff1bDelegate\u50cf\u505a\u5e02\u5546\uff0c\u6ce8\u5165\u7406\u6027\u3001\u5e15\u7d2f\u6258\u6539\u8fdb\u63d0\u6848\u91cd\u6784\u4ea4\u6613\u73af\u5883\u3002", "conclusion": "\u4ee3\u7406\u80fd\u529b\u4e0e\u5b9e\u73b0\u7684\u7fa4\u4f53\u798f\u5229\u5b58\u5728\u5dee\u8ddd\uff0c\u81ea\u4e3b\u4ee3\u7406\u867d\u6709\u8d85\u4eba\u7c7b\u6218\u7565\u8868\u73b0\uff0c\u4f46\u798f\u5229\u589e\u76ca\u53d7\u754c\u9762\u3001\u7528\u6237\u8ba4\u77e5\u548c\u91c7\u7528\u969c\u788d\u9650\u5236\uff1b\u8f85\u52a9\u6a21\u5f0f\u5e94\u8bbe\u8ba1\u4e3a\u5185\u751f\u4ea7\u751f\u53c2\u4e0e\u7684\u673a\u5236\uff0c\u91c7\u7528\u517c\u5bb9\u7684\u4ea4\u4e92\u89c4\u5219\u662f\u81ea\u52a8\u5316\u8f85\u52a9\u6539\u5584\u4eba\u7c7b\u798f\u5229\u7684\u524d\u63d0\u3002"}}
{"id": "2602.11941", "pdf": "https://arxiv.org/pdf/2602.11941", "abs": "https://arxiv.org/abs/2602.11941", "authors": ["Benjamin Clavi\u00e9", "Atoof Shakir", "Jonah Turner", "Sean Lee", "Aamir Shakir", "Makoto P. Kato"], "title": "IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \\textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.", "AI": {"tldr": "\u73b0\u6709\u97f3\u4e50\u68c0\u7d22\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f15\u5165IncompeBench\u57fa\u51c6\u53ca\u516c\u5f00\u76f8\u5173\u6570\u636e\u96c6\u548c\u63d0\u793a\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b1574\u4e2a\u97f3\u4e50\u7247\u6bb5\u3001500\u4e2a\u67e5\u8be2\u548c\u8d85125000\u4e2a\u76f8\u5173\u6027\u5224\u65ad\u7684\u591a\u9636\u6bb5\u6807\u6ce8\u7ba1\u9053\u3002", "result": "\u521b\u5efa\u4e86\u9ad8\u4e00\u81f4\u6027\u7684\u6807\u6ce8\uff0c\u6570\u636e\u96c6\u548c\u63d0\u793a\u516c\u5f00\u3002", "conclusion": "IncompeBench\u53ef\u7528\u4e8e\u8bc4\u4f30\u97f3\u4e50\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2602.11476", "pdf": "https://arxiv.org/pdf/2602.11476", "abs": "https://arxiv.org/abs/2602.11476", "authors": ["R. Jay Martin"], "title": "Bounded Local Generator Classes for Deterministic State Evolution", "categories": ["cs.OS", "cs.DS"], "comment": "38 pages. Formal operator-class result", "summary": "We formalize a constructive subclass of locality-preserving deterministic operators acting on graph-indexed state systems. We define the class of Bounded Local Generator Classes (BLGC), consisting of finite-range generators operating on bounded state spaces under deterministic composition. Within this class, incremental update cost is independent of total system dimension. We prove that, under the BLGC assumptions, per-step operator work satisfies W_t = O(1) as the number of nodes M \\to \\infty, establishing a structural decoupling between global state size and incremental computational effort. The framework admits a Hilbert-space embedding in \\ell^2(V; \\mathbb{R}^d) and yields bounded operator norms on admissible subspaces. The result applies specifically to the defined subclass and does not claim universality beyond the stated locality and boundedness constraints.", "AI": {"tldr": "\u672c\u6587\u5f62\u5f0f\u5316\u4e86\u4f5c\u7528\u4e8e\u56fe\u7d22\u5f15\u72b6\u6001\u7cfb\u7edf\u7684\u5c40\u90e8\u4fdd\u6301\u786e\u5b9a\u6027\u7b97\u5b50\u7684\u6784\u9020\u6027\u5b50\u7c7b\uff0c\u5b9a\u4e49BLGC\u7c7b\uff0c\u8bc1\u660e\u5728\u8be5\u7c7b\u5047\u8bbe\u4e0b\u6bcf\u6b65\u7b97\u5b50\u5de5\u4f5c\u91cf\u7684\u60c5\u51b5\u5e76\u5f97\u5230\u76f8\u5173\u7ed3\u679c\u3002", "motivation": "\u5bf9\u4f5c\u7528\u4e8e\u56fe\u7d22\u5f15\u72b6\u6001\u7cfb\u7edf\u7684\u5c40\u90e8\u4fdd\u6301\u786e\u5b9a\u6027\u7b97\u5b50\u7684\u6784\u9020\u6027\u5b50\u7c7b\u8fdb\u884c\u5f62\u5f0f\u5316\u7814\u7a76\u3002", "method": "\u5b9a\u4e49Bounded Local Generator Classes (BLGC)\u7c7b\uff0c\u5728\u8be5\u7c7b\u5047\u8bbe\u4e0b\u8fdb\u884c\u5206\u6790\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u5728BLGC\u5047\u8bbe\u4e0b\uff0c\u5f53\u8282\u70b9\u6570\u8d8b\u4e8e\u65e0\u7a77\u65f6\uff0c\u6bcf\u6b65\u7b97\u5b50\u5de5\u4f5c\u91cf\u6ee1\u8db3W_t = O(1)\uff0c\u6846\u67b6\u53ef\u8fdb\u884c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5d4c\u5165\u5e76\u5728\u5141\u8bb8\u5b50\u7a7a\u95f4\u4e0a\u6709\u6709\u754c\u7b97\u5b50\u8303\u6570\u3002", "conclusion": "\u7ed3\u679c\u9002\u7528\u4e8e\u5b9a\u4e49\u7684\u5b50\u7c7b\uff0c\u5728\u6240\u8ff0\u5c40\u90e8\u6027\u548c\u6709\u754c\u6027\u7ea6\u675f\u4e4b\u5916\u4e0d\u5177\u6709\u666e\u904d\u6027\u3002"}}
{"id": "2602.12029", "pdf": "https://arxiv.org/pdf/2602.12029", "abs": "https://arxiv.org/abs/2602.12029", "authors": ["Sunghyeon Woo", "Hoseung Kim", "Sunghwan Shim", "Minjung Jo", "Hyunjoon Jeong", "Jeongtae Lee", "Joonghoon Kim", "Sungjae Lee", "Baeseong Park", "Se Jung Kwon", "Dongsoo Lee"], "title": "PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving", "categories": ["cs.LG", "cs.DC"], "comment": "Preprint. 13 pages, 6 figures", "summary": "Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.", "AI": {"tldr": "\u63d0\u51fa PrefillShare \u7b97\u6cd5\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u591a\u8bed\u8a00\u6a21\u578b\u6267\u884c\u65f6\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5728\u591a\u6a21\u578b\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u964d\u4f4e\u5ef6\u8fdf\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u6267\u884c\u6a21\u5f0f\u5b58\u5728\u5197\u4f59\u8ba1\u7b97\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u589e\u52a0\u8d1f\u8f7d\u548c\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa PrefillShare \u7b97\u6cd5\uff0c\u5c06\u6a21\u578b\u5206\u89e3\u4e3a\u9884\u586b\u5145\u548c\u89e3\u7801\u6a21\u5757\uff0c\u51bb\u7ed3\u9884\u586b\u5145\u6a21\u5757\uff0c\u4ec5\u5fae\u8c03\u89e3\u7801\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u8def\u7531\u673a\u5236\u3002", "result": "PrefillShare \u5728\u5e7f\u6cdb\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8fbe\u5230\u5168\u5fae\u8c03\u7684\u51c6\u786e\u7387\uff0c\u5728\u591a\u6a21\u578b\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d p95 \u5ef6\u8fdf\u964d\u4f4e 4.5 \u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad8 3.9 \u500d\u3002", "conclusion": "PrefillShare \u80fd\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u591a\u8bed\u8a00\u6a21\u578b\u6267\u884c\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.11750", "pdf": "https://arxiv.org/pdf/2602.11750", "abs": "https://arxiv.org/abs/2602.11750", "authors": ["Jiazheng Sun", "Mingxuan Li", "Yingying Zhang", "Jiayang Niu", "Yachen Wu", "Ruihan Jin", "Shuyu Lei", "Pengrongrui Tan", "Zongyu Zhang", "Ruoyi Wang", "Jiachen Yang", "Boyu Yang", "Jiacheng Liu", "Xin Peng"], "title": "AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "21 pages, 7 figures", "summary": "Benchmarks are paramount for gauging progress in the domain of Mobile GUI Agents. In practical scenarios, users frequently fail to articulate precise directives containing full task details at the onset, and their expressions are typically ambiguous. Consequently, agents are required to converge on the user's true intent via active clarification and interaction during execution. However, existing benchmarks predominantly operate under the idealized assumption that user-issued instructions are complete and unequivocal. This paradigm focuses exclusively on assessing single-turn execution while overlooking the alignment capability of the agent. To address this limitation, we introduce AmbiBench, the first benchmark incorporating a taxonomy of instruction clarity to shift evaluation from unidirectional instruction following to bidirectional intent alignment. Grounded in Cognitive Gap theory, we propose a taxonomy of four clarity levels: Detailed, Standard, Incomplete, and Ambiguous. We construct a rigorous dataset of 240 ecologically valid tasks across 25 applications, subject to strict review protocols. Furthermore, targeting evaluation in dynamic environments, we develop MUSE (Mobile User Satisfaction Evaluator), an automated framework utilizing an MLLM-as-a-judge multi-agent architecture. MUSE performs fine-grained auditing across three dimensions: Outcome Effectiveness, Execution Quality, and Interaction Quality. Empirical results on AmbiBench reveal the performance boundaries of SoTA agents across different clarity levels, quantify the gains derived from active interaction, and validate the strong correlation between MUSE and human judgment. This work redefines evaluation standards, laying the foundation for next-generation agents capable of truly understanding user intent.", "AI": {"tldr": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u5b58\u5728\u5c40\u9650\uff0c\u672c\u6587\u5f15\u5165AmbiBench\u548cMUSE\u6765\u8bc4\u4f30\u4ee3\u7406\u80fd\u529b\uff0c\u91cd\u65b0\u5b9a\u4e49\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5047\u5b9a\u7528\u6237\u6307\u4ee4\u5b8c\u6574\u660e\u786e\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7406\u7684\u610f\u56fe\u5bf9\u9f50\u80fd\u529b\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u542b\u6307\u4ee4\u6e05\u6670\u5ea6\u5206\u7c7b\u7684AmbiBench\uff0c\u5206\u4e3a\u56db\u4e2a\u6e05\u6670\u5ea6\u7b49\u7ea7\uff0c\u6784\u5efa240\u4e2a\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u8fd8\u5f00\u53d1\u4e86\u7528MLLM-as-a-judge\u67b6\u6784\u7684MUSE\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u6e05\u6670\u5ea6\u4e0bSoTA\u4ee3\u7406\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u91cf\u5316\u4e86\u4e3b\u52a8\u4ea4\u4e92\u7684\u6536\u76ca\uff0c\u9a8c\u8bc1\u4e86MUSE\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u91cd\u65b0\u5b9a\u4e49\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u7406\u89e3\u7528\u6237\u610f\u56fe\u7684\u4e0b\u4e00\u4ee3\u4ee3\u7406\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.11437", "pdf": "https://arxiv.org/pdf/2602.11437", "abs": "https://arxiv.org/abs/2602.11437", "authors": ["Chengrui Qu", "Christopher Yeh", "Kishan Panaganti", "Eric Mazumdar", "Adam Wierman"], "title": "Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization", "categories": ["cs.AI", "cs.MA"], "comment": "ICLR 2026", "summary": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.", "AI": {"tldr": "\u63d0\u51faDistributionally robust IGM (DrIGM)\u539f\u5219\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u73af\u5883\u63d0\u5347\u5206\u5e03\u5916\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u573a\u666f\u56e0\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0d\u53ef\u9760\u3002", "method": "\u5f15\u5165DrIGM\u539f\u5219\uff0c\u63a8\u5bfc\u73b0\u6709\u4ef7\u503c\u5206\u89e3\u67b6\u6784\u7684DrIGM\u517c\u5bb9\u9c81\u68d2\u53d8\u4f53\uff0c\u8bad\u7ec3\u57fa\u4e8e\u9c81\u68d2Q\u76ee\u6807\u3002", "result": "\u5728\u9ad8\u4fdd\u771fSustainGym\u6a21\u62df\u5668\u548c\u661f\u9645\u4e89\u9738\u6e38\u620f\u73af\u5883\u4e2d\uff0c\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u5206\u5e03\u5916\u6027\u80fd\u3002", "conclusion": "DrIGM\u539f\u5219\u53ef\u884c\uff0c\u80fd\u4e3a\u7cfb\u7edf\u63d0\u4f9b\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u6709\u826f\u597d\u8868\u73b0\u3002"}}
{"id": "2602.11947", "pdf": "https://arxiv.org/pdf/2602.11947", "abs": "https://arxiv.org/abs/2602.11947", "authors": ["Apoorva Narula", "Santanu S. Dey", "Yao Xie"], "title": "Mixed-Integer Programming for Change-point Detection", "categories": ["math.OC", "stat.ML"], "comment": null, "summary": "We present a new mixed-integer programming (MIP) approach for offline multiple change-point detection by casting the problem as a globally optimal piecewise linear (PWL) fitting problem. Our main contribution is a family of strengthened MIP formulations whose linear programming (LP) relaxations admit integral projections onto the segment assignment variables, which encode the segment membership of each data point. This property yields provably tighter relaxations than existing formulations for offline multiple change-point detection. We further extend the framework to two settings of active research interest: (i) multidimensional PWL models with shared change-points, and (ii) sparse change-point detection, where only a subset of dimensions undergo structural change. Extensive computational experiments on benchmark real-world datasets demonstrate that the proposed formulations achieve reductions in solution times under both $\\ell_1$ and $\\ell_2$ loss functions in comparison to the state-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u79bb\u7ebf\u591a\u53d8\u5316\u70b9\u68c0\u6d4b\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\u65b9\u6cd5\uff0c\u52a0\u5f3a\u4e86MIP\u516c\u5f0f\uff0c\u6269\u5c55\u5230\u4e24\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6c42\u89e3\u65f6\u95f4\u4e0a\u6709\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u591a\u53d8\u5316\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u4f9b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u65b9\u6848\u3002", "method": "\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5168\u5c40\u6700\u4f18\u5206\u6bb5\u7ebf\u6027\u62df\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u52a0\u5f3a\u7684MIP\u516c\u5f0f\uff0c\u5e76\u6269\u5c55\u5230\u591a\u7ef4\u548c\u7a00\u758f\u53d8\u5316\u70b9\u68c0\u6d4b\u573a\u666f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u5728L1\u548cL2\u635f\u5931\u51fd\u6570\u4e0b\uff0c\u6240\u63d0\u516c\u5f0f\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u51cf\u5c11\u4e86\u89e3\u51b3\u65b9\u6848\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u79bb\u7ebf\u591a\u53d8\u5316\u70b9\u68c0\u6d4b\u65b9\u6cd5\u5728\u6c42\u89e3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u548c\u7a00\u758f\u53d8\u5316\u70b9\u68c0\u6d4b\u573a\u666f\u3002"}}
{"id": "2602.11215", "pdf": "https://arxiv.org/pdf/2602.11215", "abs": "https://arxiv.org/abs/2602.11215", "authors": ["Lintao Wang", "Zhuqiang Lu", "Yilin Zhu", "Kun Hu", "Zhenfei Yin", "Shixiang Tang", "Zhiyong Wang", "Wanli Ouyang", "Xinzhu Ma"], "title": "Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning", "categories": ["cs.LG"], "comment": null, "summary": "While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u5b66\u79d1\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u6784\u5efa\u4e94\u5b66\u79d1\u8bed\u6599\u5e93\u5206\u6790\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5b66\u4e60\u6a21\u5f0f\uff0c\u603b\u7ed3\u51fa\u56db\u6761\u7ecf\u9a8c\u6cd5\u5219\uff0c\u4e3a\u591a\u5b66\u79d1\u5fae\u8c03\u53ca\u901a\u7528\u79d1\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u5b66\u79d1\u5fae\u8c03\u8868\u73b0\u826f\u597d\uff0c\u4f46\u591a\u5b66\u79d1\u60c5\u5883\u4e0b\u5b66\u4e60\u52a8\u6001\u4ecd\u4e0d\u6e05\u695a\uff0c\u671f\u671b\u901a\u8fc7\u8de8\u9886\u57df\u77e5\u8bc6\u534f\u540c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u6027\u3002", "method": "\u6784\u5efa\u4e94\u5b66\u79d1\u8bed\u6599\u5e93\uff0c\u5206\u6790\u5168\u5fae\u8c03\u3001LoRA\u3001LoRA - MoE\u548cLoRA\u7ec4\u5408\u7684\u5b66\u4e60\u6a21\u5f0f\u3002", "result": "\u591a\u5b66\u79d1\u5b66\u4e60\u6bd4\u5355\u5b66\u79d1\u8bad\u7ec3\u66f4\u5177\u53ef\u53d8\u6027\uff0c\u603b\u7ed3\u51fa\u56db\u6761\u7ecf\u9a8c\u6cd5\u5219\uff0c\u5373Balance - then - Diversity\u3001Merge - then - Align\u3001Optimize - then - Scale\u3001Share - then - Specialize\u3002", "conclusion": "\u8fd9\u4e9b\u6cd5\u5219\u4e3a\u591a\u5b66\u79d1\u5fae\u8c03\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\uff0c\u4e3a\u5f00\u53d1\u53ef\u6cdb\u5316\u7684\u79d1\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u53ef\u884c\u6307\u5bfc\u3002"}}
{"id": "2602.12118", "pdf": "https://arxiv.org/pdf/2602.12118", "abs": "https://arxiv.org/abs/2602.12118", "authors": ["Johannes Brustle", "Paul Duetting", "Stefano Leonardi", "Tomasz Ponitka", "Matteo Russo"], "title": "Anonymous Contracts", "categories": ["cs.GT"], "comment": "37 pages, 2 figures", "summary": "We study a multi-agent contracting problem where agents exert costly effort to achieve individually observable binary outcomes. While the principal can theoretically extract the full social welfare using a discriminatory contract that tailors payments to individual costs, such contracts may be perceived as unfair. In this work, we introduce and analyze anonymous contracts, where payments depend solely on the total number of successes, ensuring identical treatment of agents.\n  We first establish that every anonymous contract admits a pure Nash equilibrium. However, because general anonymous contracts can suffer from multiple equilibria with unbounded gaps in principal utility, we identify uniform anonymous contracts as a desirable subclass. We prove that uniform anonymous contracts guarantee a unique equilibrium, thereby providing robust performance guarantees.\n  In terms of efficiency, we prove that under limited liability, anonymous contracts cannot generally approximate the social welfare better than a factor logarithmic in the spread of agent success probabilities. We show that uniform contracts are sufficient to match this theoretical limit. Finally, we demonstrate that removing limited liability significantly boosts performance: anonymous contracts generally achieve an $O(\\log n)$ approximation to the social welfare and, surprisingly, can extract the full welfare whenever agents' success probabilities are distinct. This reveals a structural reversal: widely spread probabilities are the hardest case under limited liability, whereas identical probabilities become the hardest case when limited liability is removed.", "AI": {"tldr": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5951\u7ea6\u95ee\u9898\uff0c\u5f15\u5165\u533f\u540d\u5951\u7ea6\uff0c\u5206\u6790\u5176\u5747\u8861\u3001\u6548\u7387\u7b49\u7279\u6027\uff0c\u63a2\u8ba8\u6709\u9650\u8d23\u4efb\u5bf9\u7ee9\u6548\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5dee\u522b\u5951\u7ea6\u53ef\u80fd\u88ab\u8ba4\u4e3a\u4e0d\u516c\u5e73\uff0c\u5f15\u5165\u533f\u540d\u5951\u7ea6\u786e\u4fdd\u5bf9\u667a\u80fd\u4f53\u4e00\u89c6\u540c\u4ec1\u3002", "method": "\u5148\u8bc1\u660e\u533f\u540d\u5951\u7ea6\u5b58\u5728\u7eaf\u7eb3\u4ec0\u5747\u8861\uff0c\u786e\u5b9a\u7edf\u4e00\u533f\u540d\u5951\u7ea6\u5b50\u7c7b\uff1b\u5206\u6790\u6548\u7387\u65f6\u5bf9\u6bd4\u6709\u9650\u8d23\u4efb\u548c\u65e0\u6709\u9650\u8d23\u4efb\u60c5\u51b5\u3002", "result": "\u7edf\u4e00\u533f\u540d\u5951\u7ea6\u4fdd\u8bc1\u552f\u4e00\u5747\u8861\uff1b\u6709\u9650\u8d23\u4efb\u4e0b\u533f\u540d\u5951\u7ea6\u8fd1\u4f3c\u793e\u4f1a\u798f\u5229\u6709\u5bf9\u6570\u56e0\u5b50\u9650\u5236\uff0c\u7edf\u4e00\u5951\u7ea6\u53ef\u8fbe\u5230\u6b64\u7406\u8bba\u6781\u9650\uff1b\u65e0\u6709\u9650\u8d23\u4efb\u65f6\u533f\u540d\u5951\u7ea6\u7ee9\u6548\u63d0\u5347\uff0c\u6982\u7387\u4e0d\u540c\u65f6\u53ef\u63d0\u53d6\u5168\u90e8\u798f\u5229\u3002", "conclusion": "\u6709\u9650\u8d23\u4efb\u5f71\u54cd\u533f\u540d\u5951\u7ea6\u7ee9\u6548\uff0c\u6982\u7387\u5206\u5e03\u5728\u4e0d\u540c\u8d23\u4efb\u60c5\u51b5\u4e0b\u5bf9\u7ee9\u6548\u7684\u5f71\u54cd\u6709\u7ed3\u6784\u53cd\u8f6c\u3002"}}
{"id": "2602.12041", "pdf": "https://arxiv.org/pdf/2602.12041", "abs": "https://arxiv.org/abs/2602.12041", "authors": ["Heng Yu", "Xiangjun Zhou", "Jie Xia", "Heng Zhao", "Anxin Wu", "Yu Zhao", "Dongying Kong"], "title": "Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems", "categories": ["cs.IR"], "comment": "11 pages, 3 figures", "summary": "Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.", "AI": {"tldr": "\u63d0\u51faMLCC\u53caMC - MLCC\u6a21\u578b\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6709\u826f\u597d\u6269\u5c55\u6027\uff0c\u5728\u7ebf\u6d4b\u8bd5\u9a8c\u8bc1\u6709\u6548\u6027\u5e76\u5728B\u7ad9\u5e7f\u544a\u7cfb\u7edf\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u6a21\u5757\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u4ea4\u4e92\u80fd\u529b\u3001\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u826f\u597d\u6269\u5c55\u6027\uff0c\u5728\u751f\u4ea7\u7ea6\u675f\u4e0b\u6a21\u578b\u6269\u5c55ROI\u6709\u9650\u3002", "method": "\u63d0\u51faMLCC\u7ed3\u6784\u5316\u7279\u5f81\u4ea4\u4e92\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u538b\u7f29\u548c\u52a8\u6001\u7ec4\u5408\u7ec4\u7ec7\u7279\u5f81\u4ea4\u53c9\uff1b\u5f15\u5165MC - MLCC\u591a\u901a\u9053\u6269\u5c55\uff0c\u5c06\u7279\u5f81\u4ea4\u4e92\u5206\u89e3\u5230\u5e76\u884c\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u964d\u4f4e\u53c2\u6570\u548cFLOPs\uff1b\u6269\u5c55\u6027\u5206\u6790\u663e\u793a\u57fa\u4e8e\u901a\u9053\u6269\u5c55\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u6709\u6548\uff0c\u80fd\u5728\u4e25\u683c\u5ef6\u8fdf\u548c\u8d44\u6e90\u7ea6\u675f\u4e0b\u5e94\u7528\uff0c\u5df2\u88abB\u7ad9\u5e7f\u544a\u7cfb\u7edf\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2602.12028", "pdf": "https://arxiv.org/pdf/2602.12028", "abs": "https://arxiv.org/abs/2602.12028", "authors": ["Althaf P", "Amit Chattopadhyay", "Osamu Saeki"], "title": "An Improved FPT Algorithm for Computing the Interleaving Distance between Merge Trees via Path-Preserving Maps", "categories": ["cs.CG", "cs.DS"], "comment": "42 pages", "summary": "A merge tree is a fundamental topological structure used to capture the sub-level set (and similarly, super-level set) topology in scalar data analysis. The interleaving distance is a theoretically sound, stable metric for comparing merge trees. However, computing this distance exactly is NP-hard. First fixed-parameter tractable (FPT) algorithm for it's exact computation introduces the concept of an $\\varepsilon$-good map between two merge trees, where $\\varepsilon$ is a candidate value for the interleaving distance. The complexity of their algorithm is $O(2^{2\u03c4}(2\u03c4)^{2\u03c4+2}\\cdot n^2\\log^3n)$ where $\u03c4$ is the degree-bound parameter and $n$ is the total number of nodes in both the merge trees. Their algorithm exhibits exponential complexity in $\u03c4$, which increases with the increasing value of $\\varepsilon$. In the current paper, we propose an improved FPT algorithm for computing the $\\varepsilon$-good map between two merge trees. Our algorithm introduces two new parameters, $\u03b7_f$ and $\u03b7_g$, corresponding to the numbers of leaf nodes in the merge trees $M_f$ and $M_g$, respectively. This parametrization is motivated by the observation that a merge tree can be decomposed into a collection of unique leaf-to-root paths. The proposed algorithm achieves a complexity of $O\\!\\left(n^2\\log n+\u03b7_g^{\u03b7_f}(\u03b7_f+\u03b7_g)\\, n \\log n \\right)$. To obtain this reduced complexity, we assume that number of possible $\\varepsilon$-good maps from $M_f$ to $M_g$ does not exceed that from $M_g$ to $M_f$. Notably, the parameters $\u03b7_f$ and $\u03b7_g$ are independent of the choice of $\\varepsilon$. Compared to their algorithm, our approach substantially reduces the search space for computing an optimal $\\varepsilon$-good map. We also provide a formal proof of correctness for the proposed algorithm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.12260", "pdf": "https://arxiv.org/pdf/2602.12260", "abs": "https://arxiv.org/abs/2602.12260", "authors": ["Oghenekaro Elem", "Nimrod Talmon"], "title": "Legitimate Overrides in Decentralized Protocols", "categories": ["cs.CR", "cs.CY", "cs.DC"], "comment": "38 pages, 8 figures", "summary": "Decentralized protocols claim immutable, rule-based execution, yet many embed emergency mechanisms such as chain-level freezes, protocol pauses, and account quarantines. These overrides are crucial for responding to exploits and systemic failures, but they expose a core tension: when does intervention preserve trust and when is it perceived as illegitimate discretion? With approximately $10$ billion in technical exploit losses potentially addressable by onchain intervention (2016--2026), the design of these mechanisms has high practical stakes, but current approaches remain ad hoc and ideologically charged. We address this gap by developing a Scope $\\times$ Authority taxonomy that maps the design space of emergency architectures along two dimensions: the precision of the intervention and the concentration of trigger authority. We formalize the resulting tradeoffs of a standing centralization cost versus containment speed and collateral disruption as a stochastic cost-minimization problem; and derive three testable predictions. Assessing these predictions against 705 documented exploit incidents, we find that containment time varies systematically by authority type; that losses follow a heavy-tailed distribution ($\u03b1\\approx 1.33$) concentrating risk in rare catastrophic events; and that community sentiment measurably modulates the effective cost of maintaining intervention capability. The analysis yields concrete design principles that move emergency governance from ideological debate towards quantitative engineering.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u53bb\u4e2d\u5fc3\u5316\u534f\u8bae\u4e2d\u7684\u7d27\u6025\u673a\u5236\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Scope \u00d7 Authority\u5206\u7c7b\u6cd5\uff0c\u8fdb\u884c\u6210\u672c\u6700\u5c0f\u5316\u5206\u6790\u5e76\u5f97\u51fa\u9884\u6d4b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u8bc4\u4f30\u540e\u7ed9\u51fa\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u534f\u8bae\u7684\u7d27\u6025\u673a\u5236\u867d\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4e14\u5b58\u5728\u4e89\u8bae\uff0c\u7ea6100\u4ebf\u7f8e\u5143\u7684\u6280\u672f\u6f0f\u6d1e\u635f\u5931\u9700\u6709\u6548\u673a\u5236\u5e94\u5bf9\u3002", "method": "\u5f00\u53d1Scope \u00d7 Authority\u5206\u7c7b\u6cd5\u7ed8\u5236\u7d27\u6025\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\uff1b\u5c06\u6743\u8861\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u968f\u673a\u6210\u672c\u6700\u5c0f\u5316\u95ee\u9898\u5e76\u5f97\u51fa\u9884\u6d4b\uff1b\u7ed3\u5408705\u4e2a\u6f0f\u6d1e\u4e8b\u4ef6\u8bc4\u4f30\u9884\u6d4b\u3002", "result": "\u53d1\u73b0\u904f\u5236\u65f6\u95f4\u968f\u89e6\u53d1\u6743\u5a01\u7c7b\u578b\u7cfb\u7edf\u53d8\u5316\uff1b\u635f\u5931\u5448\u91cd\u5c3e\u5206\u5e03\uff1b\u793e\u533a\u60c5\u7eea\u4f1a\u8c03\u8282\u7ef4\u6301\u5e72\u9884\u80fd\u529b\u7684\u6709\u6548\u6210\u672c\u3002", "conclusion": "\u5206\u6790\u5f97\u51fa\u5177\u4f53\u8bbe\u8ba1\u539f\u5219\uff0c\u63a8\u52a8\u7d27\u6025\u6cbb\u7406\u4ece\u610f\u8bc6\u5f62\u6001\u8fa9\u8bba\u8f6c\u5411\u5b9a\u91cf\u5de5\u7a0b\u3002"}}
{"id": "2602.11887", "pdf": "https://arxiv.org/pdf/2602.11887", "abs": "https://arxiv.org/abs/2602.11887", "authors": ["Javier Ron", "Martin Monperrus"], "title": "Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96f6\u77e5\u8bc6\u865a\u62df\u673a\uff08zkVM\uff09\u7684\u53ef\u9a8c\u8bc1\u6e90\u4ee3\u7801\u51fa\u5904\u65b0\u65b9\u6cd5\uff0c\u7ecf\u6d4b\u8bd5\u53ef\u5e94\u7528\u4e8e\u5b9e\u9645\u8f6f\u4ef6\u5e76\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u6e90\u4ee3\u7801\u51fa\u5904\u5728\u5b9e\u8df5\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6d41\u884c\u6280\u672f\u9700\u56f0\u96be\u7684\u6784\u5efa\u5de5\u5177\u94fe\u548c\u73af\u5883\u5339\u914d\u4e0e\u91cd\u65b0\u6267\u884c\u3002", "method": "\u5728zkVM\u5185\u6267\u884c\u7f16\u8bd1\u5668\uff0c\u751f\u6210\u7f16\u8bd1\u8f93\u51fa\u548c\u52a0\u5bc6\u8bc1\u660e\uff0c\u8bc1\u660e\u7f16\u8bd1\u662f\u5728\u58f0\u79f0\u7684\u6e90\u4ee3\u7801\u548c\u7f16\u8bd1\u5668\u4e0a\u8fdb\u884c\u7684\u3002", "result": "\u4f7f\u7528RISC Zero zkVM\u548cChibiCC C\u7f16\u8bd1\u5668\u5b9e\u73b0\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5728200\u4e2a\u5408\u6210\u7a0b\u5e8f\u300131\u4e2aOpenSSL\u548c21\u4e2alibsodium\u6e90\u6587\u4ef6\u4e0a\u8bc4\u4f30\uff0c\u6210\u529f\u963b\u6b62\u9488\u5bf9\u7f16\u8bd1\u5668\u66ff\u6362\u3001\u6e90\u7be1\u6539\u3001\u8f93\u51fa\u64cd\u7eb5\u548c\u91cd\u653e\u653b\u51fb\u7684\u5bf9\u6297\u6d4b\u8bd5\u3002", "conclusion": "zk\u7f16\u8bd1\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5e76\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\u3002"}}
{"id": "2602.11455", "pdf": "https://arxiv.org/pdf/2602.11455", "abs": "https://arxiv.org/abs/2602.11455", "authors": ["Zhengbo Jiao", "Shaobo Wang", "Zifan Zhang", "Wei Wang", "Bing Zhao", "Hu Wei", "Linfeng Zhang"], "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning", "categories": ["cs.AI"], "comment": "20pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.", "AI": {"tldr": "\u7814\u7a76\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c6\u89c9\u8bc1\u636e\u6574\u5408\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u9ad8\u8fde\u901a\u6027\u6807\u8bb0\u7684\u4f5c\u7528\uff0c\u63d0\u51fa AT - RL \u6846\u67b6\uff0c\u8bc1\u660e\u63a8\u7406\u8d28\u91cf\u53d6\u51b3\u4e8e\u8de8\u6a21\u6001\u951a\u5b9a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u89c6\u89c9\u8bc1\u636e\u6574\u5408\u65b9\u5f0f\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u5176\u5728\u63a8\u7406\u65f6\u7684\u89c6\u89c9\u8bc1\u636e\u6574\u5408\u673a\u5236\u3002", "method": "\u4ece\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u8fde\u63a5\u7684\u89d2\u5ea6\u63a2\u7d22\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u62d3\u6251\u56fe\u805a\u7c7b\u7684 Anchor - Token \u5f3a\u5316\u5b66\u4e60\uff08AT - RL\uff09\u6846\u67b6\uff0c\u9009\u62e9\u6027\u5730\u5f3a\u5316\u9ad8\u8fde\u901a\u6027\u6807\u8bb0\u3002", "result": "AT - RL \u5f15\u5165\u4ec5 1.2% \u7684\u5f00\u9500\uff0c\u4f7f 32B \u6a21\u578b\u5728 MathVista \u4e0a\u8d85\u8fc7 72B - Instruct \u57fa\u7ebf\uff0c\u5728 STEM\u3001\u89c6\u9891\u548c\u901a\u7528\u4efb\u52a1\u4e2d\u5747\u6709\u63d0\u5347\uff1b\u4ec5\u8bad\u7ec3\u4f4e\u8fde\u901a\u6027\u6807\u8bb0\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "conclusion": "\u591a\u6a21\u6001\u63a8\u7406\u8d28\u91cf\u7531\u8de8\u6a21\u6001\u951a\u5b9a\u7684\u51c6\u786e\u6027\u51b3\u5b9a\uff0c\u800c\u975e\u6807\u8bb0\u6570\u91cf\u3002"}}
{"id": "2602.12023", "pdf": "https://arxiv.org/pdf/2602.12023", "abs": "https://arxiv.org/abs/2602.12023", "authors": ["Yechan Park", "Xiaodong Yang"], "title": "Decomposition of Spillover Effects Under Misspecification:Pseudo-true Estimands and a Local--Global Extension", "categories": ["econ.EM", "math.ST", "stat.ML"], "comment": null, "summary": "Applied work with interference typically models outcomes as functions of own treatment and a low-dimensional exposure mapping of others' treatments, even when that mapping may be misspecified. This raises a basic question: what policy object are exposure-based estimands implicitly targeting, and how should we interpret their direct and spillover components relative to the underlying policy question? We take as primitive the marginal policy effect, defined as the effect of a small change in the treatment probability under the actual experimental design, and show that any researcher-chosen exposure mapping induces a unique pseudo-true outcome model. This model is the best approximation to the underlying potential outcomes that depends only on the user-chosen exposure. Utilizing that representation, the marginal policy effect admits a canonical decomposition into exposure-based direct and spillover effects, and each component provides its optimal approximation to the corresponding oracle objects that would be available if interference were fully known. We then focus on a setting that nests important empirical and theoretical applications in which both local network spillovers and global spillovers, such as market equilibrium, operate. There, the marginal policy effect further decomposes asymptotically into direct, local, and global channels. An important implication is that many existing methods are more robust than previously understood once we reinterpret their targets as channel-specific components of this pseudo-true policy estimand. Simulations and a semi-synthetic experiment calibrated to a large cash-transfer experiment show that these components can be recovered in realistic experimental designs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u57fa\u4e8e\u66b4\u9732\u7684\u4f30\u8ba1\u91cf\u9690\u542b\u7684\u653f\u7b56\u76ee\u6807\u53ca\u89e3\u91ca\u5176\u76f4\u63a5\u548c\u6ea2\u51fa\u6548\u5e94\uff0c\u63d0\u51fa\u8fb9\u9645\u653f\u7b56\u6548\u5e94\u5206\u89e3\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u66f4\u7a33\u5065\u4e14\u6a21\u62df\u663e\u793a\u53ef\u5728\u73b0\u5b9e\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\u6062\u590d\u76f8\u5173\u6210\u5206\u3002", "motivation": "\u5e94\u7528\u5de5\u4f5c\u4e2d\u57fa\u4e8e\u66b4\u9732\u5efa\u6a21\u53ef\u80fd\u5b58\u5728\u9519\u8bef\u8bbe\u5b9a\uff0c\u9700\u660e\u786e\u57fa\u4e8e\u66b4\u9732\u7684\u4f30\u8ba1\u91cf\u9690\u542b\u7684\u653f\u7b56\u76ee\u6807\u53ca\u5982\u4f55\u89e3\u91ca\u5176\u76f4\u63a5\u548c\u6ea2\u51fa\u6548\u5e94\u3002", "method": "\u4ee5\u8fb9\u9645\u653f\u7b56\u6548\u5e94\u4e3a\u57fa\u7840\uff0c\u63a8\u5bfc\u5176\u5728\u4e0d\u540c\u60c5\u51b5\u4e0b\u7684\u5206\u89e3\uff0c\u5305\u62ec\u57fa\u4e8e\u66b4\u9732\u7684\u5206\u89e3\u548c\u6e10\u8fd1\u5206\u89e3\u3002", "result": "\u8fb9\u9645\u653f\u7b56\u6548\u5e94\u53ef\u5206\u89e3\u4e3a\u76f4\u63a5\u548c\u6ea2\u51fa\u6548\u5e94\uff0c\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u53ef\u8fdb\u4e00\u6b65\u6e10\u8fd1\u5206\u89e3\uff1b\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u66f4\u7a33\u5065\uff1b\u6a21\u62df\u548c\u534a\u5408\u6210\u5b9e\u9a8c\u8868\u660e\u53ef\u5728\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u6062\u590d\u76f8\u5173\u6210\u5206\u3002", "conclusion": "\u91cd\u65b0\u89e3\u91ca\u57fa\u4e8e\u66b4\u9732\u4f30\u8ba1\u91cf\u7684\u76ee\u6807\u540e\uff0c\u73b0\u6709\u65b9\u6cd5\u6bd4\u4e4b\u524d\u7406\u89e3\u7684\u66f4\u5177\u7a33\u5065\u6027\uff0c\u4e14\u53ef\u5728\u73b0\u5b9e\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\u6062\u590d\u5206\u89e3\u6210\u5206\u3002"}}
{"id": "2602.11216", "pdf": "https://arxiv.org/pdf/2602.11216", "abs": "https://arxiv.org/abs/2602.11216", "authors": ["Panagiotis Antoniadis", "Beatrice Pavesi", "Simon Olsson", "Ole Winther"], "title": "Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators", "categories": ["cs.LG", "physics.bio-ph"], "comment": "24 pages, 12 figures and 7 tables", "summary": "Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u751f\u6210\u5f0f\u5206\u5b50\u52a8\u529b\u5b66\u53ef\u8f6c\u79fb\u6027\u6709\u9650\uff0c\u63d0\u51fa\u7ed3\u5408\u8f85\u52a9\u4fe1\u606f\u6539\u8fdbTITO\uff0cPLaTITO\u5728\u86cb\u767d\u8d28\u7cfb\u7edf\u5e73\u8861\u91c7\u6837\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u7814\u7a76\u989d\u5916\u6761\u4ef6\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u751f\u6210\u5f0f\u5206\u5b50\u52a8\u529b\u5b66\u53ef\u8f6c\u79fb\u6027\u6709\u9650\uff0c\u9700\u6539\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u8f85\u52a9\u4fe1\u606f\u6539\u8fdb\u53ef\u8f6c\u79fb\u9690\u5f0f\u8f6c\u79fb\u7b97\u5b50\uff08TITO\uff09\uff0c\u4f7f\u7528\u7c97\u7c92\u5ea6TITO\u6a21\u578b\u5e76\u878d\u5165\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08pLM\uff09\u5d4c\u5165\u3002", "result": "\u7c97\u7c92\u5ea6TITO\u6a21\u578b\u6bd4\u73bb\u5c14\u5179\u66fc\u6a21\u62df\u5668\u66f4\u5177\u6570\u636e\u6548\u7387\uff0c\u878d\u5165pLM\u5d4c\u5165\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0cPLaTITO\u5728\u5206\u5e03\u5916\u86cb\u767d\u8d28\u7cfb\u7edf\u5e73\u8861\u91c7\u6837\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u8f85\u52a9\u4fe1\u606f\u80fd\u63d0\u9ad8\u5206\u5b50\u52a8\u529b\u5b66TITO\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u989d\u5916\u6761\u4ef6\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u5f71\u54cd\u3002"}}
{"id": "2602.12181", "pdf": "https://arxiv.org/pdf/2602.12181", "abs": "https://arxiv.org/abs/2602.12181", "authors": ["Anas Barakat", "Ioannis Panageas", "Antonios Varvitsiotis"], "title": "Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria", "categories": ["cs.GT", "cs.LG", "cs.MA"], "comment": "AISTATS 2026", "summary": "Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utility Markov Games (GUMGs), capturing new applications requiring coupling between agents' occupancy measures. We prove that in GUMGs, Nash equilibria coincide with the fixed points of projected pseudo-gradient dynamics (i.e., first-order stationary points), enabled by a novel agent-wise gradient domination property. This insight also yields a simple proof of NE existence using Brouwer's fixed-point theorem. We further show the existence of Markov perfect equilibria. Building on this characterization, we establish a policy gradient theorem for GUMGs and design a model-free policy gradient algorithm. For potential GUMGs, we establish iteration complexity guarantees for computing approximate-NE under exact gradients and provide sample complexity bounds in both the generative model and on-policy settings. Our results extend beyond prior work restricted to zero-sum cMGs, providing the first theoretical analysis of common-interest cMGs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e7f\u4e49\u6548\u7528\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff08GUMGs\uff09\uff0c\u8bc1\u660e\u7eb3\u4ec0\u5747\u8861\u4e0e\u6295\u5f71\u4f2a\u68af\u5ea6\u52a8\u529b\u5b66\u4e0d\u52a8\u70b9\u91cd\u5408\uff0c\u7ed9\u51fa\u5b58\u5728\u6027\u8bc1\u660e\u3001\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u5e76\u7ed9\u51fa\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u62d3\u5c55\u4e86\u5148\u524d\u96f6\u548c\u51f8\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u7814\u7a76\u3002", "motivation": "\u51f8\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff08cMGs\uff09\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u7eb3\u4ec0\u5747\u8861\u7ed3\u6784\u548c\u5b66\u4e60\u7b97\u6cd5\u4fdd\u8bc1\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5bf9\u5176\u6269\u5c55GUMGs\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u8bc1\u660eGUMGs\u4e2d\u7eb3\u4ec0\u5747\u8861\u4e0e\u6295\u5f71\u4f2a\u68af\u5ea6\u52a8\u529b\u5b66\u4e0d\u52a8\u70b9\u91cd\u5408\uff0c\u5229\u7528\u5e03\u52b3\u5a01\u5c14\u4e0d\u52a8\u70b9\u5b9a\u7406\u8bc1\u660eNE\u5b58\u5728\u6027\uff0c\u5efa\u7acb\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u8bbe\u8ba1\u65e0\u6a21\u578b\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u7ed9\u51fa\u8fed\u4ee3\u548c\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u7eb3\u4ec0\u5747\u8861\u7279\u6027\u3001NE\u548c\u9a6c\u5c14\u53ef\u592b\u5b8c\u7f8e\u5747\u8861\u7684\u5b58\u5728\u6027\uff0c\u5efa\u7acb\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\u548c\u7b97\u6cd5\uff0c\u7ed9\u51fa\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u7ed3\u679c\u6269\u5c55\u4e86\u5148\u524d\u96f6\u548ccMGs\u7684\u5de5\u4f5c\uff0c\u9996\u6b21\u5bf9\u5171\u540c\u5229\u76cacMGs\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2602.12129", "pdf": "https://arxiv.org/pdf/2602.12129", "abs": "https://arxiv.org/abs/2602.12129", "authors": ["Rahin Arefin Ahmed", "Md. Anik Chowdhury", "Sakil Ahmed Sheikh Reza", "Devnil Bhattacharjee", "Muhammad Abdullah Adnan", "Nafis Sadeq"], "title": "Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.\n  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u56fe\u4e66\u63a8\u8350\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6RokomariBG\uff0c\u5e76\u5bf9\u591a\u79cd\u63a8\u8350\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u6587\u5b66\u4e2a\u6027\u5316\u56fe\u4e66\u63a8\u8350\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u521b\u5efa\u591a\u5b9e\u4f53\u5f02\u6784\u56fe\u4e66\u56fe\u6570\u636e\u96c6RokomariBG\uff0c\u5e76\u5728Top - N\u63a8\u8350\u4efb\u52a1\u4e0a\u5bf9\u591a\u79cd\u63a8\u8350\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5229\u7528\u591a\u5173\u7cfb\u7ed3\u6784\u548c\u6587\u672c\u8f85\u52a9\u4fe1\u606f\u5f88\u91cd\u8981\uff0c\u795e\u7ecf\u68c0\u7d22\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cNDCG@10 = 0.204\u3002", "conclusion": "\u4e3a\u5b5f\u52a0\u62c9\u8bed\u56fe\u4e66\u63a8\u8350\u7814\u7a76\u5efa\u7acb\u57fa\u7840\u57fa\u51c6\u548c\u516c\u5f00\u8d44\u6e90\uff0c\u4fbf\u4e8e\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u548c\u672a\u6765\u4f4e\u8d44\u6e90\u6587\u5316\u9886\u57df\u63a8\u8350\u7814\u7a76\u3002"}}
{"id": "2602.12209", "pdf": "https://arxiv.org/pdf/2602.12209", "abs": "https://arxiv.org/abs/2602.12209", "authors": ["Alessandro Epasto", "Xin Lyu", "Pasin Manurangsi"], "title": "Keeping a Secret Requires a Good Memory: Space Lower-Bounds for Private Algorithms", "categories": ["cs.CR", "cs.CC", "cs.DS"], "comment": "comments welcome", "summary": "We study the computational cost of differential privacy in terms of memory efficiency. While the trade-off between accuracy and differential privacy is well-understood, the inherent cost of privacy regarding memory use remains largely unexplored. This paper establishes for the first time an unconditional space lower bound for user-level differential privacy by introducing a novel proof technique based on a multi-player communication game.\n  Central to our approach, this game formally links the hardness of low-memory private algorithms to the necessity of ``contribution capping'' -- tracking and limiting the users who disproportionately impact the dataset. We demonstrate that winning this communication game requires transmitting information proportional to the number of over-active users, which translates directly to memory lower bounds.\n  We apply this framework, as an example, to the fundamental problem of estimating the number of distinct elements in a stream and we prove that any private algorithm requires almost $\\widetilde\u03a9(T^{1/3})$ space to achieve certain error rates in a promise variant of the problem. This resolves an open problem in the literature (by Jain et al. NeurIPS 2023 and Cummings et al. ICML 2025) and establishes the first exponential separation between the space complexity of private algorithms and their non-private $\\widetilde{O}(1)$ counterparts for a natural statistical estimation task. Furthermore, we show that this communication-theoretic technique generalizes to broad classes of problems, yielding lower bounds for private medians, quantiles, and max-select.", "AI": {"tldr": "\u672c\u6587\u4ece\u5185\u5b58\u6548\u7387\u89d2\u5ea6\u7814\u7a76\u5dee\u5206\u9690\u79c1\u8ba1\u7b97\u6210\u672c\uff0c\u7528\u591a\u73a9\u5bb6\u901a\u4fe1\u6e38\u620f\u8bc1\u660e\u7528\u6237\u7ea7\u5dee\u5206\u9690\u79c1\u7a7a\u95f4\u4e0b\u754c\uff0c\u89e3\u51b3\u516c\u5f00\u95ee\u9898\u5e76\u62d3\u5c55\u5230\u591a\u7c7b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5dee\u5206\u9690\u79c1\u5728\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u7684\u56fa\u6709\u6210\u672c\u63a2\u7d22\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8be5\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u591a\u73a9\u5bb6\u901a\u4fe1\u6e38\u620f\u7684\u8bc1\u660e\u6280\u672f\uff0c\u5c06\u4f4e\u5185\u5b58\u79c1\u6709\u7b97\u6cd5\u7684\u96be\u5ea6\u4e0e\u201c\u8d21\u732e\u4e0a\u9650\u201d\u8054\u7cfb\u8d77\u6765\uff0c\u901a\u8fc7\u5206\u6790\u6e38\u620f\u83b7\u80dc\u6240\u9700\u4f20\u8f93\u4fe1\u606f\u5f97\u51fa\u5185\u5b58\u4e0b\u754c\u3002", "result": "\u4ee5\u6d41\u4e2d\u4e0d\u540c\u5143\u7d20\u6570\u91cf\u4f30\u8ba1\u95ee\u9898\u4e3a\u4f8b\uff0c\u8bc1\u660e\u79c1\u6709\u7b97\u6cd5\u8fbe\u5230\u7279\u5b9a\u9519\u8bef\u7387\u6240\u9700\u7a7a\u95f4\uff0c\u89e3\u51b3\u6587\u732e\u4e2d\u516c\u5f00\u95ee\u9898\uff0c\u5efa\u7acb\u79c1\u6709\u4e0e\u975e\u79c1\u6709\u7b97\u6cd5\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u6307\u6570\u5206\u79bb\uff0c\u8fd8\u5c06\u6280\u672f\u63a8\u5e7f\u5230\u5176\u4ed6\u95ee\u9898\u3002", "conclusion": "\u8be5\u901a\u4fe1\u7406\u8bba\u6280\u672f\u53ef\u7528\u4e8e\u89e3\u51b3\u591a\u79cd\u81ea\u7136\u7edf\u8ba1\u4f30\u8ba1\u4efb\u52a1\u4e2d\u79c1\u6709\u7b97\u6cd5\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5dee\u5206\u9690\u79c1\u5185\u5b58\u6210\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.11904", "pdf": "https://arxiv.org/pdf/2602.11904", "abs": "https://arxiv.org/abs/2602.11904", "authors": ["Weixing Zhang", "Bowen Jiang", "Yuhong Fu", "Anne Koziolek", "Regina Hebig", "Daniel Str\u00fcber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6587\u672c\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSLs\uff09\u8bed\u6cd5\u548c\u5b9e\u4f8b\u534f\u540c\u6f14\u5316\u7684\u6f5c\u529b\uff0c\u7ed3\u679c\u8868\u660e\u5c0f\u5c3a\u5ea6\u6848\u4f8b\u8868\u73b0\u597d\uff0c\u5927\u5c3a\u5ea6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u6280\u672f\u4e0d\u9002\u5408\u6587\u672cDSLs\uff0c\u4e14\u53ef\u80fd\u4e22\u5931\u5e03\u5c40\u548c\u6ce8\u91ca\u7b49\u4eba\u7c7b\u76f8\u5173\u4fe1\u606f\uff0c\u56e0\u6b64\u8bc4\u4f30LLMs\u5728\u6b64\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Claude Sonnet 4.5\u548cGPT - 5.2\u5728\u5341\u79cd\u6848\u4f8b\u8bed\u8a00\u4e0a\u5404\u8fdb\u884c\u5341\u6b21\u8fd0\u884c\uff0c\u8bc4\u4f30\u6b63\u786e\u6027\u548c\u4eba\u7c7b\u5bfc\u5411\u4fe1\u606f\u7684\u4fdd\u7559\u60c5\u51b5\u3002", "result": "\u5c0f\u5c3a\u5ea6\u6848\u4f8b\u8868\u73b0\u597d\uff08\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u226594%\uff09\uff0c\u5927\u5c3a\u5ea6\u6027\u80fd\u4e0b\u964d\uff0cClaude\u572840\u884c\u65f6\u4fdd\u630185%\u53ec\u56de\u7387\uff0cGPT\u5728\u6700\u5927\u5b9e\u4f8b\u4e0a\u5931\u8d25\uff0c\u54cd\u5e94\u65f6\u95f4\u968f\u5b9e\u4f8b\u5927\u5c0f\u663e\u8457\u589e\u52a0\uff0c\u8bed\u6cd5\u6f14\u5316\u590d\u6742\u6027\u548c\u5220\u9664\u7c92\u5ea6\u5bf9\u6027\u80fd\u5f71\u54cd\u5927\u4e8e\u53d8\u66f4\u7c7b\u578b\u3002", "conclusion": "\u660e\u786e\u4e86\u57fa\u4e8eLLM\u7684\u534f\u540c\u6f14\u5316\u4f55\u65f6\u6709\u6548\u4ee5\u53ca\u5f53\u524d\u7684\u5c40\u9650\u6027\u6240\u5728\u3002"}}
{"id": "2602.11510", "pdf": "https://arxiv.org/pdf/2602.11510", "abs": "https://arxiv.org/abs/2602.11510", "authors": ["Faouzi El Yagoubi", "Ranwa Al Mallah", "Godwin Badu-Marfo"], "title": "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems", "categories": ["cs.AI"], "comment": "17 pages, 10 figures, 13 tables. Code and dataset available at https://github.com/Privatris/AgentLeak", "summary": "Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.", "AI": {"tldr": "\u63d0\u51faAgentLeak\u57fa\u51c6\u6d4b\u8bd5\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u9690\u79c1\u6cc4\u6f0f\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u867d\u51cf\u5c11\u8f93\u51fa\u6cc4\u6f0f\u4f46\u589e\u52a0\u5185\u90e8\u6cc4\u6f0f\uff0cClaude 3.5 Sonnet\u6cc4\u6f0f\u7387\u4f4e\uff0c\u5f3a\u8c03\u9700\u5185\u90e8\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u65e0\u6cd5\u8861\u91cf\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\u3002", "method": "\u5f15\u5165AgentLeak\u57fa\u51c6\uff0c\u542b1000\u4e2a\u573a\u666f\u300132\u7c7b\u653b\u51fb\u5206\u7c7b\u548c\u4e09\u5c42\u68c0\u6d4b\u7ba1\u9053\uff0c\u6d4b\u8bd55\u79cd\u6a21\u578b\u3002", "result": "\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u964d\u4f4e\u5355\u901a\u9053\u8f93\u51fa\u6cc4\u6f0f\u4f46\u589e\u52a0\u7cfb\u7edf\u603b\u66b4\u9732\uff0c\u5185\u90e8\u901a\u9053\u6cc4\u6f0f\u4e25\u91cd\uff0cClaude 3.5 Sonnet\u6cc4\u6f0f\u7387\u6700\u4f4e\uff0cC2 > C1\u6a21\u5f0f\u4e00\u81f4\u3002", "conclusion": "\u9700\u8981\u7eb3\u5165\u5185\u90e8\u901a\u9053\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u8c03\u6846\u67b6\uff0c\u52a0\u5f3a\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u7684\u9690\u79c1\u63a7\u5236\u3002"}}
{"id": "2602.12043", "pdf": "https://arxiv.org/pdf/2602.12043", "abs": "https://arxiv.org/abs/2602.12043", "authors": ["Sunny R. Karim", "Morten \u00d8rregaard Nielsen", "James G. MacKinnon", "Matthew D. Webb"], "title": "Improved Inference for CSDID Using the Cluster Jackknife", "categories": ["econ.EM", "stat.ME", "stat.ML"], "comment": null, "summary": "Obtaining reliable inferences with traditional difference-in-differences (DiD) methods can be difficult. Problems can arise when both outcomes and errors are serially correlated, when there are few clusters or few treated clusters, when cluster sizes vary greatly, and in various other cases. In recent years, recognition of the ``staggered adoption'' problem has shifted the focus away from inference towards consistent estimation of treatment effects. One of the most popular new estimators is the CSDID procedure of Callaway and Sant'Anna (2021). We find that the issues of over-rejection with few clusters and/or few treated clusters are at least as severe for CSDID as for traditional DiD methods. We also propose using a cluster jackknife for inference with CSDID, which simulations suggest greatly improves inference. We provide software packages in Stata csdidjack and R didjack to calculate cluster-jackknife standard errors easily.", "AI": {"tldr": "\u4f20\u7edf\u53cc\u91cd\u5dee\u5206\u6cd5\u83b7\u53ef\u9760\u63a8\u65ad\u6709\u56f0\u96be\uff0c\u8fd1\u5e74\u5173\u6ce8\u8f6c\u79fb\u5230\u5904\u7406\u6548\u5e94\u4f30\u8ba1\uff0c\u6d41\u884c\u7684CSDID\u4ecd\u6709\u8fc7\u62d2\u7edd\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u7528\u805a\u7c7b\u5200\u5207\u6cd5\u6539\u5584\u63a8\u65ad\u5e76\u63d0\u4f9b\u8f6f\u4ef6\u5305\u3002", "motivation": "\u4f20\u7edf\u53cc\u91cd\u5dee\u5206\u6cd5\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u96be\u83b7\u53ef\u9760\u63a8\u65ad\uff0c\u6d41\u884c\u7684CSDID\u5728\u5c11\u91cf\u805a\u7c7b\u6216\u5904\u7406\u7ec4\u805a\u7c7b\u6570\u5c11\u65f6\u6709\u8fc7\u62d2\u7edd\u95ee\u9898\u3002", "method": "\u4e3aCSDID\u65b9\u6cd5\u63d0\u51fa\u805a\u7c7b\u5200\u5207\u6cd5\u8fdb\u884c\u63a8\u65ad\uff0c\u5e76\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\u3002", "result": "\u6a21\u62df\u663e\u793a\u805a\u7c7b\u5200\u5207\u6cd5\u80fd\u5927\u5927\u6539\u5584\u63a8\u65ad\u3002", "conclusion": "\u805a\u7c7b\u5200\u5207\u6cd5\u6709\u52a9\u4e8e\u89e3\u51b3CSDID\u65b9\u6cd5\u7684\u63a8\u65ad\u95ee\u9898\uff0c\u4e14\u63d0\u4f9b\u4e86\u65b9\u4fbf\u8ba1\u7b97\u805a\u7c7b\u5200\u5207\u6cd5\u6807\u51c6\u8bef\u5dee\u7684\u8f6f\u4ef6\u5305\u3002"}}
{"id": "2602.11217", "pdf": "https://arxiv.org/pdf/2602.11217", "abs": "https://arxiv.org/abs/2602.11217", "authors": ["Simin Fan", "Dimitris Paparas", "Natasha Noy", "Binbin Xiong", "Noveen Sachdeva", "Berivan Isik"], "title": "The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.", "AI": {"tldr": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4ece\u9884\u8bad\u7ec3\u5230\u76d1\u7763\u5fae\u8c03\u7684\u8f6c\u79fb\uff0c\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u8f6c\u79fb\u53ef\u9760\u6027\u56e0\u80fd\u529b\u7c7b\u522b\u3001\u57fa\u51c6\u548c\u89c4\u6a21\u800c\u5f02\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u7b49\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4ece\u9884\u8bad\u7ec3\u5230\u76d1\u7763\u5fae\u8c03\u7684\u8f6c\u79fb\u5bf9\u9ad8\u6548\u6a21\u578b\u5f00\u53d1\u548c\u6570\u636e\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u76f8\u5173\u534f\u8bae\uff0c\u5bf9\u4e0d\u540c\u6570\u636e\u6df7\u5408\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8f6c\u79fb\u53ef\u9760\u6027\u5728\u80fd\u529b\u7c7b\u522b\u3001\u57fa\u51c6\u548c\u89c4\u6a21\u4e0a\u5dee\u5f02\u5de8\u5927\uff0c\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6709\u4e0d\u540c\u7684\u7f29\u653e\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u51b3\u7b56\u548c\u4e0b\u6e38\u7ed3\u679c\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u57fa\u51c6\u9009\u62e9\u3001\u6570\u636e\u7ba1\u7406\u548c\u9ad8\u6548\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2602.12187", "pdf": "https://arxiv.org/pdf/2602.12187", "abs": "https://arxiv.org/abs/2602.12187", "authors": ["Sunghwan Kim", "Wooseok Jeong", "Serin Kim", "Sangam Lee", "Dongha Lee"], "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization", "categories": ["cs.IR", "cs.AI"], "comment": "Work in Progress", "summary": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdSAGEO Arena\u7528\u4e8eSAGEO\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4e0d\u5b9e\u7528\uff0c\u7ed3\u6784\u4fe1\u606f\u53ef\u7f13\u89e3\u5c40\u9650\uff0c\u6709\u6548SAGEO\u9700\u9488\u5bf9\u5404\u9636\u6bb5\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u73af\u5883\u65e0\u6cd5\u652f\u6301\u5168\u9762\u7684SAGEO\u7814\u7a76\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u53ef\u89c1\u6027\u8bc4\u4f30\u4e14\u5ffd\u7565\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u6574\u5408\u5927\u89c4\u6a21\u542b\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\u7684\u7f51\u9875\u6587\u6863\u7684\u751f\u6210\u5f0f\u641c\u7d22\u7ba1\u9053\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4e0d\u5b9e\u7528\uff0c\u5e38\u964d\u4f4e\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u6027\u80fd\uff0c\u7ed3\u6784\u4fe1\u606f\u53ef\u7f13\u89e3\u5c40\u9650\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684SAGEO\u8bc4\u4f30\u548c\u4f18\u5316\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.11911", "pdf": "https://arxiv.org/pdf/2602.11911", "abs": "https://arxiv.org/abs/2602.11911", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Improving Code Generation via Small Language Model-as-a-judge", "categories": ["cs.SE"], "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.", "AI": {"tldr": "\u7814\u7a76\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u751f\u6210\u7814\u7a76\u5c40\u9650\uff0c\u8bad\u7ec3\u73b0\u4ee3SLMs\u4f5c\u4ee3\u7801\u6b63\u786e\u6027\u5224\u65ad\u5668\uff0c\u8868\u73b0\u4f18\u4e8eRankEF\u4e14\u6210\u672c\u4f4e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u8bc4\u4f30T5\u6392\u5e8f\u5668\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u6a21\u578b\u65e7\uff0c\u4e0d\u6e05\u695a\u80fd\u5426\u52a9\u516c\u53f8\u4f4e\u6210\u672c\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u5668\u3002", "method": "\u8bad\u7ec3\u591a\u4e2a\u6700\u5148\u8fdb\u7684SLMs\u4f5c\u4e3a\u4ee3\u7801\u6b63\u786e\u6027\u5224\u65ad\u5668\uff0c\u8bc4\u4f30\u5176\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u5b9e\u73b0\u7684\u80fd\u529b\u3002", "result": "\u73b0\u4ee3SLMs\u5373\u4f7f\u4e0d\u5229\u7528\u57fa\u4e8e\u6267\u884c\u7684\u4fe1\u606f\u4e5f\u4f18\u4e8eRankEF\uff0c\u4f5c\u4e3a\u4ee3\u7801\u6392\u5e8f\u5668\u65f6\u6027\u80fd\u63d0\u5347\u66f4\u9ad8\uff0c\u80fd\u4ee5\u4f4e\u6210\u672c\u4e0e\u5927\u5f97\u591a\u7684LLMs\u7ade\u4e89\u3002", "conclusion": "\u73b0\u4ee3SLMs\u53ef\u6709\u6548\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5c40\u9650\uff0c\u4e3a\u516c\u53f8\u4f4e\u6210\u672c\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u5668\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2602.11516", "pdf": "https://arxiv.org/pdf/2602.11516", "abs": "https://arxiv.org/abs/2602.11516", "authors": ["Hong Su"], "title": "Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems", "categories": ["cs.AI"], "comment": null, "summary": "Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.", "AI": {"tldr": "\u63d0\u51fa\u53d7\u4eba\u7c7b\u542f\u53d1\u7684\u8fde\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u7b49\u7edf\u4e00\uff0c\u4ee5\u5185\u90e8\u601d\u7ef4\u8fc7\u7a0b\u4e3a\u5b66\u4e60\u5bf9\u8c61\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4efb\u52a1\u4e2d\u51cf\u5c11\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u5185\u90e8\u63a8\u7406\u7ed3\u6784\u7b49\u6301\u7eed\u4f18\u5316\uff0c\u4e3a\u5f00\u53d1\u80fd\u5728\u52a8\u6001\u73af\u5883\u6301\u7eed\u9002\u5e94\u7684AI\u7cfb\u7edf\uff0c\u9700\u5b66\u4e60\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4eba\u7c7b\u542f\u53d1\u7684\u8fde\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u63a8\u7406\u3001\u884c\u52a8\u3001\u53cd\u601d\u548c\u9a8c\u8bc1\uff0c\u5c06\u5185\u90e8\u601d\u7ef4\u8fc7\u7a0b\u4f5c\u4e3a\u5b66\u4e60\u5bf9\u8c61\uff0c\u8bb0\u5f55\u63a8\u7406\u8f68\u8ff9\u548c\u4ea4\u4e92\u4e3a\u5b66\u4e60\u6750\u6599\uff0c\u652f\u6301\u66ff\u6362\u9884\u5b9a\u4e49\u903b\u8f91\uff0c\u5f15\u5165\u5206\u5c42\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728\u6e29\u5ea6\u4f20\u611f\u5668\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u5f15\u5165\u5185\u90e8\u8fc7\u7a0b\u5b66\u4e60\u4f7f\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1123.9%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u8ba9\u7cfb\u7edf\u5728\u4fdd\u6301\u8fd0\u884c\u7a33\u5b9a\u7684\u540c\u65f6\uff0c\u9010\u6b65\u8fdb\u5316\u5185\u90e8\u8ba4\u77e5\u67b6\u6784\u3002"}}
{"id": "2602.12082", "pdf": "https://arxiv.org/pdf/2602.12082", "abs": "https://arxiv.org/abs/2602.12082", "authors": ["Jihao Andreas Lin", "Sebastian Ament", "Louis C. Tiao", "David Eriksson", "Maximilian Balandat", "Eytan Bakshy"], "title": "Empirical Gaussian Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.", "AI": {"tldr": "\u7814\u7a76\u7ecf\u9a8c\u9ad8\u65af\u8fc7\u7a0b\uff0c\u4ece\u5386\u53f2\u6570\u636e\u4f30\u8ba1\u5747\u503c\u548c\u534f\u65b9\u5dee\u51fd\u6570\uff0c\u7406\u8bba\u6709\u6536\u655b\u6027\uff0c\u5e76\u6709\u7b97\u6cd5\u53ef\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u5728\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u5b9e\u8df5\u4e2d\u6548\u679c\u53d7\u6838\u51fd\u6570\u9009\u62e9\u9650\u5236\uff0c\u4f20\u7edf\u6838\u51fd\u6570\u9700\u4e13\u5bb6\u77e5\u8bc6\u3001\u9002\u5e94\u6027\u6709\u9650\u3001\u5047\u8bbe\u5f3a\u3002", "method": "\u4ece\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u4e2d\u7ecf\u9a8c\u6027\u4f30\u8ba1\u5747\u503c\u548c\u534f\u65b9\u5dee\u51fd\u6570\uff1b\u5c06\u4ece\u72ec\u7acb\u6570\u636e\u96c6\u5b66\u4e60GP\u5148\u9a8c\u95ee\u9898\u8f6c\u5316\u4e3a\u4f3c\u7136\u4f30\u8ba1\uff0c\u5e76\u63a8\u5bfc\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u3002", "result": "\u7ecf\u9a8c\u9ad8\u65af\u8fc7\u7a0b\u5728\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "conclusion": "\u7ecf\u9a8c\u9ad8\u65af\u8fc7\u7a0b\u6784\u5efa\u4e86\u7075\u6d3b\u3001\u6570\u636e\u9a71\u52a8\u7684GP\u5148\u9a8c\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.11219", "pdf": "https://arxiv.org/pdf/2602.11219", "abs": "https://arxiv.org/abs/2602.11219", "authors": ["Tanmoy Mukherjee", "Marius Kloft", "Pierre Marquis", "Zied Bouraoui"], "title": "Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528credal - set\u516c\u5f0f\u5316\u65b9\u6cd5\u5206\u79bb\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u6807\u6ce8\u57fa\u51c6\u4e0a\u6548\u679c\u826f\u597d\u3002", "motivation": "\u591a\u6570\u65b9\u6cd5\u4ece\u76f8\u540c\u9884\u6d4b\u5206\u5e03\u4f30\u8ba1\u8ba4\u77e5\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u4e8c\u8005\u4f30\u8ba1\u901a\u5e38\u5f3a\u76f8\u5173\uff0c\u6a21\u7cca\u4e86\u8bed\u4e49\uff0c\u5f71\u54cd\u53ef\u9760\u51b3\u7b56\u3002", "method": "\u63d0\u51facredal - set\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5728\u53d8\u5206\u53ef\u4fe1\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e2d\u7528\u4e24\u4e2a\u4e0d\u76f8\u4ea4\u7684\u4e0d\u786e\u5b9a\u6027\u5934\uff0c\u901a\u8fc7\u4e0d\u76f8\u4ea4\u7684\u76ee\u6807\u548c\u975e\u91cd\u53e0\u7684\u68af\u5ea6\u8def\u5f84\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4e0e\u6807\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6807\u6ce8\u57fa\u51c6\u4e0a\u4f7f\u8ba4\u77e5\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u7684\u76f8\u5173\u6027\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u540c\u65f6\u6539\u5584\u4e86\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0e\u9884\u6d4b\u8bef\u5dee\u3001\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4e0e\u771f\u5b9e\u6a21\u7cca\u6027\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5206\u79bb\u8ba4\u77e5\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u51b3\u7b56\u53ef\u9760\u6027\u3002"}}
{"id": "2602.12231", "pdf": "https://arxiv.org/pdf/2602.12231", "abs": "https://arxiv.org/abs/2602.12231", "authors": ["Robert Bredereck", "Bin Sun", "Eyal Briman", "Nimrod Talmon"], "title": "Adjusted Winner: from Splitting to Selling", "categories": ["cs.GT"], "comment": null, "summary": "The Adjusted Winner (AW) method is a fundamental procedure for the fair division of indivisible resources between two agents. However, its reliance on splitting resources can lead to practical complications. To address this limitation, we propose an extension of AW that allows the sale of selected resources under a budget constraint, with the proceeds subsequently redistributed, thereby aiming for allocations that remain as equitable as possible. Alongside developing this extended framework, we provide an axiomatic analysis that examines how equitability and envy-freeness are modified in our setting. We then formally define the resulting combinatorial problems, establish their computational complexity, and design a fully polynomial-time approximation scheme (FPTAS) to mitigate their inherent intractability. Finally, we complement our theoretical results with computer-based simulations.", "AI": {"tldr": "\u63d0\u51faAW\u65b9\u6cd5\u6269\u5c55\u5141\u8bb8\u6309\u9884\u7b97\u7ea6\u675f\u51fa\u552e\u8d44\u6e90\u5e76\u91cd\u65b0\u5206\u914d\uff0c\u8fdb\u884c\u516c\u7406\u5206\u6790\uff0c\u5b9a\u4e49\u7ec4\u5408\u95ee\u9898\uff0c\u786e\u5b9a\u590d\u6742\u5ea6\u5e76\u8bbe\u8ba1FPTAS\uff0c\u8f85\u4ee5\u6a21\u62df\u3002", "motivation": "\u539fAW\u65b9\u6cd5\u4f9d\u8d56\u5206\u5272\u8d44\u6e90\u4f1a\u5e26\u6765\u5b9e\u9645\u590d\u6742\u6027\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u5141\u8bb8\u6309\u9884\u7b97\u7ea6\u675f\u51fa\u552e\u8d44\u6e90\u5e76\u91cd\u65b0\u5206\u914d\u7684AW\u6269\u5c55\u6846\u67b6\uff0c\u8fdb\u884c\u516c\u7406\u5206\u6790\uff0c\u5b9a\u4e49\u7ec4\u5408\u95ee\u9898\uff0c\u8bbe\u8ba1FPTAS\uff0c\u7528\u8ba1\u7b97\u673a\u6a21\u62df\u3002", "result": "\u786e\u5b9a\u7ec4\u5408\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5f97\u5230\u7406\u8bba\u7ed3\u679c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u3002", "conclusion": "\u6269\u5c55\u7684AW\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u4f9b\u66f4\u516c\u5e73\u7684\u4e0d\u53ef\u5206\u5272\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u51cf\u5c0f\u539f\u65b9\u6cd5\u5c40\u9650\u6027\u3002"}}
{"id": "2602.12278", "pdf": "https://arxiv.org/pdf/2602.12278", "abs": "https://arxiv.org/abs/2602.12278", "authors": ["David Jiahao Fu", "Lam Thanh Do", "Jiayu Li", "Kevin Chen-Chuan Chang"], "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.", "AI": {"tldr": "\u63d0\u51faAttentionRetriever\u7528\u4e8e\u957f\u6587\u6863\u68c0\u7d22\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u8fdc\u8d85\u73b0\u6709\u6a21\u578b\u4e14\u6548\u7387\u4e0e\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u672a\u9488\u5bf9\u957f\u6587\u6863\u68c0\u7d22\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u56e0\u679c\u4f9d\u8d56\u548c\u68c0\u7d22\u8303\u56f4\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faAttentionRetriever\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8e\u5b9e\u4f53\u7684\u68c0\u7d22\u4e3a\u957f\u6587\u6863\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u5d4c\u5165\u5e76\u786e\u5b9a\u68c0\u7d22\u8303\u56f4\u3002", "result": "AttentionRetriever\u5728\u957f\u6587\u6863\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u68c0\u7d22\u6a21\u578b\uff0c\u4e14\u6548\u7387\u4e0e\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "AttentionRetriever\u662f\u4e00\u79cd\u6709\u6548\u7684\u957f\u6587\u6863\u68c0\u7d22\u6a21\u578b\u3002"}}
{"id": "2602.11925", "pdf": "https://arxiv.org/pdf/2602.11925", "abs": "https://arxiv.org/abs/2602.11925", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Studying Quality Improvements Recommended via Manual and Automated Code Review", "categories": ["cs.SE"], "comment": "Accepted at the 34th International Conference on Program Comprehension (ICPC 2026)", "summary": "Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.", "AI": {"tldr": "\u7814\u7a76\u4eba\u7c7b\u4e0eChatGPT-4\u4ee3\u7801\u5ba1\u67e5\u5dee\u5f02\uff0c\u53d1\u73b0\u4e8c\u8005\u4e92\u8865\uff0cDL\u6280\u672f\u53ef\u4f5c\u989d\u5916\u8d28\u91cf\u68c0\u67e5\u4f46\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u3002", "motivation": "\u63a2\u7a76DL\u65b9\u6cd5\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u63a8\u8350\u8d28\u91cf\u6539\u8fdb\u7684\u7a0b\u5ea6\u3002", "method": "\u6536\u96c6240\u4e2aPR\u4e2d\u4eba\u7c7b\u5ba1\u67e5\u8005\u7684739\u6761\u8bc4\u8bba\uff0c\u8ba9ChatGPT\u5ba1\u67e5\u76f8\u540cPR\u5e76\u5bf9\u6bd4\u63a8\u8350\u7ed3\u679c\u3002", "result": "ChatGPT\u5e73\u5747\u63a8\u8350\u4ee3\u7801\u66f4\u6539\u6570\u662f\u4eba\u7c7b\u76842.4\u500d\uff0c\u4f46\u4ec5\u80fd\u53d1\u73b0\u4eba\u7c7b\u62a5\u544a\u8d28\u91cf\u95ee\u9898\u768410%\uff0c\u7ea640%\u989d\u5916\u8bc4\u8bba\u6307\u51fa\u6709\u610f\u4e49\u95ee\u9898\u3002", "conclusion": "DL\u4ee3\u7801\u5ba1\u67e5\u53ef\u4f5c\u4eba\u7c7b\u5ba1\u67e5\u7684\u989d\u5916\u68c0\u67e5\uff0c\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\uff0c\u4e5f\u4e0d\u80fd\u8282\u7701\u5ba1\u67e5\u65f6\u95f4\u3002"}}
{"id": "2602.11527", "pdf": "https://arxiv.org/pdf/2602.11527", "abs": "https://arxiv.org/abs/2602.11527", "authors": ["Jiawei Zhu", "Wei Chen", "Ruichu Cai"], "title": "CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference", "categories": ["cs.AI"], "comment": "Accepted by IUI 2026", "summary": "Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.", "AI": {"tldr": "\u9488\u5bf9\u4f20\u7edf\u56e0\u679c\u5206\u6790\u6d41\u7a0b\u5b58\u5728\u6280\u672f\u969c\u788d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u56e0\u679c\u667a\u80fd\u4f53CausalAgent\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7b49\u521b\u65b0\u96c6\u6210\u5b9e\u73b0\u7aef\u5230\u7aef\u56e0\u679c\u63a8\u7406\uff0c\u964d\u4f4e\u56e0\u679c\u5206\u6790\u95e8\u69db\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u5206\u6790\u6d41\u7a0b\u5b58\u5728\u663e\u8457\u6280\u672f\u58c1\u5792\uff0c\u8981\u6c42\u7814\u7a76\u8005\u5177\u5907\u7edf\u8ba1\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u53cc\u91cd\u80cc\u666f\uff0c\u9700\u624b\u52a8\u9009\u62e9\u7b97\u6cd5\u3001\u5904\u7406\u6570\u636e\u8d28\u91cf\u95ee\u9898\u548c\u89e3\u91ca\u590d\u6742\u7ed3\u679c\u3002", "method": "\u63d0\u51faCausalAgent\uff0c\u521b\u65b0\u6027\u5730\u96c6\u6210Multi - Agent Systems (MAS)\u3001Retrieval - Augmented Generation (RAG)\u548cModel Context Protocol (MCP)\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u4ece\u6570\u636e\u6e05\u7406\u5230\u62a5\u544a\u751f\u6210\u7684\u81ea\u52a8\u5316\u3002", "result": "\u7528\u6237\u53ea\u9700\u4e0a\u4f20\u6570\u636e\u96c6\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\uff0c\u5373\u53ef\u83b7\u5f97\u4e25\u8c28\u3001\u4ea4\u4e92\u5f0f\u7684\u5206\u6790\u62a5\u544a\u3002", "conclusion": "CausalAgent\u4f5c\u4e3a\u4e00\u79cd\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4eba\u673a\u534f\u4f5c\u65b0\u8303\u5f0f\uff0c\u660e\u786e\u5efa\u6a21\u5206\u6790\u6d41\u7a0b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u663e\u8457\u964d\u4f4e\u56e0\u679c\u5206\u6790\u7684\u5165\u95e8\u95e8\u69db\uff0c\u540c\u65f6\u4fdd\u8bc1\u8fc7\u7a0b\u7684\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.12107", "pdf": "https://arxiv.org/pdf/2602.12107", "abs": "https://arxiv.org/abs/2602.12107", "authors": ["Haolin Liu", "Braham Snyder", "Chen-Yu Wei"], "title": "On the Complexity of Offline Reinforcement Learning with $Q^\\star$-Approximation and Partial Coverage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We study offline reinforcement learning under $Q^\\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: \"Are $Q^\\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?\"\n  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\\star$ estimation procedures, modularizing and generalizing existing approaches.\n  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $\u03b5^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $\u03b5^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\\star$-realizability and Bellman completeness beyond the tabular case.", "AI": {"tldr": "\u7814\u7a76Q*-\u8fd1\u4f3c\u548c\u90e8\u5206\u8986\u76d6\u4e0b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed9\u51fa\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u5f15\u5165\u901a\u7528\u6846\u67b6\uff0c\u6709\u591a\u9879\u6539\u8fdb\u548c\u9996\u6b21\u5206\u6790\u3002", "motivation": "\u56de\u7b54Q*-\u53ef\u5b9e\u73b0\u6027\u548cBellman\u5b8c\u5907\u6027\u5728\u90e8\u5206\u8986\u76d6\u4e0b\u5bf9\u6837\u672c\u9ad8\u6548\u79bb\u7ebfRL\u662f\u5426\u5145\u5206\u7684\u5f00\u653e\u95ee\u9898\uff0c\u8be5\u8bbe\u7f6e\u6709\u5b9e\u9645\u7b97\u6cd5\u4f46\u7406\u8bba\u5173\u6ce8\u5c11\u3002", "method": "\u5efa\u7acb\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u5f15\u5165\u53d7\u5728\u7ebfRL\u6a21\u578b\u65e0\u5173\u51b3\u7b56\u4f30\u8ba1\u7cfb\u6570\u542f\u53d1\u7684\u901a\u7528\u6846\u67b6\uff0c\u5f00\u53d1\u4e8c\u9636\u6027\u80fd\u5dee\u5f02\u5f15\u7406\u3002", "result": "\u5426\u5b9a\u56de\u7b54\u5f00\u653e\u95ee\u9898\uff1b\u6846\u67b6\u6062\u590d\u5e76\u6539\u8fdb\u5df2\u6709\u4fdd\u8bc1\u7684\u76f8\u5173\u91cf\uff1b\u8f6fQ\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u4ece\u03b5\u207b\u2074\u63d0\u5347\u5230\u03b5\u207b\u00b2\uff1b\u53bb\u9664\u989d\u5916\u5728\u7ebf\u4ea4\u4e92\u9700\u6c42\uff1b\u9996\u6b21\u523b\u753b\u65e0Bellman\u5b8c\u5907\u6027\u7684\u4e00\u822c\u4f4eBellman\u79e9MDP\u7684\u79bb\u7ebf\u53ef\u5b66\u4e60\u6027\uff1b\u9996\u6b21\u5bf9\u975e\u8868\u683c\u60c5\u5f62\u4e0bCQL\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u591a\u4e2a\u65b9\u9762\u6709\u6539\u8fdb\u548c\u62d3\u5c55\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2602.11220", "pdf": "https://arxiv.org/pdf/2602.11220", "abs": "https://arxiv.org/abs/2602.11220", "authors": ["Jiacheng Wang", "Ping Jian", "Zhen Yang", "Zirong Chen", "Keren Liao", "Zhongbin Guo"], "title": "Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u91cd\u5199\u4ee3\u7406\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u53ef\u51cf\u5c11\u975e\u4e0b\u6e38\u57fa\u51c6\u9057\u5fd8\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0b\u6e38\u5fae\u8c03\u65f6\u6570\u636e\u5206\u5e03\u504f\u79fb\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u73b0\u6709\u6570\u636e\u91cd\u5199\u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u4e0e\u6a21\u578b\u81ea\u7136\u95ee\u7b54\u751f\u6210\u5206\u5e03\u4e0d\u5339\u914d\u3001\u591a\u6837\u6027\u574d\u584c\u95ee\u9898\u3002", "method": "\u5c06\u6570\u636e\u91cd\u5199\u89c6\u4e3a\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u91cd\u5199\u5206\u5e03\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u91cd\u5199\u4ee3\u7406\u3002", "result": "\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u589e\u76ca\u4e0e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u76f8\u5f53\uff0c\u5e73\u5747\u51cf\u5c11\u975e\u4e0b\u6e38\u57fa\u51c6\u9057\u5fd812.34%\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6784\u5efa\u9ad8\u8d28\u91cf\u91cd\u5199\u6570\u636e\u96c6\u7528\u4e8e\u4e0b\u6e38\u5fae\u8c03\uff0c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2602.12253", "pdf": "https://arxiv.org/pdf/2602.12253", "abs": "https://arxiv.org/abs/2602.12253", "authors": ["Yang Cai", "Haipeng Luo", "Chen-Yu Wei", "Weiqiang Zheng"], "title": "Is Online Linear Optimization Sufficient for Strategic Robustness?", "categories": ["cs.GT", "cs.LG"], "comment": "26 pages", "summary": "We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller's manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.\n  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\\sqrt{T \\log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\\sqrt{T (\\log K+\\log(T/\u03b4)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u7b80\u5355\u5728\u7ebf\u7ebf\u6027\u4f18\u5316\u7b97\u6cd5\u7528\u4e8e\u5177\u6709\u6218\u7565\u9c81\u68d2\u6027\u548c\u4f4e\u540e\u6094\u503c\u7684\u6295\u6807\u7b97\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u6784\u9020\u9ed1\u76d2\u7ea6\u51cf\u65b9\u6cd5\u5c06OLO\u7b97\u6cd5\u8f6c\u5316\u4e3a\u6218\u7565\u9c81\u68d2\u7684\u65e0\u540e\u6094\u6295\u6807\u7b97\u6cd5\u5e76\u7ed9\u51fa\u4e0d\u540c\u60c5\u51b5\u4e0b\u7684\u540e\u6094\u503c\u7ed3\u679c\u3002", "motivation": "\u5df2\u6709\u8fbe\u5230\u6700\u4f18\u540e\u6094\u503c\u7684\u6295\u6807\u7b97\u6cd5\u5728\u5e94\u5bf9\u5356\u5bb6\u64cd\u7eb5\u7684\u6218\u7565\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u57fa\u4e8e\u65e0\u4ea4\u6362\u540e\u6094\u7b97\u6cd5\u7684\u6295\u6807\u7b97\u6cd5\u5728\u7edf\u8ba1\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4e0d\u4f73\uff0c\u56e0\u6b64\u63a2\u7d22\u7b80\u5355\u5728\u7ebf\u7ebf\u6027\u4f18\u5316 (OLO) \u7b97\u6cd5\u662f\u5426\u80fd\u5b9e\u73b0\u7406\u60f3\u5c5e\u6027\u3002", "method": "\u6784\u5efa\u7b80\u5355\u7684\u9ed1\u76d2\u7ea6\u51cf\u65b9\u6cd5\uff0c\u5c06\u4efb\u4f55OLO\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5177\u6709\u6218\u7565\u9c81\u68d2\u6027\u7684\u65e0\u540e\u6094\u6295\u6807\u7b97\u6cd5\u3002", "result": "\u5728\u5df2\u77e5\u548c\u672a\u77e5\u4ef7\u503c\u5206\u5e03\u8bbe\u7f6e\u4e0b\u5747\u80fd\u5b9e\u73b0\uff0c\u5df2\u77e5\u5206\u5e03\u65f6\u8fbeO(\u221a(T log K))\u540e\u6094\u503c\u548c\u6218\u7565\u9c81\u68d2\u6027\uff0c\u672a\u77e5\u5206\u5e03\u65f6\u5177\u9ad8\u6982\u7387O(\u221a(T (log K + log(T/\u03b4))))\u540e\u6094\u503c\u548c\u6218\u7565\u9c81\u68d2\u6027\uff0c\u4e14\u53bb\u9664\u5047\u8bbe\u3002", "conclusion": "\u6b21\u7ebf\u6027\u7ebf\u6027\u5316\u540e\u6094\u8db3\u4ee5\u5b9e\u73b0\u6218\u7565\u9c81\u68d2\u6027\uff0c\u7b80\u5355OLO\u7b97\u6cd5\u53ef\u7528\u4e8e\u6784\u9020\u5177\u6709\u8fd9\u4e24\u4e2a\u7406\u60f3\u5c5e\u6027\u7684\u6295\u6807\u7b97\u6cd5\u3002"}}
{"id": "2602.11156", "pdf": "https://arxiv.org/pdf/2602.11156", "abs": "https://arxiv.org/abs/2602.11156", "authors": ["Sungmoon Kim", "Hyuna Jeon", "Dahye Kim", "Mingyu Kim", "Dong-Kyu Chae", "Jiwoong Kim"], "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.", "AI": {"tldr": "\u63d0\u51faHybridRAG\u6846\u67b6\u5904\u7406\u975e\u7ed3\u6784\u5316PDF\u6587\u6863\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6bd4\u6807\u51c6RAG\u57fa\u7ebf\u8d28\u91cf\u66f4\u9ad8\u3001\u5ef6\u8fdf\u66f4\u4f4e\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u804a\u5929\u673a\u5668\u4eba\u5e94\u7528\u3002", "motivation": "\u73b0\u6709RAG\u7814\u7a76\u5047\u5b9a\u6587\u672c\u6e90\u7ed3\u6784\u826f\u597d\uff0c\u5728\u67e5\u8be2\u65f6\u8fdb\u884c\u68c0\u7d22\u548c\u751f\u6210\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u804a\u5929\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7OCR\u548c\u5e03\u5c40\u5206\u6790\u5904\u7406\u539f\u59cb\u975e\u7ed3\u6784\u5316PDF\u6587\u6863\uff0c\u8f6c\u5316\u4e3a\u5206\u5c42\u6587\u672c\u5757\uff0c\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u751f\u6210\u95ee\u7b54\u77e5\u8bc6\u5e93\uff0c\u67e5\u8be2\u65f6\u5148\u5339\u914d\u77e5\u8bc6\u5e93\uff0c\u65e0\u5339\u914d\u518d\u5373\u65f6\u751f\u6210\u3002", "result": "\u5728OHRBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHybridRAG\u6bd4\u6807\u51c6RAG\u57fa\u7ebf\u7b54\u6848\u8d28\u91cf\u66f4\u9ad8\u3001\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "HybridRAG\u662f\u5904\u7406\u5927\u91cf\u975e\u7ed3\u6784\u5316\u6587\u6863\u548c\u5927\u91cf\u7528\u6237\u3001\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u73b0\u5b9e\u804a\u5929\u673a\u5668\u4eba\u5e94\u7528\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11988", "pdf": "https://arxiv.org/pdf/2602.11988", "abs": "https://arxiv.org/abs/2602.11988", "authors": ["Thibaud Gloaguen", "Niels M\u00fcndler", "Mark M\u00fcller", "Veselin Raychev", "Martin Vechev"], "title": "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.\n  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.", "AI": {"tldr": "\u7814\u7a76\u4ee3\u7801\u4ee3\u7406\u4f7f\u7528\u4e0a\u4e0b\u6587\u6587\u4ef6\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u964d\u4f4e\u4efb\u52a1\u6210\u529f\u7387\u3001\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u5efa\u8bae\u4e0a\u4e0b\u6587\u6587\u4ef6\u4ec5\u63cf\u8ff0\u6700\u4f4e\u8981\u6c42\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u4ee3\u7406\u4f7f\u7528\u4e0a\u4e0b\u6587\u6587\u4ef6\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u6709\u6548\u6027\u7684\u4e25\u683c\u7814\u7a76\u3002", "method": "\u5728\u4e24\u79cd\u4e92\u8865\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u7684\u4efb\u52a1\u5b8c\u6210\u6027\u80fd\uff0c\u5305\u62ec\u6d41\u884c\u4ed3\u5e93\u7684\u65e2\u5b9aSWE - bench\u4efb\u52a1\u548c\u5305\u542b\u5f00\u53d1\u8005\u63d0\u4ea4\u4e0a\u4e0b\u6587\u6587\u4ef6\u4ed3\u5e93\u7684\u65b0\u95ee\u9898\u96c6\u3002", "result": "\u4e0a\u4e0b\u6587\u6587\u4ef6\u76f8\u6bd4\u4e0d\u63d0\u4f9b\u4ed3\u5e93\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u4efb\u52a1\u6210\u529f\u7387\uff0c\u589e\u52a0\u63a8\u7406\u6210\u672c\u8d8520%\uff0c\u4f1a\u4fc3\u4f7f\u66f4\u5e7f\u6cdb\u63a2\u7d22\u4e14\u4ee3\u7801\u4ee3\u7406\u503e\u5411\u9075\u5faa\u6307\u4ee4\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u6587\u4ef6\u7684\u4e0d\u5fc5\u8981\u8981\u6c42\u4f7f\u4efb\u52a1\u66f4\u96be\uff0c\u4eba\u7c7b\u7f16\u5199\u7684\u4e0a\u4e0b\u6587\u6587\u4ef6\u5e94\u4ec5\u63cf\u8ff0\u6700\u4f4e\u8981\u6c42\u3002"}}
{"id": "2602.11541", "pdf": "https://arxiv.org/pdf/2602.11541", "abs": "https://arxiv.org/abs/2602.11541", "authors": ["Hanbing Liu", "Chunhao Tian", "Nan An", "Ziyuan Wang", "Pinyan Lu", "Changyuan Yu", "Qi Qi"], "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.", "AI": {"tldr": "\u7814\u7a76\u9884\u7b97\u7ea6\u675f\u4e0b\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\uff0c\u63d0\u51faINTENT\u6846\u67b6\u89e3\u51b3\u76f8\u5173\u6311\u6218\uff0c\u5728\u6210\u672c\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u4e14\u5177\u9c81\u68d2\u6027", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e25\u683c\u8d27\u5e01\u9884\u7b97\u4e0b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u89e3\u51b3\u591a\u6b65\u4efb\u52a1\u7684\u96be\u9898\uff0c\u4f20\u7edf\u76f4\u63a5\u89c4\u5212\u56e0\u5404\u79cd\u56e0\u7d20\u96be\u4ee5\u5b9e\u65bd", "method": "\u63d0\u51faINTENT\u63a8\u7406\u65f6\u95f4\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u610f\u56fe\u611f\u77e5\u5206\u5c42\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u5de5\u5177\u4f7f\u7528\u3001\u8fdb\u884c\u98ce\u9669\u6821\u51c6\u6210\u672c\u5e76\u5728\u7ebf\u5f15\u5bfc\u51b3\u7b56", "result": "\u5728\u6210\u672c\u589e\u5f3a\u7684StableToolBench\u4e0a\uff0cINTENT\u4e25\u683c\u6ee1\u8db3\u9884\u7b97\u53ef\u884c\u6027\uff0c\u5927\u5e45\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5728\u52a8\u6001\u5e02\u573a\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "INTENT\u80fd\u6709\u6548\u5e94\u5bf9\u9884\u7b97\u7ea6\u675f\u4e0b\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u76f8\u5173\u95ee\u9898\uff0c\u5728\u4efb\u52a1\u6267\u884c\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.11234", "pdf": "https://arxiv.org/pdf/2602.11234", "abs": "https://arxiv.org/abs/2602.11234", "authors": ["Ankita Paul", "Wenyi Wang"], "title": "Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.", "AI": {"tldr": "\u63d0\u51faTopoGBM\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u591a\u53c2\u65703D MRI\u6355\u83b7\u5f02\u8d28\u6027\u4fdd\u7559\u3001\u626b\u63cf\u4eea\u9c81\u68d2\u7684\u8868\u5f81\uff0c\u5728\u591a\u4e2a\u961f\u5217\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u7ed3\u5408\u62d3\u6251\u5148\u9a8c\u53ef\u5b66\u4e60\u5f62\u6001\u5fe0\u5b9e\u5d4c\u5165\u3002", "motivation": "GBM\u7cbe\u786e\u9884\u540e\u53d7\u80bf\u7624\u5f02\u8d28\u6027\u548cMRI\u91c7\u96c6\u534f\u8bae\u4e0d\u4e00\u81f4\u5f71\u54cd\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u5f62\u6001\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faTopoGBM\u6846\u67b6\uff0c\u7528\u62d3\u6251\u6b63\u5219\u5316\u76843D\u5377\u79ef\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4fdd\u7559\u80bf\u7624\u6d41\u5f62\u7684\u590d\u6742\u975e\u6b27\u4e0d\u53d8\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u961f\u5217\u8bc4\u4f30\u4e2d\uff0cTopoGBM\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u91cd\u5efa\u6b8b\u5dee\u9ad8\u5ea6\u5b9a\u4f4d\u5728\u75c5\u7406\u5f02\u8d28\u6027\u533a\u57df\uff0c\u7ea650%\u9884\u540e\u4fe1\u53f7\u5b9a\u4f4d\u5728\u80bf\u7624\u53ca\u5468\u56f4\u5fae\u73af\u5883\u3002", "conclusion": "\u7ed3\u5408\u62d3\u6251\u5148\u9a8c\u80fd\u5b66\u4e60\u6355\u83b7\u80bf\u7624\u5f02\u8d28\u6027\u4e14\u4fdd\u6301\u8de8\u673a\u6784\u9c81\u68d2\u6027\u7684\u5f62\u6001\u5fe0\u5b9e\u5d4c\u5165\u3002"}}
{"id": "2602.11754", "pdf": "https://arxiv.org/pdf/2602.11754", "abs": "https://arxiv.org/abs/2602.11754", "authors": ["Keita Nishimoto", "Kimitaka Asatani", "Ichiro Sakata"], "title": "Cooperation Breakdown in LLM Agents Under Communication Delays", "categories": ["cs.MA", "cs.AI", "cs.GT"], "comment": null, "summary": "LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. We propose the FLCOA framework (Five Layers for Cooperation/Coordination among Autonomous Agents) to conceptualize how cooperation and coordination emerge in groups of autonomous agents, and highlight that the influence of lower-layer factors - especially computational and communication resources - has been largely overlooked. To examine the effect of communication delay, we introduce a Continuous Prisoner's Dilemma with Communication Delay and conduct simulations with LLM-based agents. As delay increases, agents begin to exploit slower responses even without explicit instructions. Interestingly, excessive delay reduces cycles of exploitation, yielding a U-shaped relationship between delay magnitude and mutual cooperation. These results suggest that fostering cooperation requires attention not only to high-level institutional design but also to lower-layer factors such as communication delay and resource allocation, pointing to new directions for MAS research.", "AI": {"tldr": "\u63d0\u51faFLCOA\u6846\u67b6\u7814\u7a76LLM - MAS\u4e2d\u534f\u4f5c\u95ee\u9898\uff0c\u5f15\u5165\u5e26\u901a\u4fe1\u5ef6\u8fdf\u7684\u8fde\u7eed\u56da\u5f92\u56f0\u5883\u6a21\u62df\uff0c\u53d1\u73b0\u5ef6\u8fdf\u4e0e\u5408\u4f5c\u5448U\u5f62\u5173\u7cfb\uff0c\u6307\u51faMAS\u7814\u7a76\u65b0\u65b9\u5411\u3002", "motivation": "\u4e3a\u4f7fLLM - MAS\u80fd\u5728\u73b0\u5b9e\u4e2d\u90e8\u7f72\uff0c\u9700\u8ba9\u667a\u80fd\u4f53\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u5408\u4f5c\u4e0e\u534f\u8c03\u3002", "method": "\u63d0\u51faFLCOA\u6846\u67b6\u7406\u8bba\u5316\u667a\u80fd\u4f53\u534f\u4f5c\u8fc7\u7a0b\uff1b\u5f15\u5165\u5e26\u901a\u4fe1\u5ef6\u8fdf\u7684\u8fde\u7eed\u56da\u5f92\u56f0\u5883\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\u3002", "result": "\u5ef6\u8fdf\u589e\u52a0\u65f6\u667a\u80fd\u4f53\u5f00\u59cb\u5229\u7528\u53cd\u5e94\u6162\u6765\u83b7\u5229\uff1b\u8fc7\u5ea6\u5ef6\u8fdf\u51cf\u5c11\u5265\u524a\u5faa\u73af\uff0c\u5ef6\u8fdf\u5e45\u5ea6\u4e0e\u76f8\u4e92\u5408\u4f5c\u5448U\u5f62\u5173\u7cfb\u3002", "conclusion": "\u4fc3\u8fdb\u5408\u4f5c\u4e0d\u4ec5\u8981\u5173\u6ce8\u9ad8\u5c42\u5236\u5ea6\u8bbe\u8ba1\uff0c\u8fd8\u9700\u5173\u6ce8\u901a\u4fe1\u5ef6\u8fdf\u548c\u8d44\u6e90\u5206\u914d\u7b49\u5e95\u5c42\u56e0\u7d20\u3002"}}
{"id": "2602.11160", "pdf": "https://arxiv.org/pdf/2602.11160", "abs": "https://arxiv.org/abs/2602.11160", "authors": ["Alexanne Worm", "Florian Marchal", "Sylvain Castagnos"], "title": "BIRD: A Museum Open Dataset Combining Behavior Patterns and Identity Types to Better Model Visitors' Experience", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": null, "summary": "Lack of data is a recurring problem in Artificial Intelligence, as it is essential for training and validating models. This is particularly true in the field of cultural heritage, where the number of open datasets is relatively limited and where the data collected does not always allow for holistic modeling of visitors' experience due to the fact that data are ad hoc (i.e. restricted to the sole characteristics required for the evaluation of a specific model). To overcome this lack, we conducted a study between February and March 2019 aimed at obtaining comprehensive and detailed information about visitors, their visit experience and their feedback. We equipped 51 participants with eye-tracking glasses, leaving them free to explore the 3 floors of the museum for an average of 57 minutes, and to discover an exhibition of more than 400 artworks. On this basis, we built an open dataset combining contextual data (demographic data, preferences, visiting habits, motivations, social context. . . ), behavioral data (spatiotemporal trajectories, gaze data) and feedback (satisfaction, fatigue, liked artworks, verbatim. . . ). Our analysis made it possible to re-enact visitor identities combining the majority of characteristics found in the literature and to reproduce the Veron and Levasseur profiles. This dataset will ultimately make it possible to improve the quality of recommended paths in museums by personalizing the number of points of interest (POIs), the time spent at these different POIs, and the amount of information to be provided to each visitor based on their level of interest.", "AI": {"tldr": "\u7814\u7a76\u89e3\u51b3\u6587\u5316\u9057\u4ea7\u9886\u57df\u4eba\u5de5\u667a\u80fd\u6570\u636e\u7f3a\u4e4f\u95ee\u9898\uff0c\u6784\u5efa\u5f00\u653e\u6570\u636e\u96c6\u4ee5\u6539\u5584\u535a\u7269\u9986\u63a8\u8350\u8def\u5f84\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6a21\u578b\u9700\u5927\u91cf\u6570\u636e\uff0c\u4f46\u6587\u5316\u9057\u4ea7\u9886\u57df\u5f00\u653e\u6570\u636e\u96c6\u6709\u9650\uff0c\u73b0\u6709\u6570\u636e\u96be\u4ee5\u5168\u9762\u5efa\u6a21\u6e38\u5ba2\u4f53\u9a8c\u3002", "method": "2019\u5e742 - 3\u6708\u5bf951\u540d\u53c2\u4e0e\u8005\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u773c\u955c\u6536\u96c6\u6570\u636e\uff0c\u8ba9\u5176\u81ea\u7531\u53c2\u89c2\u535a\u7269\u9986\uff0c\u7136\u540e\u7ed3\u5408\u8bed\u5883\u3001\u884c\u4e3a\u548c\u53cd\u9988\u6570\u636e\u6784\u5efa\u5f00\u653e\u6570\u636e\u96c6\u3002", "result": "\u53ef\u91cd\u65b0\u6784\u5efa\u6e38\u5ba2\u8eab\u4efd\uff0c\u91cd\u73b0Veron\u548cLevasseur\u6863\u6848\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u80fd\u57fa\u4e8e\u6e38\u5ba2\u5174\u8da3\u4e2a\u6027\u5316\u8c03\u6574\uff0c\u63d0\u9ad8\u535a\u7269\u9986\u63a8\u8350\u8def\u5f84\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.12038", "pdf": "https://arxiv.org/pdf/2602.12038", "abs": "https://arxiv.org/abs/2602.12038", "authors": ["Yuejun Guo", "Qiang Hu", "Qiang Tang", "Yves Le Traon"], "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection", "categories": ["cs.SE", "cs.AI"], "comment": "This paper was accepted by the 28th European Symposium on Research in Computer Security (ESORICS), 2023", "summary": "Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u6a21\u578b\u8868\u73b0\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u662f\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\uff0c\u5e76\u7814\u7a76\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5404\u65b9\u6848\u8868\u73b0\u4e0d\u540c\u4e14\u65e0\u5168\u9762\u4f18\u79c0\u8005\uff0c\u8fd8\u63a2\u8ba8\u5916\u90e8\u5f71\u54cd\u4ee5\u5f00\u53d1\u65b0\u65b9\u6848\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7528\u4e8e\u6f0f\u6d1e\u68c0\u6d4b\u867d\u6709\u524d\u666f\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u63a8\u6d4b\u662f\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u5bfc\u81f4\uff0c\u56e0\u6b64\u5f00\u5c55\u7814\u7a76\u3002", "method": "\u5bf9\u4e5d\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u8bc1\u5b9e\u6570\u636e\u4e0d\u5e73\u8861\u662f\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u7684\u6838\u5fc3\u539f\u56e0\uff1b\u4e0d\u540c\u7684\u4e0d\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u540c\uff0c\u5982Focal loss\u9002\u5408\u63d0\u9ad8\u7cbe\u5ea6\u7b49\uff0c\u4e14\u65e0\u65b9\u6848\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u51fa\u8272\u3002", "conclusion": "\u6570\u636e\u4e0d\u5e73\u8861\u662f\u6df1\u5ea6\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u7684\u5173\u952e\u56e0\u7d20\uff0c\u73b0\u6709\u4e0d\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u6709\u5c40\u9650\u6027\uff0c\u9700\u63a2\u7d22\u5916\u90e8\u5f71\u54cd\u4ee5\u5f00\u53d1\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.11569", "pdf": "https://arxiv.org/pdf/2602.11569", "abs": "https://arxiv.org/abs/2602.11569", "authors": ["Zhenlin Qin", "Yancheng Ling", "Leizhen Wang", "Francisco C\u00e2mara Pereira", "Zhenliang Ma"], "title": "SemaPop: Semantic-Persona Conditioned Population Synthesis", "categories": ["cs.AI"], "comment": null, "summary": "Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and statistical constraints, leaving a gap in semantic-conditioned population generation that can capture abstract behavioral patterns implicitly in survey data. This study proposes SemaPop, a semantic-statistical population synthesis model that integrates large language models (LLMs) with generative population modeling. SemaPop derives high-level persona representations from individual survey records and incorporates them as semantic conditioning signals for population generation, while marginal regularization is introduced to enforce alignment with target population marginals. In this study, the framework is instantiated using a Wasserstein GAN with gradient penalty (WGAN-GP) backbone, referred to as SemaPop-GAN. Extensive experiments demonstrate that SemaPop-GAN achieves improved generative performance, yielding closer alignment with target marginal and joint distributions while maintaining sample-level feasibility and diversity under semantic conditioning. Ablation studies further confirm the contribution of semantic persona conditioning and architectural design choices to balancing marginal consistency and structural realism. These results demonstrate that SemaPop-GAN enables controllable and interpretable population synthesis through effective semantic-statistical information fusion. SemaPop-GAN also provides a promising modular foundation for developing generative population projection systems that integrate individual-level behavioral semantics with population-level statistical constraints.", "AI": {"tldr": "\u63d0\u51faSemaPop\u6a21\u578b\u7528\u4e8e\u4eba\u53e3\u5408\u6210\uff0c\u4ee5\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u751f\u6210\u5f0f\u5efa\u6a21\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u52bf\u53ca\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u6761\u4ef6\u4eba\u53e3\u751f\u6210\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6355\u6349\u8c03\u67e5\u6570\u636e\u4e2d\u7684\u62bd\u8c61\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51faSemaPop\u6a21\u578b\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u751f\u6210\u5f0f\u4eba\u53e3\u5efa\u6a21\uff0c\u4ee5Wasserstein GAN\u4e3a\u9aa8\u5e72\u5b9e\u73b0SemaPop - GAN\uff0c\u5f15\u5165\u8fb9\u9645\u6b63\u5219\u5316\u3002", "result": "SemaPop - GAN\u751f\u6210\u6027\u80fd\u63d0\u5347\uff0c\u4e0e\u76ee\u6807\u5206\u5e03\u66f4\u5339\u914d\uff0c\u4fdd\u6301\u6837\u672c\u53ef\u884c\u6027\u548c\u591a\u6837\u6027\uff1b\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u8bed\u4e49\u89d2\u8272\u6761\u4ef6\u548c\u67b6\u6784\u8bbe\u8ba1\u7684\u4f5c\u7528\u3002", "conclusion": "SemaPop - GAN\u80fd\u901a\u8fc7\u6709\u6548\u878d\u5408\u8bed\u4e49\u548c\u7edf\u8ba1\u4fe1\u606f\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u7684\u4eba\u53e3\u5408\u6210\uff0c\u4e3a\u751f\u6210\u5f0f\u4eba\u53e3\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2602.11237", "pdf": "https://arxiv.org/pdf/2602.11237", "abs": "https://arxiv.org/abs/2602.11237", "authors": ["Mujeeb Ur Rehman", "Imran Rehan", "Sohail Khalid"], "title": "AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u7528\u4e8e\u8bca\u65ad2\u578b\u7cd6\u5c3f\u75c5\u7684AI - CDSS\uff0c\u7ed3\u679c\u663e\u793a\u5176\u51c6\u786e\u7387\u9ad8\uff0c\u6216\u53ef\u6210\u8bca\u65ad\u6709\u7528\u5de5\u5177\u3002", "motivation": "\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u6709\u6311\u6218\uff0cAI - CDSS\u53ef\u52a9\u529b\u533b\u7597\u4eba\u5458\u51c6\u786e\u8bca\u65ad\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u7279\u5b9a\u7684AI - CDSS\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u4e13\u5bb6\u89c1\u89e3\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u75281298\u4f8b\u60a3\u8005\u6570\u636e\u5f00\u53d1\u548c\u6d4b\u8bd5AI - CDSS\uff0c\u5229\u7528\u5173\u952e\u7279\u5f81\u9884\u6d4b\uff0c\u5f00\u5c55105\u4f8b\u60a3\u8005\u7684\u4e34\u5e8a\u8bd5\u70b9\u7814\u7a76\u3002", "result": "AI - CDSS\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u6d4b\u8bd5\u6570\u636e\u96c6\u91cc\u4e0e\u5185\u5206\u6ccc\u4e13\u5bb6\u4e00\u81f4\u6027\u8fbe98.8%\uff0c\u8bd5\u70b9\u7814\u7a76\u4e2d\u4e0e\u7cd6\u5c3f\u75c5\u4e13\u5bb6\u4e00\u81f4\u6027\u8fbe98.5%\uff0c\u8fdc\u8d85\u975e\u5185\u5206\u6ccc\u4e13\u5bb6\u768485% \u3002", "conclusion": "AI - CDSS\u6709\u6f5c\u529b\u6210\u4e3a\u51c6\u786e\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u7684\u6709\u7528\u5de5\u5177\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u7cd6\u5c3f\u75c5\u4e13\u5bb6\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.11829", "pdf": "https://arxiv.org/pdf/2602.11829", "abs": "https://arxiv.org/abs/2602.11829", "authors": ["Juan Agustin Duque", "Razvan Ciuca", "Ayoub Echchahed", "Hugo Larochelle", "Aaron Courville"], "title": "Towards Sustainable Investment Policies Informed by Opponent Shaping", "categories": ["cs.LG", "cs.GT"], "comment": "Accepted at ICLR 2026", "summary": "Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76InvestESG\u6a21\u62df\u4e2d\u7684\u8de8\u671f\u793e\u4f1a\u56f0\u5883\uff0c\u5e94\u7528Advantage Alignment\u7b97\u6cd5\u5f71\u54cd\u4ee3\u7406\u5b66\u4e60\uff0c\u8bc1\u660e\u7b56\u7565\u6027\u5851\u9020\u5b66\u4e60\u8fc7\u7a0b\u53ef\u5e26\u6765\u66f4\u597d\u7ed3\u679c\u4ee5\u52a9\u529b\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u9700\u5168\u7403\u534f\u8c03\uff0c\u4f46\u7ecf\u6d4e\u4e3b\u4f53\u5e38\u91cd\u77ed\u671f\u5229\u76ca\u81f4\u793e\u4f1a\u56f0\u5883\uff0c\u8981\u7814\u7a76InvestESG\u6a21\u62df\u4e2d\u7684\u793e\u4f1a\u56f0\u5883\u53ca\u89e3\u51b3\u529e\u6cd5\u3002", "method": "\u5bf9InvestESG\u51fa\u73b0\u8de8\u671f\u793e\u4f1a\u56f0\u5883\u7684\u6761\u4ef6\u8fdb\u884c\u5f62\u5f0f\u5316\u523b\u753b\uff0c\u63a8\u5bfc\u7406\u8bba\u9608\u503c\uff0c\u5e94\u7528Advantage Alignment\u7b97\u6cd5\u5f71\u54cd\u4ee3\u7406\u5b66\u4e60\u3002", "result": "Advantage Alignment\u80fd\u4f7f\u5b66\u4e60\u52a8\u6001\u504f\u5411\u5408\u4f5c\u7ed3\u679c\uff0c\u7b56\u7565\u6027\u5851\u9020\u7ecf\u6d4e\u4e3b\u4f53\u5b66\u4e60\u8fc7\u7a0b\u53ef\u5e26\u6765\u66f4\u597d\u7ed3\u679c\u3002", "conclusion": "\u7b56\u7565\u6027\u5851\u9020\u7ecf\u6d4e\u4e3b\u4f53\u5b66\u4e60\u8fc7\u7a0b\u7684\u7ed3\u679c\u53ef\u4e3a\u653f\u7b56\u673a\u5236\u63d0\u4f9b\u4fe1\u606f\uff0c\u4f7f\u5e02\u573a\u6fc0\u52b1\u4e0e\u957f\u671f\u53ef\u6301\u7eed\u76ee\u6807\u66f4\u597d\u5bf9\u9f50\u3002"}}
{"id": "2602.12058", "pdf": "https://arxiv.org/pdf/2602.12058", "abs": "https://arxiv.org/abs/2602.12058", "authors": ["Zhiyong Chen", "Jialun Cao", "Chang Xu", "Shing-Chi Cheung"], "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair", "categories": ["cs.SE", "cs.AI", "cs.FL"], "comment": "Accepted by FM 2026 Research Track (Tool)", "summary": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.", "AI": {"tldr": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u73af\u5883ModelWisdom\uff0c\u7528\u53ef\u89c6\u5316\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8ba9TLA+\u6a21\u578b\u68c0\u67e5\u66f4\u6613\u89e3\u91ca\u548c\u64cd\u4f5c\uff0c\u6539\u5584\u8f93\u51fa\u89e3\u8bfb\u548c\u8c03\u8bd5\u6548\u679c\u3002", "motivation": "\u5f53\u524dTLA+\u6a21\u578b\u68c0\u67e5\u4e2d\uff0c\u4ece\u4e1a\u8005\u5728\u89e3\u8bfb\u53cd\u4f8b\u3001\u7406\u89e3\u72b6\u6001\u8f6c\u6362\u56fe\u548c\u4fee\u590d\u6a21\u578b\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u539f\u59cb\u8f93\u51fa\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u624b\u52a8\u56de\u6eaf\u56f0\u96be\uff0c\u73b0\u6709\u5de5\u5177\u529f\u80fd\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1ModelWisdom\uff0c\u5177\u5907\u6a21\u578b\u53ef\u89c6\u5316\u3001\u56fe\u4f18\u5316\u3001\u6a21\u578b\u6458\u8981\u548c\u6a21\u578b\u4fee\u590d\u7b49\u529f\u80fd\u3002", "result": "\u5c06\u539f\u59cb\u6a21\u578b\u68c0\u67e5\u8f93\u51fa\u8f6c\u5316\u4e3a\u53ef\u4ea4\u4e92\u3001\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6539\u5584\u5bf9\u975e\u5e73\u51e1TLA+\u89c4\u8303\u7684\u7406\u89e3\u5e76\u51cf\u5c11\u8c03\u8bd5\u5de5\u4f5c\u91cf\u3002", "conclusion": "ModelWisdom\u63d0\u5347\u4e86TLA+\u6a21\u578b\u68c0\u67e5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u95ee\u9898\u3002"}}
{"id": "2602.11574", "pdf": "https://arxiv.org/pdf/2602.11574", "abs": "https://arxiv.org/abs/2602.11574", "authors": ["Aditya Taparia", "Som Sagar", "Ransalu Senanayake"], "title": "Learning to Configure Agentic AI Systems", "categories": ["cs.AI"], "comment": "21 pages, 13 figures", "summary": "Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARC\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5206\u5c42\u7b56\u7565\u52a8\u6001\u914d\u7f6e\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8bc1\u660e\u9010\u67e5\u8be2\u914d\u7f6e\u4ee3\u7406\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7cfb\u7edf\u914d\u7f6e\u91c7\u7528\u56fa\u5b9a\u6a21\u677f\u6216\u624b\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u884c\u4e3a\u8106\u5f31\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u5c06\u4ee3\u7406\u914d\u7f6e\u95ee\u9898\u8868\u8ff0\u4e3a\u9010\u67e5\u8be2\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5206\u5c42\u7b56\u7565\u52a8\u6001\u914d\u7f6e\u3002", "result": "\u5728\u63a8\u7406\u548c\u5de5\u5177\u589e\u5f3a\u95ee\u7b54\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u548c\u5176\u4ed6\u57fa\u7ebf\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534725%\uff0c\u5e76\u964d\u4f4e\u4e86\u4ee4\u724c\u548c\u8fd0\u884c\u65f6\u6210\u672c\u3002", "conclusion": "\u9010\u67e5\u8be2\u5b66\u4e60\u4ee3\u7406\u914d\u7f6e\u662f\u201c\u4e00\u5200\u5207\u201d\u8bbe\u8ba1\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.11243", "pdf": "https://arxiv.org/pdf/2602.11243", "abs": "https://arxiv.org/abs/2602.11243", "authors": ["Alina Shutova", "Alexandra Olenina", "Ivan Vinogradov", "Anton Sinitsin"], "title": "Evaluating Memory Structure in LLM Agents", "categories": ["cs.LG", "cs.CL"], "comment": "Preprint, work in progress", "summary": "Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.", "AI": {"tldr": "\u63d0\u51faStructMemEval\u57fa\u51c6\u6d4b\u8bd5LLM\u4ee3\u7406\u7ec4\u7ec7\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u521d\u59cb\u5b9e\u9a8c\u663e\u793a\u7b80\u5355\u68c0\u7d22\u589e\u5f3aLLM\u6709\u56f0\u96be\uff0c\u73b0\u4ee3LLM\u5728\u65e0\u63d0\u793a\u65f6\u4e0d\u80fd\u8bc6\u522b\u8bb0\u5fc6\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u57fa\u51c6\u96be\u4ee5\u5206\u6790\u590d\u6742\u8bb0\u5fc6\u67b6\u6784\u80fd\u529b\uff0c\u65e0\u6cd5\u6d4b\u8bd5\u590d\u6742\u8bb0\u5fc6\u5c42\u6b21\uff0c\u9700\u65b0\u57fa\u51c6\u3002", "method": "\u63d0\u51faStructMemEval\u57fa\u51c6\uff0c\u6536\u96c6\u4eba\u7c7b\u901a\u8fc7\u7279\u5b9a\u7ed3\u6784\u7ec4\u7ec7\u77e5\u8bc6\u89e3\u51b3\u7684\u4efb\u52a1\u3002", "result": "\u7b80\u5355\u68c0\u7d22\u589e\u5f3aLLM\u96be\u4ee5\u5b8c\u6210\u4efb\u52a1\uff0c\u8bb0\u5fc6\u4ee3\u7406\u5728\u63d0\u793a\u4e0b\u53ef\u89e3\u51b3\uff0c\u73b0\u4ee3LLM\u65e0\u63d0\u793a\u65f6\u4e0d\u8bc6\u522b\u8bb0\u5fc6\u7ed3\u6784\u3002", "conclusion": "\u6307\u51faLLM\u8bad\u7ec3\u548c\u8bb0\u5fc6\u6846\u67b6\u672a\u6765\u6539\u8fdb\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.12180", "pdf": "https://arxiv.org/pdf/2602.12180", "abs": "https://arxiv.org/abs/2602.12180", "authors": ["Yurong Chen", "Yu He", "Michael I. Jordan", "Fan Yao"], "title": "How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.", "AI": {"tldr": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u504f\u597d\u5bf9\u9f50\u4e2d\u91c7\u6837\u548c\u53c2\u8003\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u4ece\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u76f8\u5173\u7ed3\u8bba\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6807\u51c6\u65b9\u6cd5\u4e2d\uff0c\u91c7\u6837\u548c\u53c2\u8003\u9009\u62e9\u7684\u5f71\u54cd\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002", "method": "\u901a\u8fc7Identity Preference Optimization\u6846\u67b6\u7814\u7a76\u91c7\u6837\u548c\u53c2\u8003\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u5206\u6790\u8fed\u4ee3\u5bf9\u9f50\u52a8\u6001\uff0c\u8bc1\u660e\u7279\u5b9a\u53c2\u6570\u4e0b\u7684\u60c5\u51b5\u53ca\u7a33\u5b9a\u6027\u533a\u57df\u3002", "result": "\u9002\u5f53\u7684\u5b9e\u4f8b\u76f8\u5173\u91c7\u6837\u80fd\u6709\u66f4\u5f3a\u6392\u540d\u4fdd\u8bc1\uff0c\u504f\u659c\u7684\u7b56\u7565\u91c7\u6837\u5728\u7ed3\u6784\u5316\u504f\u597d\u4e0b\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u96c6\u4e2d\uff0c\u8fed\u4ee3\u5bf9\u9f50\u52a8\u6001\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u6709\u6301\u7eed\u632f\u8361\u6216\u71b5\u574d\u584c\u60c5\u51b5\u3002", "conclusion": "\u7406\u8bba\u89c1\u89e3\u53ef\u6269\u5c55\u5230Direct Preference Optimization\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u53d1\u73b0\u7684\u73b0\u8c61\u5728\u66f4\u5e7f\u6cdb\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u5e38\u89c1\u3002"}}
{"id": "2602.11764", "pdf": "https://arxiv.org/pdf/2602.11764", "abs": "https://arxiv.org/abs/2602.11764", "authors": ["Nilesh Vyas", "Fabien Geyer", "Svetoslav Duhovnikov"], "title": "Reliable and Private Anonymous Routing for Satellite Constellations", "categories": ["cs.CR", "cs.ET", "cs.IR", "cs.NI"], "comment": "14 Pages, 16 Figures", "summary": "Shared, dynamic network infrastructures, such as dual-use LEO satellite constellations, pose critical threats to metadata privacy, particularly for state actors operating in mixed-trust environments. This work proposes an enhanced anonymity architecture, evolving the Loopix mix-network, to provide robust security and reliability in these volatile topologies. We introduce three primary contributions: (1) A multi-path transport protocol utilizing $(n, k)$ erasure codes, which is demonstrated to counteract the high link volatility and intermittent connectivity that renders standard mix-networks unreliable. (2) The integration of a computationally efficient Private Information Retrieval (PIR) protocol during route discovery. (3) The introduction of adaptive, centrality-based delay strategies that efficiently mitigate the inherent topological bias of LEO networks, providing a superior anonymity-to-latency trade-off. This mechanism provably prevents metadata leakage at the user-provider directory, mitigating profiling and correlation attacks. We validate this architecture via high-fidelity, packet-level simulations of a LEO constellation. Empirical results show our multi-path transport achieves near-zero message loss, establishing a quantifiable trade-off between reliability and bandwidth overhead. Furthermore, microbenchmarks of the PIR protocol quantify its computational and latency overheads, confirming its feasibility for practical deployment. This work provides a validated blueprint for deployable high-anonymity communication systems, demonstrating the viability of securely multiplexing sensitive operations within large-scale commercial network infrastructures.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5171\u4eab\u52a8\u6001\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u5143\u6570\u636e\u9690\u79c1\u7684\u5a01\u80c1\uff0c\u63d0\u51fa\u589e\u5f3a\u533f\u540d\u67b6\u6784\uff0c\u4ecb\u7ecd\u4e09\u9879\u4e3b\u8981\u8d21\u732e\uff0c\u7ecf\u6a21\u62df\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u533f\u540d\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "motivation": "\u5171\u4eab\u52a8\u6001\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u5143\u6570\u636e\u9690\u79c1\u6784\u6210\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4fe1\u4efb\u73af\u5883\u4e0b\uff0c\u9700\u8981\u589e\u5f3a\u7684\u5b89\u5168\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7684\u533f\u540d\u67b6\u6784\uff0c\u5305\u62ec\u4f7f\u7528(n, k)\u64e6\u9664\u7801\u7684\u591a\u8def\u5f84\u4f20\u8f93\u534f\u8bae\u3001\u96c6\u6210\u8ba1\u7b97\u9ad8\u6548\u7684PIR\u534f\u8bae\u3001\u5f15\u5165\u81ea\u9002\u5e94\u57fa\u4e8e\u4e2d\u5fc3\u6027\u7684\u5ef6\u8fdf\u7b56\u7565\u3002", "result": "\u591a\u8def\u5f84\u4f20\u8f93\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u6d88\u606f\u4e22\u5931\uff0c\u91cf\u5316\u4e86\u53ef\u9760\u6027\u4e0e\u5e26\u5bbd\u5f00\u9500\u7684\u6743\u8861\uff1bPIR\u534f\u8bae\u7684\u5fae\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u5176\u5b9e\u8df5\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u9ad8\u533f\u540d\u901a\u4fe1\u7cfb\u7edf\u7684\u6709\u6548\u84dd\u56fe\uff0c\u8bc1\u660e\u5728\u5927\u89c4\u6a21\u5546\u4e1a\u7f51\u7edc\u4e2d\u5b89\u5168\u590d\u7528\u654f\u611f\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.12079", "pdf": "https://arxiv.org/pdf/2602.12079", "abs": "https://arxiv.org/abs/2602.12079", "authors": ["Alessandro Aneggi", "Vincenzo Stoico", "Andrea Janes"], "title": "Performance Antipatterns: Angel or Devil for Power Consumption?", "categories": ["cs.SE"], "comment": null, "summary": "Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.", "AI": {"tldr": "\u672c\u6587\u5b9e\u8bc1\u7814\u7a76\u4e86\u6027\u80fd\u53cd\u6a21\u5f0f\u5bf9\u5fae\u670d\u52a1\u7cfb\u7edf\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u90e8\u5206\u53cd\u6a21\u5f0f\u4f1a\u663e\u8457\u589e\u52a0\u80fd\u8017\u3002", "motivation": "\u63a2\u7a76\u5e7f\u6cdb\u7814\u7a76\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u662f\u5426\u4f1a\u5bf9\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u80fd\u8017\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u5c06\u5341\u79cd\u53cd\u6a21\u5f0f\u5b9e\u73b0\u4e3a\u72ec\u7acb\u5fae\u670d\u52a1\uff0c\u5728\u53ef\u63a7\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u591a\u6b21\u6536\u96c6\u6027\u80fd\u3001CPU\u548cDRAM\u80fd\u8017\u53ca\u8d44\u6e90\u5229\u7528\u7387\u6570\u636e\u3002", "result": "\u6240\u6709\u53cd\u6a21\u5f0f\u90fd\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u4f46\u53ea\u6709\u90e8\u5206\u53cd\u6a21\u5f0f\u7684\u54cd\u5e94\u65f6\u95f4\u4e0e\u80fd\u8017\u589e\u52a0\u6709\u663e\u8457\u5173\u7cfb\uff0c\u4e00\u4e9b\u53cd\u6a21\u5f0f\u8fbe\u5230CPU\u9971\u548c\uff0c\u80fd\u8017\u4e0d\u518d\u968f\u54cd\u5e94\u65f6\u95f4\u4e0a\u5347\u3002", "conclusion": "\u4e3a\u8bc6\u522b\u517c\u5177\u80fd\u8017\u53cd\u6a21\u5f0f\u7279\u5f81\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u7840\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u8282\u80fd\u7684\u5fae\u670d\u52a1\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u884c\u5efa\u8bae\u3002"}}
{"id": "2602.11583", "pdf": "https://arxiv.org/pdf/2602.11583", "abs": "https://arxiv.org/abs/2602.11583", "authors": ["Jingdi Chen", "Hanqing Yang", "Zongjun Liu", "Carlee Joe-Wong"], "title": "The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at Transactions on Machine Learning Research (TMLR), 2026", "summary": "Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u591a\u667a\u80fd\u4f53\u901a\u4fe1\uff08MA - Comm\uff09\uff0c\u8ffd\u6eaf\u4e09\u79cd\u8303\u5f0f\u4e0b\u901a\u4fe1\u65b9\u6cd5\u6f14\u53d8\uff0c\u6307\u51fa\u4e0d\u540c\u9009\u62e9\u5bf9\u901a\u4fe1\u8bbe\u8ba1\u7684\u5f71\u54cd\uff0c\u63d0\u70bc\u8bbe\u8ba1\u6a21\u5f0f\u4e0e\u6311\u6218\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u987a\u5e8f\u51b3\u7b56\u5728\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\uff0c\u901a\u4fe1\u91cd\u8981\u4f46\u73b0\u6709\u901a\u4fe1\u534f\u8bae\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u3001\u96be\u89e3\u91ca\u3001\u96be\u6cdb\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u884c\u7814\u7a76\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u4e94\u4e2aW\uff08\u8c01\u4e0e\u8c01\u901a\u4fe1\u3001\u901a\u4fe1\u5185\u5bb9\u3001\u901a\u4fe1\u65f6\u95f4\u3001\u901a\u4fe1\u4e3a\u4f55\u6709\u76ca\uff09\u6765\u7efc\u8ff0\u591a\u667a\u80fd\u4f53\u901a\u4fe1\uff0c\u8ffd\u6eaf\u5176\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u3001\u65b0\u5174\u8bed\u8a00\uff08EL\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e09\u79cd\u8303\u5f0f\u4e0b\u7684\u6f14\u53d8\u3002", "result": "\u660e\u786e\u4e0d\u540c\u8303\u5f0f\u4e0b\u901a\u4fe1\u65b9\u6cd5\u7684\u7279\u70b9\u3001\u9762\u4e34\u95ee\u9898\uff0c\u6307\u51fa\u4e0d\u540c\u9009\u62e9\u5bf9\u901a\u4fe1\u8bbe\u8ba1\u7684\u5f71\u54cd\u53ca\u4e3b\u8981\u6743\u8861\u70b9\u3002", "conclusion": "\u63d0\u70bc\u51fa\u5b9e\u7528\u8bbe\u8ba1\u6a21\u5f0f\u548c\u5f00\u653e\u6311\u6218\uff0c\u4ee5\u652f\u6301\u7ed3\u5408\u5b66\u4e60\u3001\u8bed\u8a00\u548c\u63a7\u5236\u7684\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6df7\u5408\u7cfb\u7edf\u3002"}}
{"id": "2602.11246", "pdf": "https://arxiv.org/pdf/2602.11246", "abs": "https://arxiv.org/abs/2602.11246", "authors": ["Nikhil Garg", "Jon Kleinberg", "Kenny Peng"], "title": "How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.CO"], "comment": null, "summary": "We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.\n  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = \u03a9_\u03b5(\\frac{k^2}{\\log k}\\log (m/k))$ is required while $d = O_\u03b5(k^2\\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the \"superposition hypothesis\" (Elhage et al., 2022).\n  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Tur\u00e1n's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.", "AI": {"tldr": "\u672c\u6587\u4e3a\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u5f15\u5165\u6570\u5b66\u6846\u67b6\uff0c\u5206\u79bb\u51fa\u7ebf\u6027\u8868\u793a\u548c\u7ebf\u6027\u53ef\u8bbf\u95ee\u4e24\u4e2a\u4e3b\u5f20\uff0c\u7ed9\u51fa\u7ebf\u6027\u538b\u7f29\u611f\u77e5\u7684\u4e0a\u4e0b\u754c\uff0c\u8bc1\u660e\u7ebf\u6027\u53ef\u8bbf\u95ee\u66f4\u5f3a\uff0c\u5e76\u4e3a'\u53e0\u52a0\u5047\u8bbe'\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u7ebf\u6027\u8868\u793a\u7279\u5f81\u65f6\uff0c\u786e\u5b9a\u7ebf\u6027\u8868\u793a\u548c\u7ebf\u6027\u8bbf\u95eem\u4e2a\u7279\u5f81\u6240\u9700\u7684\u6700\u5c11\u795e\u7ecf\u5143\u6570\u91cf\u3002", "method": "\u4e0a\u754c\u8bc1\u660e\u4f7f\u7528\u6807\u51c6\u968f\u673a\u6784\u9020\u5217\u8fd1\u4f3c\u6b63\u4ea4\u7684\u77e9\u9635\uff0c\u4e0b\u754c\u8bc1\u660e\u7ed3\u5408\u8fd1\u5355\u4f4d\u77e9\u9635\u7684\u79e9\u754c\u548c\u56fe\u8bba\u7684Tur\u00e1n\u5b9a\u7406\uff0c\u8fd8\u5c06\u7ed3\u679c\u62d3\u5c55\u5230\u6fc0\u6d3b\u51fd\u6570\u548c\u504f\u7f6e\u7684\u89e3\u7801\u5668\u3002", "result": "\u8bc1\u660e\u7ebf\u6027\u538b\u7f29\u611f\u77e5\u4e2d\uff0c\u4e0b\u754c $d = \u03a9_\u03b5(\\frac{k^2}{\\log k}\\log (m/k))$\uff0c\u4e0a\u754c $d = O_\u03b5(k^2\\log m)$\u3002", "conclusion": "\u7ebf\u6027\u53ef\u8bbf\u95ee\u6bd4\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u66f4\u5f3a\uff0c\u795e\u7ecf\u5143\u5728LRH\u4e0b\u53ef\u5b58\u50a8\u6307\u6570\u6570\u91cf\u7684\u7279\u5f81\uff0c\u4e14\u5c55\u793a\u4e86\u7ed3\u679c\u5bf9\u7279\u5f81\u8868\u793a\u51e0\u4f55\u7684\u7ea6\u675f\u60c5\u51b5\u3002"}}
{"id": "2602.12270", "pdf": "https://arxiv.org/pdf/2602.12270", "abs": "https://arxiv.org/abs/2602.12270", "authors": ["Annie Liang", "Jay Lu"], "title": "Creative Ownership in the Age of AI", "categories": ["econ.TH", "cs.AI", "cs.GT"], "comment": null, "summary": "Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \\emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.", "AI": {"tldr": "\u73b0\u884c\u7248\u6743\u6cd5\u805a\u7126\u5b9e\u8d28\u6027\u76f8\u4f3c\uff0c\u4f46\u751f\u6210\u5f0fAI\u53ef\u6a21\u4eff\u98ce\u683c\u4e0d\u590d\u5236\u5185\u5bb9\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u4fb5\u6743\u5224\u5b9a\u6807\u51c6\u5e76\u5206\u6790\u8bb8\u53ef\u751f\u6210\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u7248\u6743\u4fb5\u6743\u5b9a\u4e49\u4e0d\u9002\u5408\u751f\u6210\u5f0fAI\u60c5\u51b5\uff0c\u9700\u8981\u65b0\u5224\u5b9a\u6807\u51c6\u3002", "method": "\u5c06\u751f\u6210\u7cfb\u7edf\u5efa\u6a21\u4e3a\u95ed\u5305\u7b97\u5b50\uff0c\u5c06\u73b0\u6709\u4f5c\u54c1\u8bed\u6599\u6620\u5c04\u5230\u65b0\u4f5c\u54c1\u8f93\u51fa\u3002", "result": "\u63cf\u8ff0\u4e86\u8bb8\u53ef\u751f\u6210\u7684\u7ed3\u6784\u7279\u6027\uff0c\u63ed\u793a\u4e86\u6e10\u8fd1\u4e8c\u5206\u6cd5\uff1a\u6709\u673a\u521b\u4f5c\u8f7b\u5c3e\u65f6\uff0cAI\u751f\u6210\u4e0d\u53d7\u9650\uff1b\u91cd\u5c3e\u65f6\uff0c\u76d1\u7ba1\u6301\u7eed\u7ea6\u675f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u5224\u5b9a\u6807\u51c6\u80fd\u7528\u4e8e\u5206\u6790\u751f\u6210\u5f0fAI\u5728\u7248\u6743\u4e0b\u7684\u751f\u6210\u60c5\u51b5\u3002"}}
{"id": "2602.12081", "pdf": "https://arxiv.org/pdf/2602.12081", "abs": "https://arxiv.org/abs/2602.12081", "authors": ["Alessandro Aneggi", "Xiaozhou Li", "Andrea Janes"], "title": "PPTAM$\u03b7$: Energy Aware CI/CD Pipeline for Container Based Applications", "categories": ["cs.SE"], "comment": null, "summary": "Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$\u03b7$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$\u03b7$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.", "AI": {"tldr": "\u63d0\u51faPPTAM$\u03b7$\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5c06\u529f\u7387\u548c\u80fd\u8017\u6d4b\u91cf\u96c6\u6210\u5230GitLab CI\u4e2d\uff0c\u5bf9JWT\u8ba4\u8bc1API\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u5bb9\u5668\u7684\u5fae\u670d\u52a1\u5feb\u901f\u90e8\u7f72\uff0c\u4f46CI/CD\u7ba1\u9053\u5f88\u5c11\u6d4b\u91cf\u80fd\u8017\uff0c\u800c\u8bbe\u8ba1\u6a21\u5f0f\u7b49\u4f1a\u5f71\u54cd\u80fd\u6548\u3002", "method": "\u6784\u5efaPPTAM$\u03b7$\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u534f\u8c03\u8d1f\u8f7d\u751f\u6210\u3001\u5bb9\u5668\u76d1\u63a7\u548c\u786c\u4ef6\u529f\u7387\u63a2\u9488\uff0c\u5728\u6bcf\u6b21\u63d0\u4ea4\u65f6\u6536\u96c6\u53ef\u6bd4\u6307\u6807\u3002", "result": "\u5728JWT\u8ba4\u8bc1API\u7684\u56db\u4e2a\u63d0\u4ea4\u4e0a\u8bc4\u4f30PPTAM$\u03b7$\uff0c\u6536\u96c6\u6027\u80fd\u548c\u80fd\u8017\u6307\u6807\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4f7f\u5f00\u53d1\u4eba\u5458\u53ef\u89c1\u80fd\u8017\uff0c\u652f\u6301\u6d4b\u8bd5\u5de5\u7a0b\u5e08\u8fdb\u884c\u7248\u672c\u6bd4\u8f83\uff0c\u4fbf\u4e8e\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u8d8b\u52bf\u5206\u6790\u3002"}}
{"id": "2602.11596", "pdf": "https://arxiv.org/pdf/2602.11596", "abs": "https://arxiv.org/abs/2602.11596", "authors": ["Nikhil Verma", "Minjung Kim", "JooYoung Yoo", "Kyung-Min Jin", "Manasa Bharadwaj", "Kevin Ferreira", "Ko Keun Kim", "Youngjoon Kim"], "title": "MAPLE: Modality-Aware Post-training and Learning Ecosystem", "categories": ["cs.AI"], "comment": "31 pages", "summary": "Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.", "AI": {"tldr": "\u63d0\u51faMAPLE\u751f\u6001\u7cfb\u7edf\u7528\u4e8e\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u7f29\u5c0f\u4e86\u5355/\u591a\u6a21\u6001\u51c6\u786e\u6027\u5dee\u8ddd\uff0c\u52a0\u901f\u6536\u655b\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5ffd\u7565\u4efb\u52a1\u5bf9\u6a21\u6001\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5bfc\u81f4\u7b56\u7565\u68af\u5ea6\u65b9\u5dee\u589e\u5927\u3001\u6536\u655b\u7f13\u6162\u548c\u9c81\u68d2\u6027\u4e0b\u964d\u3002", "method": "\u5f15\u5165MAPLE\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ecMAPLE - bench\u57fa\u51c6\u3001MAPO\u4f18\u5316\u6846\u67b6\u3001\u81ea\u9002\u5e94\u52a0\u6743\u548c\u8bfe\u7a0b\u8c03\u5ea6\u3002", "result": "MAPLE\u7f29\u5c0f\u5355/\u591a\u6a21\u6001\u7cbe\u5ea6\u5dee\u8ddd30.24%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad83.18\u500d\uff0c\u5728\u4e0d\u540c\u4fe1\u53f7\u7ec4\u5408\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "MAPLE\u662f\u9002\u7528\u4e8e\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11287", "pdf": "https://arxiv.org/pdf/2602.11287", "abs": "https://arxiv.org/abs/2602.11287", "authors": ["Yuanyong Luo", "Jing Huang", "Yu Cheng", "Ziwei Yu", "Kaihua Zhang", "Kehong Hong", "Xinda Ma", "Xin Wang", "Anping Tong", "Guipeng Hu", "Yun Xu", "Mehran Taghian", "Peng Wu", "Guanglin Li", "Yunke Peng", "Tianchi Hu", "Minqi Chen", "Michael Bi Mi", "Hu Liu", "Xiping Zhou", "Junsong Wang", "Qiang Lin", "Heng Liao"], "title": "HiFloat4 Format for Language Model Inference", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "8 pages, 4 figures", "summary": "This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5b9a\u5236\u7684HiFloat4\u6570\u636e\u683c\u5f0f\uff0c\u7ecf\u5b9e\u9a8c\u5728\u591a\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u9ad8\u4e8eNVFP4\u3002", "motivation": "\u63d0\u51fa\u9002\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u80fd\u63d0\u9ad8\u6570\u636e\u8868\u793a\u7a7a\u95f4\u5229\u7528\u7387\u3001\u964d\u4f4e\u786c\u4ef6\u80fd\u8017\u7684\u6570\u636e\u683c\u5f0f\u3002", "method": "\u63d0\u51faHiFloat4\u6570\u636e\u683c\u5f0f\uff0c\u6bcf\u4e2a\u5355\u5143\u5305\u542b64\u4e2a4\u4f4d\u5143\u7d20\u548c32\u4f4d\u5171\u4eab\u7f29\u653e\u5143\u6570\u636e\uff0c\u5143\u6570\u636e\u6307\u5b9a\u4e09\u7ea7\u7f29\u653e\u5c42\u6b21\uff1b\u5bf9LLaMA\u3001Qwen\u7b49\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u5b9e\u9a8c\u3002", "result": "HiF4\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6bd4NVFP4\u6709\u66f4\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "HiFloat4\u662f\u4e00\u79cd\u6027\u80fd\u826f\u597d\u7684\u9002\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u6570\u636e\u683c\u5f0f\u3002"}}
{"id": "2602.12144", "pdf": "https://arxiv.org/pdf/2602.12144", "abs": "https://arxiv.org/abs/2602.12144", "authors": ["Muhammad Ahmad Khan", "Hasnain Ali", "Muneeb Rana", "Muhammad Saqib Ilyas", "Abdul Ali Bangash"], "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at MSR 2026 Mining Challenge track", "summary": "AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.", "AI": {"tldr": "\u5bf9\u5f00\u6e90\u79fb\u52a8\u5e94\u7528\u9879\u76ee\u4e2dAI\u751f\u6210\u4ee3\u7801\u8fdb\u884c\u7c7b\u522b\u7ea7\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u5e73\u53f0\u3001\u4ee3\u7406\u548c\u4efb\u52a1\u7c7b\u522b\u7684PR\u63a5\u53d7\u884c\u4e3a\u3002", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u5bf9\u79fb\u52a8\u5f00\u53d1\u7684\u5f71\u54cd\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\uff0c\u672c\u6587\u8fdb\u884c\u76f8\u5173\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u5206\u6790AIDev\u6570\u636e\u96c6\u4e2d193\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5b89\u5353\u548ciOS\u5f00\u6e90GitHub\u4ed3\u5e93\u4e2d2901\u4e2aAI\u7f16\u5199\u7684\u62c9\u53d6\u8bf7\u6c42\uff08PR\uff09\u3002", "result": "\u5b89\u5353\u9879\u76ee\u6536\u5230\u7684AI\u7f16\u5199PR\u66f4\u591a\uff0c\u63a5\u53d7\u7387\u66f4\u9ad8\uff1b\u5e38\u89c4\u4efb\u52a1PR\u63a5\u53d7\u7387\u6700\u9ad8\uff1b\u5b89\u5353PR\u89e3\u51b3\u65f6\u95f4\u57282025\u5e74\u5e74\u4e2d\u6709\u6240\u6539\u5584\u540e\u53c8\u4e0b\u964d\u3002", "conclusion": "\u4e3aAI\u4ee3\u7406\u5bf9\u5f00\u6e90\u79fb\u52a8\u9879\u76ee\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u7279\u5f81\u63cf\u8ff0\uff0c\u4e3a\u8bc4\u4f30\u4ee3\u7406\u751f\u6210\u7684\u8d21\u732e\u5efa\u7acb\u4e86\u5b9e\u8bc1\u57fa\u7ebf\u3002"}}
{"id": "2602.11609", "pdf": "https://arxiv.org/pdf/2602.11609", "abs": "https://arxiv.org/abs/2602.11609", "authors": ["Yiming Gao", "Zhen Wang", "Jefferson Chen", "Mark Antkowiak", "Mengzhou Hu", "JungHo Kong", "Dexter Pratt", "Jieyuan Liu", "Enze Ma", "Zhiting Hu", "Eric P. Xing"], "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery", "categories": ["cs.AI", "q-bio.GN"], "comment": "Accepted at NeurIPS 2025 Main Conference", "summary": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.\n  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.\n  Code, data, and package are available at https://github.com/maitrix-org/scPilot", "AI": {"tldr": "\u63d0\u51fascPilot\u6846\u67b6\u8fdb\u884c\u7ec4\u5b66\u539f\u751f\u63a8\u7406\uff0c\u53d1\u5e03scBench\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u663e\u793a\u53ef\u63d0\u5347\u51c6\u786e\u7387\u5e76\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u5355\u7ec6\u80de\u5206\u6790\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u68c0\u67e5\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u548c\u751f\u7269\u4fe1\u606f\u5b66\u5de5\u5177\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\uff0c\u89e3\u51b3\u6838\u5fc3\u5355\u7ec6\u80de\u5206\u6790\u95ee\u9898\u3002", "method": "\u5c06\u6838\u5fc3\u5355\u7ec6\u80de\u5206\u6790\u8f6c\u5316\u4e3a\u9010\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u53d1\u5e03scBench\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8fed\u4ee3\u7ec4\u5b66\u539f\u751f\u63a8\u7406\u63d0\u5347\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u5e73\u5747\u51c6\u786e\u738711%\uff0cGemini - 2.5 - Pro\u51cf\u5c11\u8f68\u8ff9\u56fe\u7f16\u8f91\u8ddd\u79bb30%\uff0c\u751f\u6210\u900f\u660e\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "scPilot\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u539f\u59cb\u7ec4\u5b66\u6570\u636e\uff0c\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u3001\u53ef\u89e3\u91ca\u548c\u6709\u8bca\u65ad\u4fe1\u606f\u7684\u5355\u7ec6\u80de\u5206\u6790\u3002"}}
{"id": "2602.11320", "pdf": "https://arxiv.org/pdf/2602.11320", "abs": "https://arxiv.org/abs/2602.11320", "authors": ["Jamie Mahowald", "Brian Bell", "Alex Ho", "Michael Geyer"], "title": "Efficient Analysis of the Distilled Neural Tangent Kernel", "categories": ["cs.LG"], "comment": "27 pages, 9 figures", "summary": "Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.", "AI": {"tldr": "\u63d0\u51fa\u84b8\u998f\u795e\u7ecf\u5207\u7ebf\u6838 (DNTK) \u7ed3\u5408\u6570\u636e\u96c6\u84b8\u998f\u548c\u6295\u5f71\u65b9\u6cd5\u964d\u4f4e NTK \u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709 NTK \u65b9\u6cd5\u53d7\u5927\u89c4\u6a21\u96c5\u53ef\u6bd4\u77e9\u9635\u8ba1\u7b97\u9650\u5236\uff0c\u9700\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528 NTK \u8c03\u4f18\u7684\u6570\u636e\u96c6\u84b8\u998f\u538b\u7f29\u6570\u636e\u7ef4\u5ea6\uff0c\u7ed3\u5408\u6700\u5148\u8fdb\u6295\u5f71\u65b9\u6cd5\uff0c\u63d0\u51fa DNTK\u3002", "result": "\u6570\u636e\u96c6\u84b8\u998f\u4f7f\u96c5\u53ef\u6bd4\u8ba1\u7b97\u91cf\u51cf\u5c11 20 - 100 \u500d\uff0cDNTK \u6700\u591a\u964d\u4f4e NTK \u4e94\u9636\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "DNTK \u5728\u964d\u4f4e\u590d\u6742\u5ea6\u540c\u65f6\u80fd\u4fdd\u6301\u6838\u7ed3\u6784\u548c\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.12256", "pdf": "https://arxiv.org/pdf/2602.12256", "abs": "https://arxiv.org/abs/2602.12256", "authors": ["Alex Chudic", "G\u00fcl \u00c7al\u0131kl\u0131"], "title": "Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting", "categories": ["cs.SE"], "comment": "13 pages, 3 figures, accepted to ICPC 2026 (34th International Conference on Program Comprehension)", "summary": "Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.", "AI": {"tldr": "\u672c\u6587\u5b9e\u8bc1\u7814\u7a76\u4e0d\u540c\u6d4b\u8bd5\u5de5\u4ef6\u6e90\u7684\u5c11\u6837\u672c\u63d0\u793a\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\uff0c\u4eba\u5de5\u7f16\u5199\u793a\u4f8b\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u5de5\u5177\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u7f3a\u4e4f\u53ef\u8bfb\u6027\u7b49\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u80fd\u529b\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5c11\u6837\u672c\u5b66\u4e60\u6f5c\u529b\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7814\u7a76\u4e0d\u540c\u6d4b\u8bd5\u6e90\u7684\u5c11\u6837\u672c\u63d0\u793a\u5bf9\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u5728HumanEval\u548cClassEval\u6570\u636e\u96c6\u4e0a\u4f7f\u7528GPT - 4o\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u57fa\u4e8e\u68c0\u7d22\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\uff0c\u4eba\u5de5\u7f16\u5199\u793a\u4f8b\u4ea7\u751f\u6700\u4f73\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\uff0c\u57fa\u4e8e\u95ee\u9898\u63cf\u8ff0\u548c\u4ee3\u7801\u7ec4\u5408\u76f8\u4f3c\u5ea6\u9009\u62e9\u793a\u4f8b\u80fd\u5f97\u5230\u6700\u6709\u6548\u7684\u5c11\u6837\u672c\u63d0\u793a\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u5355\u5143\u6d4b\u8bd5\uff0c\u4e0d\u540c\u6d4b\u8bd5\u6e90\u7684\u5c11\u6837\u672c\u63d0\u793a\u6548\u679c\u6709\u5dee\u5f02\uff0c\u4eba\u5de5\u7f16\u5199\u793a\u4f8b\u66f4\u4f18\u3002"}}
{"id": "2602.11619", "pdf": "https://arxiv.org/pdf/2602.11619", "abs": "https://arxiv.org/abs/2602.11619", "authors": ["Aman Mehta"], "title": "When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents", "categories": ["cs.AI"], "comment": "5 pages, 2 figures", "summary": "Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.", "AI": {"tldr": "\u8fd0\u884c\u540c\u4e00\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u6267\u884c\u76f8\u540c\u4efb\u52a1\uff0c\u884c\u4e3a\u5e38\u4e0d\u4e00\u81f4\u3002\u7814\u7a76\u53d1\u73b0\u5176\u884c\u4e3a\u65b9\u5dee\u4e0e\u5931\u8d25\u76f8\u5173\uff0c\u65b9\u5dee\u6e90\u4e8e\u65e9\u671f\u51b3\u7b56\uff0c\u76d1\u6d4b\u884c\u4e3a\u4e00\u81f4\u6027\u53ef\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u7a76\u540c\u4e00LLM\u667a\u80fd\u4f53\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u662f\u5426\u4f1a\u6709\u76f8\u540c\u884c\u4e3a\uff0c\u4ee5\u53ca\u884c\u4e3a\u4e00\u81f4\u6027\u4e0e\u4efb\u52a1\u7ed3\u679c\u7684\u5173\u7cfb\u3002", "method": "\u5728HotpotQA\u4e0a\u5bf9\u4e09\u4e2a\u6a21\u578b\uff08Llama 3.1 70B\u3001GPT - 4o\u548cClaude Sonnet 4.5\uff09\u8fdb\u884c3000\u6b21\u667a\u80fd\u4f53\u8fd0\u884c\u5b9e\u9a8c\u3002", "result": "ReAct\u98ce\u683c\u667a\u80fd\u4f53\u5e73\u5747\u6bcf10\u6b21\u8fd0\u884c\u4ea7\u751f2.0 - 4.2\u4e2a\u4e0d\u540c\u52a8\u4f5c\u5e8f\u5217\uff1b\u884c\u4e3a\u4e00\u81f4\u7684\u4efb\u52a1\u51c6\u786e\u738780 - 92%\uff0c\u9ad8\u5ea6\u4e0d\u4e00\u81f4\u7684\u4efb\u52a1\u51c6\u786e\u7387\u4ec525 - 60%\uff1b69%\u7684\u5206\u6b67\u53d1\u751f\u5728\u7b2c\u4e8c\u6b65\uff08\u9996\u6b21\u641c\u7d22\u67e5\u8be2\uff09\u3002", "conclusion": "\u6267\u884c\u671f\u95f4\u76d1\u6d4b\u884c\u4e3a\u4e00\u81f4\u6027\u53ef\u5b9e\u73b0\u65e9\u671f\u9519\u8bef\u68c0\u6d4b\uff0c\u63d0\u9ad8\u667a\u80fd\u4f53\u53ef\u9760\u6027\u3002"}}
{"id": "2602.11203", "pdf": "https://arxiv.org/pdf/2602.11203", "abs": "https://arxiv.org/abs/2602.11203", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Compositionality of Systems and Partially Ordered Runs", "categories": ["cs.LO", "cs.SE"], "comment": "15 pages, 16 figures, submitted to PETRI NETS 2026", "summary": "In the late 1970s, C.A. Petri introduced partially ordered event occurrences (runs), then called \\emph{processes}, as the appropriate model to describe the individual evolutions of distributed systems. Here, we present a unified framework for handling Petri nets and their runs, specifically to compose and decompose them. It is shown that, for nets $M$ and $N$, the set of runs of the composed net $M \\bullet N$ equals the composition of the runs of $M$ and $N$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5904\u7406Petri\u7f51\u53ca\u5176\u8fd0\u884c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c55\u793a\u7ec4\u5408\u7f51\u8fd0\u884c\u96c6\u548c\u539f\u7f51\u8fd0\u884c\u7ec4\u5408\u7684\u5173\u7cfb\u3002", "motivation": "\u627e\u5230\u5408\u9002\u6a21\u578b\u63cf\u8ff0\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2a\u4f53\u6f14\u5316\uff0c\u9700\u5904\u7406Petri\u7f51\u53ca\u5176\u8fd0\u884c\u7684\u7ec4\u5408\u4e0e\u5206\u89e3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406Petri\u7f51\u53ca\u5176\u8fd0\u884c\uff0c\u8fdb\u884c\u7ec4\u5408\u548c\u5206\u89e3\u64cd\u4f5c\u3002", "result": "\u5bf9\u4e8e\u7f51M\u548cN\uff0c\u7ec4\u5408\u7f51M \u2022 N\u7684\u8fd0\u884c\u96c6\u7b49\u4e8eM\u548cN\u8fd0\u884c\u7684\u7ec4\u5408\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406Petri\u7f51\u53ca\u5176\u8fd0\u884c\u7684\u7ec4\u5408\u4e0e\u5206\u89e3\u3002"}}
{"id": "2602.11630", "pdf": "https://arxiv.org/pdf/2602.11630", "abs": "https://arxiv.org/abs/2602.11630", "authors": ["Yipeng Huang", "Dejun Xu", "Zexin Lin", "Zhenzhong Wang", "Min Jiang"], "title": "Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families", "categories": ["cs.AI"], "comment": null, "summary": "Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box\" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\\sim$35.7% increase in accuracy while providing interpretable analytical solutions.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u8f85\u52a9\u591a\u4efb\u52a1\u7b26\u53f7PDE\u6c42\u89e3\u5668\u6846\u67b6NMIPS\u89e3\u51b3PDE\u65cf\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u7cbe\u5ea6\u63d0\u5347\u4e14\u6709\u53ef\u89e3\u91ca\u89e3\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u6c42\u89e3PDE\u65cf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u673a\u5668\u5b66\u4e60PDE\u6c42\u89e3\u5668\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faNMIPS\u6846\u67b6\uff0c\u7528\u591a\u56e0\u5b50\u4f18\u5316\u540c\u65f6\u53d1\u73b0PDE\u89e3\u6790\u89e3\uff0c\u8bbe\u8ba1\u4eff\u5c04\u8f6c\u79fb\u65b9\u6cd5\u907f\u514d\u91cd\u590d\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6bd4\u73b0\u6709\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u7cbe\u5ea6\u63d0\u5347\u8fbe\u7ea635.7%\uff0c\u4e14\u63d0\u4f9b\u53ef\u89e3\u91ca\u89e3\u6790\u89e3\u3002", "conclusion": "NMIPS\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3PDE\u65cf\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.11346", "pdf": "https://arxiv.org/pdf/2602.11346", "abs": "https://arxiv.org/abs/2602.11346", "authors": ["Esha Singh", "Dongxia Wu", "Chien-Yi Yang", "Tajana Rosing", "Rose Yu", "Yi-An Ma"], "title": "Divide and Learn: Multi-Objective Combinatorial Optimization at Scale", "categories": ["cs.LG", "cs.AI"], "comment": "Tech report. Code URL coming soon", "summary": "Multi-objective combinatorial optimization seeks Pareto-optimal solutions over exponentially large discrete spaces, yet existing methods sacrifice generality, scalability, or theoretical guarantees. We reformulate it as an online learning problem over a decomposed decision space, solving position-wise bandit subproblems via adaptive expert-guided sequential construction. This formulation admits regret bounds of $O(d\\sqrt{T \\log T})$ depending on subproblem dimensionality \\(d\\) rather than combinatorial space size. On standard benchmarks, our method achieves 80--98\\% of specialized solvers performance while achieving two to three orders of magnitude improvement in sample and computational efficiency over Bayesian optimization methods. On real-world hardware-software co-design for AI accelerators with expensive simulations, we outperform competing methods under fixed evaluation budgets. The advantage grows with problem scale and objective count, establishing bandit optimization over decomposed decision spaces as a principled alternative to surrogate modeling or offline training for multi-objective optimization.", "AI": {"tldr": "\u5c06\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u5206\u89e3\u51b3\u7b56\u7a7a\u95f4\u4e0a\u89e3\u51b3\uff0c\u6709\u8f83\u597d\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u727a\u7272\u4e86\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u6216\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u5206\u89e3\u51b3\u7b56\u7a7a\u95f4\u4e0a\u901a\u8fc7\u81ea\u9002\u5e94\u4e13\u5bb6\u5f15\u5bfc\u7684\u987a\u5e8f\u6784\u9020\u89e3\u51b3\u4f4d\u7f6e\u5f0f\u591a\u81c2\u8001\u864e\u673a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e13\u4e1a\u6c42\u89e3\u566880 - 98%\u7684\u6027\u80fd\uff0c\u6837\u672c\u548c\u8ba1\u7b97\u6548\u7387\u6bd4\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e24\u5230\u4e09\u4e2a\u6570\u91cf\u7ea7\uff1b\u5728\u5b9e\u9645\u786c\u4ef6 - \u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u4e2d\uff0c\u5728\u56fa\u5b9a\u8bc4\u4f30\u9884\u7b97\u4e0b\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u5728\u5206\u89e3\u51b3\u7b56\u7a7a\u95f4\u4e0a\u8fdb\u884c\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u662f\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u66ff\u4ee3\u4ee3\u7406\u5efa\u6a21\u6216\u79bb\u7ebf\u8bad\u7ec3\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.11213", "pdf": "https://arxiv.org/pdf/2602.11213", "abs": "https://arxiv.org/abs/2602.11213", "authors": ["Shuyu Chang", "Haiping Huang", "Yanjun Zhang", "Yujin Huang", "Fu Xiao", "Leo Yu Zhang"], "title": "Transferable Backdoor Attacks for Code Models via Sharpness-Aware Adversarial Perturbation", "categories": ["cs.CR", "cs.SE"], "comment": "9 pages, 5 figures, Accepted at AAAI 2026", "summary": "Code models are increasingly adopted in software development but remain vulnerable to backdoor attacks via poisoned training data. Existing backdoor attacks on code models face a fundamental trade-off between transferability and stealthiness. Static trigger-based attacks insert fixed dead code patterns that transfer well across models and datasets but are easily detected by code-specific defenses. In contrast, dynamic trigger-based attacks adaptively generate context-aware triggers to evade detection but suffer from poor cross-dataset transferability. Moreover, they rely on unrealistic assumptions of identical data distributions between poisoned and victim training data, limiting their practicality. To overcome these limitations, we propose Sharpness-aware Transferable Adversarial Backdoor (STAB), a novel attack that achieves both transferability and stealthiness without requiring complete victim data. STAB is motivated by the observation that adversarial perturbations in flat regions of the loss landscape transfer more effectively across datasets than those in sharp minima. To this end, we train a surrogate model using Sharpness-Aware Minimization to guide model parameters toward flat loss regions, and employ Gumbel-Softmax optimization to enable differentiable search over discrete trigger tokens for generating context-aware adversarial triggers. Experiments across three datasets and two code models show that STAB outperforms prior attacks in terms of transferability and stealthiness. It achieves a 73.2% average attack success rate after defense, outperforming static trigger-based attacks that fail under defense. STAB also surpasses the best dynamic trigger-based attack by 12.4% in cross-dataset attack success rate and maintains performance on clean inputs.", "AI": {"tldr": "\u73b0\u6709\u4ee3\u7801\u6a21\u578b\u540e\u95e8\u653b\u51fb\u5728\u53ef\u8fc1\u79fb\u6027\u548c\u9690\u853d\u6027\u4e0a\u5b58\u5728\u6743\u8861\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faSTAB\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u53ef\u8fc1\u79fb\u6027\u548c\u9690\u853d\u6027\u4e0a\u4f18\u4e8e\u5148\u524d\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u89e6\u53d1\u5668\u548c\u52a8\u6001\u89e6\u53d1\u5668\u7684\u4ee3\u7801\u6a21\u578b\u540e\u95e8\u653b\u51fb\u5b58\u5728\u53ef\u8fc1\u79fb\u6027\u4e0e\u9690\u853d\u6027\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u4e14\u52a8\u6001\u653b\u51fb\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u6570\u636e\u5206\u5e03\u5047\u8bbe\u3002", "method": "\u63d0\u51faSharpness - aware Transferable Adversarial Backdoor (STAB) \u653b\u51fb\uff0c\u4f7f\u7528Sharpness - Aware Minimization\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\u5f15\u5bfc\u53c2\u6570\u5230\u5e73\u5766\u635f\u5931\u533a\u57df\uff0c\u7528Gumbel - Softmax\u4f18\u5316\u641c\u7d22\u79bb\u6563\u89e6\u53d1\u5668\u4ee4\u724c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u6297\u89e6\u53d1\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4ee3\u7801\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAB\u5728\u53ef\u8fc1\u79fb\u6027\u548c\u9690\u853d\u6027\u4e0a\u4f18\u4e8e\u5148\u524d\u653b\u51fb\uff0c\u9632\u5fa1\u540e\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u8fbe73.2%\uff0c\u8de8\u6570\u636e\u96c6\u653b\u51fb\u6210\u529f\u7387\u6bd4\u6700\u4f73\u52a8\u6001\u89e6\u53d1\u5668\u653b\u51fb\u9ad812.4%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5e72\u51c0\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "STAB\u65e0\u9700\u5b8c\u6574\u53d7\u5bb3\u8005\u6570\u636e\uff0c\u80fd\u540c\u65f6\u5b9e\u73b0\u53ef\u8fc1\u79fb\u6027\u548c\u9690\u853d\u6027\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u4ee3\u7801\u6a21\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002"}}
{"id": "2602.11635", "pdf": "https://arxiv.org/pdf/2602.11635", "abs": "https://arxiv.org/abs/2602.11635", "authors": ["Shuo Lu", "Jianjie Cheng", "Yinuo Xu", "Yongcan Yu", "Lijun Sheng", "Peijie Wang", "Siru Jiang", "Yongguan Hu", "Run Ling", "Yihua Shao", "Ao Ma", "Wei Feng", "Lingxiao He", "Meng Wang", "Qianlong Xie", "Xingxing Wang", "Ran He", "Jian Liang"], "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\\% accuracy, but we find that most leading MLLMs fail to reach even 60\\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7a7a\u95f4\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51faMathSpatial\u6846\u67b6\u8bc4\u4f30\u548c\u6539\u8fdb\u5176\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5fae\u8c03\u6a21\u578b\u6548\u679c\u826f\u597d\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0d\u660e\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\uff0c\u9700\u7814\u7a76\u6539\u8fdb\u3002", "method": "\u6784\u5efaMathSpatial\u6846\u67b6\uff0c\u5305\u542bMathSpatial - Bench\u57fa\u51c6\u3001MathSpatial - Corpus\u8bad\u7ec3\u6570\u636e\u96c6\u548cMathSpatial - SRT\u63a8\u7406\u6a21\u578b\u3002", "result": "\u5728MathSpatial\u4e0a\u5fae\u8c03Qwen2.5 - VL - 7B\u6a21\u578b\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c1125%\u7684token\u3002", "conclusion": "MathSpatial\u662f\u9996\u4e2a\u5206\u79bb\u611f\u77e5\u548c\u63a8\u7406\u7684\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u80fd\u7cbe\u786e\u6d4b\u91cf\u548c\u5168\u9762\u7406\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.11350", "pdf": "https://arxiv.org/pdf/2602.11350", "abs": "https://arxiv.org/abs/2602.11350", "authors": ["Tomer Meir", "Ori Linial", "Danny Eytan", "Uri Shalit"], "title": "Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes", "categories": ["cs.LG"], "comment": null, "summary": "Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u673a\u5236 - \u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4f30\u8ba1\u5e72\u9884\u7ed3\u679c\uff0c\u5728\u5468\u671f\u6446\u548c\u4e19\u6cca\u915a\u63a8\u6ce8\u573a\u666f\u8868\u73b0\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u673a\u5236\u65b9\u6cd5\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u5206\u5e03\u5916\u8868\u73b0\u4e0d\u4f73\uff0c\u673a\u5236\u6a21\u578b\u53ef\u80fd\u8fc7\u4e8e\u7b80\u5316\uff0c\u9700\u66f4\u597d\u65b9\u6cd5\u4f30\u8ba1\u52a8\u529b\u7cfb\u7edf\u5e72\u9884\u6548\u679c\u4ee5\u5b9e\u73b0\u7ed3\u679c\u4f18\u5316\u3002", "method": "\u5c06\u52a8\u529b\u7cfb\u7edf\u7684\u8f6c\u79fb\u7b97\u5b50\u5206\u89e3\u4e3a\u53c2\u6570\u548c\u975e\u53c2\u6570\u7ec4\u4ef6\uff0c\u533a\u5206\u5e72\u9884\u76f8\u5173\u548c\u65e0\u5173\u52a8\u6001\uff0c\u5bf9\u673a\u5236\u53c2\u6570\u672a\u77e5\u60c5\u51b5\u91c7\u7528\u4e24\u9636\u6bb5\u7a0b\u5e8f\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u8868\u73b0\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u673a\u5236\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u673a\u5236 - \u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u52a8\u529b\u7cfb\u7edf\u7684\u5e72\u9884\u4f18\u5316\u4e2d\u6709\u6f5c\u529b\u3002"}}
{"id": "2602.11232", "pdf": "https://arxiv.org/pdf/2602.11232", "abs": "https://arxiv.org/abs/2602.11232", "authors": ["Animesh Singh", "K Shiv Kumar", "S. VenkataKeerthy", "Pragna Mamidipaka", "R V B R N Aaseesh", "Sayandeep Sen", "Palanivel Kodeswaran", "Theophilus A. Benson", "Ramakrishna Upadrasta", "Praveen Tammana"], "title": "Yaksha-Prashna: Understanding eBPF Bytecode Network Function Behavior", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": null, "summary": "Many cloud infrastructure organizations increasingly rely on third-party eBPF-based network functions for use cases like security, observability, and load balancing, so that not everyone requires a team of highly skilled eBPF experts. However, the network functions from third parties (e.g., F5, Palo Alto) are available in bytecode format to cloud operators, giving little or no understanding of their functional correctness and interaction with other network functions in a chain. Also, eBPF developers want to provide proof of functional correctness for their developed network functions without disclosing the source code to the operators. We design Yaksha-Prashna, a system that allows operators/developers to assert and query bytecode's conformance to its specification and dependencies on other bytecodes. Our work builds domain-specific models that enable us to employ scalable program analysis to extract and model eBPF programs. Using Yaksha-Prashna language, we express 24 properties on standard and non-standard eBPF-based network functions with 200-1000x speedup over the state-of-the-art work.", "AI": {"tldr": "\u8bbe\u8ba1Yaksha - Prashna\u7cfb\u7edf\u7528\u4e8e\u65ad\u8a00\u548c\u67e5\u8be2eBPF\u5b57\u8282\u7801\u7684\u89c4\u8303\u4e00\u81f4\u6027\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u73b0\u6709\u5de5\u4f5c\u5feb200 - 1000\u500d\u3002", "motivation": "\u7b2c\u4e09\u65b9eBPF\u7f51\u7edc\u529f\u80fd\u4ee5\u5b57\u8282\u7801\u5f62\u5f0f\u63d0\u4f9b\uff0c\u4e91\u8fd0\u8425\u5546\u96be\u4ee5\u4e86\u89e3\u5176\u529f\u80fd\u6b63\u786e\u6027\u53ca\u4e0e\u5176\u4ed6\u7f51\u7edc\u529f\u80fd\u7684\u4ea4\u4e92\uff0c\u5f00\u53d1\u8005\u60f3\u5728\u4e0d\u66b4\u9732\u6e90\u7801\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u529f\u80fd\u6b63\u786e\u6027\u8bc1\u660e\u3002", "method": "\u8bbe\u8ba1Yaksha - Prashna\u7cfb\u7edf\uff0c\u6784\u5efa\u7279\u5b9a\u9886\u57df\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7a0b\u5e8f\u5206\u6790\u6765\u63d0\u53d6\u548c\u5efa\u6a21eBPF\u7a0b\u5e8f\u3002", "result": "\u4f7f\u7528Yaksha - Prashna\u8bed\u8a00\u5728\u6807\u51c6\u548c\u975e\u6807\u51c6eBPF\u7f51\u7edc\u529f\u80fd\u4e0a\u8868\u8fbe24\u4e2a\u5c5e\u6027\uff0c\u6bd4\u73b0\u6709\u5de5\u4f5c\u5feb200 - 1000\u500d\u3002", "conclusion": "Yaksha - Prashna\u7cfb\u7edf\u53ef\u6709\u6548\u5e2e\u52a9\u8fd0\u8425\u5546\u548c\u5f00\u53d1\u8005\u5904\u7406eBPF\u5b57\u8282\u7801\u7684\u76f8\u5173\u95ee\u9898\u3002"}}
{"id": "2602.11661", "pdf": "https://arxiv.org/pdf/2602.11661", "abs": "https://arxiv.org/abs/2602.11661", "authors": ["Tianxiang Xu", "Jiayi Liu", "Yixuan Tong", "Jialu Xu", "Yunqing Wei", "Kaiwen Feng", "PanPan Hou", "Kangping Yin", "Jiyuan Hu", "Hao Zhou", "Zhenxin Ma", "Jian Xu", "Guanjun Jiang"], "title": "Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm", "categories": ["cs.AI"], "comment": null, "summary": "While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.", "AI": {"tldr": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u7528\u4e8e\u533b\u5b66\u95ee\u7b54\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u9c81\u68d2\u533b\u5b66\u5bf9\u9f50\u8303\u5f0f\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u7528\u4e8e\u533b\u5b66\u95ee\u7b54\u5b58\u5728\u8303\u5f0f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5982\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u6210\u672c\u9ad8\u3001\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u6709\u6548\u9a8c\u8bc1\u5668\uff0c\u4e14\u533b\u5b66\u5bf9\u9f50\u591a\u76ee\u6807\u5956\u52b1\u4fe1\u53f7\u5b58\u5728\u95ee\u9898\u3002", "method": "\u6784\u5efa\u6574\u4f53\u591a\u7ef4\u533b\u5b66\u5bf9\u9f50\u77e9\u9635\uff0c\u4e3a\u8fed\u4ee3\u4f18\u5316\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\u4fe1\u53f7\uff1b\u63d0\u51fa\u7edf\u4e00\u4f18\u5316\u673a\u5236\uff0c\u91c7\u7528\u53c2\u8003\u51bb\u7ed3\u5f52\u4e00\u5316\u5bf9\u9f50\u5956\u52b1\u5c3a\u5ea6\uff0c\u5b9e\u65bd\u4e09\u56e0\u7d20\u81ea\u9002\u5e94\u52a8\u6001\u52a0\u6743\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u6240\u63d0\u8303\u5f0f\u5728\u73b0\u5b9e\u533b\u5b66\u573a\u666f\u8bc4\u4f30\u4e2d\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u8303\u5f0f\u4e3a\u5782\u76f4\u9886\u57df\u590d\u6742\u5bf9\u9f50\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.11729", "pdf": "https://arxiv.org/pdf/2602.11729", "abs": "https://arxiv.org/abs/2602.11729", "authors": ["Thomas Jiralerspong", "Trenton Bricken"], "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs", "categories": ["cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06crosscoders\u5e94\u7528\u4e8e\u8de8\u67b6\u6784\u6a21\u578bdiffing\uff0c\u5e76\u5f15\u5165DFCs\u6280\u672f\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u7684\u7279\u5f81\uff0c\u63a8\u52a8\u8de8\u67b6\u6784crosscoder\u6a21\u578bdiffing\u6210\u4e3a\u8bc6\u522bAI\u6a21\u578b\u5dee\u5f02\u7684\u6709\u6548\u65b9\u6cd5\u3002", "motivation": "\u6a21\u578bdiffing\u5728\u65b0\u6a21\u578b\u5b89\u5168\u5173\u952e\u884c\u4e3a\u53d1\u73b0\u4e0a\u6709\u524d\u666f\uff0c\u4f46\u73b0\u6709\u5e94\u7528\u591a\u5728\u57fa\u7840\u6a21\u578b\u548c\u5fae\u8c03\u6a21\u578b\u6bd4\u8f83\uff0c\u65b0LLM\u5e38\u4e3a\u65b0\u67b6\u6784\uff0c\u56e0\u6b64\u9700\u8981\u8de8\u67b6\u6784\u65b9\u6cd5\u3002", "method": "\u9996\u6b21\u5c06crosscoders\u5e94\u7528\u4e8e\u8de8\u67b6\u6784\u6a21\u578bdiffing\uff0c\u5f15\u5165Dedicated Feature Crosscoders (DFCs)\u6280\u672f\u3002", "result": "\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u53d1\u73b0Qwen3 - 8B\u7b49\u6a21\u578b\u7684\u7279\u5b9a\u7279\u5f81\u3002", "conclusion": "\u8de8\u67b6\u6784crosscoder\u6a21\u578bdiffing\u662f\u8bc6\u522bAI\u6a21\u578b\u6709\u610f\u4e49\u884c\u4e3a\u5dee\u5f02\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.11666", "pdf": "https://arxiv.org/pdf/2602.11666", "abs": "https://arxiv.org/abs/2602.11666", "authors": ["E Fan", "Lisong Shi", "Zhengtong Li", "Chih-yung Wen"], "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics", "categories": ["cs.AI", "cs.CL"], "comment": "30 pages, 10 figures", "summary": "The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to \"context poisoning,\" where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPhyNiKCE\u6846\u67b6\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728CFD\u4e2d\u5e94\u7528\u7684\u95ee\u9898\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u80fd\u63d0\u5347\u6027\u80fd\u5e76\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u5de5\u4e1a\u81ea\u52a8\u5316\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6982\u7387\u6027\u53ca\u8bed\u4e49-\u7269\u7406\u8131\u8282\u95ee\u9898\u9650\u5236\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728CFD\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u5de5\u7a0b\u5e94\u7528\u3002", "method": "\u5f15\u5165PhyNiKCE\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u89c4\u5212\u4e0e\u7b26\u53f7\u9a8c\u8bc1\u5206\u79bb\uff0c\u4f7f\u7528\u7b26\u53f7\u77e5\u8bc6\u5f15\u64ce\u5c06\u6a21\u62df\u8bbe\u7f6e\u89c6\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u786e\u5b9a\u6027RAG\u5f15\u64ce\u6267\u884c\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5728OpenFOAM\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u670996%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u51cf\u5c1159%\u7684\u81ea\u4e3b\u81ea\u6211\u4fee\u6b63\u5faa\u73af\uff0c\u964d\u4f4e17%\u7684LLM\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "\u5c06\u795e\u7ecf\u751f\u6210\u4e0e\u7b26\u53f7\u7ea6\u675f\u6267\u884c\u5206\u79bb\u53ef\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u8be5\u67b6\u6784\u4e3a\u66f4\u5e7f\u6cdb\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u7684\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u5ba1\u8ba1\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.11374", "pdf": "https://arxiv.org/pdf/2602.11374", "abs": "https://arxiv.org/abs/2602.11374", "authors": ["Aviv Bick", "Eric P. Xing", "Albert Gu"], "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.", "AI": {"tldr": "\u63d0\u51fa\u68c0\u7d22\u611f\u77e5\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u6362\u4e3a\u6df7\u5408\u6a21\u578b\uff0c\u4fdd\u7559\u5c11\u91cf\u5173\u952e\u6ce8\u610f\u529b\u5934\u6062\u590d\u6559\u5e08\u6a21\u578b\u6027\u80fd\uff0c\u964d\u4f4e\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u9700\u8981\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u843d\u540e\u4e8eTransformer\uff0c\u89e3\u51b3SSMs\u96be\u4ee5\u91cd\u73b0G&A\u6ce8\u610f\u529b\u5934\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u68c0\u7d22\u611f\u77e5\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u68c0\u7d22\u4efb\u52a1\u7684\u6d88\u878d\u5b9e\u9a8c\u786e\u5b9a\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u6362\u4e3a\u6df7\u5408\u5b66\u751f\u6a21\u578b\u3002", "result": "\u4fdd\u75592%\u7684\u6ce8\u610f\u529b\u5934\u53ef\u5728\u68c0\u7d22\u5bc6\u96c6\u4efb\u52a1\u4e0a\u6062\u590d\u8d8595%\u7684\u6559\u5e08\u6a21\u578b\u6027\u80fd\uff0c\u6df7\u5408\u6a21\u578b\u5185\u5b58\u6548\u7387\u63d0\u9ad85 - 6\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7f29\u5c0f\u4e86Transformer\u548cSSM\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u8f83\u4f4e\u7684\u5185\u5b58\u6210\u672c\u5b9e\u73b0\u4e86\u8f83\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.11674", "pdf": "https://arxiv.org/pdf/2602.11674", "abs": "https://arxiv.org/abs/2602.11674", "authors": ["Longyuan Zhu", "Hairan Hua", "Linlin Miao", "Bing Zhao"], "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs", "categories": ["cs.AI"], "comment": "42 pages, 8 figures, 7 tables. Code and website available at https://github.com/SKYLENAGE-AI/benchmark-health-index", "summary": "Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.", "AI": {"tldr": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u53ef\u9760\uff0c\u63d0\u51fa\u57fa\u51c6\u5065\u5eb7\u6307\u6570\uff08BHI\uff09\u6846\u67b6\u5ba1\u8ba1\u8bc4\u4f30\u96c6\uff0c\u7cfb\u7edf\u523b\u753b\u8bc4\u4f30\u683c\u5c40\uff0c\u53ef\u7528\u4e8e\u57fa\u51c6\u9009\u62e9\u548c\u7ba1\u7406\u3002", "motivation": "\u5f53\u524d\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u56e0\u5206\u6570\u81a8\u80c0\u548c\u9009\u62e9\u6027\u62a5\u544a\u53d8\u5f97\u4e0d\u53ef\u9760\uff0c\u793e\u533a\u96be\u4ee5\u786e\u5b9a\u54ea\u4e9b\u8bc4\u4f30\u7ed3\u679c\u53ef\u4fe1\u3002", "method": "\u5f15\u5165BHI\u6846\u67b6\uff0c\u4ece\u80fd\u529b\u533a\u5206\u3001\u6297\u9971\u548c\u3001\u5f71\u54cd\u4e09\u4e2a\u6b63\u4ea4\u4e92\u8865\u8f74\u5ba1\u8ba1\u8bc4\u4f30\u96c6\uff0c\u63d0\u70bc2025\u5e7491\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u6280\u672f\u62a5\u544a\u4e2d\u7684106\u4e2a\u6709\u6548\u57fa\u51c6\u3002", "result": "\u7cfb\u7edf\u523b\u753b\u4e86\u8bc4\u4f30\u683c\u5c40\u3002", "conclusion": "BHI\u662f\u9996\u4e2a\u5b8f\u89c2\u91cf\u5316\u57fa\u51c6\u5065\u5eb7\u7684\u6846\u67b6\uff0c\u4e3a\u57fa\u51c6\u9009\u62e9\u63d0\u4f9b\u539f\u5219\u57fa\u7840\uff0c\u53ef\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u8bc4\u4f30\u534f\u8bae\u7684\u52a8\u6001\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002"}}
{"id": "2602.11775", "pdf": "https://arxiv.org/pdf/2602.11775", "abs": "https://arxiv.org/abs/2602.11775", "authors": ["Mersedeh Sadeghi", "Simon Scholz", "Max Unterbusch", "Andreas Vogelsang"], "title": "V-SHiNE: A Virtual Smart Home Framework for Explainability Evaluation", "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Explanations are essential for helping users interpret and trust autonomous smart-home decisions, yet evaluating their quality and impact remains methodologically difficult in this domain. V-SHiNE addresses this gap: a browser-based smarthome simulation framework for scalable and realistic assessment of explanations. It allows researchers to configure environments, simulate behaviors, and plug in custom explanation engines, with flexible delivery modes and rich interaction logging. A study with 159 participants demonstrates its feasibility. V-SHiNE provides a lightweight, reproducible platform for advancing user-centered evaluation of explainable intelligent systems", "AI": {"tldr": "V-SHiNE\u662f\u7528\u4e8e\u667a\u80fd\u5bb6\u5c45\u89e3\u91ca\u8bc4\u4f30\u7684\u6d4f\u89c8\u5668\u6a21\u62df\u6846\u67b6\uff0c\u7ecf\u7814\u7a76\u9a8c\u8bc1\u53ef\u884c\uff0c\u4e3a\u53ef\u89e3\u91ca\u667a\u80fd\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u5e73\u53f0\u3002", "motivation": "\u8bc4\u4f30\u667a\u80fd\u5bb6\u5c45\u89e3\u91ca\u7684\u8d28\u91cf\u548c\u5f71\u54cd\u5728\u65b9\u6cd5\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "method": "\u6784\u5efa\u6d4f\u89c8\u5668\u7aef\u667a\u80fd\u5bb6\u5c45\u6a21\u62df\u6846\u67b6V - SHiNE\uff0c\u53ef\u914d\u7f6e\u73af\u5883\u3001\u6a21\u62df\u884c\u4e3a\u3001\u63a5\u5165\u81ea\u5b9a\u4e49\u89e3\u91ca\u5f15\u64ce\uff0c\u6709\u7075\u6d3b\u4ea4\u4ed8\u6a21\u5f0f\u548c\u4e30\u5bcc\u4ea4\u4e92\u65e5\u5fd7\u3002", "result": "159\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u8bc1\u660e\u4e86V - SHiNE\u7684\u53ef\u884c\u6027\u3002", "conclusion": "V - SHiNE\u4e3a\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u667a\u80fd\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u3001\u53ef\u91cd\u590d\u7684\u5e73\u53f0\u3002"}}
{"id": "2602.11675", "pdf": "https://arxiv.org/pdf/2602.11675", "abs": "https://arxiv.org/abs/2602.11675", "authors": ["Edward Y. Chang"], "title": "Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs", "categories": ["cs.AI"], "comment": "18 pages, 6 tables, 3 figures", "summary": "Machine learning systems that are \"right for the wrong reasons\" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.", "AI": {"tldr": "\u6307\u51fa\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u2018\u56e0\u9519\u8bef\u539f\u56e0\u800c\u6b63\u786e\u2019\u95ee\u9898\u7684\u56e0\u679c\u6839\u6e90\uff0c\u63d0\u51fa\u8ba4\u77e5\u540e\u6094\u6700\u5c0f\u5316\uff08ERM\uff09\u65b9\u6cd5\u53ca\u76f8\u5173\u67b6\u6784\uff0c\u8bc1\u660e\u7406\u8bba\u7ed3\u679c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u901a\u8fc7\u6377\u5f84\u83b7\u9ad8\u6027\u80fd\u4f46\u5728\u5206\u5e03\u53d8\u5316\u65f6\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u660e\u786e\u5176\u56e0\u679c\u6839\u6e90\u5e76\u6539\u8fdb\u3002", "method": "\u63d0\u51faEpistemic Regret Minimization\uff08ERM\uff09\u76ee\u6807\uff0c\u6784\u5efa\u4e09\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u8bc1\u660e\u7269\u7406\u57fa\u7840\u5b9a\u7406\u3001\u5c06ERM\u4f5c\u4e3a\u6ee1\u8db3AGM\u516c\u7406\u7684\u56e0\u679c\u4fe1\u5ff5\u4fee\u6b63\u7b97\u5b50\u3001\u5efa\u7acb\u6545\u969c\u6a21\u5f0f\u5206\u7c7b\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u771f\u5b9e\u5e72\u9884\u5206\u5e03\u7684\u6e10\u8fd1\u6062\u590d\u53ca\u6709\u9650\u6837\u672c\u754c\uff1b\u5b9e\u9a8c\u8868\u660eRung Collapse\u5728\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u4e2d\u4ecd\u5b58\u5728\uff0c\u9ad8\u7ea7\u6a21\u578b\u96be\u7ea0\u6b63\uff0cERM\u53cd\u9988\u80fd\u6062\u590d\u5927\u90e8\u5206\u6839\u6df1\u8482\u56fa\u7684\u9519\u8bef\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u2018\u56e0\u9519\u8bef\u539f\u56e0\u800c\u6b63\u786e\u2019\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u6709\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2602.11383", "pdf": "https://arxiv.org/pdf/2602.11383", "abs": "https://arxiv.org/abs/2602.11383", "authors": ["Christopher Kverne", "Mayur Akewar", "Yuqian Huo", "Tirthak Patel", "Janki Bhimani"], "title": "WSBD: Freezing-Based Optimizer for Quantum Neural Networks", "categories": ["cs.LG", "quant-ph"], "comment": "Accepted to AISTATS 2026. 9 pages main, 24 pages total", "summary": "The training of Quantum Neural Networks (QNNs) is hindered by the high computational cost of gradient estimation and the barren plateau problem, where optimization landscapes become intractably flat. To address these challenges, we introduce Weighted Stochastic Block Descent (WSBD), a novel optimizer with a dynamic, parameter-wise freezing strategy. WSBD intelligently focuses computational resources by identifying and temporarily freezing less influential parameters based on a gradient-derived importance score. This approach significantly reduces the number of forward passes required per training step and helps navigate the optimization landscape more effectively. Unlike pruning or layer-wise freezing, WSBD maintains full expressive capacity while adapting throughout training. Our extensive evaluation shows that WSBD converges on average 63.9% faster than Adam for the popular ground-state-energy problem, an advantage that grows with QNN size. We provide a formal convergence proof for WSBD and show that parameter-wise freezing outperforms traditional layer-wise approaches in QNNs. Project page: https://github.com/Damrl-lab/WSBD-Stochastic-Freezing-Optimizer.", "AI": {"tldr": "\u63d0\u51faWSBD\u4f18\u5316\u5668\u89e3\u51b3\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u95ee\u9898\uff0c\u6bd4Adam\u6536\u655b\u5feb\uff0c\u6709\u6536\u655b\u8bc1\u660e\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u68af\u5ea6\u4f30\u8ba1\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\u3002", "method": "\u5f15\u5165WSBD\u4f18\u5316\u5668\uff0c\u91c7\u7528\u52a8\u6001\u3001\u9010\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\uff0c\u6839\u636e\u68af\u5ea6\u91cd\u8981\u6027\u5206\u6570\u8bc6\u522b\u5e76\u4e34\u65f6\u51bb\u7ed3\u5f71\u54cd\u8f83\u5c0f\u7684\u53c2\u6570\u3002", "result": "WSBD\u5728\u57fa\u6001\u80fd\u91cf\u95ee\u9898\u4e0a\u6bd4Adam\u5e73\u5747\u6536\u655b\u5feb63.9%\uff0c\u4e14\u4f18\u52bf\u968f\u7f51\u7edc\u89c4\u6a21\u589e\u5927\u800c\u589e\u52a0\u3002", "conclusion": "WSBD\u5728\u4fdd\u6301\u5168\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u80fd\u6709\u6548\u5bfc\u822a\u4f18\u5316\u7a7a\u95f4\uff0c\u9010\u53c2\u6570\u51bb\u7ed3\u4f18\u4e8e\u4f20\u7edf\u9010\u5c42\u65b9\u6cd5\u3002"}}
{"id": "2602.11782", "pdf": "https://arxiv.org/pdf/2602.11782", "abs": "https://arxiv.org/abs/2602.11782", "authors": ["Yihao Liu", "Ziyun Zhang", "Zile He", "Huaqian Cai"], "title": "FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.", "AI": {"tldr": "\u63d0\u51faES\u6846\u67b6\u5c06\u4efb\u52a1\u6267\u884c\u548c\u5de5\u4f5c\u6d41\u6784\u5efa\u89e3\u8026\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u4e14\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u901a\u8fc7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5c06\u89e3\u51b3\u65b9\u6848\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u5177\u6709\u6311\u6218\uff0c\u4e14\u73b0\u6709\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "method": "\u63d0\u51faExecute - Summarize(ES)\u6846\u67b6\uff0c\u5148\u4f7f\u7528\u53ef\u7528\u5de5\u5177\u5b8c\u6210\u4efb\u52a1\uff0c\u518d\u4ece\u6267\u884c\u8f68\u8ff9\u72ec\u7acb\u91cd\u5efa\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u5f15\u5165FlowBench\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u7531\u5f62\u5f0f\u7684\u63a8\u7406\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u9760\u8303\u4f8b\uff0c\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6d41\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.11678", "pdf": "https://arxiv.org/pdf/2602.11678", "abs": "https://arxiv.org/abs/2602.11678", "authors": ["Chengwei Ma", "Zhen Tian", "Zhou Zhou", "Zhixian Xu", "Xiaowei Zhu", "Xia Hua", "Si Shi", "F. Richard Yu"], "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing", "categories": ["cs.AI", "cs.CV"], "comment": "4 pages, 3 figures. Accepted to ICASSP 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at https://github.com/gm-embodied/V2G-Audit.", "AI": {"tldr": "MLLMs\u6709\u7ed3\u6784\u76f2\u533a\uff0c\u63d0\u51faV2G\u7ba1\u9053\u8f6c\u6362CAD\u56fe\u4e3a\u5c5e\u6027\u56fe\uff0c\u5728\u7535\u6c14\u5408\u89c4\u68c0\u67e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6548\u679c\u597d\uff0c\u51f8\u663e\u50cf\u7d20\u65b9\u6cd5\u4e0d\u8db3\uff0c\u53d1\u5e03\u57fa\u51c6\u548c\u4ee3\u7801\u3002", "motivation": "\u89e3\u51b3MLLMs\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u5b58\u5728\u7684\u7ed3\u6784\u76f2\u533a\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u5904\u7406\u5de5\u7a0b\u793a\u610f\u56fe\u4e2d\u7684\u62d3\u6251\u548c\u7b26\u53f7\u903b\u8f91\u3002", "method": "\u63d0\u51faVector-to-Graph (V2G) \u7ba1\u9053\uff0c\u5c06CAD\u56fe\u8f6c\u6362\u4e3a\u5c5e\u6027\u56fe\uff0c\u4f7f\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u660e\u786e\u3002", "result": "\u5728\u7535\u6c14\u5408\u89c4\u68c0\u67e5\u7684\u8bca\u65ad\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cV2G\u5728\u6240\u6709\u9519\u8bef\u7c7b\u522b\u4e0a\u90fd\u6709\u5927\u5e45\u51c6\u786e\u7387\u63d0\u5347\uff0c\u800c\u9886\u5148\u7684MLLMs\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002", "conclusion": "\u50cf\u7d20\u57fa\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff0c\u7ed3\u6784\u611f\u77e5\u8868\u793a\u4e3a\u591a\u6a21\u6001AI\u5728\u5de5\u7a0b\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u9014\u5f84\u3002"}}
{"id": "2602.11387", "pdf": "https://arxiv.org/pdf/2602.11387", "abs": "https://arxiv.org/abs/2602.11387", "authors": ["Anirudh Satheesh", "Ziyi Chen", "Furong Huang", "Heng Huang"], "title": "Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization", "categories": ["cs.LG"], "comment": "30 pages", "summary": "We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\\tilde{\\mathcal{O}}(\u03b5^{-2})$ sample complexity, a factor of $\\mathcal{O}(\u03b5^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\\mathcal{O}(\u03b5^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\\mathcal{O}(\u03b5^{-4})$ discounted, $\\mathcal{O}(\u03b5^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.", "AI": {"tldr": "\u7814\u7a76\u5e26\u901a\u7528\u7b56\u7565\u53c2\u6570\u5316\u7684\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u95ee\u9898\uff0c\u901a\u8fc7\u65b9\u6cd5\u6539\u8fdb\u6837\u672c\u590d\u6742\u5ea6\u548c\u7b97\u6cd5\u590d\u6742\u5ea6\u3002", "motivation": "\u8fc7\u5f80\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u8868\u683c\u7b56\u7565\uff0c\u7f3a\u4e4f\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u5c06\u5e73\u5747\u5956\u52b1 RMDPs \u8f6c\u5316\u4e3a\u71b5\u6b63\u5219\u5316\u6298\u6263\u9c81\u68d2 MDPs\uff0c\u8bc1\u660e\u901a\u7528\u7b56\u7565\u53c2\u6570\u5316\u7279\u6027\uff0c\u5f15\u5165\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u8bbe\u8ba1\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u548c Frank - Wolfe \u7b97\u6cd5\u3002", "result": "\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u5668\u6837\u672c\u590d\u6742\u5ea6\u63d0\u5347\uff0c\u7b97\u6cd5\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u590d\u6742\u5ea6\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u9996\u6b21\u4e3a\u975e (s, a) - \u77e9\u5f62\u7684\u901a\u7528\u7b56\u7565\u53c2\u6570\u5316 RMDPs \u63d0\u4f9b\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u5728\u5e73\u5747\u5956\u52b1\u548c\u6298\u6263\u8bbe\u7f6e\u4e0a\u90fd\u6709\u7a81\u7834\u3002"}}
{"id": "2602.12183", "pdf": "https://arxiv.org/pdf/2602.12183", "abs": "https://arxiv.org/abs/2602.12183", "authors": ["Shan Ali", "Feifei Niu", "Paria Shirani", "Lionel C. Briand"], "title": "Unknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach", "categories": ["cs.CR", "cs.SE"], "comment": "13 pages, 2 figures", "summary": "The rapid evolution of cyberattacks continues to drive the emergence of unknown (zero-day) threats, posing significant challenges for network intrusion detection systems in Internet of Things (IoT) networks. Existing machine learning and deep learning approaches typically rely on large labeled datasets, payload inspection, or closed-set classification, limiting their effectiveness under data scarcity, encrypted traffic, and distribution shifts. Consequently, detecting unknown attacks in realistic IoT deployments remains difficult. To address these limitations, we propose SiamXBERT, a robust and data-efficient Siamese meta-learning framework empowered by a transformer-based language model for unknown attack detection. The proposed approach constructs a dual-modality feature representation by integrating flow-level and packet-level information, enabling richer behavioral modeling while remaining compatible with encrypted traffic. Through meta-learning, the model rapidly adapts to new attack types using only a small number of labeled samples and generalizes to previously unseen behaviors. Extensive experiments on representative IoT intrusion datasets demonstrate that SiamXBERT consistently outperforms state-of-the-art baselines under both within-dataset and cross-dataset settings while requiring significantly less training data, achieving up to \\num{78.8}\\% improvement in unknown F1-score. These results highlight the practicality of SiamXBERT for robust unknown attack detection in real-world IoT environments.", "AI": {"tldr": "\u73b0\u6709\u7269\u8054\u7f51\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u5e94\u5bf9\u672a\u77e5\u653b\u51fb\u6709\u5c40\u9650\uff0c\u63d0\u51faSiamXBERT\u6846\u67b6\u68c0\u6d4b\u672a\u77e5\u653b\u51fb\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u8868\u73b0\u4f18\u4e14\u6240\u9700\u8bad\u7ec3\u6570\u636e\u5c11\u3002", "motivation": "\u7f51\u7edc\u653b\u51fb\u6f14\u53d8\u5bfc\u81f4\u672a\u77e5\u5a01\u80c1\u51fa\u73b0\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u3001\u52a0\u5bc6\u6d41\u91cf\u548c\u5206\u5e03\u53d8\u5316\u65f6\u6548\u679c\u53d7\u9650\uff0c\u68c0\u6d4b\u672a\u77e5\u653b\u51fb\u56f0\u96be\u3002", "method": "\u63d0\u51faSiamXBERT\u6846\u67b6\uff0c\u7ed3\u5408\u6d41\u7ea7\u548c\u5305\u7ea7\u4fe1\u606f\u6784\u5efa\u53cc\u6a21\u6001\u7279\u5f81\u8868\u793a\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u5b9e\u73b0\u5bf9\u65b0\u653b\u51fb\u7c7b\u578b\u7684\u5feb\u901f\u9002\u5e94\u548c\u6cdb\u5316\u3002", "result": "\u5728\u4ee3\u8868\u6027\u7269\u8054\u7f51\u5165\u4fb5\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cSiamXBERT\u5728\u6570\u636e\u96c6\u5185\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u672a\u77e5F1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe78.8%\uff0c\u4e14\u6240\u9700\u8bad\u7ec3\u6570\u636e\u5c11\u3002", "conclusion": "SiamXBERT\u9002\u7528\u4e8e\u73b0\u5b9e\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u9c81\u68d2\u68c0\u6d4b\u3002"}}
{"id": "2602.11683", "pdf": "https://arxiv.org/pdf/2602.11683", "abs": "https://arxiv.org/abs/2602.11683", "authors": ["Xin Xu", "Tong Yu", "Xiang Chen", "Haoliang Wang", "Julian McAuley", "Saayan Mitra"], "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u6f5c\u5728\u63a8\u7406\u4e2d\u6a21\u578b\u7f6e\u4fe1\u5ea6\u52a8\u6001\uff0c\u63d0\u51fa\u63a8\u7406\u65f6\u7f6e\u4fe1\u5ea6\u611f\u77e5\u8def\u7531\u673a\u5236ThinkRouter\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6821\u51c6\u9519\u8bef\u5e76\u52a0\u901f\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u6709\u6548\u6027\u56e0\u8bbe\u7f6e\u800c\u5f02\uff0c\u4f4e\u7f6e\u4fe1\u5ea6\u601d\u8003\u66ff\u4ee3\u65b9\u6848\u7684\u8f6f\u5d4c\u5165\u53ef\u80fd\u5f15\u5165\u5e76\u4f20\u64ad\u566a\u58f0\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u63a8\u7406\u8f68\u8ff9\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faThinkRouter\uff0c\u5728\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u5c06\u601d\u8003\u8def\u7531\u5230\u79bb\u6563\u4ee4\u724c\u7a7a\u95f4\uff0c\u5426\u5219\u8def\u7531\u5230\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728STEM\u63a8\u7406\u548c\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThinkRouter\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u663e\u5f0fCoT\u3001\u968f\u673a\u8def\u7531\u548c\u6f5c\u5728\u63a8\u7406\u57fa\u7ebf\uff0cPass@1\u5e73\u5747\u63d0\u9ad819.70\u70b9\uff0c\u751f\u6210\u957f\u5ea6\u6700\u591a\u51cf\u5c1115.55%\u3002", "conclusion": "ThinkRouter\u80fd\u6821\u51c6\u663e\u5f0fCoT\u548c\u6f5c\u5728\u63a8\u7406\u4ea7\u751f\u7684\u8bef\u5dee\uff0c\u901a\u8fc7\u5168\u5c40\u964d\u4f4e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u52a0\u901f\u601d\u8003\u7ed3\u675f\u4ee4\u724c\u7684\u751f\u6210\u3002"}}
{"id": "2602.11388", "pdf": "https://arxiv.org/pdf/2602.11388", "abs": "https://arxiv.org/abs/2602.11388", "authors": ["Dibyanayan Bandyopadhyay", "Asif Ekbal"], "title": "Sparse Semantic Dimension as a Generalization Certificate for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Work in progress (17 pages)", "summary": "Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive \"feature sharpness\" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable \"feature explosion\" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.", "AI": {"tldr": "\u6807\u51c6\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u8fc7\u62df\u5408\uff0c\u4f46\u5b9e\u9645\u80fd\u7a33\u5065\u6cdb\u5316\u3002\u672c\u6587\u63d0\u51fa\u6709\u6548\u5bb9\u91cf\u7531\u6a21\u578b\u5185\u90e8\u8868\u5f81\u51e0\u4f55\u51b3\u5b9a\uff0c\u5f15\u5165SSD\u8861\u91cf\u590d\u6742\u5ea6\uff0c\u5728GPT - 2\u548cGemma - 2B\u4e0a\u9a8c\u8bc1\uff0c\u53d1\u73b0\u201c\u7279\u5f81\u9510\u5ea6\u201d\u5b9a\u5f8b\uff0c\u8fd8\u53ef\u4f5c\u5b89\u5168\u76d1\u6d4b\u5de5\u5177\u3002", "motivation": "\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u8fdc\u8d85\u8bad\u7ec3token\u6570\u5374\u80fd\u7a33\u5065\u6cdb\u5316\u7684\u73b0\u8c61\u3002", "method": "\u5f15\u5165Sparse Semantic Dimension (SSD)\uff0c\u7ed3\u5408\u8bad\u7ec3\u7684Sparse Autoencoder (SAE)\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u548cSAE\u89c6\u4e3a\u56fa\u5b9a\u7684\u9884\u8a00\u673a\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728GPT - 2 Small\u548cGemma - 2B\u4e0a\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\uff0c\u53d1\u73b0Gemma - 2B\u6bd4GPT - 2\u8bc6\u522b\u6d3b\u8dc3\u6d41\u5f62\u6240\u9700\u6821\u51c6\u6837\u672c\u5c11\uff0c\u8fd8\u53d1\u73b0\u5206\u5e03\u5916\u8f93\u5165\u89e6\u53d1\u201c\u7279\u5f81\u7206\u70b8\u201d\u3002", "conclusion": "\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u53d6\u51b3\u4e8e\u5b57\u5178\u7a00\u758f\u6027\u800c\u975e\u53c2\u6570\u603b\u6570\uff0c\u6846\u67b6\u53ef\u4f5c\u4e3a\u5b89\u5168\u76d1\u6d4b\u5de5\u5177\u3002"}}
{"id": "2602.11717", "pdf": "https://arxiv.org/pdf/2602.11717", "abs": "https://arxiv.org/abs/2602.11717", "authors": ["Weihong Lin", "Lin Sun", "Qilong Shi", "Aomufei Yuan", "Yuxuan Tian", "Zhengyang Wang", "Guangxiang Zhao", "Xiangzheng Zhang", "Tong Yang"], "title": "Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging", "categories": ["cs.AI"], "comment": null, "summary": "Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.", "AI": {"tldr": "\u63d0\u51faSCF - RKL\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u3001\u5206\u5e03\u611f\u77e5\u66f4\u65b0\u63a7\u5236\u529f\u80fd\u5e72\u6270\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4f9d\u8d56\u53c2\u6570\u7a7a\u95f4\u542f\u53d1\u5f0f\uff0c\u4f1a\u5f15\u5165\u4e25\u91cd\u5e72\u6270\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u548c\u751f\u6210\u884c\u4e3a\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faSCF - RKL\u6846\u67b6\uff0c\u7528\u53cd\u5411Kullback - Leibler\u6563\u5ea6\u8861\u91cf\u6a21\u578b\u529f\u80fd\u5dee\u5f02\uff0c\u9009\u62e9\u6027\u5408\u5e76\u4e92\u8865\u53c2\u6570\u3002", "result": "\u572824\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSCF - RKL\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u4e14\u4fdd\u6301\u5f3a\u6cdb\u5316\u548c\u751f\u6210\u7a33\u5b9a\u6027\u3002", "conclusion": "SCF - RKL\u662f\u4e00\u79cd\u6709\u6548\u63a7\u5236\u529f\u80fd\u5e72\u6270\u7684\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u80fd\u5728\u96c6\u6210\u65b0\u80fd\u529b\u65f6\u4fdd\u7559\u7a33\u5b9a\u8868\u793a\u3002"}}
{"id": "2602.11395", "pdf": "https://arxiv.org/pdf/2602.11395", "abs": "https://arxiv.org/abs/2602.11395", "authors": ["Qingsong Wang", "Mikhail Belkin", "Yusu Wang"], "title": "General and Efficient Steering of Unconditional Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u68af\u5ea6\u5f15\u5bfc\u9ad8\u6548\u63a7\u5236\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u5f15\u5bfc\u4e14\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5f15\u5bfc\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u9ad8\u6548\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u566a\u58f0\u5bf9\u9f50\u548c\u53ef\u8f6c\u79fb\u6982\u5ff5\u5411\u91cf\u4e24\u4e2a\u89c2\u5bdf\uff0c\u7528\u9012\u5f52\u7279\u5f81\u673a\uff08RFM\uff09\u8bc6\u522b\u6982\u5ff5\u65b9\u5411\u3002", "result": "\u5728CIFAR - 10\u3001ImageNet\u548cCelebA\u4e0a\u5b9e\u9a8c\uff0c\u7cbe\u5ea6/\u8d28\u91cf\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u5f15\u5bfc\uff0c\u4e14\u63a8\u7406\u663e\u8457\u52a0\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u9ad8\u6548\u5f15\u5bfc\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u5feb\u901f\u53ef\u63a7\u751f\u6210\u3002"}}
{"id": "2602.11399", "pdf": "https://arxiv.org/pdf/2602.11399", "abs": "https://arxiv.org/abs/2602.11399", "authors": ["Chongyi Zheng", "Royina Karegoudra Jayanth", "Benjamin Eysenbach"], "title": "Can We Really Learn One Representation to Optimize All Rewards?", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.", "AI": {"tldr": "\u672c\u6587\u63ed\u5f00FB\u8868\u5f81\u5b66\u4e60\u795e\u79d8\u9762\u7eb1\uff0c\u63d0\u51fa\u4e00\u6b65FB\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u964d\u4f4e\u8bef\u5dee\u3001\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u4f7f\u7528\u5927\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4f46FB\u8868\u5f81\u5b66\u4e60\u7684\u8bad\u7ec3\u76ee\u6807\u548c\u5b66\u4e60\u884c\u4e3a\u672a\u77e5\uff0c\u9700\u6df1\u5165\u4e86\u89e3\u3002", "method": "\u5206\u6790FB\u8868\u5f81\u5b58\u5728\u6761\u4ef6\u3001\u8bad\u7ec3\u76ee\u6807\u53ca\u6536\u655b\u60c5\u51b5\uff0c\u63d0\u51fa\u4e00\u6b65FB\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u4e00\u6b65FB\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u9886\u57df\u4e2d\u6536\u655b\u8bef\u5dee\u5c0f10^5\uff0c\u5e73\u5747\u96f6\u6837\u672c\u6027\u80fd\u63d0\u534724%\u3002", "conclusion": "\u4e00\u6b65FB\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u9884\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2602.11745", "pdf": "https://arxiv.org/pdf/2602.11745", "abs": "https://arxiv.org/abs/2602.11745", "authors": ["Songlin Lyu", "Lujie Ban", "Zihang Wu", "Tianqi Luo", "Jirong Liu", "Chenhao Ma", "Yuyu Luo", "Nan Tang", "Shipeng Qi", "Heng Lin", "Yongchao Liu", "Chuntao Hong"], "title": "Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]", "categories": ["cs.AI"], "comment": null, "summary": "Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.", "AI": {"tldr": "\u63d0\u51faText2GQL - Bench\u7edf\u4e00\u57fa\u51c6\uff0c\u542b\u591aGQL\u6570\u636e\u96c6\u4e0e\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8bc4\u4f30\u53d1\u73b0ISO - GQL\u751f\u6210\u5b58\u5728\u65b9\u8a00\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709Text - to - GQL\u7cfb\u7edf\u6570\u636e\u96c6\u5728\u9886\u57df\u8986\u76d6\u3001\u652f\u6301\u7684\u56fe\u67e5\u8be2\u8bed\u8a00\u548c\u8bc4\u4f30\u8303\u56f4\u6709\u9650\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efaText2GQL - Bench\uff0c\u5305\u542b\u591aGQL\u6570\u636e\u96c6\u548c\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5f15\u5165\u7efc\u5408\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0ISO - GQL\u751f\u6210\u6709\u65b9\u8a00\u5dee\u8ddd\uff0c\u96f6\u6837\u672c\u6267\u884c\u51c6\u786e\u7387\u81f3\u591a4%\uff0c3 - shot\u63d0\u793a\u63d0\u5347\u81f3\u7ea650%\uff0c\u8bed\u6cd5\u6709\u6548\u6027\u4f4e\u4e8e70%\uff1b\u5fae\u8c03\u76848B\u6a21\u578bEX\u8fbe45.1%\uff0c\u8bed\u6cd5\u6709\u6548\u6027\u8fbe90.8%\u3002", "conclusion": "\u63a5\u89e6\u8db3\u591fISO - GQL\u793a\u4f8b\u53ef\u5927\u5e45\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.11410", "pdf": "https://arxiv.org/pdf/2602.11410", "abs": "https://arxiv.org/abs/2602.11410", "authors": ["David Pardoe", "Neil Daftary", "Miro Furtado", "Aditya Aiyer", "Yu Wang", "Liuqing Li", "Tao Song", "Lars Hertel", "Young Jin Yun", "Senthil Radhakrishnan", "Zhiwei Wang", "Tommy Li", "Khai Tran", "Ananth Nagarajan", "Ali Naqvi", "Yue Zhang", "Renpeng Fang", "Avi Romascanu", "Arjun Kulothungun", "Deepak Kumar", "Praneeth Boda", "Fedor Borisyuk", "Ruoyan Wang"], "title": "CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer", "categories": ["cs.LG"], "comment": null, "summary": "Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.", "AI": {"tldr": "\u63d0\u51faCADET\u7528\u4e8e\u5e7f\u544aCTR\u9884\u6d4b\uff0c\u6709\u591a\u9879\u521b\u65b0\uff0c\u7ebf\u4e0aA/B\u6d4b\u8bd5CTR\u63d0\u534711.04%\uff0c\u5df2\u5728\u9886\u82f1\u5e7f\u544a\u5e73\u53f0\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u67b6\u6784\u5e94\u7528\u4e8e\u5e7f\u544aCTR\u9884\u6d4b\u9762\u4e34\u5904\u7406\u8bc4\u5206\u540e\u4e0a\u4e0b\u6587\u4fe1\u53f7\u3001\u4fdd\u6301\u79bb\u7ebf\u5728\u7ebf\u4e00\u81f4\u6027\u548c\u6269\u5c55\u5230\u5de5\u4e1a\u5de5\u4f5c\u8d1f\u8f7d\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faCADET\u6a21\u578b\uff0c\u6709\u4e0a\u4e0b\u6587\u6761\u4ef6\u89e3\u7801\u67b6\u6784\u3001\u81ea\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u65f6\u95f4\u6233\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u3001\u4f1a\u8bdd\u63a9\u7801\u7b56\u7565\u548c\u751f\u4ea7\u5de5\u7a0b\u6280\u672f\u7b49\u521b\u65b0\u3002", "result": "\u7ebf\u4e0aA/B\u6d4b\u8bd5\u4e2d\uff0cCADET\u8f83\u751f\u4ea7LiRank\u57fa\u7ebf\u6a21\u578bCTR\u63d0\u534711.04%\u3002", "conclusion": "CADET\u6a21\u578b\u5728\u5e7f\u544aCTR\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5df2\u6210\u529f\u90e8\u7f72\u5728\u9886\u82f1\u5e7f\u544a\u5e73\u53f0\u3002"}}
{"id": "2602.11749", "pdf": "https://arxiv.org/pdf/2602.11749", "abs": "https://arxiv.org/abs/2602.11749", "authors": ["Zibo Xiao", "Jun Sun", "Junjie Chen"], "title": "AIR: Improving Agent Safety through Incident Response", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7cfb\u7edf\u7684\u4e8b\u4ef6\u54cd\u5e94\u6846\u67b6AIR\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u68c0\u6d4b\u3001\u4fee\u590d\u548c\u6839\u9664\u6210\u529f\u7387\u8d8590%\uff0c\u8bc1\u660e\u4e8b\u4ef6\u54cd\u5e94\u53ef\u6709\u6548\u63d0\u5347\u4ee3\u7406\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5b89\u5168\u673a\u5236\u4fa7\u91cd\u4e8e\u4e8b\u524d\u9884\u9632\uff0c\u5bf9\u4e8b\u4ef6\u53d1\u751f\u540e\u7684\u54cd\u5e94\u3001\u63a7\u5236\u548c\u6062\u590d\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5b9a\u4e49\u7279\u5b9a\u9886\u57df\u8bed\u8a00\uff0c\u5c06\u5176\u96c6\u6210\u5230\u4ee3\u7406\u6267\u884c\u5faa\u73af\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u67e5\u68c0\u6d4b\u4e8b\u4ef6\uff0c\u5f15\u5bfc\u4ee3\u7406\u6267\u884c\u63a7\u5236\u548c\u6062\u590d\u52a8\u4f5c\uff0c\u5728\u6839\u9664\u9636\u6bb5\u5408\u6210\u62a4\u680f\u89c4\u5219\u3002", "result": "\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u4ee3\u7406\u7c7b\u578b\u4e0a\u8bc4\u4f30\uff0c\u68c0\u6d4b\u3001\u4fee\u590d\u548c\u6839\u9664\u6210\u529f\u7387\u8d8590%\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u5173\u952e\u8bbe\u8ba1\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3001\u53ca\u65f6\u6027\u548c\u9002\u5ea6\u5f00\u9500\uff0cLLM\u751f\u6210\u89c4\u5219\u63a5\u8fd1\u5f00\u53d1\u8005\u7f16\u5199\u89c4\u5219\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e8b\u4ef6\u54cd\u5e94\u4f5c\u4e3a\u63d0\u5347\u4ee3\u7406\u5b89\u5168\u6027\u7684\u4e00\u7ea7\u673a\u5236\u662f\u53ef\u884c\u4e14\u5fc5\u8981\u7684\u3002"}}
{"id": "2602.11413", "pdf": "https://arxiv.org/pdf/2602.11413", "abs": "https://arxiv.org/abs/2602.11413", "authors": ["Md Rakibul Haque", "Vishwa Goudar", "Shireen Elhabian", "Warren Woodrich Pettine"], "title": "TimeSynth: A Framework for Uncovering Systematic Biases in Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is a fundamental tool with wide ranging applications, yet recent debates question whether complex nonlinear architectures truly outperform simple linear models. Prior claims of dominance of the linear model often stem from benchmarks that lack diverse temporal dynamics and employ biased evaluation protocols. We revisit this debate through TimeSynth, a structured framework that emulates key properties of real world time series,including non-stationarity, periodicity, trends, and phase modulation by creating synthesized signals whose parameters are derived from real-world time series. Evaluating four model families Linear, Multi Layer Perceptrons (MLP), Convolutional Neural Networks (CNNs), and Transformers, we find a systematic bias in linear models: they collapse to simple oscillation regardless of signal complexity. Nonlinear models avoid this collapse and gain clear advantages as signal complexity increases. Notably, Transformers and CNN based models exhibit slightly greater adaptability to complex modulated signals compared to MLPs. Beyond clean forecasting, the framework highlights robustness differences under distribution and noise shifts and removes biases of prior benchmarks by using independent instances for train, test, and validation for each signal family. Collectively, TimeSynth provides a principled foundation for understanding when different forecasting approaches succeed or fail, moving beyond oversimplified claims of model equivalence.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7TimeSynth\u6846\u67b6\u91cd\u65b0\u63a2\u8ba8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u590d\u6742\u975e\u7ebf\u6027\u67b6\u6784\u4e0e\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u53d1\u73b0\u7ebf\u6027\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u504f\u5dee\uff0c\u975e\u7ebf\u6027\u6a21\u578b\u5728\u4fe1\u53f7\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u8fd1\u671f\u5173\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u590d\u6742\u975e\u7ebf\u6027\u67b6\u6784\u662f\u5426\u771f\u7684\u4f18\u4e8e\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u5b58\u5728\u4e89\u8bae\uff0c\u4ee5\u5f80\u7ebf\u6027\u6a21\u578b\u5360\u4f18\u7684\u8bf4\u6cd5\u5e38\u6e90\u4e8e\u7f3a\u4e4f\u591a\u6837\u65f6\u95f4\u52a8\u6001\u548c\u6709\u504f\u5dee\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u4f7f\u7528TimeSynth\u6846\u67b6\uff0c\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u7684\u5173\u952e\u5c5e\u6027\uff0c\u521b\u5efa\u5408\u6210\u4fe1\u53f7\u5bf9\u7ebf\u6027\u3001\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u548cTransformer\u56db\u79cd\u6a21\u578b\u65cf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ebf\u6027\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u504f\u5dee\uff0c\u4e0d\u8bba\u4fe1\u53f7\u590d\u6742\u5ea6\u5982\u4f55\u90fd\u4f1a\u9000\u5316\u4e3a\u7b80\u5355\u632f\u8361\uff1b\u975e\u7ebf\u6027\u6a21\u578b\u907f\u514d\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u4e14\u968f\u4fe1\u53f7\u590d\u6742\u5ea6\u589e\u52a0\u4f18\u52bf\u660e\u663e\uff1bTransformers\u548c\u57fa\u4e8eCNN\u7684\u6a21\u578b\u5bf9\u590d\u6742\u8c03\u5236\u4fe1\u53f7\u7684\u9002\u5e94\u6027\u7565\u5f3a\u4e8eMLP\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u51f8\u663e\u4e86\u5206\u5e03\u548c\u566a\u58f0\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u5dee\u5f02\uff0c\u6d88\u9664\u4e86\u5148\u524d\u57fa\u51c6\u7684\u504f\u5dee\u3002", "conclusion": "TimeSynth\u4e3a\u7406\u89e3\u4e0d\u540c\u9884\u6d4b\u65b9\u6cd5\u4f55\u65f6\u6210\u529f\u6216\u5931\u8d25\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u57fa\u7840\uff0c\u8d85\u8d8a\u4e86\u5bf9\u6a21\u578b\u7b49\u4ef7\u6027\u7684\u8fc7\u5ea6\u7b80\u5316\u8bba\u65ad\u3002"}}
{"id": "2602.11767", "pdf": "https://arxiv.org/pdf/2602.11767", "abs": "https://arxiv.org/abs/2602.11767", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Holger Boche"], "title": "TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.", "AI": {"tldr": "\u63d0\u51faTSR\u65b9\u6cd5\u63d0\u9ad8\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4e2d\u6bcf\u8f6e\u7684\u6eda\u52a8\u751f\u6210\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u548c\u66f4\u7a33\u5b9a\u5b66\u4e60\u3002", "motivation": "\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u5956\u52b1\u7a00\u758f\u3001\u5ef6\u8fdf\uff0c\u73af\u5883\u968f\u673a\u7b49\u95ee\u9898\uff0c\u6734\u7d20\u8f68\u8ff9\u91c7\u6837\u6709\u5f0a\u7aef\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTSR\u65b9\u6cd5\uff0c\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6811\u72b6\u641c\u7d22\uff0c\u5229\u7528\u7279\u5b9a\u4efb\u52a1\u53cd\u9988\u9009\u62e9\u9ad8\u5f97\u5206\u52a8\u4f5c\u6784\u5efa\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u53ef\u4e0e\u4e0d\u540c\u641c\u7d22\u7b56\u7565\u548c\u4f18\u5316\u5668\u642d\u914d\u3002", "result": "\u5728Sokoban\u3001FrozenLake\u548cWebShop\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u591a15%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5b66\u4e60\u66f4\u7a33\u5b9a\uff0c\u8bad\u7ec3\u8ba1\u7b97\u6709\u4e00\u6b21\u6027\u589e\u52a0\u3002", "conclusion": "TSR\u5c06\u641c\u7d22\u4ece\u63a8\u7406\u9636\u6bb5\u79fb\u5230\u8bad\u7ec3\u7684\u6eda\u52a8\u9636\u6bb5\uff0c\u4e3a\u591a\u8f6e\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u7b80\u5355\u901a\u7528\u673a\u5236\uff0c\u4e0e\u73b0\u6709\u6846\u67b6\u548c\u65b9\u6cd5\u4e92\u8865\u3002"}}
{"id": "2602.11439", "pdf": "https://arxiv.org/pdf/2602.11439", "abs": "https://arxiv.org/abs/2602.11439", "authors": ["Ziyuan Huang", "Lina Alkarmi", "Mingyan Liu"], "title": "Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics", "categories": ["cs.LG"], "comment": "Preprint. 8 pages (8 figures) plus appendix", "summary": "Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.", "AI": {"tldr": "\u4e0d\u540c\u4e8e\u73b0\u6709\u987a\u5e8f\u6218\u7565\u5206\u7c7b\u91cd\u6743\u91cd\u4f18\u5316\u7684\u7814\u7a76\uff0c\u672c\u6587\u5206\u6790\u591a\u7ea7\u5347\u964d\u6846\u67b6\u4e0b\u5206\u7c7b\u9608\u503c\u548c\u96be\u5ea6\u8fdb\u5c55\u8bbe\u8ba1\uff0c\u8bc1\u660e\u5728\u6b64\u673a\u5236\u4e0b\u4ee3\u7406\u53ef\u901a\u8fc7\u52aa\u529b\u8fbe\u5230\u4efb\u610f\u9ad8\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u987a\u5e8f\u6218\u7565\u5206\u7c7b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u52a8\u6001\u5206\u7c7b\u5668\u6743\u91cd\u4f18\u5316\uff0c\u672c\u6587\u5e0c\u671b\u4ece\u5206\u7c7b\u5668\u9608\u503c\u548c\u96be\u5ea6\u8fdb\u5c55\u8bbe\u8ba1\u7684\u89d2\u5ea6\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u5728\u591a\u7ea7\u5347\u964d\u6846\u67b6\u4e0b\u5206\u6790\u5206\u7c7b\u5668\u9608\u503c\u548c\u96be\u5ea6\u8fdb\u5c55\u8bbe\u8ba1\uff0c\u5efa\u7acb\u6a21\u578b\u6355\u6349\u4ee3\u7406\u4eba\u7684\u8de8\u671f\u6fc0\u52b1\u3002", "result": "\u523b\u753b\u4e86\u4ee3\u7406\u4eba\u7684\u6700\u4f18\u957f\u671f\u7b56\u7565\uff0c\u8bc1\u660e\u59d4\u6258\u4eba\u80fd\u8bbe\u8ba1\u9608\u503c\u5e8f\u5217\u6709\u6548\u6fc0\u52b1\u8bda\u5b9e\u52aa\u529b\u3002", "conclusion": "\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\uff0c\u8be5\u673a\u5236\u80fd\u4f7f\u4ee3\u7406\u4eba\u4ec5\u901a\u8fc7\u771f\u6b63\u7684\u6539\u8fdb\u52aa\u529b\u8fbe\u5230\u4efb\u610f\u9ad8\u7684\u6c34\u5e73\u3002"}}
{"id": "2602.11771", "pdf": "https://arxiv.org/pdf/2602.11771", "abs": "https://arxiv.org/abs/2602.11771", "authors": ["S\u00e9bastien Gigot--L\u00e9andri", "Ga\u00e9tan Morand", "Alexis Joly", "Fran\u00e7ois Munoz", "David Mouillot", "Christophe Botella", "Maximilien Servajean"], "title": "How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?", "categories": ["cs.AI"], "comment": null, "summary": "Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of species prevalence and community composition. We present MaxExp, a decision-driven binarization framework that selects the most probable species assemblage by directly maximizing a chosen evaluation metric. MaxExp requires no calibration data and is flexible across several scores. We also introduce the Set Size Expectation (SSE) method, a computationally efficient alternative that predicts assemblages based on expected species richness. Using three case studies spanning diverse taxa, species counts, and performance metrics, we show that MaxExp consistently matches or surpasses widely used thresholding and calibration methods, especially under strong class imbalance and high rarity. SSE offers a simpler yet competitive option. Together, these methods provide robust, reproducible tools for multispecies SDM binarization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMaxExp\u4e0eSSE\u65b9\u6cd5\u7528\u4e8e\u7269\u79cd\u5206\u5e03\u6a21\u578b\u4e8c\u503c\u5316\uff0c\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edf\u7269\u79cd\u5206\u5e03\u6a21\u578b\u4e8c\u503c\u5316\u6b65\u9aa4\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u4f1a\u626d\u66f2\u7269\u79cd\u6d41\u884c\u7387\u548c\u7fa4\u843d\u7ec4\u6210\u4f30\u8ba1\u3002", "method": "\u63d0\u51faMaxExp\u51b3\u7b56\u9a71\u52a8\u4e8c\u503c\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u6700\u5927\u5316\u6240\u9009\u8bc4\u4f30\u6307\u6807\uff1b\u5f15\u5165Set Size Expectation (SSE)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9884\u671f\u7269\u79cd\u4e30\u5bcc\u5ea6\u9884\u6d4b\u7ec4\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cMaxExp\u8868\u73b0\u4f18\u4e8e\u5e38\u7528\u65b9\u6cd5\uff0cSSE\u662f\u66f4\u7b80\u5355\u4e14\u6709\u7ade\u4e89\u529b\u7684\u9009\u62e9\u3002", "conclusion": "\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e3a\u591a\u7269\u79cdSDM\u4e8c\u503c\u5316\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u91cd\u590d\u7684\u5de5\u5177\u3002"}}
{"id": "2602.11448", "pdf": "https://arxiv.org/pdf/2602.11448", "abs": "https://arxiv.org/abs/2602.11448", "authors": ["Nghia Nguyen", "Tianjiao Ding", "Ren\u00e9 Vidal"], "title": "Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \\& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.", "AI": {"tldr": "\u63d0\u51faHierarchical Concept Embedding & Pursuit (HCEP)\u6846\u67b6\uff0c\u5c06\u5c42\u6b21\u7ed3\u6784\u5f15\u5165\u7a00\u758f\u7f16\u7801\uff0c\u5728\u6982\u5ff5\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5e26\u6765\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6982\u5ff5\u6062\u590d\u65b9\u6cd5\u5ffd\u7565\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u6b63\u786e\u4f46\u89e3\u91ca\u4e0e\u5c42\u6b21\u7ed3\u6784\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faHCEP\u6846\u67b6\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5165\u6982\u5ff5\u5d4c\u5165\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u7528\u5c42\u6b21\u7a00\u758f\u7f16\u7801\u6765\u6062\u590d\u56fe\u50cf\u4e2d\u7684\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHCEP\u5728\u6982\u5ff5\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6837\u672c\u6709\u9650\u65f6\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6982\u5ff5\u6062\u590d\u80fd\u529b\u66f4\u4f18\u3002", "conclusion": "\u5c06\u5c42\u6b21\u7ed3\u6784\u878d\u5165\u7a00\u758f\u7f16\u7801\u53ef\u4ea7\u751f\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2602.11780", "pdf": "https://arxiv.org/pdf/2602.11780", "abs": "https://arxiv.org/abs/2602.11780", "authors": ["Jinfang Wang", "Jiajie Liu", "Jianwei Wu", "Ziqin Luo", "Zhen Chen", "Chunlei Li", "Biao Han", "Tao Deng", "Yi Li", "Shuanglong Li", "Lin Liu"], "title": "RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation", "categories": ["cs.AI"], "comment": "10 pages, 3 figures", "summary": "In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-through rate(CTR). This separation often leads to misaligned optimization objectives and low funnel efficiency, limiting global optimality.\n  To address these limitations, we propose RELATE, a reinforcement learning-based end-to-end framework that unifies generation and objective alignment within a single model. Instead of decoupling text generation from downstream metric alignment, RELATE integrates performance and compliance objectives directly into the generation process via policy learning. To better capture ultimate advertiser value beyond click-level signals, We incorporate conversion-oriented metrics into the objective and jointly model them with compliance constraints as multi-dimensional rewards, enabling the model to generate high-quality ad texts that improve conversion performance under policy constraints.\n  Extensive experiments on large-scale industrial datasets demonstrate that RELATE consistently outperforms baselines. Furthermore, online deployment on a production advertising platform yields statistically significant improvements in click-through conversion rate(CTCVR) under strict policy constraints, validating the robustness and real-world effectiveness of the proposed framework.", "AI": {"tldr": "\u73b0\u6709\u5728\u7ebf\u5e7f\u544a\u6587\u672c\u751f\u6210\u7cfb\u7edf\u4e24\u9636\u6bb5\u8303\u5f0f\u6709\u5c40\u9650\uff0c\u63d0\u51fa RELATE \u6846\u67b6\u7edf\u4e00\u751f\u6210\u4e0e\u76ee\u6807\u5bf9\u9f50\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u597d\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u7cfb\u7edf\u4e24\u9636\u6bb5\u8303\u5f0f\u5bfc\u81f4\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6f0f\u6597\u6548\u7387\u4f4e\uff0c\u9650\u5236\u5168\u5c40\u6700\u4f18\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6846\u67b6 RELATE\uff0c\u5c06\u6027\u80fd\u548c\u5408\u89c4\u76ee\u6807\u901a\u8fc7\u7b56\u7565\u5b66\u4e60\u96c6\u6210\u5230\u751f\u6210\u8fc7\u7a0b\uff0c\u7eb3\u5165\u8f6c\u5316\u6307\u6807\u5e76\u4f5c\u4e3a\u591a\u7ef4\u5ea6\u5956\u52b1\u8054\u5408\u5efa\u6a21\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d RELATE \u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u751f\u4ea7\u5e7f\u544a\u5e73\u53f0\u5728\u7ebf\u90e8\u7f72\u4f7f\u70b9\u51fb\u7387\u8f6c\u5316\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RELATE \u6846\u67b6\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002"}}
{"id": "2602.11465", "pdf": "https://arxiv.org/pdf/2602.11465", "abs": "https://arxiv.org/abs/2602.11465", "authors": ["Jared Levy", "Aarti Lalwani", "Elijah Wyckoff", "Kenneth J. Loh", "Sara P. Gombatto", "Rose Yu", "Emilia Farcas"], "title": "Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.", "AI": {"tldr": "\u63d0\u51faMT - AIM\u6a21\u578b\u5bf9\u4e0b\u80cc\u90e8\u8fd0\u52a8\u5206\u7c7b\uff0c\u89e3\u51b3\u8fd0\u52a8\u76d1\u6d4b\u96be\u9898\u3002", "motivation": "\u80cc\u75db\u666e\u904d\uff0c\u9700\u8bc4\u4f30\u4e0b\u80cc\u90e8\u8fd0\u52a8\u8f85\u52a9\u7406\u7597\uff0c\u4f46\u8fdc\u7a0b\u76d1\u6d4b\u96be\uff0c\u73b0\u6709\u4f20\u611f\u5668\u4e0d\u9002\u7528\u4e8e\u65e5\u5e38\u751f\u6d3b\uff0cMT\u4f20\u611f\u5668\u867d\u6709\u4f18\u52bf\u4f46\u6570\u636e\u6709\u5c40\u9650\u3002", "method": "\u63d0\u51faMT - AIM\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u7ba1\u9053\uff0c\u5229\u7528\u6761\u4ef6\u751f\u6210\u6a21\u578b\u751f\u6210\u5408\u6210MT\u6570\u636e\uff0c\u9884\u6d4b\u5173\u8282\u8fd0\u52a8\u5b66\u7279\u5f81\u4f5c\u4e3a\u8865\u5145\u3002", "result": "MT - AIM\u5728\u5bf9\u4e0b\u80cc\u90e8\u8fd0\u52a8\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "MT - AIM\u5f25\u5408\u4e86\u751f\u7406\u4f20\u611f\u548c\u8fd0\u52a8\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.11467", "pdf": "https://arxiv.org/pdf/2602.11467", "abs": "https://arxiv.org/abs/2602.11467", "authors": ["Yining Jiao", "Sreekalyani Bhamidi", "Carlton Jude Zdanski", "Julia S Kimbell", "Andrew Prince", "Cameron P Worden", "Samuel Kirse", "Christopher Rutter", "Benjamin H Shields", "Jisan Mahmud", "Marc Niethammer"], "title": "PRISM: A 3D Probabilistic Neural Representation for Interpretable Shape Modeling", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Understanding how anatomical shapes evolve in response to developmental covariates and quantifying their spatially varying uncertainties is critical in healthcare research. Existing approaches typically rely on global time-warping formulations that ignore spatially heterogeneous dynamics. We introduce PRISM, a novel framework that bridges implicit neural representations with uncertainty-aware statistical shape analysis. PRISM models the conditional distribution of shapes given covariates, providing spatially continuous estimates of both the population mean and covariate-dependent uncertainty at arbitrary locations. A key theoretical contribution is a closed-form Fisher Information metric that enables efficient, analytically tractable local temporal uncertainty quantification via automatic differentiation. Experiments on three synthetic datasets and one clinical dataset demonstrate PRISM's strong performance across diverse tasks within a unified framework, while providing interpretable and clinically meaningful uncertainty estimates.", "AI": {"tldr": "\u63d0\u51faPRISM\u6846\u67b6\u7528\u4e8e\u89e3\u5256\u5f62\u72b6\u6f14\u53d8\u5206\u6790\uff0c\u5728\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u73b0\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7a7a\u95f4\u5f02\u8d28\u52a8\u6001\uff0c\u9700\u65b0\u65b9\u6cd5\u7406\u89e3\u89e3\u5256\u5f62\u72b6\u6f14\u53d8\u53ca\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165PRISM\u6846\u67b6\uff0c\u7ed3\u5408\u9690\u5f0f\u795e\u7ecf\u8868\u5f81\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7edf\u8ba1\u5f62\u72b6\u5206\u6790\uff0c\u7ed9\u51fa\u95ed\u5f62\u5f0fFisher\u4fe1\u606f\u5ea6\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cPRISM\u5728\u7edf\u4e00\u6846\u67b6\u4e0d\u540c\u4efb\u52a1\u6709\u5f3a\u6027\u80fd\uff0c\u7ed9\u51fa\u53ef\u89e3\u91ca\u4e14\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "PRISM\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u89e3\u5256\u5f62\u72b6\u6f14\u53d8\u5206\u6790\u95ee\u9898\uff0c\u65b0\u65b9\u6cd5\u53ef\u884c\u4e14\u6709\u6548\u3002"}}
{"id": "2602.11790", "pdf": "https://arxiv.org/pdf/2602.11790", "abs": "https://arxiv.org/abs/2602.11790", "authors": ["Lingyong Yan", "Jiulong Wu", "Dong Xie", "Weixian Shi", "Deguo Xia", "Jizhou Huang"], "title": "Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation", "categories": ["cs.AI", "cs.CL"], "comment": "For more information, visit the project website: https://robitsg.github.io/LASEV/", "summary": "Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf LAVES \u7528\u4e8e\u4ece\u6559\u80b2\u95ee\u9898\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u5b66\u89c6\u9891\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u751f\u4ea7\uff0c\u6210\u672c\u5927\u5e45\u964d\u4f4e\u4e14\u541e\u5410\u91cf\u9ad8\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u9700\u8981\u4e25\u683c\u903b\u8f91\u548c\u7cbe\u786e\u77e5\u8bc6\u8868\u793a\u7684\u573a\u666f\uff08\u5982\u6559\u5b66\u548c\u6559\u80b2\u5a92\u4f53\uff09\u4e2d\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5c06\u6559\u80b2\u89c6\u9891\u751f\u6210\u4f5c\u4e3a\u591a\u76ee\u6807\u4efb\u52a1\uff0c\u5c06\u751f\u6210\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u7531\u4e2d\u592e\u534f\u8c03\u667a\u80fd\u4f53\u76d1\u7763\u5e76\u8bbe\u7f6e\u8d28\u91cf\u95e8\u548c\u8fed\u4ee3\u6279\u5224\u673a\u5236\uff0c\u6784\u5efa\u53ef\u6267\u884c\u89c6\u9891\u811a\u672c\u3002", "result": "\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\uff0cLAVES \u65e5\u4ea7\u91cf\u8d85\u767e\u4e07\u89c6\u9891\uff0c\u6210\u672c\u6bd4\u5f53\u524d\u884c\u4e1a\u6807\u51c6\u65b9\u6cd5\u964d\u4f4e\u8d85 95%\uff0c\u4e14\u4fdd\u6301\u9ad8\u63a5\u53d7\u7387\u3002", "conclusion": "LAVES \u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u6559\u80b2\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u751f\u4ea7\u3002"}}
{"id": "2602.11482", "pdf": "https://arxiv.org/pdf/2602.11482", "abs": "https://arxiv.org/abs/2602.11482", "authors": ["Kazuki Haishima", "Kyohei Suzuki", "Konstantinos Slavakis"], "title": "External Division of Two Bregman Proximity Operators for Poisson Inverse Problems", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u542b\u6cca\u677e\u566a\u58f0\u7ebf\u6027\u6a21\u578b\u6062\u590d\u7a00\u758f\u5411\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u5d4c\u5165\u65b0\u7b97\u5b50\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4ece\u542b\u6cca\u677e\u566a\u58f0\u7ebf\u6027\u6a21\u578b\u6062\u590d\u7a00\u758f\u5411\u91cf\u95ee\u9898\uff0c\u51cf\u8f7b\u7ecf\u5178l1\u8303\u6570\u6b63\u5219\u5316\u4f30\u8ba1\u504f\u5dee\u3002", "method": "\u5f15\u5165\u5916\u90e8\u9664\u6cd5\u5b9a\u4e49\u7684\u7b97\u5b50\u4fc3\u8fdb\u7a00\u758f\u89e3\uff0c\u5c06\u5176\u5d4c\u5165NoLips\u7b97\u6cd5\uff1b\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u91cd\u8868\u8ff0\u9610\u660e\u7b97\u5b50\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u65b0\u65b9\u6cd5\u6536\u655b\u66f4\u7a33\u5b9a\uff0c\u5728\u5408\u6210\u6570\u636e\u548c\u56fe\u50cf\u6062\u590d\u95ee\u9898\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8eKL\u6563\u5ea6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u6062\u590d\u542b\u6cca\u677e\u566a\u58f0\u7ebf\u6027\u6a21\u578b\u7684\u7a00\u758f\u5411\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.11792", "pdf": "https://arxiv.org/pdf/2602.11792", "abs": "https://arxiv.org/abs/2602.11792", "authors": ["Hongbo Zhang", "Yue Yang", "Jianhao Yan", "Guangsheng Bao", "Yue Zhang", "Yue Zhang"], "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u6570\u636e\u672a\u516c\u5f00\u5f15\u53d1\u57fa\u51c6\u6c61\u67d3\u62c5\u5fe7\uff0c\u63d0\u51faMin - kNN Distance\u68c0\u6d4b\u5668\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u6709\u6548\u533a\u5206RL\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u548c\u672a\u89c1\u8fc7\u7684\u793a\u4f8b\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RLVR\u8bad\u7ec3\u6570\u636e\u672a\u516c\u5f00\u5f15\u53d1\u57fa\u51c6\u6c61\u67d3\u62c5\u5fe7\uff0c\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u68c0\u6d4b\u65b9\u6cd5\u5bf9RLVR\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165Min - kNN Distance\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u9ed1\u76d2\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5bf9\u7ed9\u5b9a\u63d0\u793a\u91c7\u6837\u591a\u4e2a\u8865\u5168\u7ed3\u679c\u5e76\u8ba1\u7b97k\u4e2a\u6700\u5c0f\u6700\u8fd1\u90bb\u7f16\u8f91\u8ddd\u79bb\u7684\u5e73\u5747\u503c\u6765\u91cf\u5316\u751f\u6210\u7684\u6536\u655b\u6027\uff0c\u65e0\u9700\u8bbf\u95ee\u53c2\u8003\u6a21\u578b\u6216\u6807\u8bb0\u6982\u7387\u3002", "result": "\u5728\u591a\u4e2aRLVR\u8bad\u7ec3\u7684\u63a8\u7406\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMin - kNN Distance\u80fd\u53ef\u9760\u5730\u533a\u5206RL\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u548c\u672a\u89c1\u8fc7\u7684\u793a\u4f8b\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u548cRL\u6c61\u67d3\u68c0\u6d4b\u57fa\u7ebf\u3002", "conclusion": "Min - kNN Distance\u662f\u4e00\u79cd\u6709\u6548\u7684\u68c0\u6d4bRLVR\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.11491", "pdf": "https://arxiv.org/pdf/2602.11491", "abs": "https://arxiv.org/abs/2602.11491", "authors": ["Xuan Yu", "Xu Wang", "Rui Zhu", "Yudong Zhang", "Yang Wang"], "title": "Exploring Multiple High-Scoring Subspaces in Generative Flow Networks", "categories": ["cs.LG"], "comment": null, "summary": "As a probabilistic sampling framework, Generative Flow Networks (GFlowNets) show strong potential for constructing complex combinatorial objects through the sequential composition of elementary components. However, existing GFlowNets often suffer from excessive exploration over vast state spaces, leading to over-sampling of low-reward regions and convergence to suboptimal distributions. Effectively biasing GFlowNets toward high-reward solutions remains a non-trivial challenge. In this paper, we propose CMAB-GFN, which integrates a combinatorial multi-armed bandit (CMAB) framework with GFlowNet policies. The CMAB component prunes low-quality actions, yielding compact high-scoring subspaces for exploration. Restricting GFNs to these compact high-scoring subspaces accelerates the discovery of high-value candidates, while the exploration of different subspaces ensures that diversity is not sacrificed. Experimental results on multiple tasks demonstrate that CMAB-GFN generates higher-reward candidates than existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCMAB - GFN\uff0c\u5c06\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e0eGFlowNet\u7b56\u7565\u96c6\u6210\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u751f\u6210\u66f4\u9ad8\u5956\u52b1\u7684\u5019\u9009\u65b9\u6848\u3002", "motivation": "\u73b0\u6709Generative Flow Networks\u5728\u5e9e\u5927\u72b6\u6001\u7a7a\u95f4\u8fc7\u5ea6\u63a2\u7d22\uff0c\u5bfc\u81f4\u4f4e\u5956\u52b1\u533a\u57df\u8fc7\u91c7\u6837\u548c\u6536\u655b\u5230\u6b21\u4f18\u5206\u5e03\uff0c\u9700\u8981\u6709\u6548\u5f15\u5bfcGFlowNets\u5bfb\u627e\u9ad8\u5956\u52b1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCMAB - GFN\uff0c\u7528\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u7ec4\u4ef6\u4fee\u526a\u4f4e\u8d28\u91cf\u52a8\u4f5c\uff0c\u5f97\u5230\u7d27\u51d1\u7684\u9ad8\u5206\u6570\u5b50\u7a7a\u95f4\u4f9bGenerative Flow Networks\u63a2\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCMAB - GFN\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u9ad8\u5956\u52b1\u7684\u5019\u9009\u65b9\u6848\u3002", "conclusion": "CMAB - GFN\u80fd\u6709\u6548\u5f15\u5bfcGenerative Flow Networks\u627e\u5230\u9ad8\u5956\u52b1\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u4e0d\u727a\u7272\u591a\u6837\u6027\u3002"}}
{"id": "2602.11799", "pdf": "https://arxiv.org/pdf/2602.11799", "abs": "https://arxiv.org/abs/2602.11799", "authors": ["Pingjun Pan", "Tingting Zhou", "Peiyao Lu", "Tingting Fei", "Hongxiang Chen", "Chuanjiang Luo"], "title": "Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation", "categories": ["cs.AI"], "comment": null, "summary": "Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.", "AI": {"tldr": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u8350\u8bed\u4e49ID\u65b9\u6cd5\u5b58\u5728\u5206\u8bcd\u6b20\u4f73\u548c\u67b6\u6784\u4e0e\u6570\u636e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51faHi - SAM\u6846\u67b6\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u4f18\u4e8eSOTA\u57fa\u7ebf\uff0c\u4e0a\u7ebf\u5e73\u53f0\u6838\u5fc3\u6307\u6807\u63d0\u53476.55%\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u4e49ID\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u8350\u4e2d\u5b58\u5728\u7684\u5206\u8bcd\u6b20\u4f73\uff08\u7f3a\u4e4f\u8de8\u6a21\u6001\u8bed\u4e49\u548c\u7279\u5b9a\u6a21\u6001\u7ec6\u8282\u89e3\u8026\uff09\u548c\u67b6\u6784\u4e0e\u6570\u636e\u4e0d\u5339\u914d\uff08\u5ffd\u7565\u7528\u6237\u4ea4\u4e92\u3001\u7269\u54c1\u548c\u4ee4\u724c\u5c42\u6b21\u7ed3\u6784\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51faHi - SAM\u6846\u67b6\uff0c\u5305\u542b\u89e3\u7ea0\u7f20\u8bed\u4e49\u5206\u8bcd\u5668\uff08DST\uff09\u7edf\u4e00\u6a21\u6001\u5e76\u91cf\u5316\uff0c\u4ee5\u53ca\u5206\u5c42\u8bb0\u5fc6\u951a\u5b9a\u53d8\u538b\u5668\uff08HMAT\uff09\u901a\u8fc7\u5206\u5c42RoPE\u6062\u590d\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u6548\u679c\u4f18\u4e8eSOTA\u57fa\u7ebf\uff0c\u5728\u51b7\u542f\u52a8\u573a\u666f\u8868\u73b0\u7a81\u51fa\uff1b\u5728\u5927\u89c4\u6a21\u793e\u4ea4\u5e73\u53f0\u4e0a\u7ebf\u6838\u5fc3\u5728\u7ebf\u6307\u6807\u63d0\u53476.55%\u3002", "conclusion": "Hi - SAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u6001\u63a8\u8350\u8bed\u4e49ID\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2602.11498", "pdf": "https://arxiv.org/pdf/2602.11498", "abs": "https://arxiv.org/abs/2602.11498", "authors": ["Xuan Yu", "Xu Wang", "Rui Zhu", "Yudong Zhang", "Yang Wang"], "title": "Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning", "categories": ["cs.LG"], "comment": null, "summary": "Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \\modelname converges faster than existing works on large state spaces. Furthermore, \\modelname not only generates candidates with higher rewards but also significantly improves their diversity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9650\u5236\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNets\uff09\u884c\u52a8\u8005\u63a2\u7d22\u8303\u56f4\u7684\u65b9\u6cd5\uff0c\u80fd\u4f7f\u6a21\u578b\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u751f\u6210\u66f4\u9ad8\u5956\u52b1\u548c\u66f4\u591a\u6837\u7684\u5019\u9009\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709GFlowNets\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u81ea\u7531\u63a2\u7d22\u65f6\u5b58\u5728\u663e\u8457\u6536\u655b\u6311\u6218\u3002", "method": "\u5f15\u5165\u89c4\u5212\u5668\u5c06\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u91cd\u53e0\u7684\u90e8\u5206\u72b6\u6001\u7a7a\u95f4\uff0c\u8ba9\u884c\u52a8\u8005\u5728\u5176\u4e2d\u9ad8\u6548\u8bc6\u522b\u9ad8\u5956\u52b1\u5b50\u533a\u57df\uff1b\u91c7\u7528\u542f\u53d1\u5f0f\u7b56\u7565\u5207\u6362\u90e8\u5206\u533a\u57df\uff0c\u907f\u514d\u884c\u52a8\u8005\u6d6a\u8d39\u65f6\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\uff0c\u751f\u6210\u7684\u5019\u9009\u7ed3\u679c\u5956\u52b1\u66f4\u9ad8\u4e14\u591a\u6837\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9650\u5236\u884c\u52a8\u8005\u63a2\u7d22\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3GFlowNets\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u6536\u655b\u95ee\u9898\u3002"}}
{"id": "2602.11807", "pdf": "https://arxiv.org/pdf/2602.11807", "abs": "https://arxiv.org/abs/2602.11807", "authors": ["Lianjun Wu", "Shengchen Zhu", "Yuxuan Liu", "Liuyu Kai", "Xiaoduan Feng", "Duomin Wang", "Wenshuo Liu", "Jingxuan Zhang", "Kelvin Li", "Bin Wang"], "title": "PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts", "categories": ["cs.AI"], "comment": null, "summary": "Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25\u00b0) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.", "AI": {"tldr": "\u63d0\u51faPuYun - LDM\u6a21\u578b\u4ee5\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u96c6\u5408\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u53ef\u6269\u6563\u6027\u95ee\u9898\uff0c\u52a0\u901f\u9884\u62a5\u6548\u7387\u3002", "motivation": "\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u96c6\u5408\u5929\u6c14\u9884\u62a5\u4e2d\u53ef\u6269\u6563\u6027\u6709\u9650\uff0c\u6c14\u8c61\u9886\u57df\u7f3a\u5c11\u9002\u7528\u57fa\u7840\u6a21\u578b\uff0c\u73b0\u6709\u9891\u7387\u65b9\u6cd5\u5728\u591a\u53d8\u91cf\u6570\u636e\u4e2d\u5b58\u5728\u6b63\u5219\u5316\u5f3a\u5ea6\u4e0d\u5747\u95ee\u9898\u3002", "method": "\u63d0\u51fa3D - MAE\u7f16\u7801\u5929\u6c14\u72b6\u6001\u6f14\u53d8\u7279\u5f81\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u989d\u5916\u6761\u4ef6\uff0c\u91c7\u7528VA - MFM\u7b56\u7565\u6839\u636e\u5404\u53d8\u91cf\u9891\u8c31\u80fd\u91cf\u5206\u5e03\u81ea\u9002\u5e94\u9009\u62e9\u9608\u503c\uff0c\u7ed3\u5408\u63d0\u51faPuYun - LDM\u3002", "result": "PuYun - LDM\u589e\u5f3a\u4e86\u6f5c\u5728\u53ef\u6269\u6563\u6027\uff0c\u77ed\u671f\u9884\u62a5\u6027\u80fd\u8d85ENS\uff0c\u957f\u671f\u4e0eENS\u76f8\u5f53\uff0c\u80fd\u5728\u5355GPU\u4e0a\u9ad8\u6548\u751f\u621015\u5929\u5168\u7403\u9884\u62a5\u3002", "conclusion": "PuYun - LDM\u80fd\u6709\u6548\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u96c6\u5408\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u62a5\u6548\u7387\u3002"}}
{"id": "2602.11500", "pdf": "https://arxiv.org/pdf/2602.11500", "abs": "https://arxiv.org/abs/2602.11500", "authors": ["Diptarka Chakraborty", "Kushagra Chatterjee", "Debarati Das", "Tien-Long Nguyen"], "title": "A Generic Framework for Fair Consensus Clustering in Streams", "categories": ["cs.LG"], "comment": "Accepted in AAMAS 2026", "summary": "Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.\n  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.", "AI": {"tldr": "\u7814\u7a76\u6d41\u5f0f\u6a21\u578b\u4e0b\u7684\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\uff0c\u8bbe\u8ba1\u9996\u4e2a\u5e38\u6570\u56e0\u5b50\u7b97\u6cd5\uff0c\u63d0\u51fa\u901a\u7528\u6846\u67b6\u5e76\u6269\u5c55\u5230k - \u4e2d\u503c\u5171\u8bc6\u805a\u7c7b\u95ee\u9898\u3002", "motivation": "Chakraborty\u7b49\u4eba\u7684\u79bb\u7ebf\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u65b9\u6cd5\u5b58\u50a8\u6240\u6709\u8f93\u5165\u805a\u7c7b\u4ee3\u4ef7\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u9700\u8981\u5728\u6d41\u5f0f\u6a21\u578b\u4e0b\u7814\u7a76\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u3002", "method": "\u8bbe\u8ba1\u9996\u4e2a\u5e38\u6570\u56e0\u5b50\u7b97\u6cd5\u5904\u7406\u6d41\u6570\u636e\uff0c\u4ec5\u5b58\u50a8\u5bf9\u6570\u6570\u91cf\u7684\u8f93\u5165\uff1b\u5f15\u5165\u65b0\u7684\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u5c06\u6700\u63a5\u8fd1\u516c\u5e73\u805a\u7c7b\u4e0e\u805a\u7c7b\u62df\u5408\u76f8\u7ed3\u5408\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u548c\u6846\u67b6\u5728\u6d41\u5f0f\u548c\u79bb\u7ebf\u573a\u666f\u4e0b\u90fd\u6709\u6539\u8fdb\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u4e14\u6846\u67b6\u5bf9\u516c\u5e73\u6027\u5b9a\u4e49\u4e0d\u654f\u611f\u3002", "conclusion": "\u53ef\u4ee5\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684k - \u4e2d\u503c\u5171\u8bc6\u805a\u7c7b\u95ee\u9898\u3002"}}
{"id": "2602.11812", "pdf": "https://arxiv.org/pdf/2602.11812", "abs": "https://arxiv.org/abs/2602.11812", "authors": ["Huanyi Xie", "Yubin Chen", "Liangyu Wang", "Lijie Hu", "Di Wang"], "title": "Predicting LLM Output Length via Entropy-Guided Representations", "categories": ["cs.AI"], "comment": null, "summary": "The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic \"one-to-many\" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.", "AI": {"tldr": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u548c\u5f3a\u5316\u5b66\u4e60\u91c7\u6837\u4e2d\u5e8f\u5217\u957f\u5ea6\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u8ba1\u7b97\u6d6a\u8d39\uff0c\u672c\u6587\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6846\u67b6\u9884\u6d4b\u957f\u5ea6\uff0c\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u548c\u5f3a\u5316\u5b66\u4e60\u91c7\u6837\u4e2d\u5e8f\u5217\u957f\u5ea6\u957f\u5c3e\u5206\u5e03\u56e0\u586b\u5145\u5bfc\u81f4\u7684\u8ba1\u7b97\u6d6a\u8d39\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5f00\u9500\u9ad8\u3001\u6cdb\u5316\u6027\u5dee\u7b49\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u7528\u4e3b\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u9884\u6d4b\u957f\u5ea6\uff0c\u5305\u542b\u71b5\u5f15\u5bfc\u4ee4\u724c\u6c60\u5316\uff08EGTP\uff09\u548c\u6e10\u8fdb\u957f\u5ea6\u9884\u6d4b\uff08PLP\uff09\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u6784\u5efa\u5e76\u53d1\u5e03\u7efc\u5408\u57fa\u51c6ForeLen\u3002", "result": "EGTP\u5728ForeLen\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u964d\u4f4e\u4e8629.16%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u4e0e\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u5668\u96c6\u6210\u5e26\u6765\u4e86\u663e\u8457\u7684\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u548c\u8bc4\u4f30\u57fa\u7ebf\u3002"}}
{"id": "2602.11505", "pdf": "https://arxiv.org/pdf/2602.11505", "abs": "https://arxiv.org/abs/2602.11505", "authors": ["Jiangkai Xiong", "Kalyan Talluri", "Hanzhao Wang"], "title": "Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice", "categories": ["cs.LG"], "comment": null, "summary": "Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u4f01\u4e1a\u7f3a\u5931\u6d88\u8d39\u8005\u5916\u90e8\u9009\u62e9\u4fe1\u606f\u95ee\u9898\uff0c\u7814\u7a76\u5229\u7528\u6709\u504f\u5dee\u7684\u5916\u90e8\u9009\u62e9\u6982\u7387\u9884\u6d4b\u5668\uff0c\u5f00\u53d1\u6821\u51c6\u65b9\u6cd5\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u6548\u65e0\u8d2d\u4e70\u4f30\u8ba1\uff0c\u5e76\u5206\u6790\u5bf9\u4e0b\u6e38\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u4f01\u4e1a\u901a\u5e38\u65e0\u6cd5\u83b7\u53d6\u5173\u952e\u6d88\u8d39\u8005\u884c\u4e3a\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8f85\u52a9\u6570\u636e\uff0c\u800c\u672c\u6587\u7814\u7a76\u5229\u7528\u6709\u504f\u5dee\u7684\u9ed1\u76d2\u8f85\u52a9\u9884\u6d4b\u5668\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6821\u51c6\u65b9\u6cd5\uff0c\u4e00\u662f\u5728\u5bf9\u6570\u7a7a\u95f4\u4eff\u5c04\u5931\u51c6\u4e0b\u7528\u7b80\u5355\u56de\u5f52\u8bc6\u522b\u5916\u9009\u6548\u7528\u53c2\u6570\uff1b\u4e8c\u662f\u5728\u8fd1\u5355\u8c03\u6761\u4ef6\u4e0b\u63d0\u51fa\u57fa\u4e8e\u6392\u540d\u7684\u6821\u51c6\u65b9\u6cd5\u5e76\u63a8\u5bfc\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u6539\u8fdb\u4e86\u65e0\u8d2d\u4e70\u4f30\u8ba1\u548c\u4e0b\u6e38\u54c1\u7c7b\u51b3\u7b56\uff0c\u8fd8\u8ba8\u8bba\u4e86\u591a\u9884\u6d4b\u5668\u7684\u7a33\u5065\u805a\u5408\u6269\u5c55\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6821\u51c6\u65b9\u6cd5\u80fd\u5c06\u6709\u504f\u5dee\u9884\u6d4b\u8f6c\u5316\u4e3a\u6709\u6548\u65e0\u8d2d\u4e70\u4f30\u8ba1\uff0c\u5206\u6790\u4e86\u4f30\u8ba1\u8bef\u5dee\u5bf9\u4e0b\u6e38\u51b3\u7b56\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u660e\u786e\u4e86\u5404\u8bef\u5dee\u6765\u6e90\u7684\u4e3b\u5bfc\u60c5\u51b5\u3002"}}
{"id": "2602.11824", "pdf": "https://arxiv.org/pdf/2602.11824", "abs": "https://arxiv.org/abs/2602.11824", "authors": ["Jialin Wu", "Wei Shi", "Han Shen", "Peigui Qi", "Kunsheng Tang", "Zhicong Huang", "Binghao Wang", "Zhou Yang"], "title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.", "AI": {"tldr": "\u63d0\u51faREVIS\u6846\u67b6\u89e3\u51b3\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u51cf\u5c11\u5e7b\u89c9\u7387", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e38\u51fa\u73b0\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u539f\u56e0\u662f\u89c6\u89c9\u7279\u5f81\u548c\u9884\u8bad\u7ec3\u6587\u672c\u8868\u5f81\u5728\u6df1\u5c42\u7f51\u7edc\u5c42\u4ea4\u7ec7\uff0c\u9700\u89e3\u51b3\u8be5\u95ee\u9898", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684REVIS\u6846\u67b6\uff0c\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u63d0\u53d6\u7eaf\u89c6\u89c9\u4fe1\u606f\u5411\u91cf\uff0c\u91c7\u7528\u6821\u51c6\u7b56\u7565\u5728\u4fe1\u606f\u88ab\u6291\u5236\u7684\u6df1\u5ea6\u8fdb\u884c\u7a00\u758f\u5e72\u9884", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREVIS\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u5c06\u7269\u4f53\u5e7b\u89c9\u7387\u964d\u4f4e\u4e86\u7ea619%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u822c\u63a8\u7406\u80fd\u529b", "conclusion": "REVIS\u80fd\u4ee5\u6700\u5c0f\u8ba1\u7b97\u6210\u672c\u6709\u6548\u6062\u590d\u89c6\u89c9\u4fe1\u606f\uff0c\u51cf\u5c11\u7269\u4f53\u5e7b\u89c9\u7387"}}
{"id": "2602.11852", "pdf": "https://arxiv.org/pdf/2602.11852", "abs": "https://arxiv.org/abs/2602.11852", "authors": ["Yordan Yordanov", "Matteo Forasassi", "Bayar Menzat", "Ruizhi Wang", "Chang Qi", "Markus Kaltenberger", "Amine M'Charrak", "Tommaso Salvatori", "Thomas Lukasiewicz"], "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Preprint under review. Equal contribution: Yordan Yordanov and Matteo Forasassi. 39 pages, 25 figures, 22 tables", "summary": "While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. \"woman\") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.", "AI": {"tldr": "\u63d0\u51faPrototype Transformer (ProtoT) \u67b6\u6784\uff0c\u53ef\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\u3001\u6709\u8ba1\u7b97\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u6027\u80fd\u63a5\u8fd1SOTA\u4e14\u80fd\u5c55\u73b0\u9c81\u68d2\u6027\u6765\u6e90\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5b58\u5728\u6b3a\u9a97\u548c\u5e7b\u89c9\u7b49\u98ce\u9669\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u67b6\u6784\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u539f\u578b\uff08\u53c2\u6570\u5411\u91cf\uff09\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578bProtoT\uff0c\u901a\u8fc7\u8f93\u5165\u5e8f\u5217\u548c\u539f\u578b\u7684\u53cc\u5411\u901a\u4fe1\u5de5\u4f5c\u3002", "result": "ProtoT\u53ef\u81ea\u52a8\u6355\u6349\u53ef\u547d\u540d\u6982\u5ff5\uff0c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u4e3a\u7ebf\u6027\uff0c\u5728\u6587\u672c\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u5bf9\u8f93\u5165\u6270\u52a8\u6709\u9c81\u68d2\u6027\u4e14\u80fd\u5c55\u793a\u9c81\u68d2\u6027\u548c\u654f\u611f\u6027\u6765\u6e90\u3002", "conclusion": "ProtoT\u4e3a\u521b\u5efa\u53ef\u89e3\u91ca\u7684\u9ad8\u6027\u80fd\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.11523", "pdf": "https://arxiv.org/pdf/2602.11523", "abs": "https://arxiv.org/abs/2602.11523", "authors": ["Li He", "Qiang Qu", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Unifying Stable Optimization and Reference Regularization in RLHF", "categories": ["cs.LG"], "comment": "ICLR 2026", "summary": "Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \\textbf{reward hacking} and \\textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($\u03c0_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($\u03c0_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $\u03c0_0$ and $\u03c0_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6b63\u5219\u5316\u65b9\u6cd5\u5e73\u8861\u9632\u6b62\u5956\u52b1\u7834\u89e3\u548c\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u76ee\u6807\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dRLHF\u5b58\u5728\u5956\u52b1\u7834\u89e3\u548c\u7a33\u5b9a\u4f18\u5316\u4e24\u5927\u6311\u6218\uff0c\u4e14\u540c\u65f6\u5bf9$\u03c0_0$\u548c$\u03c0_t$\u6b63\u5219\u5316\u7684\u9690\u5f0f\u6743\u8861\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5f97\u5230\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u635f\u5931\uff0c\u660e\u786e\u5e73\u8861\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8eRLHF\u548c\u5728\u7ebf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u6b64\u7edf\u4e00\u65b9\u6cd5\u80fd\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u6539\u5584\u5bf9\u9f50\u7ed3\u679c\u548c\u5b9e\u73b0\u590d\u6742\u5ea6\u3002"}}
{"id": "2602.11860", "pdf": "https://arxiv.org/pdf/2602.11860", "abs": "https://arxiv.org/abs/2602.11860", "authors": ["Lu Tao", "Jinxuan Luo", "Yousuke Watanabe", "Zhengshu Zhou", "Yuhuan Lu", "Shen Ying", "Pan Zhang", "Fei Zhao", "Hiroaki Takada"], "title": "Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models", "categories": ["cs.AI"], "comment": "Submitted to IEEE TITS. Under review", "summary": "Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdVRCsim\u6a21\u62df\u6846\u67b6\u751f\u6210\u6570\u636e\uff0c\u6784\u5efaVRC - QA\u6570\u636e\u96c6\uff0c\u63d0\u51faTalk2DM\u6a21\u5757\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6709\u826f\u597d\u51c6\u786e\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u5730\u56fe\uff08DM\uff09\u7cfb\u7edf\u7f3a\u4e4f\u81ea\u7136\u8bed\u8a00\u652f\u6301\uff08NLS\uff09\u7684\u4eba\u673a\u754c\u9762\uff0c\u9700\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u5f15\u5165VRCsim\u6846\u67b6\uff0c\u6784\u5efaVRC - QA\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8eCoP\u673a\u5236\u7684Talk2DM\u6a21\u5757\u3002", "result": "Talk2DM\u80fd\u5207\u6362\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7531Qwen3:8B\u7b49\u6a21\u578b\u9a71\u52a8\u65f6\uff0cNLS\u67e5\u8be2\u51c6\u786e\u7387\u8d8593%\uff0c\u5e73\u5747\u54cd\u5e94\u65f6\u95f42 - 5\u79d2\u3002", "conclusion": "Talk2DM\u5177\u6709\u8f83\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5927\u6a21\u578b\u867d\u51c6\u786e\u7387\u9ad8\u4f46\u6548\u7387\u4f4e\u3002"}}
{"id": "2602.11524", "pdf": "https://arxiv.org/pdf/2602.11524", "abs": "https://arxiv.org/abs/2602.11524", "authors": ["Congmin Zheng", "Xiaoyun Mo", "Xinbei Ma", "Qiqiang Lin", "Yin Zhao", "Jiachen Zhu", "Xingyu Lou", "Jun Wang", "Zhaoxiang Wang", "Weiwen Liu", "Zhuosheng Zhang", "Yong Yu", "Weinan Zhang"], "title": "Adaptive Milestone Reward for GUI Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.", "AI": {"tldr": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u4fdd\u771f\u5ea6\u548c\u5bc6\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51faADMIRE\u673a\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u63d0\u5347\u6210\u529f\u7387\u4e14\u6cdb\u5316\u6027\u5f3a", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u4e0a\u6709\u95ee\u9898\uff0c\u5956\u52b1\u4fdd\u771f\u5ea6\u548c\u5bc6\u5ea6\u96be\u4ee5\u6743\u8861", "method": "\u63d0\u51faADMIRE\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u951a\u5b9a\u5230\u52a8\u6001\u63d0\u70bc\u7684\u91cc\u7a0b\u7891\u6765\u6784\u5efa\u53ef\u9a8c\u8bc1\u3001\u81ea\u9002\u5e94\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u5e76\u96c6\u6210\u975e\u5bf9\u79f0\u4fe1\u7528\u5206\u914d\u7b56\u7565", "result": "\u5728AndroidWorld\u4e0a\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684\u6210\u529f\u7387\u6709\u8d8510%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u5728\u591a\u79cdRL\u7b97\u6cd5\u548c\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d", "conclusion": "ADMIRE\u673a\u5236\u6709\u6548\u89e3\u51b3\u5956\u52b1\u6743\u8861\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u6027\u80fd\u548c\u6cdb\u5316\u6027"}}
{"id": "2602.11865", "pdf": "https://arxiv.org/pdf/2602.11865", "abs": "https://arxiv.org/abs/2602.11865", "authors": ["Nenad Toma\u0161ev", "Matija Franklin", "Simon Osindero"], "title": "Intelligent AI Delegation", "categories": ["cs.AI"], "comment": null, "summary": "AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u667a\u80fdAI\u59d4\u6258\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u59d4\u6258\u7f51\u7edc\u4e2d\u7684\u4eba\u7c7b\u548cAI\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5206\u89e3\u548c\u59d4\u6258\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u73af\u5883\u53d8\u5316\u548c\u5904\u7406\u610f\u5916\u6545\u969c\uff0c\u4e3a\u8ba9AI\u4ee3\u7406\u5b9e\u73b0\u66f4\u5b8f\u4f1f\u76ee\u6807\uff0c\u9700\u66f4\u597d\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5305\u542b\u4efb\u52a1\u5206\u914d\u51b3\u7b56\u3001\u6743\u9650\u8d23\u4efb\u8f6c\u79fb\u3001\u660e\u786e\u89d2\u8272\u8fb9\u754c\u3001\u610f\u56fe\u8bf4\u660e\u548c\u5efa\u7acb\u4fe1\u4efb\u673a\u5236\u3002", "result": "\u65e0\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u590d\u6742\u59d4\u6258\u7f51\u7edc\u4e2d\u7684\u4eba\u7c7b\u548cAI\u59d4\u6258\u65b9\u4e0e\u88ab\u59d4\u6258\u65b9\uff0c\u6709\u52a9\u4e8e\u65b0\u5174\u4ee3\u7406\u7f51\u7edc\u534f\u8bae\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.11530", "pdf": "https://arxiv.org/pdf/2602.11530", "abs": "https://arxiv.org/abs/2602.11530", "authors": ["Eunyeong Cho", "Jehyeon Bang", "Ranggi Hwang", "Minsoo Rhu"], "title": "PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models", "categories": ["cs.LG", "cs.AR"], "comment": "Accepted for publication at the 32nd IEEE International Symposium on High-Performance Computer Architecture (HPCA-32), 2026", "summary": "The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.", "AI": {"tldr": "\u73b0\u6709LLM\u670d\u52a1\u6846\u67b6\u5728\u5904\u7406\u63a8\u7406\u578b\u5927\u6a21\u578b\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u63d0\u51faPASCAL\u8c03\u5ea6\u7b97\u6cd5\uff0c\u53ef\u964d\u4f4eTTFT\u5e76\u4fdd\u6301QoE\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u91c7\u7528CoT\u63a8\u7406\u5e26\u6765\u670d\u52a1\u6311\u6218\uff0c\u73b0\u6709LLM\u670d\u52a1\u6846\u67b6\u65e0\u6cd5\u533a\u5206\u63a8\u7406\u548c\u56de\u7b54\u9636\u6bb5\uff0c\u5728GPU\u5185\u5b58\u53d7\u9650\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faPASCAL\u76f8\u4f4d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\uff0c\u4f18\u5148\u5904\u7406\u63a8\u7406\u4ee5\u964d\u4f4eTTFT\uff0c\u5728\u56de\u7b54\u9636\u6bb5\u4f7f\u7528\u53ef\u63a7\u62a2\u5360\u548c\u4ee4\u724c pacing \u6765\u4fdd\u8bc1QoE\uff1b\u91c7\u7528\u5206\u5c42\u8c03\u5ea6\u5668\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u653e\u7f6e\u548c\u5b9e\u4f8b\u5185\u6267\u884c\uff0c\u5e76\u5728\u9636\u6bb5\u8fb9\u754c\u8fdb\u884c\u52a8\u6001\u8fc1\u79fb\u3002", "result": "\u5728\u4f7f\u7528DeepSeek - R1 - Distill - Qwen - 32B\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPASCAL\u6700\u591a\u53ef\u5c06\u5c3e\u90e8TTFT\u964d\u4f4e72%\uff0c\u5e76\u4fdd\u6301\u56de\u7b54\u9636\u6bb5SLO\u8fbe\u6807\u3002", "conclusion": "\u76f8\u4f4d\u611f\u77e5\u8c03\u5ea6\u5bf9\u4e8e\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5f88\u91cd\u8981\u3002"}}
{"id": "2602.11881", "pdf": "https://arxiv.org/pdf/2602.11881", "abs": "https://arxiv.org/abs/2602.11881", "authors": ["Yifan Luo", "Yang Zhan", "Jiedong Jiang", "Tianyang Liu", "Mingrui Wu", "Zhennan Zhou", "Bin Dong"], "title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders", "categories": ["cs.AI"], "comment": null, "summary": "Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"feature splitting\" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.", "AI": {"tldr": "\u4e3a\u6355\u6349\u5927\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u5c42\u6b21\u7ed3\u6784\uff0c\u63d0\u51fa\u5206\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668HSAE\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u4e14\u4fdd\u7559\u6807\u51c6SAEs\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b64\u7acb\u8bc6\u522b\u7279\u5f81\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u6709\u5c42\u6b21\u5316\u7684\u5185\u5728\u7ed3\u6784\uff0c\u9700\u65b0\u65b9\u6cd5\u6355\u6349\u3002", "method": "\u63d0\u51faHSAE\uff0c\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u635f\u5931\u548c\u968f\u673a\u7279\u5f81\u6270\u52a8\u673a\u5236\u52a0\u5f3a\u7236\u5b50\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5c42\u7684\u5b9e\u9a8c\u4e2d\uff0cHSAE\u80fd\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4fdd\u7559\u6807\u51c6SAEs\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "HSAE\u662f\u53d1\u73b0\u548c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u8868\u793a\u4e2d\u591a\u5c3a\u5ea6\u6982\u5ff5\u7ed3\u6784\u7684\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u5de5\u5177\u3002"}}
