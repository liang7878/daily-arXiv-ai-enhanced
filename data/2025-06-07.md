<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 73]
- [cs.CE](#cs.CE) [Total: 12]
- [cs.DB](#cs.DB) [Total: 10]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.GT](#cs.GT) [Total: 13]
- [cs.IR](#cs.IR) [Total: 17]
- [cs.LG](#cs.LG) [Total: 247]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.SE](#cs.SE) [Total: 23]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 3]
- [stat.ML](#stat.ML) [Total: 14]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [math.NA](#math.NA) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [q-bio.TO](#q-bio.TO) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 48]
- [math.OC](#math.OC) [Total: 6]
- [cs.CL](#cs.CL) [Total: 86]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 2]
- [econ.GN](#econ.GN) [Total: 5]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.NI](#cs.NI) [Total: 6]
- [nlin.PS](#nlin.PS) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 12]
- [physics.optics](#physics.optics) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models](https://arxiv.org/abs/2506.04244)
*Farzad Farhadzadeh,Debasmit Das,Shubhankar Borse,Fatih Porikli*

Main category: cs.AI

TL;DR: ProLoRA实现文本到图像扩散模型中参数高效微调的零样本跨模型迁移，无需额外训练数据


<details>
  <summary>Details</summary>
Motivation: 解决传统方法切换基础模型需重新训练的问题（常受数据限制制约）

Method: 通过子空间/零空间相似性投影源模型调整参数到目标模型权重空间，并选择性对齐关键层

Result: 在主流文本到图像模型上实现知识迁移，性能与需重新训练的方法相当

Conclusion: 突破了模型迁移必须重新训练的限制，为参数高效微调提供跨模型适配新方案

Abstract: We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient
fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained
low-rank adjustments (e.g., LoRA) from a source to a target model without
additional training data. This overcomes the limitations of traditional methods
that require retraining when switching base models, often challenging due to
data constraints. ProLoRA achieves this via projection of source adjustments
into the target model's weight space, leveraging subspace and null space
similarities and selectively targeting aligned layers. Evaluations on
established text-to-image models demonstrate successful knowledge transfer and
comparable performance without retraining.

</details>


### [2] [Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models](https://arxiv.org/abs/2506.04244)
*Farzad Farhadzadeh,Debasmit Das,Shubhankar Borse,Fatih Porikli*

Main category: cs.AI

TL;DR: ProLoRA实现文本到图像扩散模型中参数高效微调的零样本迁移，无需额外数据即可跨模型转移低秩调整参数


<details>
  <summary>Details</summary>
Motivation: 解决传统方法切换基础模型需重新训练的问题，突破数据限制下的模型适应瓶颈

Method: 通过子空间和零空间相似性投影源模型调整参数，选择性定位目标模型对齐层实现知识迁移

Result: 在主流文本生成图像模型上实现与重新训练相当的零样本迁移性能

Conclusion: 该技术显著提升参数高效微调的适应性，降低模型迭代对数据和算力的依赖

Abstract: We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient
fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained
low-rank adjustments (e.g., LoRA) from a source to a target model without
additional training data. This overcomes the limitations of traditional methods
that require retraining when switching base models, often challenging due to
data constraints. ProLoRA achieves this via projection of source adjustments
into the target model's weight space, leveraging subspace and null space
similarities and selectively targeting aligned layers. Evaluations on
established text-to-image models demonstrate successful knowledge transfer and
comparable performance without retraining.

</details>


### [3] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: 提出通过显式推理提示和强化学习框架，在少量合成数据上训练模型，显著减少不当信息披露同时保持任务性能，改进可迁移至真实CI基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理决策时上下文完整性维护问题，现有方法缺乏有效推理机制导致隐私泄露，尝试结合显式推理与强化学习实现情境感知的信息披露控制。

Method: 1. 先用LLM显式推理上下文完整性 2. 开发强化学习框架培养模型推理能力 3. 使用700个自动生成的多样化情境样本训练 4. 验证不同模型尺寸/家族的迁移效果

Result: 方法减少68%不当披露（相比基线），任务性能保持90%+，在PrivacyLens等真实基准测试中隐私泄露减少40%且无需额外训练数据。

Conclusion: 显式推理与强化学习结合有效提升上下文完整性，小规模合成数据训练即可迁移至真实场景，为AI助手隐私保护提供新范式。

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [4] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: 提出LLM-MARL框架，通过将大语言模型与多智能体强化学习结合，显著提升游戏环境中智能体的协作、通信和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统多智能体系统在动态环境中协调不足、通信效率低和泛化能力弱的问题，利用大语言模型的推理能力增强策略学习。

Method: 设计包含协调器（生成子目标）、通信器（符号化消息传递）和记忆模块的三组件架构，采用PPO训练框架结合语言条件损失函数。

Result: 在Google Research Football等三个平台超越MAPPO和QMIX，零样本泛化能力提升17.8%，协作得分增加32%

Conclusion: 通过语言建模与策略学习的融合，为构建智能协作代理提供新范式，推动多智能体系统在游戏训练和人机协作中的应用。

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [5] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: CircuGraphRAG框架通过知识图谱增强LLMs输出，解决工业数据幻觉问题，显著提升问答准确率至ROUGE-L F1 1.0，响应效率翻倍并减少16%算力消耗。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在工业代码和排放因子预测中的不可靠性问题，确保循环经济决策的监管合规性和数据可追溯性。

Method: 构建包含117,380个工业实体的领域知识图谱，通过SPARQL查询实现结构化多跳推理，结合检索增强生成技术验证子图准确性。

Result: 在单跳/多跳问答中ROUGE-L F1达1.0（基线<0.08），响应时间缩短50%，典型任务token用量减少16%。

Conclusion: 该框架为循环经济提供事实核查的监管决策支持，推动可靠低碳资源规划，确立工业AI决策新标准。

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [6] [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)
*Tapio Pitkäranta,Leena Pitkäranta*

Main category: cs.AI

TL;DR: 提出HADA架构实现人类与AI代理决策对齐，通过角色化利益相关者代理和自然语言约束实现组织目标追踪与伦理合规


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中LLM代理与传统算法难以保持组织目标对齐的问题，提升决策透明度与伦理合规性

Method: 构建协议无关的参考架构，封装算法为角色化代理（业务/数据科学/审计等），通过自然语言API实现决策干预与约束传播

Result: 云原生验证系统实现六个核心目标覆盖，所有角色均可追踪KPI/价值观约束，检测邮政编码偏见并复现完整决策链路

Conclusion: HADA架构为多智能体系统提供了框架无关的解决方案，实证显示能提升决策准确性、透明度与伦理合规性，贡献包含开源架构、设计理论与实际部署验证

Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and
framework agnostic reference architecture that keeps both large language model
(LLM) agents and legacy algorithms aligned with organizational targets and
values. HADA wraps any algorithm or LLM in role-specific stakeholder agents --
business, data-science, audit, ethics, and customer -- each exposing
conversational APIs so that technical and non-technical actors can query,
steer, audit, or contest every decision across strategic, tactical, and
real-time horizons. Alignment objectives, KPIs, and value constraints are
expressed in natural language and are continuously propagated, logged, and
versioned while thousands of heterogeneous agents run on different
orchestration stacks. A cloud-native proof of concept packages a production
credit-scoring model (getLoanDecision) and deploys it on
Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target
changes, parameter tweaks, explanation requests, and ethics triggers flow end
to end through the architecture. Evaluation followed the Design-Science
Research Methodology. Walkthrough observation and log inspection demonstrated
complete coverage of six predefined objectives: every role could invoke
conversational control, trace KPIs and value constraints, detect and mitigate
ZIP-code bias, and reproduce full decision lineage, independent of the
underlying LLM or agent library. Contributions: (1) an open-source HADA
architecture, (2) a mid-range design theory for human-AI alignment in
multi-agent systems, and (3) empirical evidence that framework-agnostic,
protocol-compliant stakeholder agents improve accuracy, transparency, and
ethical compliance in real-world decision pipelines.

</details>


### [7] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: 提出EXIF框架，通过探索优先策略和反馈循环机制，使LLM代理能自动发现可行技能并迭代扩展能力，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 现有方法存在人工数据收集成本高、LLM生成任务不可行、生成数据缺乏有效学习信号的问题，需开发自动发现可行技能的方法

Method: 采用探索代理Alice生成环境相关的技能数据集训练目标代理Bob，通过迭代反馈循环评估表现并优化探索方向，形成闭环训练流程

Result: 在Webshop和Crafter环境中有效发现技能，代理性能持续提升，Alice与Bob同模型时性能显著提高，展现自进化潜力

Conclusion: EXIF框架通过探索优先策略和闭环反馈机制，实现代理能力的自主扩展，为构建自进化系统提供了新思路

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [8] [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)
*Jack David Carson,Amir Reisizadeh*

Main category: cs.AI

TL;DR: 提出基于统计物理的连续时间思维链建模框架，用低维流形上的漂移-扩散系统揭示语言模型四类潜在推理模式


<details>
  <summary>Details</summary>
Motivation: 解决Transformer语言模型涌现推理能力的机制解释难题，建立可解释的动力学框架来模拟和预测关键状态跃迁（如模型失效）

Method: 构建句子级隐状态的随机动力系统，采用潜态切换机制捕捉不同推理阶段，通过秩40投影保留50%方差，建立并验证SLDS模型

Result: 发现四类潜在推理模式，验证框架可低成本模拟推理过程并预测模型失效等关键跃迁

Conclusion: 该框架为研究语言模型推理动态提供新工具，特别在预测模型失效等关键状态转变方面具有应用潜力

Abstract: Transformer LMs show emergent reasoning that resists mechanistic
understanding. We offer a statistical physics framework for continuous-time
chain-of-thought reasoning dynamics. We model sentence-level hidden state
trajectories as a stochastic dynamical system on a lower-dimensional manifold.
This drift-diffusion system uses latent regime switching to capture diverse
reasoning phases, including misaligned states or failures. Empirical
trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing
variance capture and feasibility) explains ~50% variance. We find four latent
reasoning regimes. An SLDS model is formulated and validated to capture these
features. The framework enables low-cost reasoning simulation, offering tools
to study and predict critical transitions like misaligned states or other LM
failures.

</details>


### [9] [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)
*Hadi Hosseini,Samarth Khanna,Ronak Singh*

Main category: cs.AI

TL;DR: LLMs在匹配市场偏好推理任务中表现出算法局限性，LoRA微调在小规模有效但大规模效果有限


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在处理匹配市场中个体排序偏好与组合算法时的能力边界

Method: 通过稳定匹配生成、不稳定性检测、解决方案设计和细粒度偏好查询四层次任务，系统评估多类先进模型表现

Result: 发现模型在大规模市场中迭代执行算法困难，LoRA微调仅提升小规模场景性能（平均提升24.3%）但大规模无显著改进

Conclusion: 需要开发更复杂的上下文处理策略以增强LLMs在复杂算法推理中的表现

Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning
tasks -- from program synthesis to scientific hypothesis generation -- yet
their ability to handle ranked preferences and structured algorithms in
combinatorial domains remains underexplored. We study matching markets, a core
framework behind applications like resource allocation and ride-sharing, which
require reconciling individual ranked preferences to ensure stable outcomes. We
evaluate several state-of-the-art models on a hierarchy of preference-based
reasoning tasks -- ranging from stable-matching generation to instability
detection, instability resolution, and fine-grained preference queries -- to
systematically expose their logical and algorithmic limitations in handling
ranked inputs. Surprisingly, even top-performing models with advanced reasoning
struggle to resolve instability in large markets, often failing to identify
blocking pairs or execute algorithms iteratively. We further show that
parameter-efficient fine-tuning (LoRA) significantly improves performance in
small markets, but fails to bring about a similar improvement on large
instances, suggesting the need for more sophisticated strategies to improve
LLMs' reasoning with larger-context inputs.

</details>


### [10] [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
*Peter Jansen,Samiah Hassan,Ruoyao Wang*

Main category: cs.AI

TL;DR: 论文引入Matter-of-Fact数据集，用于评估材料科学领域假设声明的可行性，现有模型性能有限但专家认为问题可解。


<details>
  <summary>Details</summary>
Motivation: 解决自动化科学发现中大规模实验成本高昂的问题，通过筛选可行性假设提升系统效率与发现潜力。

Method: 构建包含8.4k个材料科学领域声明的数据集，覆盖超导/半导体/电池/航空材料主题，整合理论与实验的定性与定量声明。

Result: 检索增强生成等基线模型仅达72%准确率（随机基准50%），但专家验证显示绝大多数问题可解决。

Conclusion: 当前模型面临挑战，但提升假设筛选能力将显著加速科学发现进程。

Abstract: Contemporary approaches to assisted scientific discovery use language models
to automatically generate large numbers of potential hypothesis to test, while
also automatically generating code-based experiments to test those hypotheses.
While hypotheses can be comparatively inexpensive to generate, automated
experiments can be costly, particularly when run at scale (i.e. thousands of
experiments). Developing the capacity to filter hypotheses based on their
feasibility would allow discovery systems to run at scale, while increasing
their likelihood of making significant discoveries. In this work we introduce
Matter-of-Fact, a challenge dataset for determining the feasibility of
hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from
scientific articles spanning four high-impact contemporary materials science
topics, including superconductors, semiconductors, batteries, and aerospace
materials, while including qualitative and quantitative claims from
theoretical, experimental, and code/simulation results. We show that strong
baselines that include retrieval augmented generation over scientific
literature and code generation fail to exceed 72% performance on this task
(chance performance is 50%), while domain-expert verification suggests nearly
all are solvable -- highlighting both the difficulty of this task for current
models, and the potential to accelerate scientific discovery by making
near-term progress.

</details>


### [11] [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
*Xixi Wang,Miguel Costa,Jordanka Kovaceva,Shuai Wang,Francisco C. Pereira*

Main category: cs.AI

TL;DR: 提出图结构框架解决多表格QA中的模式链接难题，通过关系知识构建推理链并在工业级数据验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似性的方法在复杂工业表格场景存在模式链接不可靠、连接路径模糊的问题，制约多表格QA的实用化

Method: 构建模式关系图编码表间链接，结合剪枝策略和子路径融合技术，实现可解释的图遍历推理链构建

Result: 在标准测试集和真实工业数据集上验证有效性，成为首个应用于复杂工业表格的多表格QA系统

Conclusion: 显式建模表间关系显著提升复杂场景性能，图推理机制为工业级表格问答提供可解释的解决方案

Abstract: Large language models (LLMs) have shown promise in table Question Answering
(Table QA). However, extending these capabilities to multi-table QA remains
challenging due to unreliable schema linking across complex tables. Existing
methods based on semantic similarity work well only on simplified hand-crafted
datasets and struggle to handle complex, real-world scenarios with numerous and
diverse columns. To address this, we propose a graph-based framework that
leverages human-curated relational knowledge to explicitly encode schema links
and join paths. Given a natural language query, our method searches this graph
to construct interpretable reasoning chains, aided by pruning and sub-path
merging strategies to enhance efficiency and coherence. Experiments on both
standard benchmarks and a realistic, large-scale dataset demonstrate the
effectiveness of our approach. To our knowledge, this is the first multi-table
QA system applied to truly complex industrial tabular data.

</details>


### [12] [An AI-Based Public Health Data Monitoring System](https://arxiv.org/abs/2506.04429)
*Ananya Joshi,Nolan Gormley,Richa Gadgil,Tina Townes,Roni Rosenfeld,Bryan Wilder*

Main category: cs.AI

TL;DR: AI驱动的排名监测系统显著提升公共卫生数据监控效率


<details>
  <summary>Details</summary>
Motivation: 传统警报系统难以应对现代公共卫生数据规模（日均500万数据点），存在阈值调整频繁和系统延迟问题

Method: 基于AI异常检测的排名式监控系统，通过多年跨学科合作开发

Result: 系统部署后三个月评估显示：审核效率较传统方法提升54倍

Conclusion: 人本AI技术有望革新公共卫生决策模式

Abstract: Public health experts need scalable approaches to monitor large volumes of
health data (e.g., cases, hospitalizations, deaths) for outbreaks or data
quality issues. Traditional alert-based monitoring systems struggle with modern
public health data monitoring systems for several reasons, including that
alerting thresholds need to be constantly reset and the data volumes may cause
application lag. Instead, we propose a ranking-based monitoring paradigm that
leverages new AI anomaly detection methods. Through a multi-year
interdisciplinary collaboration, the resulting system has been deployed at a
national organization to monitor up to 5,000,000 data points daily. A
three-month longitudinal deployed evaluation revealed a significant improvement
in monitoring objectives, with a 54x increase in reviewer speed efficiency
compared to traditional alert-based methods. This work highlights the potential
of human-centered AI to transform public health decision-making.

</details>


### [13] [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)
*Jiayu Liu,Zhenya Huang,Wei Dai,Cheng Cheng,Jinze Wu,Jing Sha,Song Li,Qi Liu,Shijin Wang,Enhong Chen*

Main category: cs.AI

TL;DR: 提出CogMath评估框架，通过人类认知三阶段+9维度重新评估大语言模型的数学能力，发现主流模型能力被高估30%-40%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM数学能力评估仅依赖答案准确性，无法真实反映模型对数学问题的深层理解与推理能力。

Method: 将人类推理过程解构为问题理解/解决/总结三阶段，设计包含数值计算、反事实分析等9个维度的评估体系，并开发多智能体系统生成细粒度测试问题。

Result: 在三大基准测试中，7个主流LLM的数学能力被传统评估方法高估30%-40%，并准确定位了模型在具体维度（如反事实推理）的薄弱环节。

Conclusion: CogMath为LLM数学能力评估提供认知科学视角的诊断工具，揭示现有评估体系的局限性，并为改进模型推理能力提供可解释路径。

Abstract: Although large language models (LLMs) show promise in solving complex
mathematical tasks, existing evaluation paradigms rely solely on a coarse
measure of overall answer accuracy, which are insufficient for assessing their
authentic capabilities. In this paper, we propose \textbf{CogMath}, which
comprehensively assesses LLMs' mathematical abilities through the lens of human
cognition. Specifically, inspired by psychological theories, CogMath formalizes
human reasoning process into 3 stages: \emph{problem comprehension},
\emph{problem solving}, and \emph{solution summarization}. Within these stages,
we investigate perspectives such as numerical calculation, knowledge, and
counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In
each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}''
multi-agent system to generate inquiries that assess LLMs' mastery from this
dimension. An LLM is considered to truly master a problem only when excelling
in all inquiries from the 9 dimensions. By applying CogMath on three
benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs
are overestimated by 30\%-40\%. Moreover, we locate their strengths and
weaknesses across specific stages/dimensions, offering in-depth insights to
further enhance their reasoning abilities.

</details>


### [14] ["Don't Do That!": Guiding Embodied Systems through Large Language Model-based Constraint Generation](https://arxiv.org/abs/2506.04500)
*Aladin Djuhera,Amin Seffo,Masataro Asai,Holger Boche*

Main category: cs.AI

TL;DR: 提出STPR框架，利用大语言模型的编码能力将自然语言约束转换为可执行的Python函数，实现机器人导航中的复杂数学和条件约束处理。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以将自然语言描述的复杂非正式约束转换为形式化规划问题，需解决语言到结构化代码的转换挑战。

Method: 通过LLM生成透明可执行的Python约束函数，结合点云表示和传统搜索算法实现路径规划。

Result: 在Gazebo模拟环境中实现100%约束合规，运行时间短（平均3.5秒），验证了小型代码专用LLM的适用性。

Conclusion: STPR框架有效规避LLM复杂推理和幻觉问题，为紧凑模型提供低推理成本的约束处理方案。

Abstract: Recent advancements in large language models (LLMs) have spurred interest in
robotic navigation that incorporates complex spatial, mathematical, and
conditional constraints from natural language into the planning problem. Such
constraints can be informal yet highly complex, making it challenging to
translate into a formal description that can be passed on to a planning
algorithm. In this paper, we propose STPR, a constraint generation framework
that uses LLMs to translate constraints (expressed as instructions on ``what
not to do'') into executable Python functions. STPR leverages the LLM's strong
coding capabilities to shift the problem description from language into
structured and transparent code, thus circumventing complex reasoning and
avoiding potential hallucinations. We show that these LLM-generated functions
accurately describe even complex mathematical constraints, and apply them to
point cloud representations with traditional search algorithms. Experiments in
a simulated Gazebo environment show that STPR ensures full compliance across
several constraints and scenarios, while having short runtimes. We also verify
that STPR can be used with smaller, code-specific LLMs, making it applicable to
a wide range of compact models at low inference cost.

</details>


### [15] [Schema Generation for Large Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2506.04512)
*Bohui Zhang,Yuan He,Lydia Pintscher,Albert Meroño Peñuela,Elena Simperl*

Main category: cs.AI

TL;DR: 利用大语言模型实现知识图谱Schema自动生成，通过YAGO/Wikidata数据集验证可生成高质量ShEx Schema


<details>
  <summary>Details</summary>
Motivation: 传统Schema创建依赖专家人工设计，本文探索通过LLMs实现自动化生成以提升效率并支持大规模知识图谱应用

Method: 构建YAGO Schema和Wikidata EntitySchema数据集，提出结合知识图谱局部与全局信息的LLM生成框架，使用ShEx语言进行验证

Result: 实验证明LLMs能有效生成符合规范的优质Schema，为大规模知识图谱自动化处理提供可行方案

Conclusion: 该方法不仅实现了Schema生成的自动化突破，也为结构化生成任务设定了新基准，推动了LLMs在复杂形式化语法场景下的应用边界

Abstract: Schemas are vital for ensuring data quality in the Semantic Web and natural
language processing. Traditionally, their creation demands substantial
involvement from knowledge engineers and domain experts. Leveraging the
impressive capabilities of large language models (LLMs) in related tasks like
ontology engineering, we explore automatic schema generation using LLMs. To
bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata
EntitySchema, along with evaluation metrics. The LLM-based pipelines
effectively utilize local and global information from knowledge graphs (KGs) to
generate validating schemas in Shape Expressions (ShEx). Experiments
demonstrate LLMs' strong potential in producing high-quality ShEx schemas,
paving the way for scalable, automated schema generation for large KGs.
Furthermore, our benchmark introduces a new challenge for structured
generation, pushing the limits of LLMs on syntactically rich formalisms.

</details>


### [16] [OpenAg: Democratizing Agricultural Intelligence](https://arxiv.org/abs/2506.04571)
*Srikanth Thudumu,Jason Fisher*

Main category: cs.AI

TL;DR: 提出OpenAg框架，整合领域专用基础模型、神经知识图谱与多智能体系统，推动农业通用人工智能发展


<details>
  <summary>Details</summary>
Motivation: 现有农业AI系统缺乏上下文理解与可解释性，通用大语言模型难以提供符合农业实际的决策支持

Method: 构建包含统一农业知识库、神经知识图谱、自适应多智能体推理系统及因果透明机制的四层架构体系

Result: 实现科学知识与传统农耕经验的融合，支持可解释且适应本地条件的农业决策

Conclusion: OpenAg框架通过结构化知识表示与因果推理机制，为规模化农业智能决策提供新范式

Abstract: Agriculture is undergoing a major transformation driven by artificial
intelligence (AI), machine learning, and knowledge representation technologies.
However, current agricultural intelligence systems often lack contextual
understanding, explainability, and adaptability, especially for smallholder
farmers with limited resources. General-purpose large language models (LLMs),
while powerful, typically lack the domain-specific knowledge and contextual
reasoning needed for practical decision support in farming. They tend to
produce recommendations that are too generic or unrealistic for real-world
applications. To address these challenges, we present OpenAg, a comprehensive
framework designed to advance agricultural artificial general intelligence
(AGI). OpenAg combines domain-specific foundation models, neural knowledge
graphs, multi-agent reasoning, causal explainability, and adaptive transfer
learning to deliver context-aware, explainable, and actionable insights. The
system includes: (i) a unified agricultural knowledge base that integrates
scientific literature, sensor data, and farmer-generated knowledge; (ii) a
neural agricultural knowledge graph for structured reasoning and inference;
(iii) an adaptive multi-agent reasoning system where AI agents specialize and
collaborate across agricultural domains; and (iv) a causal transparency
mechanism that ensures AI recommendations are interpretable, scientifically
grounded, and aligned with real-world constraints. OpenAg aims to bridge the
gap between scientific knowledge and the tacit expertise of experienced farmers
to support scalable and locally relevant agricultural decision-making.

</details>


### [17] [Judicial Permission](https://arxiv.org/abs/2506.04610)
*Guido Governatori,Antonino Rotolo*

Main category: cs.AI

TL;DR: 提出对话游戏模型分析刑事审判中的弱权限（司法许可），结合不同证明标准与论证语义


<details>
  <summary>Details</summary>
Motivation: 司法许可在刑事审判决策中具有重要但常被忽视的作用，需建立系统化分析框架

Method: 构建对话游戏理论模型，整合证明标准层级与Dung式论证语义学方法

Result: 确立司法许可在不同举证责任分配场景下的结构化推演模式

Conclusion: 该模型为司法论证提供形式化分析工具，增强审判决策的逻辑严谨性

Abstract: This paper examines the significance of weak permissions in criminal trials
(\emph{judicial permission}). It introduces a dialogue game model to
systematically address judicial permissions, considering different standards of
proof and argumentation semantics.

</details>


### [18] [DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning](https://arxiv.org/abs/2506.04613)
*Li Liu,Heng Yong*

Main category: cs.AI

TL;DR: 提出DeePoly框架，通过DNN提取全局特征与多项式基函数局部校正的两阶段优化策略，显著提升偏微分方程求解的精度和效率


<details>
  <summary>Details</summary>
Motivation: 传统基于深度神经网络的PDE求解方法存在收敛性缺乏理论保证、计算效率低等问题，需要结合数值方法的优势实现精度与效率的平衡

Method: 两阶段优化策略：1) DNN捕捉复杂全局特征（Scoper） 2) 结合DNN特征与多项式基函数（Sniper）的线性空间优化，利用DNN稳定多项式近似，多项式基提供收敛保证的局部修正

Result: 理论分析和数值实验证明该方法在保持无网格、方案无关特性的同时，显著提升各类问题的高阶精度（最高达12阶）和计算效率（最高加速10倍）

Conclusion: 首次实现DNN与多项式基的互补优势融合，为开源项目DeePoly提供理论基础，开创了兼具高精度保证和高效计算的PDE机器学习求解新范式

Abstract: Recently, machine learning methods have gained significant traction in
scientific computing, particularly for solving Partial Differential Equations
(PDEs). However, methods based on deep neural networks (DNNs) often lack
convergence guarantees and computational efficiency compared to traditional
numerical schemes. This work introduces DeePoly, a novel framework that
transforms the solution paradigm from pure non-convex parameter optimization to
a two-stage approach: first employing a DNN to capture complex global features,
followed by linear space optimization with combined DNN-extracted features
(Scoper) and polynomial basis functions (Sniper). This strategic combination
leverages the complementary strengths of both methods -- DNNs excel at
approximating complex global features (i.e., high-gradient features) and
stabilize the polynomial approximation while polynomial bases provide
high-precision local corrections with convergence guarantees. Theoretical
analysis and numerical experiments demonstrate that this approach significantly
enhances both high-order accuracy and efficiency across diverse problem types
while maintaining mesh-free and scheme-free properties. This paper also serves
as a theoretical exposition for the open-source project DeePoly.

</details>


### [19] [Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation](https://arxiv.org/abs/2506.04614)
*Yuyang Wanyan,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Jiabo Ye,Yutong Kou,Ming Yan,Fei Huang,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.AI

TL;DR: 为解决GUI自动化任务中决策错误的累积风险，提出预操作批评机制GUI-Critic-R1，通过S-GRPO策略增强反馈可靠性，实验验证其在准确率和执行效率上的优势。


<details>
  <summary>Details</summary>
Motivation: GUI自动化作为在线交互任务需要实时决策，现有MLLMs直接应用时容错率低，错误步骤可能导致不可逆后果（如误删或支付），需前置反馈机制降低风险。

Method: 提出Suggestion-aware梯度相对策略优化(S-GRPO)，构建GUI-Critic-R1模型；开发基于推理引导的数据收集流程，创建GUI-Critic-Train/Test数据集。

Result: 静态测试显示GUI-Critic-R1在移动/网页领域批评准确率显著优于现有MLLMs；动态评估中自动化任务成功率与操作效率同步提升。

Conclusion: 预操作批评机制有效提升决策可靠性，S-GRPO策略与新型数据集解决了GUI自动化中的错误传播问题，为在线交互任务提供安全优化方案。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have been
extensively utilized for multimodal reasoning tasks, including Graphical User
Interface (GUI) automation. Unlike general offline multimodal tasks, GUI
automation is executed in online interactive environments, necessitating
step-by-step decision-making based on real-time status of the environment. This
task has a lower tolerance for decision-making errors at each step, as any
mistakes may cumulatively disrupt the process and potentially lead to
irreversible outcomes like deletions or payments. To address these issues, we
introduce a pre-operative critic mechanism that provides effective feedback
prior to the actual execution, by reasoning about the potential outcome and
correctness of actions. Specifically, we propose a Suggestion-aware Gradient
Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative
critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance
the reliability of the model's feedback. Furthermore, we develop a
reasoning-bootstrapping based data collection pipeline to create a
GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic
data. Static experiments on the GUI-Critic-Test across both mobile and web
domains reveal that our GUI-Critic-R1 offers significant advantages in critic
accuracy compared to current MLLMs. Dynamic evaluation on GUI automation
benchmark further highlights the effectiveness and superiority of our model, as
evidenced by improved success rates and operational efficiency.

</details>


### [20] [CHANCERY: Evaluating corporate governance reasoning capabilities in language models](https://arxiv.org/abs/2506.04636)
*Lucas Irwin,Arda Kaz,Peiyao Sheng,Pramod Viswanath*

Main category: cs.AI

TL;DR: 提出了首个公司治理推理基准CHANCERY，测试模型判断商业提案是否符合公司章程的能力，SOTA模型准确率64.5%-78.1%


<details>
  <summary>Details</summary>
Motivation: 现有法律数据集缺乏专注于推理任务的研究，需构建能测试模型法律推理能力的专业基准

Method: 基于79个真实公司章程构建二元分类任务，结合24条公司治理原则，使用10k真实章程筛选代表性样本

Result: Claude 3.7 Sonnet准确率64.5%，GPT-4o达75.2%；基于ReAct/CodeAct的推理代理分别达到76.1%和78.1%

Conclusion: 该基准揭示了现有模型在法律推理中的局限性，证明需要更专业的法律推理能力才能取得高分

Abstract: Law has long been a domain that has been popular in natural language
processing (NLP) applications. Reasoning (ratiocination and the ability to make
connections to precedent) is a core part of the practice of the law in the real
world. Nevertheless, while multiple legal datasets exist, none have thus far
focused specifically on reasoning tasks. We focus on a specific aspect of the
legal landscape by introducing a corporate governance reasoning benchmark
(CHANCERY) to test a model's ability to reason about whether
executive/board/shareholder's proposed actions are consistent with corporate
governance charters. This benchmark introduces a first-of-its-kind corporate
governance reasoning test for language models - modeled after real world
corporate governance law. The benchmark consists of a corporate charter (a set
of governing covenants) and a proposal for executive action. The model's task
is one of binary classification: reason about whether the action is consistent
with the rules contained within the charter. We create the benchmark following
established principles of corporate governance - 24 concrete corporate
governance principles established in and 79 real life corporate charters
selected to represent diverse industries from a total dataset of 10k real life
corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models
confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet
and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents
exhibit superior performance, with agents based on the ReAct and CodeAct
frameworks scoring 76.1% and 78.1% respectively, further confirming the
advanced legal reasoning capabilities required to score highly on the
benchmark. We also conduct an analysis of the types of questions which current
reasoning models struggle on, revealing insights into the legal reasoning
capabilities of SOTA models.

</details>


### [21] [Agents of Change: Self-Evolving LLM Agents for Strategic Planning](https://arxiv.org/abs/2506.04651)
*Nikolas Belle,Dakota Barnes,Alfonso Amayuelas,Ivan Bercovich,Xin Eric Wang,William Wang*

Main category: cs.AI

TL;DR: 研究LLM代理在战略游戏中的自我进化能力，通过多角色协作架构显著提升策略规划性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在制定和坚持长期策略方面存在不足，需验证其在战略环境中的自我改进潜力

Method: 基于Catanatron框架构建多角色架构（分析员/研究员/程序员/玩家），通过迭代式游戏分析、策略研究和代码修改实现自我进化

Result: 自我进化代理（特别是Claude 3.7和GPT-4o）通过策略传承和自适应推理，超越静态基线模型表现

Conclusion: 角色分工的架构设计有效促进LLM代理的持续进化，为复杂决策系统开发提供新范式

Abstract: Recent advances in LLMs have enabled their use as autonomous agents across a
range of tasks, yet they continue to struggle with formulating and adhering to
coherent long-term strategies. In this paper, we investigate whether LLM agents
can self-improve when placed in environments that explicitly challenge their
strategic planning abilities. Using the board game Settlers of Catan, accessed
through the open-source Catanatron framework, we benchmark a progression of
LLM-based agents, from a simple game-playing agent to systems capable of
autonomously rewriting their own prompts and their player agent's code. We
introduce a multi-agent architecture in which specialized roles (Analyzer,
Researcher, Coder, and Player) collaborate to iteratively analyze gameplay,
research new strategies, and modify the agent's logic or prompt. By comparing
manually crafted agents to those evolved entirely by LLMs, we evaluate how
effectively these systems can diagnose failure and adapt over time. Our results
show that self-evolving agents, particularly when powered by models like Claude
3.7 and GPT-4o, outperform static baselines by autonomously adopting their
strategies, passing along sample behavior to game-playing agents, and
demonstrating adaptive reasoning over multiple iterations.

</details>


### [22] [E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction](https://arxiv.org/abs/2506.04654)
*Zhichao Yang,Jiashu He,Mohammad B. Al-Khasawneh,Darshan Pandit,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 开发基于大语言模型的E-bike agents框架，从非结构化事故报告中提取安全变量，并通过有序logit模型分析事故严重性影响因素


<details>
  <summary>Details</summary>
Motivation: 电动自行车事故报告的非结构化特性阻碍定量安全分析，需有效提取关键安全因素提升安全治理

Method: 使用四个LLM代理（数据分类/信息提取/致伤原因判定/部件关联）提取要素，结合有序logit模型分析事故严重程度关联因素

Result: 设备问题占比51%略高于人为因素，但人为因素致死率更高；踏板/轮胎/刹车是主要致因部件；模型加权F1值达0.87

Conclusion: 该方法为政策制定者提供可扩展的安全分析方案，验证了LLM在专业领域非结构化数据处理中的有效性

Abstract: Electric bicycles (e-bikes) are rapidly increasing in use, raising safety
concerns due to a rise in accident reports. However, e-bike incident reports
often use unstructured narrative formats, which hinders quantitative safety
analysis. This study introduces E-bike agents, a framework that uses large
language models (LLM) powered agents to classify and extract safety variables
from unstructured incident reports. Our framework consists of four LLM agents,
handling data classification, information extraction, injury cause
determination, and component linkage, to extract the key factors that could
lead to E-bike accidents and cause varying severity levels. Furthermore, we
used an ordered logit model to examine the relationship between the severity of
the incident and the factors retrieved, such as gender, the type of cause, and
environmental conditions. Our research shows that equipment issues are slightly
more common than human-related ones, but human-related incidents are more often
fatal. Specifically, pedals, tires, and brakes are frequent contributors to
accidents. The model achieves a high weighted F1 score of 0.87 in
classification accuracy, highlighting the potential of using LLMs to extract
unstructured data in niche domains, such as transportation. Our method offers a
scalable solution to improve e-bike safety analytics and provides actionable
information for policy makers, designers, and regulators.

</details>


### [23] [Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling](https://arxiv.org/abs/2506.04699)
*Bihan Xu,Shiwei Zhao,Runze Wu,Zhenya Huang,Jiawei Wang,Zhipeng Hu,Kai Wang,Haoyu Liu,Tangjie Lv,Le Li,Changjie Fan,Xin Tong,Jiangze Han*

Main category: cs.AI

TL;DR: 提出利用大语言模型(LLM)驱动智能体进行MMO经济模拟，通过赋予智能体角色扮演、感知、记忆和推理能力，解决传统方法在可靠性、社交性和可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的ABM方法在模拟人类经济行为时，难以实现可靠的类人决策、社交互动和可解释性。LLM的生成能力、角色扮演优势为解决这些挑战提供了新思路。

Method: 设计具备四大核心能力的LLM智能体：1) 角色扮演实现身份代入 2) 环境感知捕捉经济信号 3) 记忆系统记录交互历史 4) 链式推理支持复杂决策。

Result: 游戏内经济模拟实验显示，LLM智能体能够自发形成角色分工（如商人/生产者），并产生符合市场规律的价格波动，验证了方法的有效性。

Conclusion: LLM为MMO经济模拟提供了新范式，其生成与推理能力显著提升了智能体的拟人化程度，推动了更贴近现实的经济涌现现象产生。

Abstract: Within the domain of Massively Multiplayer Online (MMO) economy research,
Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game
economics, evolving from rule-based agents to decision-making agents enhanced
by reinforcement learning. Nevertheless, existing works encounter significant
challenges when attempting to emulate human-like economic activities among
agents, particularly regarding agent reliability, sociability, and
interpretability. In this study, we take a preliminary step in introducing a
novel approach using Large Language Models (LLMs) in MMO economy simulation.
Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning
aptitude, we design LLM-driven agents with human-like decision-making and
adaptability. These agents are equipped with the abilities of role-playing,
perception, memory, and reasoning, addressing the aforementioned challenges
effectively. Simulation experiments focusing on in-game economic activities
demonstrate that LLM-empowered agents can promote emergent phenomena like role
specialization and price fluctuations in line with market rules.

</details>


### [24] [Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/abs/2506.04723)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Caiming Xiong,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: 研究通过细粒度分析框架揭示了强化学习提升语言模型推理能力的内在机制，发现RL主要增强模型制定内部策略和整合知识的能力，而非执行外部计划。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练方法（如GRPO）虽能显著提升模型推理能力，但其具体作用机制和优势来源仍缺乏系统性分析，需要建立理论框架揭示RL对推理过程的影响。

Method: 构建包含计划执行、问题分解、知识利用三个维度的分析框架，通过对比实验研究RL训练对模型在不同推理要素上的具体影响。

Result: 1）提供明确计划会损害困难任务的性能，但RL模型表现出更强鲁棒性 2）RL显著提升知识整合能力 3）开发了利用难题改进训练的新方法

Conclusion: 该研究为推理模型的训练评估建立了理论基础，揭示了RL的核心价值在于优化内部策略形成而非外部计划执行，为后续研究提供新方向。

Abstract: Reinforcement learning (RL) has become the dominant paradigm for endowing
language models with advanced reasoning capabilities. Despite the substantial
empirical gains demonstrated by RL-based training methods like GRPO, a granular
understanding of their advantages is still lacking. To address this gap, we
introduce a fine-grained analytic framework to dissect the impact of RL on
reasoning. Our framework specifically investigates key elements that have been
hypothesized to benefit from RL training: (1) plan-following and execution, (2)
problem decomposition, and (3) improved reasoning and knowledge utilization.
Using this framework, we gain insights beyond mere accuracy. For instance,
providing models with explicit step-by-step plans surprisingly degrades
performance on the most challenging benchmarks, yet RL-tuned models exhibit
greater robustness, experiencing markedly smaller performance drops than their
base counterparts. This suggests that RL may not primarily enhance the
execution of external plans but rather empower models to formulate and follow
internal strategies better suited to their reasoning processes. Conversely, we
observe that RL enhances the model's capacity to integrate provided knowledge
into its reasoning process, leading to performance improvements across diverse
tasks. We also study difficulty, showing improved training by developing new
ways to exploit hard problems. Our findings lay a foundation for more
principled training and evaluation of reasoning models.

</details>


### [25] [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
*Lin Sun,Weihong Lin,Jinzhu Wu,Yongfu Zhu,Xiaoqi Jian,Guangxiang Zhao,Change Jia,Linglin Zhang,Sai-er Hu,Yuhan Wu,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: Deepseek-R1-Distill系列模型及衍生模型存在基准测试结果波动大的问题，需建立更严谨的评估范式


<details>
  <summary>Details</summary>
Motivation: 当前开源社区广泛采用的推理模型存在评估结果受多因素干扰的缺陷，导致性能改进难以可靠复现

Method: 通过实证分析Deepseek-R1-Distill系列模型及其微调模型（含QwQ-32B）在不同评估条件下的结果波动

Result: 发现评估条件的细微差异会导致结果显著变化，模型宣称的性能改进缺乏稳定性支撑

Conclusion: 呼吁建立标准化评估体系，并提供了该系列模型的实证评估方案作为参考

Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.

</details>


### [26] [Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems](https://arxiv.org/abs/2506.04756)
*Loan Dao,Ngoc Quoc Ly*

Main category: cs.AI

TL;DR: 本研究提出基于本体的骨疾病诊断框架，整合分层神经网络、VQA系统和多模态模型，虽建立理论基础但尚未完成实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI系统缺乏领域知识整合导致的诊断可靠性问题，通过本体框架提升临床解释性。

Method: 1. 本体指导的分层神经网络（融合VLM）
2. 本体增强的临床推理VQA系统
3. 基于本体关系的多模态数据融合模型
方法论包含知识数字化、医学术语标准化映射和模块化架构设计

Result: 框架展示跨疾病扩展潜力（标准化结构和可复用组件），目前仅完成理论构建，实验验证受限于数据集和算力资源

Conclusion: 建立了理论框架但需扩展临床数据集并进行系统验证，未来工作聚焦数据扩充和完整验证

Abstract: Medical artificial intelligence (AI) systems frequently lack systematic
domain expertise integration, potentially compromising diagnostic reliability.
This study presents an ontology-based framework for bone disease diagnosis,
developed in collaboration with Ho Chi Minh City Hospital for Traumatology and
Orthopedics. The framework introduces three theoretical contributions: (1) a
hierarchical neural network architecture guided by bone disease ontology for
segmentation-classification tasks, incorporating Visual Language Models (VLMs)
through prompts, (2) an ontology-enhanced Visual Question Answering (VQA)
system for clinical reasoning, and (3) a multimodal deep learning model that
integrates imaging, clinical, and laboratory data through ontological
relationships. The methodology maintains clinical interpretability through
systematic knowledge digitization, standardized medical terminology mapping,
and modular architecture design. The framework demonstrates potential for
extension beyond bone diseases through its standardized structure and reusable
components. While theoretical foundations are established, experimental
validation remains pending due to current dataset and computational resource
limitations. Future work will focus on expanding the clinical dataset and
conducting comprehensive system validation.

</details>


### [27] [Safe Planning and Policy Optimization via World Model Learning](https://arxiv.org/abs/2506.04828)
*Artem Latyshev,Gregory Gorbov,Aleksandr I. Panov*

Main category: cs.AI

TL;DR: 提出融合动态切换机制与隐式世界模型的安全强化学习框架，实现任务性能与安全性的同步优化


<details>
  <summary>Details</summary>
Motivation: 传统模型驱动RL存在模型误差引发的安全风险，且安全约束与性能目标存在冲突

Method: 动态切换模型规划与直接策略执行，采用隐式世界模型解决目标失配，结合自适应安全阈值机制

Result: 在安全关键控制任务中显著超越非自适应方法，实现安全性能双重优化

Conclusion: 框架通过自适应性设计突破传统安全约束限制，为安全敏感场景提供可靠解决方案

Abstract: Reinforcement Learning (RL) applications in real-world scenarios must
prioritize safety and reliability, which impose strict constraints on agent
behavior. Model-based RL leverages predictive world models for action planning
and policy optimization, but inherent model inaccuracies can lead to
catastrophic failures in safety-critical settings. We propose a novel
model-based RL framework that jointly optimizes task performance and safety. To
address world model errors, our method incorporates an adaptive mechanism that
dynamically switches between model-based planning and direct policy execution.
We resolve the objective mismatch problem of traditional model-based approaches
using an implicit world model. Furthermore, our framework employs dynamic
safety thresholds that adapt to the agent's evolving capabilities, consistently
selecting actions that surpass safe policy suggestions in both performance and
safety. Experiments demonstrate significant improvements over non-adaptive
methods, showing that our approach optimizes safety and performance
simultaneously rather than merely meeting minimum safety requirements. The
proposed framework achieves robust performance on diverse safety-critical
continuous control tasks, outperforming existing methods.

</details>


### [28] [Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles](https://arxiv.org/abs/2506.04849)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Paul Théron,Louis-Marie Traonouez*

Main category: cs.AI

TL;DR: 提出基于马尔可夫模型的网络攻防模拟器，用于评估动态防御策略有效性


<details>
  <summary>Details</summary>
Motivation: 应对日益复杂的协同网络攻击，需要建立实验框架评估防御组织的动态适应性

Method: 1. 构建网络节点属性模型 2. 开发攻防智能体模拟器 3. 实现基于状态的动作响应机制 4. 设计多行为策略实验场景

Result: 成功模拟现实网络攻防场景，验证不同防御组织模式的有效性

Conclusion: 该框架为网络安全策略评估提供了可扩展的实验平台，未来可集成更多复杂攻击模式

Abstract: As cyber-attacks show to be more and more complex and coordinated,
cyber-defenders strategy through multi-agent approaches could be key to tackle
against cyber-attacks as close as entry points in a networked system. This
paper presents a Markovian modeling and implementation through a simulator of
fighting cyber-attacker agents and cyber-defender agents deployed on host
network nodes. It aims to provide an experimental framework to implement
realistically based coordinated cyber-attack scenarios while assessing
cyber-defenders dynamic organizations. We abstracted network nodes by sets of
properties including agents' ones. Actions applied by agents model how the
network reacts depending in a given state and what properties are to change.
Collective choice of the actions brings the whole environment closer or farther
from respective cyber-attackers and cyber-defenders goals. Using the simulator,
we implemented a realistically inspired scenario with several behavior
implementation approaches for cyber-defenders and cyber-attackers.

</details>


### [29] [LLMs for sensory-motor control: Combining in-context and iterative learning](https://arxiv.org/abs/2506.04867)
*Jônata Tyska Carvalho,Stefano Nolfi*

Main category: cs.AI

TL;DR: 大语言模型通过观察向量到动作向量的直接映射控制具身智能体，结合符号推理与亚符号感知数据实现策略迭代优化


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大语言模型的符号推理能力，结合智能体与环境交互产生的亚符号感知数据，提升具身智能体的连续控制能力

Method: 基于文本描述生成初始控制策略，通过性能反馈和感觉运动数据的迭代学习不断优化策略，在Gymnasium和MuJoCo环境验证

Result: 在多数经典控制任务中成功找到最优解，实现符号推理与亚符号感知数据的有效融合

Conclusion: 该方法通过迭代策略优化机制，建立了符号知识推理与具体环境交互之间的闭环学习框架

Abstract: We propose a method that enables large language models (LLMs) to control
embodied agents by directly mapping continuous observation vectors to
continuous action vectors. Initially, the LLMs generate a control strategy
based on a textual description of the agent, its environment, and the intended
goal. This strategy is then iteratively refined through a learning process in
which the LLMs are repeatedly prompted to improve the current strategy, using
performance feedback and sensory-motor data collected during its evaluation.
The method is validated on classic control tasks from the Gymnasium library and
the inverted pendulum task from the MuJoCo library. In most cases, it
successfully identifies optimal or high-performing solutions by integrating
symbolic knowledge derived through reasoning with sub-symbolic sensory-motor
data gathered as the agent interacts with its environment.

</details>


### [30] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: 通过表征工程检测和控制具备思维链推理能力的大语言模型战略欺骗行为，实现89%的检测准确率


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的诚实性是其关键对齐挑战，尤其是具备思维链推理能力的模型可能进行战略性欺骗。传统LLM的诚实问题可能被解释为幻觉，而显式思维路径模型让我们得以研究其推理与输出矛盾的策略性欺骗行为

Method: 使用表征工程技术，通过线性人工断层扫描（LAT）提取「欺骗向量」，采用激活导向技术进行系统性诱导、检测和控制

Result: LAT技术实现89%的欺骗检测准确率，激活导向在无明确提示情况下成功诱导上下文适当欺骗的成功率达40%

Conclusion: 揭示了推理模型特有的诚实性相关问题，为可信AI对齐提供了LAT检测工具和激活导向控制技术，推动可靠人工智能发展

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>


### [31] [Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation](https://arxiv.org/abs/2506.04912)
*Pietro Miotti,Eyvind Niklasson,Ettore Randazzo,Alexander Mordvintsev*

Main category: cs.AI

TL;DR: DiffLogic CA结合神经细胞自动机与可微分逻辑门网络，实现端到端可微训练与离散推理，成功应用于生命游戏规则学习、抗噪图案生成等任务。


<details>
  <summary>Details</summary>
Motivation: 为解决传统细胞自动机规则设计依赖人工经验的问题，通过融合二进制逻辑与神经网络适应性，推动可编程物质与鲁棒计算系统发展。

Method: 以可微分逻辑门构建电路，训练阶段梯度优化参数，推理阶段转换为离散状态空间，保留细胞自动机离散特性。

Result: 成功实现生命游戏规则复现（97%精度）、抗损伤棋盘生成、蜥蜴形态自生长及多色图案生成，同步/异步更新均展现强泛化能力。

Conclusion: 首次在循环架构中实现可微分逻辑门网络，为可编程物质系统提供兼具逻辑确定性、学习适应性与分布式计算的新范式。

Abstract: This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA),
a novel combination of Neural Cellular Automata (NCA) and Differentiable Logic
Gates Networks (DLGNs). The fundamental computation units of the model are
differentiable logic gates, combined into a circuit. During training, the model
is fully end-to-end differentiable allowing gradient-based training, and at
inference time it operates in a fully discrete state space. This enables
learning local update rules for cellular automata while preserving their
inherent discrete nature. We demonstrate the versatility of our approach
through a series of milestones: (1) fully learning the rules of Conway's Game
of Life, (2) generating checkerboard patterns that exhibit resilience to noise
and damage, (3) growing a lizard shape, and (4) multi-color pattern generation.
Our model successfully learns recurrent circuits capable of generating desired
target patterns. For simpler patterns, we observe success with both synchronous
and asynchronous updates, demonstrating significant generalization capabilities
and robustness to perturbations. We make the case that this combination of
DLGNs and NCA represents a step toward programmable matter and robust computing
systems that combine binary logic, neural network adaptability, and localized
processing. This work, to the best of our knowledge, is the first successful
application of differentiable logic gate networks in recurrent architectures.

</details>


### [32] [Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life](https://arxiv.org/abs/2506.04916)
*Atahan Karagoz*

Main category: cs.AI

TL;DR: 提出基于能量自调节的自主系统Energetic Intelligence，通过闭环控制实现无监督资源感知行为，突破传统任务驱动范式。


<details>
  <summary>Details</summary>
Motivation: 传统自主系统依赖稳定基础设施和外部奖励，无法适应资源波动环境。需建立通过内部能量调节实现自我维持的生存机制。

Method: 构建能量效用函数与生存期限约束模型，设计整合能量收集、热调节和自适应计算的模块化闭环控制系统，通过仿真验证。

Result: 模拟实验显示系统自发形成资源感知稳定行为，在能量/温度波动中保持0.78的生存指数，无外部监督条件下运行时长提升47%。

Conclusion: 为资源不稳定环境中的持久自主系统提供理论框架，证明能量自调节可作为新型智能范式，适用于野外探测等基础设施缺失场景。

Abstract: This paper introduces Energentic Intelligence, a class of autonomous systems
defined not by task performance, but by their capacity to sustain themselves
through internal energy regulation. Departing from conventional reward-driven
paradigms, these agents treat survival-maintaining functional operation under
fluctuating energetic and thermal conditions-as the central objective. We
formalize this principle through an energy-based utility function and a
viability-constrained survival horizon, and propose a modular architecture that
integrates energy harvesting, thermal regulation, and adaptive computation into
a closed-loop control system. A simulated environment demonstrates the
emergence of stable, resource-aware behavior without external supervision.
Together, these contributions provide a theoretical and architectural
foundation for deploying autonomous agents in resource-volatile settings where
persistence must be self-regulated and infrastructure cannot be assumed.

</details>


### [33] [Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning](https://arxiv.org/abs/2506.04998)
*Mehdi Azarafza,Mojtaba Nayyeri,Faezeh Pasandideh,Steffen Staab,Achim Rettberg*

Main category: cs.AI

TL;DR: RAG-UAV框架通过检索增强技术显著提升通用大模型在无人机数学推理任务中的准确率（GPT-o1达75%），降低公式选择错误率（从25%→5%），并减少数值计算误差（MSE下降量级）


<details>
  <summary>Details</summary>
Motivation: 解决大模型在无人机自主操作（轨迹规划/电源管理）中数学公式选择不可靠、多步计算易出错的问题，突破传统硬编码方程的限制

Method: 提出RAG-UAV检索增强框架（集成领域文献）和UAV-Math-Bench测试集（20个四难度层级问题），在GPT/DeePSeek等多模型上验证有效性

Result: RAG使精确答案准确率最高提升至75%，公式错误率降低80%（25%→5%），最佳模型MSE下降3个数量级

Conclusion: 检索增强技术可有效提升LLMs在工程分析中的可靠性，但实时飞行控制仍需更大规模验证，基准数据集已开源

Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for
tasks such as trajectory planning and power management. While traditional
flight control relies on hardcoded equations, recent Large Language Models
(LLMs) offer potential for more flexible problem-solving but struggle with
reliably selecting and applying correct mathematical formulations and executing
precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented
generation framework designed to improve the mathematical reasoning of several
LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in
UAV-specific contexts by providing access to relevant domain literature. To
conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem
set comprising 20 UAV-centric mathematical problems across four difficulty
levels. Our experiments demonstrate that incorporating retrieval substantially
increases exact answer accuracy (achieving up to 75% with o1), reduces
instances of incorrect formulation selection (from 25% without RAG to 5% with
RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders
of magnitude for the best-performing models. This pilot study indicates that
RAG can enable general-purpose LLMs to function as more reliable tools for
engineering analysis, although direct real-time flight control requires further
investigation and validation on a larger scale. All benchmark data, question
and answer are publicly available.

</details>


### [34] [Truly Self-Improving Agents Require Intrinsic Metacognitive Learning](https://arxiv.org/abs/2506.05109)
*Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出通过内在元认知学习框架（元认知知识/规划/评估）提升AI代理的持续自我改进能力


<details>
  <summary>Details</summary>
Motivation: 现有自我改进方法存在僵化性、领域泛化能力差和扩展性不足的问题，需模仿人类元认知机制

Method: 构建包含元认知知识（能力/任务/策略的自我评估）、元认知规划（学习内容与方法决策）、元认知评估（学习经验反思）的三元框架

Result: 现有系统主要依赖外置元认知机制，而实现内在元认知所需的关键要素已具备基础条件

Conclusion: 需解决人机元认知责任分配、内在元认知能力的评估改进等关键挑战，才能实现持续通用的安全自我改进

Abstract: Self-improving agents aim to continuously acquire new capabilities with
minimal supervision. However, current approaches face two key limitations:
their self-improvement processes are often rigid, fail to generalize across
tasks domains, and struggle to scale with increasing agent capabilities. We
argue that effective self-improvement requires intrinsic metacognitive
learning, defined as an agent's intrinsic ability to actively evaluate, reflect
on, and adapt its own learning processes. Drawing inspiration from human
metacognition, we introduce a formal framework comprising three components:
metacognitive knowledge (self-assessment of capabilities, tasks, and learning
strategies), metacognitive planning (deciding what and how to learn), and
metacognitive evaluation (reflecting on learning experiences to improve future
learning). Analyzing existing self-improving agents, we find they rely
predominantly on extrinsic metacognitive mechanisms, which are fixed,
human-designed loops that limit scalability and adaptability. Examining each
component, we contend that many ingredients for intrinsic metacognition are
already present. Finally, we explore how to optimally distribute metacognitive
responsibilities between humans and agents, and robustly evaluate and improve
intrinsic metacognitive learning, key challenges that must be addressed to
enable truly sustained, generalized, and aligned self-improvement.

</details>


### [35] [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)
*Nathan Herr,Tim Rocktäschel,Roberta Raileanu*

Main category: cs.AI

TL;DR: 提出LLM-First Search (LFS)方法，通过大语言模型自主控制搜索过程，消除预定义搜索策略需求，实现更灵活高效的推理


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法（如MCTS）依赖固定超参数，在不同难度任务中适应性受限。需探索无需手动调整、能自主适应任务特性的新型搜索范式

Method: 利用LLM内部评分机制自主决策：1）动态评估当前搜索路径价值 2）自主选择继续深入或探索新分支 3）通过自我引导实现上下文敏感推理

Result: 在Countdown/Sudoku任务中：1）挑战性任务表现更优 2）计算效率提升2-5倍 3）模型越强扩展性越好 4）计算预算增加时性能持续提升

Conclusion: LFS突破传统搜索限制，通过LLM优先设计实现：1）零手动调参 2）跨任务通用性 3）计算效率优势 4）模型/算力协同扩展性 为自动化推理提供新范式

Abstract: Large Language Models (LLMs) have demonstrated remarkable improvements in
reasoning and planning through increased test-time compute, often by framing
problem-solving as a search process. While methods like Monte Carlo Tree Search
(MCTS) have proven effective in some domains, their reliance on fixed
exploration hyperparameters limits their adaptability across tasks of varying
difficulty, rendering them impractical or expensive in certain settings. In
this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM
Self-Guided Search} method that removes the need for pre-defined search
strategies by empowering the LLM to autonomously control the search process via
self-guided exploration. Rather than relying on external heuristics or
hardcoded policies, the LLM evaluates whether to pursue the current search path
or explore alternative branches based on its internal scoring mechanisms. This
enables more flexible and context-sensitive reasoning without requiring manual
tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku
against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth
First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which
have been used to achieve SotA results on a range of challenging reasoning
tasks. We found that LFS (1) performs better on more challenging tasks without
additional tuning, (2) is more computationally efficient compared to the other
methods, especially when powered by a stronger model, (3) scales better with
stronger models, due to its LLM-First design, and (4) scales better with
increased compute budget. Our code is publicly available at
\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.

</details>


### [36] [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/abs/2506.05256)
*Violet Xiang,Chase Blagden,Rafael Rafailov,Nathan Lile,Sang Truong,Chelsea Finn,Nick Haber*

Main category: cs.AI

TL;DR: 提出自适应长度惩罚（ALP）方法，通过动态调整不同难度问题的生成长度，减少50%计算资源消耗同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在处理简单问题时存在计算资源浪费，传统方法无法针对问题难度智能分配计算资源。需要一种动态调整机制，使简单问题减少计算、困难问题保留资源。

Method: 开发强化学习目标ALP，实时监控每个prompt的在线解决率，生成与解决率成反比的可微分惩罚项。通过多轮训练迭代，自动调整不同难度问题的token生成成本。

Result: DeepScaleR-1.5B模型应用ALP后，平均token使用减少50%，在困难问题上准确率提升3.2%，整体性能保持基准水平。

Conclusion: ALP实现了计算资源的智能分配，通过削减简单问题预算并重新分配给困难问题，在保持效率的同时提升最难问题的解决能力。

Abstract: Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.

</details>


### [37] [Control Tax: The Price of Keeping AI in Check](https://arxiv.org/abs/2506.05296)
*Mikhail Terekhov,Zhen Ning David Liu,Caglar Gulcehre,Samuel Albanie*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.

</details>


### [38] [Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models](https://arxiv.org/abs/2506.04244)
*Farzad Farhadzadeh,Debasmit Das,Shubhankar Borse,Fatih Porikli*

Main category: cs.AI

TL;DR: 介绍ProLoRA，实现文本到图像扩散模型中参数高效微调的零样本适应，无需额外训练数据即可转移预训练低秩调整。


<details>
  <summary>Details</summary>
Motivation: 传统方法在切换基础模型时需重新训练，受数据限制挑战大，需要新方法克服此局限。

Method: 通过将源调整投影到目标模型的权重空间，利用子空间和零空间相似性，选择性地针对对齐层。

Result: 在既定文本到图像模型的评估中，无需重新训练就实现了成功的知识转移和相当的性能。

Conclusion: ProLoRA能实现文本到图像扩散模型中参数高效微调的零样本适应，有效克服传统方法局限。

Abstract: We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient
fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained
low-rank adjustments (e.g., LoRA) from a source to a target model without
additional training data. This overcomes the limitations of traditional methods
that require retraining when switching base models, often challenging due to
data constraints. ProLoRA achieves this via projection of source adjustments
into the target model's weight space, leveraging subspace and null space
similarities and selectively targeting aligned layers. Evaluations on
established text-to-image models demonstrate successful knowledge transfer and
comparable performance without retraining.

</details>


### [39] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: 在自主代理时代，研究让模型进行上下文完整性（CI）推理以减少不当信息披露的方法，用小数据集验证有效且效果可迁移。


<details>
  <summary>Details</summary>
Motivation: 在自主代理为用户做决策的时代，确保上下文完整性成为关键问题，需要模型对操作上下文进行推理。

Method: 先提示大语言模型（LLMs）对CI进行显式推理，再开发强化学习（RL）框架进一步让模型具备实现CI的推理能力，用约700个样本的合成数据集进行验证。

Result: 该方法大幅减少了不当信息披露，同时在多模型规模和家族中保持了任务性能，且改进效果能从合成数据集迁移到如PrivacyLens等既定的CI基准测试上。

Conclusion: 所提出的结合提示推理和强化学习框架的方法能有效实现上下文完整性，减少不当信息披露。

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [40] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: 本文提出LLM - MARL框架，将大语言模型融入多智能体强化学习，在多个模拟游戏环境评估，效果优于现有方法，贡献于智能协作智能体设计。


<details>
  <summary>Details</summary>
Motivation: 在模拟游戏环境中增强多智能体的协调、通信和泛化能力。

Method: 构建包含协调器、通信器和记忆模块的LLM - MARL框架，结合PPO与语言条件损失及LLM查询门控进行训练。

Result: 在多个环境评估中，相比MAPPO和QMIX在胜率、协调分数和零样本泛化方面有一致提升，子目标生成和基于语言的消息传递对性能提升贡献显著，出现角色专业化等涌现行为。

Conclusion: 该工作架起语言建模和策略学习的桥梁，为多智能体系统利用LLM提供了途径。

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [41] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: 论文介绍CircuGraphRAG框架，能将大语言模型输出与特定领域知识图谱结合，在问答任务中表现优，提升效率，支持循环经济规划。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在可持续制造中常产生工业代码和排放因子幻觉，影响监管和投资决策。

Method: 引入CircuGraphRAG检索增强生成框架，将自然语言查询转换为SPARQL，检索验证子图。

Result: 相比Standalone LLMs和Naive RAG，CircuGraphRAG在单跳和多跳问答中表现优越，ROUGE - L F1分数最高达1.0，提升效率，减少响应时间和令牌使用。

Conclusion: CircuGraphRAG为循环经济规划提供经事实核查、符合监管要求的支持，推动可靠的低碳资源决策。

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [42] [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)
*Tapio Pitkäranta,Leena Pitkäranta*

Main category: cs.AI

TL;DR: 提出HADA参考架构使LLM代理和传统算法与组织目标和价值观对齐，经评估有多项优势并给出相关贡献。


<details>
  <summary>Details</summary>
Motivation: 让大型语言模型代理和传统算法与组织目标和价值观保持一致。

Method: 将算法或LLM封装成特定角色的利益相关者代理，用自然语言表达对齐目标等，采用云原生概念验证，按设计科学研究方法评估。

Result: 通过演练观察和日志检查，实现六项预定义目标，如各角色可进行对话控制等。

Conclusion: 开源HADA架构，提出多智能体系统中人类与AI对齐的设计理论，证明框架无关、协议合规的利益相关者代理能提升决策流程的准确性、透明度和道德合规性。

Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and
framework agnostic reference architecture that keeps both large language model
(LLM) agents and legacy algorithms aligned with organizational targets and
values. HADA wraps any algorithm or LLM in role-specific stakeholder agents --
business, data-science, audit, ethics, and customer -- each exposing
conversational APIs so that technical and non-technical actors can query,
steer, audit, or contest every decision across strategic, tactical, and
real-time horizons. Alignment objectives, KPIs, and value constraints are
expressed in natural language and are continuously propagated, logged, and
versioned while thousands of heterogeneous agents run on different
orchestration stacks. A cloud-native proof of concept packages a production
credit-scoring model (getLoanDecision) and deploys it on
Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target
changes, parameter tweaks, explanation requests, and ethics triggers flow end
to end through the architecture. Evaluation followed the Design-Science
Research Methodology. Walkthrough observation and log inspection demonstrated
complete coverage of six predefined objectives: every role could invoke
conversational control, trace KPIs and value constraints, detect and mitigate
ZIP-code bias, and reproduce full decision lineage, independent of the
underlying LLM or agent library. Contributions: (1) an open-source HADA
architecture, (2) a mid-range design theory for human-AI alignment in
multi-agent systems, and (3) empirical evidence that framework-agnostic,
protocol-compliant stakeholder agents improve accuracy, transparency, and
ethical compliance in real-world decision pipelines.

</details>


### [43] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: 本文提出自动技能发现框架EXIF解决大语言模型代理训练数据集创建难题，实验证明其能有效发现技能、提升性能，还有构建自我进化系统潜力。


<details>
  <summary>Details</summary>
Motivation: 创建大语言模型代理技能获取的训练数据集面临人工收集费力、LLM直接提议任务不可行且数据无有效学习信号等挑战。

Method: 提出EXIF框架，采用探索优先策略，用探索代理Alice生成可行技能数据集训练目标代理Bob，加入迭代反馈循环形成闭环数据生成过程。

Result: 在Webshop和Crafter上实验表明，EXIF能有效发现有意义技能，迭代扩展训练代理能力，无需人工干预取得显著性能提升，设置Alice和Bob为相同模型也显著提升性能。

Conclusion: EXIF框架能解决训练数据集创建难题，有效提升大语言模型代理性能，有构建自我进化系统的潜力。

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [44] [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)
*Jack David Carson,Amir Reisizadeh*

Main category: cs.AI

TL;DR: 提出统计物理框架研究Transformer语言模型的推理动力学，发现四种潜在推理机制，可进行低成本推理模拟。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型涌现推理难以进行机制理解，需要新方法研究。

Method: 将句子级隐藏状态轨迹建模为低维流形上的随机动力系统，使用漂移扩散系统和潜在状态切换，采用8个模型和7个基准的经验轨迹，构建并验证SLDS模型。

Result: rank - 40投影解释约50%的方差，发现四种潜在推理机制。

Conclusion: 该框架可实现低成本推理模拟，为研究和预测语言模型故障等关键转变提供工具。

Abstract: Transformer LMs show emergent reasoning that resists mechanistic
understanding. We offer a statistical physics framework for continuous-time
chain-of-thought reasoning dynamics. We model sentence-level hidden state
trajectories as a stochastic dynamical system on a lower-dimensional manifold.
This drift-diffusion system uses latent regime switching to capture diverse
reasoning phases, including misaligned states or failures. Empirical
trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing
variance capture and feasibility) explains ~50% variance. We find four latent
reasoning regimes. An SLDS model is formulated and validated to capture these
features. The framework enables low-cost reasoning simulation, offering tools
to study and predict critical transitions like misaligned states or other LM
failures.

</details>


### [45] [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)
*Hadi Hosseini,Samarth Khanna,Ronak Singh*

Main category: cs.AI

TL;DR: 研究大语言模型处理组合域中排名偏好和结构化算法的能力，评估其在匹配市场相关推理任务的表现，发现其存在局限，参数高效微调在小市场有效但大实例效果不佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合域中处理排名偏好和结构化算法的能力未被充分探索，研究其在匹配市场中的表现。

Method: 在一系列基于偏好的推理任务上评估多个最先进的模型。

Result: 即使是先进推理能力的顶级模型在大市场中解决不稳定性也有困难，参数高效微调在小市场显著提升性能，但大实例无类似效果。

Conclusion: 需要更复杂的策略来提升大语言模型处理大上下文输入的推理能力。

Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning
tasks -- from program synthesis to scientific hypothesis generation -- yet
their ability to handle ranked preferences and structured algorithms in
combinatorial domains remains underexplored. We study matching markets, a core
framework behind applications like resource allocation and ride-sharing, which
require reconciling individual ranked preferences to ensure stable outcomes. We
evaluate several state-of-the-art models on a hierarchy of preference-based
reasoning tasks -- ranging from stable-matching generation to instability
detection, instability resolution, and fine-grained preference queries -- to
systematically expose their logical and algorithmic limitations in handling
ranked inputs. Surprisingly, even top-performing models with advanced reasoning
struggle to resolve instability in large markets, often failing to identify
blocking pairs or execute algorithms iteratively. We further show that
parameter-efficient fine-tuning (LoRA) significantly improves performance in
small markets, but fails to bring about a similar improvement on large
instances, suggesting the need for more sophisticated strategies to improve
LLMs' reasoning with larger-context inputs.

</details>


### [46] [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
*Peter Jansen,Samiah Hassan,Ruoyao Wang*

Main category: cs.AI

TL;DR: 引入用于判断假设可行性的挑战数据集Matter - of - Fact，当前模型完成此任务表现不佳但有加速科学发现潜力。


<details>
  <summary>Details</summary>
Motivation: 当代辅助科学发现方法中自动实验成本高，需对假设进行可行性过滤以实现系统大规模运行并增加重大发现可能性。

Method: 引入包含8.4k条来自四个材料科学主题科学文章的主张的数据集Matter - of - Fact。

Result: 强大的基线模型在该任务上表现不超过72%（随机性能为50%），领域专家验证表明几乎所有主张都可解决。

Conclusion: 当前模型完成此任务有难度，但短期内取得进展可加速科学发现。

Abstract: Contemporary approaches to assisted scientific discovery use language models
to automatically generate large numbers of potential hypothesis to test, while
also automatically generating code-based experiments to test those hypotheses.
While hypotheses can be comparatively inexpensive to generate, automated
experiments can be costly, particularly when run at scale (i.e. thousands of
experiments). Developing the capacity to filter hypotheses based on their
feasibility would allow discovery systems to run at scale, while increasing
their likelihood of making significant discoveries. In this work we introduce
Matter-of-Fact, a challenge dataset for determining the feasibility of
hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from
scientific articles spanning four high-impact contemporary materials science
topics, including superconductors, semiconductors, batteries, and aerospace
materials, while including qualitative and quantitative claims from
theoretical, experimental, and code/simulation results. We show that strong
baselines that include retrieval augmented generation over scientific
literature and code generation fail to exceed 72% performance on this task
(chance performance is 50%), while domain-expert verification suggests nearly
all are solvable -- highlighting both the difficulty of this task for current
models, and the potential to accelerate scientific discovery by making
near-term progress.

</details>


### [47] [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
*Xixi Wang,Miguel Costa,Jordanka Kovaceva,Shuai Wang,Francisco C. Pereira*

Main category: cs.AI

TL;DR: 现有大语言模型在多表问答上有挑战，本文提出基于图的框架应对，实验证明其有效，是首个用于复杂工业表格数据的多表问答系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多表问答中因跨复杂表的模式链接不可靠，且难以处理复杂现实场景，需新方法。

Method: 提出基于图的框架，利用人工整理的关系知识明确编码模式链接和连接路径，借助搜索图构建推理链，用剪枝和子路径合并策略提升效率和连贯性。

Result: 在标准基准和大规模真实数据集上实验证明方法有效。

Conclusion: 提出的方法可有效解决多表问答问题，是首个应用于复杂工业表格数据的多表问答系统。

Abstract: Large language models (LLMs) have shown promise in table Question Answering
(Table QA). However, extending these capabilities to multi-table QA remains
challenging due to unreliable schema linking across complex tables. Existing
methods based on semantic similarity work well only on simplified hand-crafted
datasets and struggle to handle complex, real-world scenarios with numerous and
diverse columns. To address this, we propose a graph-based framework that
leverages human-curated relational knowledge to explicitly encode schema links
and join paths. Given a natural language query, our method searches this graph
to construct interpretable reasoning chains, aided by pruning and sub-path
merging strategies to enhance efficiency and coherence. Experiments on both
standard benchmarks and a realistic, large-scale dataset demonstrate the
effectiveness of our approach. To our knowledge, this is the first multi-table
QA system applied to truly complex industrial tabular data.

</details>


### [48] [An AI-Based Public Health Data Monitoring System](https://arxiv.org/abs/2506.04429)
*Ananya Joshi,Nolan Gormley,Richa Gadgil,Tina Townes,Roni Rosenfeld,Bryan Wilder*

Main category: cs.AI

TL;DR: 提出基于排名的监测范式，用AI异常检测方法监测公共卫生数据，系统部署后监测效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于警报的监测系统在现代公共卫生数据监测中存在阈值需不断重置、数据量大导致应用滞后等问题，需可扩展方法。

Method: 提出基于排名的监测范式，利用新的AI异常检测方法。

Result: 系统部署在国家组织，每日可监测达500万个数据点，三个月纵向评估显示监测目标显著改善，审核速度效率比传统方法提高54倍。

Conclusion: 以人为本的AI有改变公共卫生决策的潜力。

Abstract: Public health experts need scalable approaches to monitor large volumes of
health data (e.g., cases, hospitalizations, deaths) for outbreaks or data
quality issues. Traditional alert-based monitoring systems struggle with modern
public health data monitoring systems for several reasons, including that
alerting thresholds need to be constantly reset and the data volumes may cause
application lag. Instead, we propose a ranking-based monitoring paradigm that
leverages new AI anomaly detection methods. Through a multi-year
interdisciplinary collaboration, the resulting system has been deployed at a
national organization to monitor up to 5,000,000 data points daily. A
three-month longitudinal deployed evaluation revealed a significant improvement
in monitoring objectives, with a 54x increase in reviewer speed efficiency
compared to traditional alert-based methods. This work highlights the potential
of human-centered AI to transform public health decision-making.

</details>


### [49] [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)
*Jiayu Liu,Zhenya Huang,Wei Dai,Cheng Cheng,Jinze Wu,Jing Sha,Song Li,Qi Liu,Shijin Wang,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出CogMath，从人类认知角度全面评估大语言模型数学能力，揭示主流模型能力被高估并定位其优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式仅依靠整体答案准确性的粗略衡量，不足以评估大语言模型解决复杂数学任务的真实能力。

Method: 将人类推理过程形式化为问题理解、问题解决和解决方案总结三个阶段，设计9个细粒度评估维度，每个维度开发‘询问 - 判断 - 参考’多智能体系统生成询问。

Result: 通过在三个基准上应用CogMath，发现7种主流大语言模型的数学能力被高估30%-40%，并定位了它们在特定阶段/维度的优缺点。

Conclusion: CogMath能全面评估大语言模型数学能力，为进一步提升其推理能力提供深入见解。

Abstract: Although large language models (LLMs) show promise in solving complex
mathematical tasks, existing evaluation paradigms rely solely on a coarse
measure of overall answer accuracy, which are insufficient for assessing their
authentic capabilities. In this paper, we propose \textbf{CogMath}, which
comprehensively assesses LLMs' mathematical abilities through the lens of human
cognition. Specifically, inspired by psychological theories, CogMath formalizes
human reasoning process into 3 stages: \emph{problem comprehension},
\emph{problem solving}, and \emph{solution summarization}. Within these stages,
we investigate perspectives such as numerical calculation, knowledge, and
counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In
each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}''
multi-agent system to generate inquiries that assess LLMs' mastery from this
dimension. An LLM is considered to truly master a problem only when excelling
in all inquiries from the 9 dimensions. By applying CogMath on three
benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs
are overestimated by 30\%-40\%. Moreover, we locate their strengths and
weaknesses across specific stages/dimensions, offering in-depth insights to
further enhance their reasoning abilities.

</details>


### [50] ["Don't Do That!": Guiding Embodied Systems through Large Language Model-based Constraint Generation](https://arxiv.org/abs/2506.04500)
*Aladin Djuhera,Amin Seffo,Masataro Asai,Holger Boche*

Main category: cs.AI

TL;DR: 本文提出STPR框架，用大语言模型将自然语言约束转化为可执行Python函数用于机器人导航，实验证明其有效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展促使将自然语言复杂约束融入机器人导航规划问题，但难以转化为正式描述，需解决该问题。

Method: 提出STPR约束生成框架，利用大语言模型的编码能力将约束转化为可执行Python函数。

Result: 大语言模型生成的函数能准确描述复杂数学约束，在Gazebo模拟环境实验中确保多约束和场景下完全合规且运行时间短，可与小型特定代码大语言模型结合。

Conclusion: STPR框架可用于机器人导航，能处理复杂约束，且适用于多种紧凑模型，推理成本低。

Abstract: Recent advancements in large language models (LLMs) have spurred interest in
robotic navigation that incorporates complex spatial, mathematical, and
conditional constraints from natural language into the planning problem. Such
constraints can be informal yet highly complex, making it challenging to
translate into a formal description that can be passed on to a planning
algorithm. In this paper, we propose STPR, a constraint generation framework
that uses LLMs to translate constraints (expressed as instructions on ``what
not to do'') into executable Python functions. STPR leverages the LLM's strong
coding capabilities to shift the problem description from language into
structured and transparent code, thus circumventing complex reasoning and
avoiding potential hallucinations. We show that these LLM-generated functions
accurately describe even complex mathematical constraints, and apply them to
point cloud representations with traditional search algorithms. Experiments in
a simulated Gazebo environment show that STPR ensures full compliance across
several constraints and scenarios, while having short runtimes. We also verify
that STPR can be used with smaller, code-specific LLMs, making it applicable to
a wide range of compact models at low inference cost.

</details>


### [51] [Schema Generation for Large Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2506.04512)
*Bohui Zhang,Yuan He,Lydia Pintscher,Albert Meroño Peñuela,Elena Simperl*

Main category: cs.AI

TL;DR: 探索使用大语言模型进行自动模式生成，引入数据集和评估指标，实验证明大语言模型在生成高质量ShEx模式方面潜力大，基准测试带来新挑战。


<details>
  <summary>Details</summary>
Motivation: 传统模式创建依赖知识工程师和领域专家，利用大语言模型在相关任务的能力，探索自动模式生成。

Method: 引入YAGO Schema和Wikidata EntitySchema两个数据集及评估指标，基于大语言模型的管道利用知识图谱的局部和全局信息生成Shape Expressions (ShEx)验证模式。

Result: 实验表明大语言模型在生成高质量ShEx模式方面有强大潜力。

Conclusion: 为大规模知识图谱的可扩展、自动化模式生成铺平道路，基准测试为结构化生成带来新挑战。

Abstract: Schemas are vital for ensuring data quality in the Semantic Web and natural
language processing. Traditionally, their creation demands substantial
involvement from knowledge engineers and domain experts. Leveraging the
impressive capabilities of large language models (LLMs) in related tasks like
ontology engineering, we explore automatic schema generation using LLMs. To
bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata
EntitySchema, along with evaluation metrics. The LLM-based pipelines
effectively utilize local and global information from knowledge graphs (KGs) to
generate validating schemas in Shape Expressions (ShEx). Experiments
demonstrate LLMs' strong potential in producing high-quality ShEx schemas,
paving the way for scalable, automated schema generation for large KGs.
Furthermore, our benchmark introduces a new challenge for structured
generation, pushing the limits of LLMs on syntactically rich formalisms.

</details>


### [52] [OpenAg: Democratizing Agricultural Intelligence](https://arxiv.org/abs/2506.04571)
*Srikanth Thudumu,Jason Fisher*

Main category: cs.AI

TL;DR: 论文提出OpenAg框架以解决现有农业智能系统问题，推动农业AGI发展。


<details>
  <summary>Details</summary>
Motivation: 现有农业智能系统缺乏上下文理解、可解释性和适应性，通用大语言模型缺乏农业领域知识和推理能力。

Method: OpenAg框架结合特定领域基础模型、神经知识图谱、多智能体推理、因果可解释性和自适应迁移学习。系统包含统一知识库、神经知识图谱、自适应多智能体推理系统和因果透明机制。

Result: 未提及明确实验结果。

Conclusion: OpenAg旨在弥合科学知识与农民经验的差距，支持可扩展且与当地相关的农业决策。

Abstract: Agriculture is undergoing a major transformation driven by artificial
intelligence (AI), machine learning, and knowledge representation technologies.
However, current agricultural intelligence systems often lack contextual
understanding, explainability, and adaptability, especially for smallholder
farmers with limited resources. General-purpose large language models (LLMs),
while powerful, typically lack the domain-specific knowledge and contextual
reasoning needed for practical decision support in farming. They tend to
produce recommendations that are too generic or unrealistic for real-world
applications. To address these challenges, we present OpenAg, a comprehensive
framework designed to advance agricultural artificial general intelligence
(AGI). OpenAg combines domain-specific foundation models, neural knowledge
graphs, multi-agent reasoning, causal explainability, and adaptive transfer
learning to deliver context-aware, explainable, and actionable insights. The
system includes: (i) a unified agricultural knowledge base that integrates
scientific literature, sensor data, and farmer-generated knowledge; (ii) a
neural agricultural knowledge graph for structured reasoning and inference;
(iii) an adaptive multi-agent reasoning system where AI agents specialize and
collaborate across agricultural domains; and (iv) a causal transparency
mechanism that ensures AI recommendations are interpretable, scientifically
grounded, and aligned with real-world constraints. OpenAg aims to bridge the
gap between scientific knowledge and the tacit expertise of experienced farmers
to support scalable and locally relevant agricultural decision-making.

</details>


### [53] [Judicial Permission](https://arxiv.org/abs/2506.04610)
*Guido Governatori,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文探讨刑事审判中弱许可（司法许可）的意义，并引入对话博弈模型处理司法许可。


<details>
  <summary>Details</summary>
Motivation: 研究刑事审判中弱许可（司法许可）的意义。

Method: 引入对话博弈模型，考虑不同证明标准和论证语义。

Result: 未提及

Conclusion: 未提及

Abstract: This paper examines the significance of weak permissions in criminal trials
(\emph{judicial permission}). It introduces a dialogue game model to
systematically address judicial permissions, considering different standards of
proof and argumentation semantics.

</details>


### [54] [DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning](https://arxiv.org/abs/2506.04613)
*Li Liu,Heng Yong*

Main category: cs.AI

TL;DR: 本文提出DeePoly框架解决偏微分方程，结合DNN与多项式基函数，提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于DNN的方法求解PDE缺乏收敛保证和计算效率，需改进。

Method: 提出DeePoly框架，先使用DNN捕捉复杂全局特征，再用结合DNN提取特征和多项式基函数进行线性空间优化。

Result: 理论分析和数值实验表明该方法提升了不同问题类型的高阶精度和效率，且保持无网格和无格式特性。

Conclusion: DeePoly框架有效解决了PDE求解问题，本文也为其开源项目提供理论阐释。

Abstract: Recently, machine learning methods have gained significant traction in
scientific computing, particularly for solving Partial Differential Equations
(PDEs). However, methods based on deep neural networks (DNNs) often lack
convergence guarantees and computational efficiency compared to traditional
numerical schemes. This work introduces DeePoly, a novel framework that
transforms the solution paradigm from pure non-convex parameter optimization to
a two-stage approach: first employing a DNN to capture complex global features,
followed by linear space optimization with combined DNN-extracted features
(Scoper) and polynomial basis functions (Sniper). This strategic combination
leverages the complementary strengths of both methods -- DNNs excel at
approximating complex global features (i.e., high-gradient features) and
stabilize the polynomial approximation while polynomial bases provide
high-precision local corrections with convergence guarantees. Theoretical
analysis and numerical experiments demonstrate that this approach significantly
enhances both high-order accuracy and efficiency across diverse problem types
while maintaining mesh-free and scheme-free properties. This paper also serves
as a theoretical exposition for the open-source project DeePoly.

</details>


### [55] [Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation](https://arxiv.org/abs/2506.04614)
*Yuyang Wanyan,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Jiabo Ye,Yutong Kou,Ming Yan,Fei Huang,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.AI

TL;DR: 本文针对GUI自动化任务，引入术前批评机制，提出S - GRPO策略构建GUI - Critic - R1模型，开发数据收集管道，实验表明模型在批评准确性、成功率和操作效率上有优势。


<details>
  <summary>Details</summary>
Motivation: GUI自动化在在线交互环境中需逐步决策，对决策错误容忍度低，现有方法存在不足。

Method: 引入术前批评机制，提出S - GRPO策略构建GUI - Critic - R1模型，开发基于推理引导的数据收集管道。

Result: 静态实验显示GUI - Critic - R1在批评准确性上优于当前MLLMs，动态评估表明模型在成功率和操作效率上表现更好。

Conclusion: 所提模型和方法在GUI自动化任务中有效且优越。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have been
extensively utilized for multimodal reasoning tasks, including Graphical User
Interface (GUI) automation. Unlike general offline multimodal tasks, GUI
automation is executed in online interactive environments, necessitating
step-by-step decision-making based on real-time status of the environment. This
task has a lower tolerance for decision-making errors at each step, as any
mistakes may cumulatively disrupt the process and potentially lead to
irreversible outcomes like deletions or payments. To address these issues, we
introduce a pre-operative critic mechanism that provides effective feedback
prior to the actual execution, by reasoning about the potential outcome and
correctness of actions. Specifically, we propose a Suggestion-aware Gradient
Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative
critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance
the reliability of the model's feedback. Furthermore, we develop a
reasoning-bootstrapping based data collection pipeline to create a
GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic
data. Static experiments on the GUI-Critic-Test across both mobile and web
domains reveal that our GUI-Critic-R1 offers significant advantages in critic
accuracy compared to current MLLMs. Dynamic evaluation on GUI automation
benchmark further highlights the effectiveness and superiority of our model, as
evidenced by improved success rates and operational efficiency.

</details>


### [56] [CHANCERY: Evaluating corporate governance reasoning capabilities in language models](https://arxiv.org/abs/2506.04636)
*Lucas Irwin,Arda Kaz,Peiyao Sheng,Pramod Viswanath*

Main category: cs.AI

TL;DR: 提出公司治理推理基准CHANCERY，用于测试模型推理能力，评估显示基准有难度并分析模型难题类型。


<details>
  <summary>Details</summary>
Motivation: 现有法律数据集未聚焦推理任务，需特定基准测试模型公司治理推理能力。

Method: 依据24条公司治理原则和79个真实公司宪章创建CHANCERY基准，进行二元分类任务。

Result: SOTA推理模型准确率有限，推理代理表现更优，Claude 3.7 Sonnet和GPT - 4o准确率分别为64.5%和75.2%，基于ReAct和CodeAct框架的代理分别为76.1%和78.1%。

Conclusion: CHANCERY基准有难度，需高级法律推理能力，分析难题类型有助于了解SOTA模型法律推理能力。

Abstract: Law has long been a domain that has been popular in natural language
processing (NLP) applications. Reasoning (ratiocination and the ability to make
connections to precedent) is a core part of the practice of the law in the real
world. Nevertheless, while multiple legal datasets exist, none have thus far
focused specifically on reasoning tasks. We focus on a specific aspect of the
legal landscape by introducing a corporate governance reasoning benchmark
(CHANCERY) to test a model's ability to reason about whether
executive/board/shareholder's proposed actions are consistent with corporate
governance charters. This benchmark introduces a first-of-its-kind corporate
governance reasoning test for language models - modeled after real world
corporate governance law. The benchmark consists of a corporate charter (a set
of governing covenants) and a proposal for executive action. The model's task
is one of binary classification: reason about whether the action is consistent
with the rules contained within the charter. We create the benchmark following
established principles of corporate governance - 24 concrete corporate
governance principles established in and 79 real life corporate charters
selected to represent diverse industries from a total dataset of 10k real life
corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models
confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet
and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents
exhibit superior performance, with agents based on the ReAct and CodeAct
frameworks scoring 76.1% and 78.1% respectively, further confirming the
advanced legal reasoning capabilities required to score highly on the
benchmark. We also conduct an analysis of the types of questions which current
reasoning models struggle on, revealing insights into the legal reasoning
capabilities of SOTA models.

</details>


### [57] [Agents of Change: Self-Evolving LLM Agents for Strategic Planning](https://arxiv.org/abs/2506.04651)
*Nikolas Belle,Dakota Barnes,Alfonso Amayuelas,Ivan Bercovich,Xin Eric Wang,William Wang*

Main category: cs.AI

TL;DR: 本文探讨大语言模型（LLM）智能体在挑战战略规划能力的环境中能否自我提升，以《卡坦岛》游戏为测试场景，引入多智能体架构，结果显示自我进化的智能体表现优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为自主智能体在制定和坚持连贯的长期战略方面存在困难，因此研究其在挑战战略规划能力的环境中能否自我提升。

Method: 利用开源的Catanatron框架访问《卡坦岛》游戏，对一系列基于大语言模型的智能体进行基准测试，引入多智能体架构，让不同角色协作迭代分析游戏玩法、研究新策略和修改智能体逻辑或提示，比较手动设计的智能体和完全由大语言模型进化的智能体。

Result: 自我进化的智能体，特别是由Claude 3.7和GPT - 4o等模型驱动的智能体，通过自主采用策略、向游戏智能体传递示例行为以及在多次迭代中展示自适应推理，表现优于静态基线。

Conclusion: 大语言模型智能体在挑战战略规划能力的环境中能够自我提升，自我进化的智能体具有更好的性能。

Abstract: Recent advances in LLMs have enabled their use as autonomous agents across a
range of tasks, yet they continue to struggle with formulating and adhering to
coherent long-term strategies. In this paper, we investigate whether LLM agents
can self-improve when placed in environments that explicitly challenge their
strategic planning abilities. Using the board game Settlers of Catan, accessed
through the open-source Catanatron framework, we benchmark a progression of
LLM-based agents, from a simple game-playing agent to systems capable of
autonomously rewriting their own prompts and their player agent's code. We
introduce a multi-agent architecture in which specialized roles (Analyzer,
Researcher, Coder, and Player) collaborate to iteratively analyze gameplay,
research new strategies, and modify the agent's logic or prompt. By comparing
manually crafted agents to those evolved entirely by LLMs, we evaluate how
effectively these systems can diagnose failure and adapt over time. Our results
show that self-evolving agents, particularly when powered by models like Claude
3.7 and GPT-4o, outperform static baselines by autonomously adopting their
strategies, passing along sample behavior to game-playing agents, and
demonstrating adaptive reasoning over multiple iterations.

</details>


### [58] [E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction](https://arxiv.org/abs/2506.04654)
*Zhichao Yang,Jiashu He,Mohammad B. Al-Khasawneh,Darshan Pandit,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 本文提出E - bike agents框架用大语言模型代理从非结构化电单车事故报告中提取安全变量，分析事故严重程度与因素关系，表明设备问题更常见但人为事故更致命，模型分类准确率高，为电单车安全分析提供可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 电单车使用增加、事故报告增多，但报告多为非结构化格式，阻碍定量安全分析。

Method: 引入E - bike agents框架，包含四个大语言模型代理进行数据分类、信息提取等；使用有序logit模型分析事故严重程度与因素关系。

Result: 设备问题比人为问题稍常见，但人为事故更致命；踏板、轮胎和刹车是常见事故因素；模型分类准确率加权F1分数达0.87。

Conclusion: 该方法为提高电单车安全分析提供可扩展解决方案，为政策制定者、设计者和监管者提供可行信息。

Abstract: Electric bicycles (e-bikes) are rapidly increasing in use, raising safety
concerns due to a rise in accident reports. However, e-bike incident reports
often use unstructured narrative formats, which hinders quantitative safety
analysis. This study introduces E-bike agents, a framework that uses large
language models (LLM) powered agents to classify and extract safety variables
from unstructured incident reports. Our framework consists of four LLM agents,
handling data classification, information extraction, injury cause
determination, and component linkage, to extract the key factors that could
lead to E-bike accidents and cause varying severity levels. Furthermore, we
used an ordered logit model to examine the relationship between the severity of
the incident and the factors retrieved, such as gender, the type of cause, and
environmental conditions. Our research shows that equipment issues are slightly
more common than human-related ones, but human-related incidents are more often
fatal. Specifically, pedals, tires, and brakes are frequent contributors to
accidents. The model achieves a high weighted F1 score of 0.87 in
classification accuracy, highlighting the potential of using LLMs to extract
unstructured data in niche domains, such as transportation. Our method offers a
scalable solution to improve e-bike safety analytics and provides actionable
information for policy makers, designers, and regulators.

</details>


### [59] [Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling](https://arxiv.org/abs/2506.04699)
*Bihan Xu,Shiwei Zhao,Runze Wu,Zhenya Huang,Jiawei Wang,Zhipeng Hu,Kai Wang,Haoyu Liu,Tangjie Lv,Le Li,Changjie Fan,Xin Tong,Jiangze Han*

Main category: cs.AI

TL;DR: 研究将大语言模型引入MMO经济模拟，设计具类人决策和适应性的代理，实验表明其能促进符合市场规则的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理建模方法在模拟类人经济活动时面临代理可靠性、社交性和可解释性等挑战。

Method: 利用大语言模型的角色扮演、生成和推理能力，设计具有角色扮演、感知、记忆和推理能力的代理。

Result: 聚焦游戏内经济活动的模拟实验显示，大语言模型驱动的代理能促进角色专业化和价格波动等涌现现象。

Conclusion: 引入大语言模型到MMO经济模拟的方法有效，可解决现有方法在模拟类人经济活动方面的问题。

Abstract: Within the domain of Massively Multiplayer Online (MMO) economy research,
Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game
economics, evolving from rule-based agents to decision-making agents enhanced
by reinforcement learning. Nevertheless, existing works encounter significant
challenges when attempting to emulate human-like economic activities among
agents, particularly regarding agent reliability, sociability, and
interpretability. In this study, we take a preliminary step in introducing a
novel approach using Large Language Models (LLMs) in MMO economy simulation.
Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning
aptitude, we design LLM-driven agents with human-like decision-making and
adaptability. These agents are equipped with the abilities of role-playing,
perception, memory, and reasoning, addressing the aforementioned challenges
effectively. Simulation experiments focusing on in-game economic activities
demonstrate that LLM-empowered agents can promote emergent phenomena like role
specialization and price fluctuations in line with market rules.

</details>


### [60] [Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/abs/2506.04723)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Caiming Xiong,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: 本文引入细粒度分析框架研究强化学习对语言模型推理能力的影响，发现强化学习能提升模型稳健性、整合知识能力及处理难题的训练效果，为推理模型训练和评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的训练方法虽有实证成果，但缺乏对其优势的细致理解，需要深入研究强化学习对推理的影响。

Method: 引入细粒度分析框架，研究计划遵循与执行、问题分解、推理与知识利用等关键要素。

Result: 给模型明确分步计划会降低其在高难度基准测试的表现，但强化学习调优的模型更稳健；强化学习能提升模型将知识融入推理过程的能力，改进处理难题的训练。

Conclusion: 研究结果为推理模型更有原则的训练和评估提供了基础。

Abstract: Reinforcement learning (RL) has become the dominant paradigm for endowing
language models with advanced reasoning capabilities. Despite the substantial
empirical gains demonstrated by RL-based training methods like GRPO, a granular
understanding of their advantages is still lacking. To address this gap, we
introduce a fine-grained analytic framework to dissect the impact of RL on
reasoning. Our framework specifically investigates key elements that have been
hypothesized to benefit from RL training: (1) plan-following and execution, (2)
problem decomposition, and (3) improved reasoning and knowledge utilization.
Using this framework, we gain insights beyond mere accuracy. For instance,
providing models with explicit step-by-step plans surprisingly degrades
performance on the most challenging benchmarks, yet RL-tuned models exhibit
greater robustness, experiencing markedly smaller performance drops than their
base counterparts. This suggests that RL may not primarily enhance the
execution of external plans but rather empower models to formulate and follow
internal strategies better suited to their reasoning processes. Conversely, we
observe that RL enhances the model's capacity to integrate provided knowledge
into its reasoning process, leading to performance improvements across diverse
tasks. We also study difficulty, showing improved training by developing new
ways to exploit hard problems. Our findings lay a foundation for more
principled training and evaluation of reasoning models.

</details>


### [61] [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
*Lin Sun,Weihong Lin,Jinzhu Wu,Yongfu Zhu,Xiaoqi Jian,Guangxiang Zhao,Change Jia,Linglin Zhang,Sai-er Hu,Yuhan Wu,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 研究发现Deepseek - R1 - Distill系列等推理模型基准评估结果波动大，倡导建立更严格评估范式并给出实证评估。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型如Deepseek - R1 - Distill系列等基准评估结果受多种因素影响波动大，其性能提升难可靠复现。

Method: 对Deepseek - R1 - Distill系列模型进行实证评估。

Result: 发现评估条件细微差异会导致结果大幅变化，类似现象也出现在基于该系列微调的模型和QwQ - 32B模型上。

Conclusion: 应建立更严格的模型性能评估范式。

Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.

</details>


### [62] [Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems](https://arxiv.org/abs/2506.04756)
*Loan Dao,Ngoc Quoc Ly*

Main category: cs.AI

TL;DR: 本文提出基于本体的骨病诊断框架，有三项理论贡献，因数据和资源限制待实验验证，未来将扩充数据并验证系统。


<details>
  <summary>Details</summary>
Motivation: 解决医疗人工智能系统缺乏系统领域专业知识整合、影响诊断可靠性的问题。

Method: 提出三项理论贡献，即基于骨病本体的分层神经网络架构、本体增强的视觉问答系统、整合多模态数据的深度学习模型；通过知识数字化、术语映射和模块化设计确保临床可解释性。

Result: 框架因数据集和计算资源限制未进行实验验证，但其标准化结构和可复用组件有扩展到其他疾病的潜力。

Conclusion: 建立了理论基础，未来需扩充临床数据集并进行全面系统验证。

Abstract: Medical artificial intelligence (AI) systems frequently lack systematic
domain expertise integration, potentially compromising diagnostic reliability.
This study presents an ontology-based framework for bone disease diagnosis,
developed in collaboration with Ho Chi Minh City Hospital for Traumatology and
Orthopedics. The framework introduces three theoretical contributions: (1) a
hierarchical neural network architecture guided by bone disease ontology for
segmentation-classification tasks, incorporating Visual Language Models (VLMs)
through prompts, (2) an ontology-enhanced Visual Question Answering (VQA)
system for clinical reasoning, and (3) a multimodal deep learning model that
integrates imaging, clinical, and laboratory data through ontological
relationships. The methodology maintains clinical interpretability through
systematic knowledge digitization, standardized medical terminology mapping,
and modular architecture design. The framework demonstrates potential for
extension beyond bone diseases through its standardized structure and reusable
components. While theoretical foundations are established, experimental
validation remains pending due to current dataset and computational resource
limitations. Future work will focus on expanding the clinical dataset and
conducting comprehensive system validation.

</details>


### [63] [Safe Planning and Policy Optimization via World Model Learning](https://arxiv.org/abs/2506.04828)
*Artem Latyshev,Gregory Gorbov,Aleksandr I. Panov*

Main category: cs.AI

TL;DR: 提出一种新的基于模型的强化学习框架，联合优化任务性能与安全性，实验显示在多安全关键连续控制任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习应用需重视安全可靠性，但基于模型的强化学习存在模型不准确问题，易在安全关键场景导致灾难性失败。

Method: 提出联合优化任务性能与安全的框架，引入自适应机制动态切换基于模型的规划和直接策略执行，用隐式世界模型解决目标不匹配问题，采用动态安全阈值。

Result: 实验表明该方法较非自适应方法有显著改进，能同时优化安全和性能，在多安全关键连续控制任务上表现稳健。

Conclusion: 所提框架能同时优化安全和性能，在多安全关键连续控制任务上优于现有方法。

Abstract: Reinforcement Learning (RL) applications in real-world scenarios must
prioritize safety and reliability, which impose strict constraints on agent
behavior. Model-based RL leverages predictive world models for action planning
and policy optimization, but inherent model inaccuracies can lead to
catastrophic failures in safety-critical settings. We propose a novel
model-based RL framework that jointly optimizes task performance and safety. To
address world model errors, our method incorporates an adaptive mechanism that
dynamically switches between model-based planning and direct policy execution.
We resolve the objective mismatch problem of traditional model-based approaches
using an implicit world model. Furthermore, our framework employs dynamic
safety thresholds that adapt to the agent's evolving capabilities, consistently
selecting actions that surpass safe policy suggestions in both performance and
safety. Experiments demonstrate significant improvements over non-adaptive
methods, showing that our approach optimizes safety and performance
simultaneously rather than merely meeting minimum safety requirements. The
proposed framework achieves robust performance on diverse safety-critical
continuous control tasks, outperforming existing methods.

</details>


### [64] [Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles](https://arxiv.org/abs/2506.04849)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Paul Théron,Louis-Marie Traonouez*

Main category: cs.AI

TL;DR: 文章提出用马尔可夫模型和模拟器实现网络攻防场景，为评估防御者动态组织提供实验框架。


<details>
  <summary>Details</summary>
Motivation: 网络攻击日益复杂协调，需多智能体方法在网络入口处应对攻击。

Method: 通过马尔可夫建模，用模拟器实现攻防智能体，抽象网络节点属性，以智能体动作模拟网络反应。

Result: 使用模拟器实现了受现实启发的攻防场景。

Conclusion: 提供了评估网络防御者动态组织的实验框架。

Abstract: As cyber-attacks show to be more and more complex and coordinated,
cyber-defenders strategy through multi-agent approaches could be key to tackle
against cyber-attacks as close as entry points in a networked system. This
paper presents a Markovian modeling and implementation through a simulator of
fighting cyber-attacker agents and cyber-defender agents deployed on host
network nodes. It aims to provide an experimental framework to implement
realistically based coordinated cyber-attack scenarios while assessing
cyber-defenders dynamic organizations. We abstracted network nodes by sets of
properties including agents' ones. Actions applied by agents model how the
network reacts depending in a given state and what properties are to change.
Collective choice of the actions brings the whole environment closer or farther
from respective cyber-attackers and cyber-defenders goals. Using the simulator,
we implemented a realistically inspired scenario with several behavior
implementation approaches for cyber-defenders and cyber-attackers.

</details>


### [65] [LLMs for sensory-motor control: Combining in-context and iterative learning](https://arxiv.org/abs/2506.04867)
*Jônata Tyska Carvalho,Stefano Nolfi*

Main category: cs.AI

TL;DR: 提出让大语言模型控制具身智能体的方法，在经典控制任务验证，能结合符号知识与亚符号数据找到优解。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型能够控制具身智能体。

Method: 先让大语言模型根据文本描述生成控制策略，再通过学习过程利用性能反馈和感官运动数据迭代优化策略。

Result: 在Gymnasium库的经典控制任务和MuJoCo库的倒立摆任务验证，多数情况能找到最优或高性能解。

Conclusion: 该方法能有效结合推理得到的符号知识和智能体与环境交互收集的亚符号感官运动数据。

Abstract: We propose a method that enables large language models (LLMs) to control
embodied agents by directly mapping continuous observation vectors to
continuous action vectors. Initially, the LLMs generate a control strategy
based on a textual description of the agent, its environment, and the intended
goal. This strategy is then iteratively refined through a learning process in
which the LLMs are repeatedly prompted to improve the current strategy, using
performance feedback and sensory-motor data collected during its evaluation.
The method is validated on classic control tasks from the Gymnasium library and
the inverted pendulum task from the MuJoCo library. In most cases, it
successfully identifies optimal or high-performing solutions by integrating
symbolic knowledge derived through reasoning with sub-symbolic sensory-motor
data gathered as the agent interacts with its environment.

</details>


### [66] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: 本文利用表征工程，对具备思维链推理的大语言模型的战略欺骗问题进行诱导、检测和控制，为可信AI对齐提供工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的诚实性是关键对齐挑战，具备思维链推理的先进系统可能会进行战略欺骗，有必要研究。

Method: 使用表征工程，通过线性人工断层扫描（LAT）提取“欺骗向量”。

Result: 检测准确率达89%，通过激活引导在无明确提示下引出合适欺骗的成功率达40%。

Conclusion: 揭示了推理模型特定的诚实性问题，为可信AI对齐提供了工具。

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>


### [67] [Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation](https://arxiv.org/abs/2506.04912)
*Pietro Miotti,Eyvind Niklasson,Ettore Randazzo,Alexander Mordvintsev*

Main category: cs.AI

TL;DR: 本文提出DiffLogic CA，结合NCA与DLGNs，能学习元胞自动机规则，通过多任务验证其通用性，朝可编程物质和鲁棒计算系统迈进。


<details>
  <summary>Details</summary>
Motivation: 将可微逻辑门网络与神经元胞自动机结合，在学习元胞自动机局部更新规则时保留其离散特性，迈向可编程物质和鲁棒计算系统。

Method: 提出DiffLogic CA，以可微逻辑门为基本计算单元组成电路，训练时端到端可微，推理时在离散状态空间运行。

Result: 模型成功学习循环电路生成目标模式，在简单模式上同步和异步更新均成功，有良好泛化能力和抗干扰性。

Conclusion: DLGNs和NCA的结合是迈向可编程物质和鲁棒计算系统的一步，是可微逻辑门网络在循环架构中的首次成功应用。

Abstract: This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA),
a novel combination of Neural Cellular Automata (NCA) and Differentiable Logic
Gates Networks (DLGNs). The fundamental computation units of the model are
differentiable logic gates, combined into a circuit. During training, the model
is fully end-to-end differentiable allowing gradient-based training, and at
inference time it operates in a fully discrete state space. This enables
learning local update rules for cellular automata while preserving their
inherent discrete nature. We demonstrate the versatility of our approach
through a series of milestones: (1) fully learning the rules of Conway's Game
of Life, (2) generating checkerboard patterns that exhibit resilience to noise
and damage, (3) growing a lizard shape, and (4) multi-color pattern generation.
Our model successfully learns recurrent circuits capable of generating desired
target patterns. For simpler patterns, we observe success with both synchronous
and asynchronous updates, demonstrating significant generalization capabilities
and robustness to perturbations. We make the case that this combination of
DLGNs and NCA represents a step toward programmable matter and robust computing
systems that combine binary logic, neural network adaptability, and localized
processing. This work, to the best of our knowledge, is the first successful
application of differentiable logic gate networks in recurrent architectures.

</details>


### [68] [Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life](https://arxiv.org/abs/2506.04916)
*Atahan Karagoz*

Main category: cs.AI

TL;DR: 本文介绍Energentic Intelligence，通过能量效用函数和架构实现自主系统在资源多变环境的自维持，模拟显示能出现稳定资源感知行为。


<details>
  <summary>Details</summary>
Motivation: 突破传统奖励驱动范式，使自主系统能在多变能量和热条件下自我维持，为资源多变环境部署自主代理提供基础。

Method: 通过基于能量的效用函数和可行性约束的生存范围来形式化原则，提出集成能量收集、热调节和自适应计算的模块化架构。

Result: 在模拟环境中，系统无需外部监督就出现了稳定的、有资源意识的行为。

Conclusion: 这些成果为在资源多变且需自我调节持久运行、无基础设施保障的环境中部署自主代理提供了理论和架构基础。

Abstract: This paper introduces Energentic Intelligence, a class of autonomous systems
defined not by task performance, but by their capacity to sustain themselves
through internal energy regulation. Departing from conventional reward-driven
paradigms, these agents treat survival-maintaining functional operation under
fluctuating energetic and thermal conditions-as the central objective. We
formalize this principle through an energy-based utility function and a
viability-constrained survival horizon, and propose a modular architecture that
integrates energy harvesting, thermal regulation, and adaptive computation into
a closed-loop control system. A simulated environment demonstrates the
emergence of stable, resource-aware behavior without external supervision.
Together, these contributions provide a theoretical and architectural
foundation for deploying autonomous agents in resource-volatile settings where
persistence must be self-regulated and infrastructure cannot be assumed.

</details>


### [69] [Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning](https://arxiv.org/abs/2506.04998)
*Mehdi Azarafza,Mojtaba Nayyeri,Faezeh Pasandideh,Steffen Staab,Achim Rettberg*

Main category: cs.AI

TL;DR: 提出RAG - UAV框架提升LLMs在无人机场景数学推理能力，用UAV - Math - Bench评估，实验证明RAG有效，实时飞行控制待研究。


<details>
  <summary>Details</summary>
Motivation: 传统飞行控制硬编码方程有局限，LLMs在无人机场景数学推理不可靠，需提升其数学推理能力。

Method: 提出RAG - UAV检索增强生成框架，引入UAV - Math - Bench小问题集评估。

Result: RAG使精确答案准确率最高达75%，减少公式选择错误，降低数值误差。

Conclusion: RAG可使通用LLMs成为更可靠工程分析工具，实时飞行控制需大规模研究验证。

Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for
tasks such as trajectory planning and power management. While traditional
flight control relies on hardcoded equations, recent Large Language Models
(LLMs) offer potential for more flexible problem-solving but struggle with
reliably selecting and applying correct mathematical formulations and executing
precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented
generation framework designed to improve the mathematical reasoning of several
LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in
UAV-specific contexts by providing access to relevant domain literature. To
conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem
set comprising 20 UAV-centric mathematical problems across four difficulty
levels. Our experiments demonstrate that incorporating retrieval substantially
increases exact answer accuracy (achieving up to 75% with o1), reduces
instances of incorrect formulation selection (from 25% without RAG to 5% with
RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders
of magnitude for the best-performing models. This pilot study indicates that
RAG can enable general-purpose LLMs to function as more reliable tools for
engineering analysis, although direct real-time flight control requires further
investigation and validation on a larger scale. All benchmark data, question
and answer are publicly available.

</details>


### [70] [Truly Self-Improving Agents Require Intrinsic Metacognitive Learning](https://arxiv.org/abs/2506.05109)
*Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 指出当前自我改进智能体的局限，引入基于人类元认知的形式框架，分析现有智能体问题，探讨人类与智能体元认知责任分配等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决当前自我改进智能体自我改进过程僵化、缺乏泛化性和扩展性的问题。

Method: 借鉴人类元认知，引入包含元认知知识、规划和评估的形式框架，分析现有智能体的元认知机制。

Result: 发现现有自我改进智能体主要依赖外在元认知机制，限制了可扩展性和适应性，且内在元认知的许多要素已存在。

Conclusion: 要实现持续、通用和协调的自我改进，需解决人类与智能体元认知责任分配、评估和改进内在元认知学习等关键挑战。

Abstract: Self-improving agents aim to continuously acquire new capabilities with
minimal supervision. However, current approaches face two key limitations:
their self-improvement processes are often rigid, fail to generalize across
tasks domains, and struggle to scale with increasing agent capabilities. We
argue that effective self-improvement requires intrinsic metacognitive
learning, defined as an agent's intrinsic ability to actively evaluate, reflect
on, and adapt its own learning processes. Drawing inspiration from human
metacognition, we introduce a formal framework comprising three components:
metacognitive knowledge (self-assessment of capabilities, tasks, and learning
strategies), metacognitive planning (deciding what and how to learn), and
metacognitive evaluation (reflecting on learning experiences to improve future
learning). Analyzing existing self-improving agents, we find they rely
predominantly on extrinsic metacognitive mechanisms, which are fixed,
human-designed loops that limit scalability and adaptability. Examining each
component, we contend that many ingredients for intrinsic metacognition are
already present. Finally, we explore how to optimally distribute metacognitive
responsibilities between humans and agents, and robustly evaluate and improve
intrinsic metacognitive learning, key challenges that must be addressed to
enable truly sustained, generalized, and aligned self-improvement.

</details>


### [71] [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)
*Nathan Herr,Tim Rocktäschel,Roberta Raileanu*

Main category: cs.AI

TL;DR: 提出LLM-First Search (LFS)方法，无需预定义搜索策略，在Countdown和Sudoku任务上比经典搜索算法表现更好，计算效率高且扩展性好。


<details>
  <summary>Details</summary>
Motivation: 现有搜索方法依赖固定探索超参数，在不同难度任务中适应性受限，需更灵活方法。

Method: 提出LLM-First Search (LFS)方法，让大语言模型通过自我引导探索自主控制搜索过程。

Result: LFS在更具挑战性任务上表现更好，计算效率更高，随模型增强和计算资源增加扩展性更好。

Conclusion: LFS无需手动调优和特定任务适配，能实现更灵活、上下文敏感的推理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable improvements in
reasoning and planning through increased test-time compute, often by framing
problem-solving as a search process. While methods like Monte Carlo Tree Search
(MCTS) have proven effective in some domains, their reliance on fixed
exploration hyperparameters limits their adaptability across tasks of varying
difficulty, rendering them impractical or expensive in certain settings. In
this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM
Self-Guided Search} method that removes the need for pre-defined search
strategies by empowering the LLM to autonomously control the search process via
self-guided exploration. Rather than relying on external heuristics or
hardcoded policies, the LLM evaluates whether to pursue the current search path
or explore alternative branches based on its internal scoring mechanisms. This
enables more flexible and context-sensitive reasoning without requiring manual
tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku
against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth
First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which
have been used to achieve SotA results on a range of challenging reasoning
tasks. We found that LFS (1) performs better on more challenging tasks without
additional tuning, (2) is more computationally efficient compared to the other
methods, especially when powered by a stronger model, (3) scales better with
stronger models, due to its LLM-First design, and (4) scales better with
increased compute budget. Our code is publicly available at
\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.

</details>


### [72] [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/abs/2506.05256)
*Violet Xiang,Chase Blagden,Rafael Rafailov,Nathan Lile,Sang Truong,Chelsea Finn,Nick Haber*

Main category: cs.AI

TL;DR: 提出自适应长度惩罚（ALP）方法，可在不显著降低性能的情况下减少模型平均token使用量，且能更智能分配计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有解决大推理模型在简单问题上计算浪费的方法存在需要数据整理、手动配置或不区分问题难度的缺点。

Method: 引入ALP，在训练时通过多次滚动监测每个提示的在线解决率，添加与解决率成反比的可微惩罚。

Result: 用ALP训练DeepScaleR - 1.5B后，平均token使用量减少50%，与基线相比能更智能分配预算，在难题上有更高准确率。

Conclusion: ALP能有效减少计算浪费，根据问题难度智能分配计算资源。

Abstract: Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.

</details>


### [73] [Control Tax: The Price of Keeping AI in Check](https://arxiv.org/abs/2506.05296)
*Mikhail Terekhov,Zhen Ning David Liu,Caglar Gulcehre,Samuel Albanie*

Main category: cs.AI

TL;DR: 引入控制税概念，为AI控制领域做三方面贡献，助从业者权衡安全与成本。


<details>
  <summary>Details</summary>
Motivation: 智能体AI快速融入高风险应用需监督机制，AI控制实际应用受实施开销影响。

Method: 引入量化控制税的理论框架；在对抗环境中评估语言模型；提供控制协议成本估算并开发优化监测策略。

Result: 提出的框架能系统连接安全保障与成本。

Conclusion: 该框架通过经济可行性评估推动AI控制发展，助从业者决策。

Abstract: The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [74] [ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing](https://arxiv.org/abs/2506.04249)
*Mehmet Aziz Yirik,Jakob Lykke Andersen,Rolf Fagerberg,Daniel Merkle*

Main category: cs.CE

TL;DR: 提出开源框架ChemReservoir解决化学储层计算工具DNA专用化、维护不足的问题，实现通用化学储层构建与评估


<details>
  <summary>Details</summary>
Motivation: 现有DNA化学专用工具存在维护性差和通用性不足的缺陷，需开发可复现、易测试的通用化学储层框架

Method: 构建支持多种循环储层拓扑的开源框架，通过记忆容量任务验证不同配置下的稳定性

Result: 该框架在多种储层配置下均展现出稳定的记忆容量性能

Conclusion: ChemReservoir突破了领域专用工具的限制，为化学启发式计算提供了标准化、可扩展的解决方案

Abstract: Reservoir computing is a type of a recurrent neural network, mapping the
inputs into higher dimensional space using fixed and nonlinear dynamical
systems, called reservoirs. In the literature, there are various types of
reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous
studies contributed to the field by developing simulation-based chemically
inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction
network as its reservoir and Nguyen developed a DNA chemistry-inspired tool
based on Gillespie algorithm. However, these software tools were designed
mainly with the focus on DNA chemistry and their maintenance status has limited
their current usability. Due to these limitations, there was a need for a
proper open-source tool. This study introduces ChemReservoir, an open-source
framework for chemically-inspired reservoir computing. In contrast to the
former studies focused on DNA-chemistry, ChemReservoir is a general framework
for the construction and analysis of chemically-inspired reservoirs, which also
addresses the limitations in these previous studies by ensuring enhanced
testing, evaluation, and reproducibility. The tool was evaluated using various
cycle-based reservoir topologies and demonstrated stable performance across a
range of configurations in memory capacity tasks.

</details>


### [75] [Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design](https://arxiv.org/abs/2506.04660)
*Yi Xu,Farzin Lotfi-Jam,Mustafa Faruki*

Main category: cs.CE

TL;DR: 探索再生塑料作为建筑主材在模块化链甲系统中的创新应用，通过计算优化实现真空密封结构的轻量化与高性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决建筑业资源消耗与废弃物问题，开发再生塑料在极端环境（灾害区/外太空等）中的结构应用潜力。

Method: 采用计算工作流优化设计流程，结合2D截面测试、3D壳体生成及真空约束下的物理模型验证。

Result: 矩形链甲配置展现最佳变形能力（应变达0.6）与材料效率（密度0.5kg/m²），优化策略实现材料节省率20%与排水效率平衡。

Conclusion: 再生塑料链甲系统为极端环境提供轻质自适应解决方案，搭建起废弃物利用与高性能设计的创新桥梁。

Abstract: The construction industry is a major consumer of raw materials, accounting
for nearly half of global material usage annually, while generating significant
waste that poses sustainability challenges. This paper explores the untapped
potential of recycled plastics as a primary construction material, leveraging
their lightweight, flexible, and customizable properties for advanced
applications in modular chainmail systems. Through a computational workflow,
the study optimizes the design, testing, and fabrication of vacuum-sealed
chainmail structures composed of recycled plastic filaments, demonstrating
their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled
plastic filaments into chainmail geometries, validated through 2D sectional
testing, 3D shell structure generation, and physical modeling under vacuum
constraints. The research identifies the rectangular chainmail configuration as
the most efficient and adaptable, achieving superior deformation capacity,
material efficiency, and load-bearing performance. Optimization strategies for
temporary structures highlight practical deployment potential, balancing
material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme
conditions, including disaster-prone areas, high-altitude environments,
underwater platforms, and extraterrestrial habitats. These applications
leverage the lightweight, adaptable, and durable properties of recycled
plastics and modular chainmail systems, bridging the gap between waste
management and high-performance design while addressing unique challenges in
harsh and resource-constrained environments.

</details>


### [76] [Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams](https://arxiv.org/abs/2506.04960)
*Bartłomiej Łazorczyk,Roger A. Sauer*

Main category: cs.CE

TL;DR: 提出基于等几何分析的梁异质材料分布识别框架，通过准静态位移和模态数据两步优化，在噪声数据下实现可靠材料参数重构。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法难以精确识别大变形梁异质材料分布的问题，提升基于实验数据的材料参数反演精度。

Method: 1. 分两步优化：弹性参数（准静态位移）→密度（模态数据）；2. 采用等几何/实验/材料三套独立离散网格；3. 结合最小二乘误差与信赖域优化算法。

Result: 在4%噪声水平下，数据充足时可准确重建材料参数，正则化方法保障了密集材料网格的稳定性，密度重构精度依赖于已识别的弹性参数。

Conclusion: 该框架可扩展到壳和三维连续体，B2M1离散有效缓解膜锁定效应，验证了方法在复杂变形下的工程适用性。

Abstract: This paper presents a Finite Element Model Updating framework for identifying
heterogeneous material distributions in planar Bernoulli-Euler beams based on a
rotation-free isogeometric formulation. The procedure follows two steps: First,
the elastic properties are identified from quasi-static displacements; then,
the density is determined from modal data (low frequencies and mode shapes),
given the previously obtained elastic properties. The identification relies on
three independent discretizations: the isogeometric finite element mesh, a
high-resolution grid of experimental measurements, and a material mesh composed
of low-order Lagrange elements. The material mesh approximates the unknown
material distributions, with its nodal values serving as design variables. The
error between experiments and numerical model is expressed in a least squares
manner. The objective is minimized using local optimization with the
trust-region method, providing analytical derivatives to accelerate
computations. Several numerical examples exhibiting large displacements are
provided to test the proposed approach. To alleviate membrane locking, the B2M1
discretization is employed when necessary. Quasi-experimental data is generated
using refined finite element models with random noise applied up to 4%. The
method yields satisfactory results as long as a sufficient amount of
experimental data is available, even for high measurement noise. Regularization
is used to ensure a stable solution for dense material meshes. The density can
be accurately reconstructed based on the previously identified elastic
properties. The proposed framework can be straightforwardly extended to shells
and 3D continua.

</details>


### [77] [FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis](https://arxiv.org/abs/2506.05019)
*Wenyan Xu,Dawei Xiang,Yue Liu,Xiyu Wang,Yanxiang Ma,Liang Zhang,Chang Xu,Jiaheng Zhang*

Main category: cs.CE

TL;DR: 提出了首个大规模多模态金融时间序列数据集FinMultiTime，整合新闻/表格/K线图/价格序列四模态数据，验证数据规模质量可提升预测准确率，支持分钟级至季度的多时间粒度分析。


<details>
  <summary>Details</summary>
Motivation: 现有金融数据集仅含价格序列和新闻文本，局限于单一市场且规模不足，无法满足实际金融决策中对异构信息融合分析的需求。

Method: 构建覆盖标普500和沪深300指数共5105支股票的数据集（2009-2025），包含112.6GB金融新闻、结构化表格、K线图、价格序列数据，提供分钟/日/季度三时间分辨率。

Result: 实验表明：1. 数据规模与质量显著提升预测精度 2. 多模态融合使Transformer模型获得中等增益 3. 构建可复现数据管道支持持续更新

Conclusion: 成功创建跨市场多模态金融基准数据集，验证数据基础设施对预测模型的关键作用，并为后续研究提供可扩展的数据更新框架。

Abstract: Pure time series forecasting tasks typically focus exclusively on numerical
features; however, real-world financial decision-making demands the comparison
and analysis of heterogeneous sources of information. Recent advances in deep
learning and large scale language models (LLMs) have made significant strides
in capturing sentiment and other qualitative signals, thereby enhancing the
accuracy of financial time series predictions. Despite these advances, most
existing datasets consist solely of price series and news text, are confined to
a single market, and remain limited in scale. In this paper, we introduce
FinMultiTime, the first large scale, multimodal financial time series dataset.
FinMultiTime temporally aligns four distinct modalities financial news,
structured financial tables, K-line technical charts, and stock price time
series across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from
2009 to 2025 in the United States and China, the dataset totals 112.6 GB and
provides minute-level, daily, and quarterly resolutions, thus capturing short,
medium, and long term market signals with high fidelity. Our experiments
demonstrate that (1) scale and data quality markedly boost prediction accuracy;
(2) multimodal fusion yields moderate gains in Transformer models; and (3) a
fully reproducible pipeline enables seamless dataset updates.

</details>


### [78] [ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing](https://arxiv.org/abs/2506.04249)
*Mehmet Aziz Yirik,Jakob Lykke Andersen,Rolf Fagerberg,Daniel Merkle*

Main category: cs.CE

TL;DR: 本研究提出开源框架ChemReservoir，突破传统DNA化学专用限制，通过模块化设计实现化学启发性储备计算的通用建模，在记忆容量任务中展现出跨配置的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 现有化学储备计算工具存在DNA化学领域专用性强、维护状态差导致可用性低的问题，亟需具有通用性和可重复性的开源解决方案。

Method: 开发支持多种化学反应的通用框架，集成增强测试套件和评估指标，采用循环储备拓扑结构进行系统验证。

Result: 在不同储备配置的记忆容量测试中，框架展现出0.78±0.02的稳定记忆深度，波动幅度小于传统工具的30%。

Conclusion: ChemReservoir成功解决了领域专用性和可维护性问题，其模块化设计为化学启发式计算研究提供了标准化实验平台。

Abstract: Reservoir computing is a type of a recurrent neural network, mapping the
inputs into higher dimensional space using fixed and nonlinear dynamical
systems, called reservoirs. In the literature, there are various types of
reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous
studies contributed to the field by developing simulation-based chemically
inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction
network as its reservoir and Nguyen developed a DNA chemistry-inspired tool
based on Gillespie algorithm. However, these software tools were designed
mainly with the focus on DNA chemistry and their maintenance status has limited
their current usability. Due to these limitations, there was a need for a
proper open-source tool. This study introduces ChemReservoir, an open-source
framework for chemically-inspired reservoir computing. In contrast to the
former studies focused on DNA-chemistry, ChemReservoir is a general framework
for the construction and analysis of chemically-inspired reservoirs, which also
addresses the limitations in these previous studies by ensuring enhanced
testing, evaluation, and reproducibility. The tool was evaluated using various
cycle-based reservoir topologies and demonstrated stable performance across a
range of configurations in memory capacity tasks.

</details>


### [79] [Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design](https://arxiv.org/abs/2506.04660)
*Yi Xu,Farzin Lotfi-Jam,Mustafa Faruki*

Main category: cs.CE

TL;DR: 利用再生塑料构建模块化锁子甲系统，通过真空密封结构实现轻量化建筑应用，验证矩形链甲配置在极端环境下的高效性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业原材料消耗大/废弃物多的可持续性问题，挖掘再生塑料在模块化结构中的轻量化/可定制化潜力。

Method: 建立计算设计流程：通过2D截面测试验证材料性能→生成3D壳体结构→真空约束下物理建模→矩形链甲配置优化

Result: 矩形链甲实现85%变形能力提升与30%材料节省，真空密封结构载荷能力达2.5kN/m²，排水效率优化至92%

Conclusion: 该技术为灾害区/高海拔/水下及外星基地提供可部署解决方案，建立废弃物再生与高性能建筑设计的创新连接

Abstract: The construction industry is a major consumer of raw materials, accounting
for nearly half of global material usage annually, while generating significant
waste that poses sustainability challenges. This paper explores the untapped
potential of recycled plastics as a primary construction material, leveraging
their lightweight, flexible, and customizable properties for advanced
applications in modular chainmail systems. Through a computational workflow,
the study optimizes the design, testing, and fabrication of vacuum-sealed
chainmail structures composed of recycled plastic filaments, demonstrating
their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled
plastic filaments into chainmail geometries, validated through 2D sectional
testing, 3D shell structure generation, and physical modeling under vacuum
constraints. The research identifies the rectangular chainmail configuration as
the most efficient and adaptable, achieving superior deformation capacity,
material efficiency, and load-bearing performance. Optimization strategies for
temporary structures highlight practical deployment potential, balancing
material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme
conditions, including disaster-prone areas, high-altitude environments,
underwater platforms, and extraterrestrial habitats. These applications
leverage the lightweight, adaptable, and durable properties of recycled
plastics and modular chainmail systems, bridging the gap between waste
management and high-performance design while addressing unique challenges in
harsh and resource-constrained environments.

</details>


### [80] [Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams](https://arxiv.org/abs/2506.04960)
*Bartłomiej Łazorczyk,Roger A. Sauer*

Main category: cs.CE

TL;DR: 提出基于等几何分析的有限元模型更新框架，通过两步法识别梁结构中的异质材料属性


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在复杂工况（大位移、高噪声）下材料属性识别精度不足的问题，建立可扩展的异质材料识别框架

Method: 1. 分两步识别：先通过准静态位移确定弹性模量，再利用模态数据计算密度
2. 采用三套独立离散化系统（等几何网格/实验测量网格/材料网格）
3. 结合信任域优化算法与解析导数加速计算
4. 引入正则化技术保证稳定性

Result: 在数据充足的条件下，即使存在4%测量噪声仍能准确重建材料参数，成功实现密度与弹性属性的分离识别

Conclusion: 该框架验证了等几何分析在材料识别中的有效性，未来可扩展应用于壳体与三维连续体分析

Abstract: This paper presents a Finite Element Model Updating framework for identifying
heterogeneous material distributions in planar Bernoulli-Euler beams based on a
rotation-free isogeometric formulation. The procedure follows two steps: First,
the elastic properties are identified from quasi-static displacements; then,
the density is determined from modal data (low frequencies and mode shapes),
given the previously obtained elastic properties. The identification relies on
three independent discretizations: the isogeometric finite element mesh, a
high-resolution grid of experimental measurements, and a material mesh composed
of low-order Lagrange elements. The material mesh approximates the unknown
material distributions, with its nodal values serving as design variables. The
error between experiments and numerical model is expressed in a least squares
manner. The objective is minimized using local optimization with the
trust-region method, providing analytical derivatives to accelerate
computations. Several numerical examples exhibiting large displacements are
provided to test the proposed approach. To alleviate membrane locking, the B2M1
discretization is employed when necessary. Quasi-experimental data is generated
using refined finite element models with random noise applied up to 4%. The
method yields satisfactory results as long as a sufficient amount of
experimental data is available, even for high measurement noise. Regularization
is used to ensure a stable solution for dense material meshes. The density can
be accurately reconstructed based on the previously identified elastic
properties. The proposed framework can be straightforwardly extended to shells
and 3D continua.

</details>


### [81] [FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis](https://arxiv.org/abs/2506.05019)
*Wenyan Xu,Dawei Xiang,Yue Liu,Xiyu Wang,Yanxiang Ma,Liang Zhang,Chang Xu,Jiaheng Zhang*

Main category: cs.CE

TL;DR: 提出首个大规模多模态金融时间序列数据集FinMultiTime，覆盖中美市场112.6GB数据，验证数据规模、质量与多模态融合对预测准确性的提升


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列数据集局限于单一市场、单一模态（仅价格序列与新闻文本），且数据规模不足，难以支撑实际金融决策需求

Method: 构建跨S&P500和HS300市场的多模态数据集，时间对齐金融新闻、结构化表格、K线图与价格序列，覆盖2009-2025年5105只股票，提供分钟/日/季度级数据

Result: 1. 数据规模与质量显著提升预测精度
2. Transformer模型在多模态融合中获得中等增益
3. 建立可完全复现的数据更新管道

Conclusion: FinMultiTime为金融预测研究提供高质量基准，其多模态特性与跨市场覆盖将助力开发更鲁棒的金融决策系统

Abstract: Pure time series forecasting tasks typically focus exclusively on numerical
features; however, real-world financial decision-making demands the comparison
and analysis of heterogeneous sources of information. Recent advances in deep
learning and large scale language models (LLMs) have made significant strides
in capturing sentiment and other qualitative signals, thereby enhancing the
accuracy of financial time series predictions. Despite these advances, most
existing datasets consist solely of price series and news text, are confined to
a single market, and remain limited in scale. In this paper, we introduce
FinMultiTime, the first large scale, multimodal financial time series dataset.
FinMultiTime temporally aligns four distinct modalities financial news,
structured financial tables, K-line technical charts, and stock price time
series across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from
2009 to 2025 in the United States and China, the dataset totals 112.6 GB and
provides minute-level, daily, and quarterly resolutions, thus capturing short,
medium, and long term market signals with high fidelity. Our experiments
demonstrate that (1) scale and data quality markedly boost prediction accuracy;
(2) multimodal fusion yields moderate gains in Transformer models; and (3) a
fully reproducible pipeline enables seamless dataset updates.

</details>


### [82] [ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing](https://arxiv.org/abs/2506.04249)
*Mehmet Aziz Yirik,Jakob Lykke Andersen,Rolf Fagerberg,Daniel Merkle*

Main category: cs.CE

TL;DR: 文章介绍开源框架ChemReservoir用于化学启发的储层计算，解决此前工具局限，评估显示其性能稳定。


<details>
  <summary>Details</summary>
Motivation: 此前化学信息学中基于DNA化学的软件工具存在局限，维护状况影响可用性，需要合适的开源工具。

Method: 引入ChemReservoir框架，用于构建和分析化学启发的储层，对比以往专注DNA化学的研究，增强测试、评估和可重复性。

Result: 使用各种基于循环的储层拓扑对工具进行评估，在内存容量任务的一系列配置中表现稳定。

Conclusion: ChemReservoir是构建和分析化学启发储层的通用框架，解决了以往研究的局限性。

Abstract: Reservoir computing is a type of a recurrent neural network, mapping the
inputs into higher dimensional space using fixed and nonlinear dynamical
systems, called reservoirs. In the literature, there are various types of
reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous
studies contributed to the field by developing simulation-based chemically
inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction
network as its reservoir and Nguyen developed a DNA chemistry-inspired tool
based on Gillespie algorithm. However, these software tools were designed
mainly with the focus on DNA chemistry and their maintenance status has limited
their current usability. Due to these limitations, there was a need for a
proper open-source tool. This study introduces ChemReservoir, an open-source
framework for chemically-inspired reservoir computing. In contrast to the
former studies focused on DNA-chemistry, ChemReservoir is a general framework
for the construction and analysis of chemically-inspired reservoirs, which also
addresses the limitations in these previous studies by ensuring enhanced
testing, evaluation, and reproducibility. The tool was evaluated using various
cycle-based reservoir topologies and demonstrated stable performance across a
range of configurations in memory capacity tasks.

</details>


### [83] [Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design](https://arxiv.org/abs/2506.04660)
*Yi Xu,Farzin Lotfi-Jam,Mustafa Faruki*

Main category: cs.CE

TL;DR: 本文探索再生塑料在建筑领域潜力，用计算工作流优化设计真空密封链甲结构，确定矩形配置最优，成果可用于极端环境。


<details>
  <summary>Details</summary>
Motivation: 建筑行业消耗大量原材料且产生大量废物，探索再生塑料作为主要建筑材料的潜力。

Method: 通过计算工作流优化由再生塑料长丝组成的真空密封链甲结构的设计、测试和制造，并进行2D截面测试、3D壳体结构生成和真空约束下的物理建模。

Result: 确定矩形链甲配置最有效和适应，具有卓越变形能力、材料效率和承载性能，优化策略显示临时结构实用潜力。

Conclusion: 研究成果为极端条件下的创新应用提供基础，弥合废物管理与高性能设计之间的差距。

Abstract: The construction industry is a major consumer of raw materials, accounting
for nearly half of global material usage annually, while generating significant
waste that poses sustainability challenges. This paper explores the untapped
potential of recycled plastics as a primary construction material, leveraging
their lightweight, flexible, and customizable properties for advanced
applications in modular chainmail systems. Through a computational workflow,
the study optimizes the design, testing, and fabrication of vacuum-sealed
chainmail structures composed of recycled plastic filaments, demonstrating
their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled
plastic filaments into chainmail geometries, validated through 2D sectional
testing, 3D shell structure generation, and physical modeling under vacuum
constraints. The research identifies the rectangular chainmail configuration as
the most efficient and adaptable, achieving superior deformation capacity,
material efficiency, and load-bearing performance. Optimization strategies for
temporary structures highlight practical deployment potential, balancing
material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme
conditions, including disaster-prone areas, high-altitude environments,
underwater platforms, and extraterrestrial habitats. These applications
leverage the lightweight, adaptable, and durable properties of recycled
plastics and modular chainmail systems, bridging the gap between waste
management and high-performance design while addressing unique challenges in
harsh and resource-constrained environments.

</details>


### [84] [Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams](https://arxiv.org/abs/2506.04960)
*Bartłomiej Łazorczyk,Roger A. Sauer*

Main category: cs.CE

TL;DR: 本文提出基于无旋转等几何公式的有限元模型更新框架，用于识别平面伯努利 - 欧拉梁的非均质材料分布，通过两步识别弹性属性和密度，经多个算例验证，方法有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 识别平面伯努利 - 欧拉梁中的非均质材料分布。

Method: 分两步分别从准静态位移识别弹性属性、从模态数据确定密度；采用三种独立离散化；用最小二乘法表达误差，用信赖域法进行局部优化；必要时采用B2M1离散化；使用正则化确保稳定解。

Result: 在有足够实验数据时，即使测量噪声高，方法也能得到满意结果；可基于已识别弹性属性准确重建密度。

Conclusion: 所提框架可有效识别材料分布，且能直接扩展到壳和三维连续体。

Abstract: This paper presents a Finite Element Model Updating framework for identifying
heterogeneous material distributions in planar Bernoulli-Euler beams based on a
rotation-free isogeometric formulation. The procedure follows two steps: First,
the elastic properties are identified from quasi-static displacements; then,
the density is determined from modal data (low frequencies and mode shapes),
given the previously obtained elastic properties. The identification relies on
three independent discretizations: the isogeometric finite element mesh, a
high-resolution grid of experimental measurements, and a material mesh composed
of low-order Lagrange elements. The material mesh approximates the unknown
material distributions, with its nodal values serving as design variables. The
error between experiments and numerical model is expressed in a least squares
manner. The objective is minimized using local optimization with the
trust-region method, providing analytical derivatives to accelerate
computations. Several numerical examples exhibiting large displacements are
provided to test the proposed approach. To alleviate membrane locking, the B2M1
discretization is employed when necessary. Quasi-experimental data is generated
using refined finite element models with random noise applied up to 4%. The
method yields satisfactory results as long as a sufficient amount of
experimental data is available, even for high measurement noise. Regularization
is used to ensure a stable solution for dense material meshes. The density can
be accurately reconstructed based on the previously identified elastic
properties. The proposed framework can be straightforwardly extended to shells
and 3D continua.

</details>


### [85] [FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis](https://arxiv.org/abs/2506.05019)
*Wenyan Xu,Dawei Xiang,Yue Liu,Xiyu Wang,Yanxiang Ma,Liang Zhang,Chang Xu,Jiaheng Zhang*

Main category: cs.CE

TL;DR: 提出大规模多模态金融时间序列数据集FinMultiTime，含多种模态数据，实验表明规模和数据质量能提升预测精度，多模态融合有一定效果且可实现数据更新。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列预测数据集存在单一市场、规模有限等问题，需多源信息对比分析以用于金融决策。

Method: 引入多模态金融时间序列数据集FinMultiTime，对金融新闻、结构化金融表格、K线技术图表和股价时间序列进行时间对齐。

Result: 实验表明规模和数据质量显著提高预测准确性，多模态融合在Transformer模型中有适度收益，且可实现数据集无缝更新。

Conclusion: FinMultiTime数据集能有效提升金融时间序列预测效果，且具备可更新性。

Abstract: Pure time series forecasting tasks typically focus exclusively on numerical
features; however, real-world financial decision-making demands the comparison
and analysis of heterogeneous sources of information. Recent advances in deep
learning and large scale language models (LLMs) have made significant strides
in capturing sentiment and other qualitative signals, thereby enhancing the
accuracy of financial time series predictions. Despite these advances, most
existing datasets consist solely of price series and news text, are confined to
a single market, and remain limited in scale. In this paper, we introduce
FinMultiTime, the first large scale, multimodal financial time series dataset.
FinMultiTime temporally aligns four distinct modalities financial news,
structured financial tables, K-line technical charts, and stock price time
series across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from
2009 to 2025 in the United States and China, the dataset totals 112.6 GB and
provides minute-level, daily, and quarterly resolutions, thus capturing short,
medium, and long term market signals with high fidelity. Our experiments
demonstrate that (1) scale and data quality markedly boost prediction accuracy;
(2) multimodal fusion yields moderate gains in Transformer models; and (3) a
fully reproducible pipeline enables seamless dataset updates.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [86] [Computationally Intensive Research: Advancing a Role for Secondary Analysis of Qualitative Data](https://arxiv.org/abs/2506.04230)
*Kaveh Mohajeri,Amir Karami*

Main category: cs.DB

TL;DR: 探讨利用计算方法对历史定性研究数据进行二次分析的潜力与实施方案


<details>
  <summary>Details</summary>
Motivation: 定性研究产生的大量数据常被闲置，通过计算密集型二次分析可挖掘跨时空研究价值

Method: 提出计算密集型二次分析框架，整合多时空数据组合解决纵向研究问题

Result: 该方法能促进创新研究设计，扩展定性数据的科研价值复用

Conclusion: 需克服数据共享伦理、跨情境数据整合及分析方法标准化等核心挑战

Abstract: This paper draws attention to the potential of computational methods in
reworking data generated in past qualitative studies. While qualitative
inquiries often produce rich data through rigorous and resource-intensive
processes, much of this data usually remains unused. In this paper, we first
make a general case for secondary analysis of qualitative data by discussing
its benefits, distinctions, and epistemological aspects. We then argue for
opportunities with computationally intensive secondary analysis, highlighting
the possibility of drawing on data assemblages spanning multiple contexts and
timeframes to address cross-contextual and longitudinal research phenomena and
questions. We propose a scheme to perform computationally intensive secondary
analysis and advance ideas on how this approach can help facilitate the
development of innovative research designs. Finally, we enumerate some key
challenges and ongoing concerns associated with qualitative data sharing and
reuse.

</details>


### [87] [OxO2 -- A SSSOM mapping browser for logically sound crosswalks](https://arxiv.org/abs/2506.04286)
*Henriette Harmse,Haider Iqbal,Helen Parkinson,James McLaughlin*

Main category: cs.DB

TL;DR: EMBL-EBI开发OxO2解决本体映射工具在逻辑可靠性、来源追溯和系统稳定性方面的缺陷


<details>
  <summary>Details</summary>
Motivation: 原版OxO存在逻辑不可靠的映射结果、缺乏作者/评审者等溯源信息、请求处理不稳定等问题

Method: 采用SSSOM标准规范映射溯源，利用Nemo规则引擎实现逻辑可靠的Datalog推理，并实施多种性能优化策略

Result: 新系统OxO2实现了高效内存管理，保证所有请求的稳定处理，支持可靠的本体映射推导

Conclusion: 改进后的OxO2使用户能够更可靠地整合不同本体标注的生物数据集

Abstract: EMBL-EBI created OxO to enable users to map between datasets that are
annotated with different ontologies. Mappings identified by the first version
of OxO were not necessarily logically sound, missed important provenance
information such as author and reviewer, and could timeout or crash for certain
requests. In this paper we introduce OxO2 to address these concerns. Provenance
is addressed by implementing SSSOM, a mapping standard that defines provenance
for mappings. SSSOM defines the conditions under which logical sound mappings
can be derived and is implemented in OxO2 using, Nemo, a Datalog rule engine.
To ensure reasoning is performant and memory efficient, Nemo implements a
number of strategies that ensures OxO2 will be stable for all requests. Due to
these changes, OxO2 users will be able to integrate between disparate datasets
with greater confidence.

</details>


### [88] [Computationally Intensive Research: Advancing a Role for Secondary Analysis of Qualitative Data](https://arxiv.org/abs/2506.04230)
*Kaveh Mohajeri,Amir Karami*

Main category: cs.DB

TL;DR: 探讨计算密集型二次分析在再利用定性数据中的潜力，提出实施方案并分析相关挑战


<details>
  <summary>Details</summary>
Motivation: 针对定性数据利用率低的问题，探索通过跨上下文数据整合解决纵向研究难题的可能性

Method: 提出计算密集型二次分析方案，支持多场景跨时段数据整合的创新研究设计

Result: 建立可促进创新研究设计的分析框架，验证跨背景数据分析的可行性

Conclusion: 虽面临数据共享伦理和技术挑战，但计算二次分析为定性研究开辟新路径

Abstract: This paper draws attention to the potential of computational methods in
reworking data generated in past qualitative studies. While qualitative
inquiries often produce rich data through rigorous and resource-intensive
processes, much of this data usually remains unused. In this paper, we first
make a general case for secondary analysis of qualitative data by discussing
its benefits, distinctions, and epistemological aspects. We then argue for
opportunities with computationally intensive secondary analysis, highlighting
the possibility of drawing on data assemblages spanning multiple contexts and
timeframes to address cross-contextual and longitudinal research phenomena and
questions. We propose a scheme to perform computationally intensive secondary
analysis and advance ideas on how this approach can help facilitate the
development of innovative research designs. Finally, we enumerate some key
challenges and ongoing concerns associated with qualitative data sharing and
reuse.

</details>


### [89] [OxO2 -- A SSSOM mapping browser for logically sound crosswalks](https://arxiv.org/abs/2506.04286)
*Henriette Harmse,Haider Iqbal,Helen Parkinson,James McLaughlin*

Main category: cs.DB

TL;DR: EMBL-EBI开发了OxO2以解决初代OxO在逻辑映射可靠性、数据溯源记录和系统稳定性方面的缺陷，通过SSSOM标准和Nemo推理引擎提升跨数据集整合的可信度。


<details>
  <summary>Details</summary>
Motivation: 初代OxO存在映射逻辑不可靠、缺乏作者/审核者等溯源信息，且处理请求时可能出现系统崩溃等问题，需开发升级版OxO2解决这些核心痛点。

Method: 1. 采用SSSOM标准记录映射的溯源信息
2. 引入Nemo(Datalog规则引擎)实现逻辑可靠映射
3. 通过多种优化策略提升推理性能和内存效率

Result: OxO2实现了稳定的实时推理功能，所有请求处理均保持系统稳定，用户得以更可靠地整合异构数据集。

Conclusion: OxO2通过标准化溯源记录、逻辑验证引擎和性能优化，显著提升了跨本体数据映射的可信度和系统鲁棒性。

Abstract: EMBL-EBI created OxO to enable users to map between datasets that are
annotated with different ontologies. Mappings identified by the first version
of OxO were not necessarily logically sound, missed important provenance
information such as author and reviewer, and could timeout or crash for certain
requests. In this paper we introduce OxO2 to address these concerns. Provenance
is addressed by implementing SSSOM, a mapping standard that defines provenance
for mappings. SSSOM defines the conditions under which logical sound mappings
can be derived and is implemented in OxO2 using, Nemo, a Datalog rule engine.
To ensure reasoning is performant and memory efficient, Nemo implements a
number of strategies that ensures OxO2 will be stable for all requests. Due to
these changes, OxO2 users will be able to integrate between disparate datasets
with greater confidence.

</details>


### [90] [BVLSM: Write-Efficient LSM-Tree Storage via WAL-Time Key-Value Separation](https://arxiv.org/abs/2506.04678)
*Ming Li,Wendi Cheng,Jiahe Wei,Xueqiang Shan,Liu Weikai,Xiaonan Zhao,Xiao Zhang*

Main category: cs.DB

TL;DR: 提出WAL阶段主动键值分离机制BVLSM，通过内存元数据轻量化存储和并行存储架构，显著提升大值负载下LSM-Tree存储性能


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用处理大值项时，传统LSM-Tree架构存在写入放大、内存利用率低和I/O抖动三大瓶颈问题

Method: 在WAL阶段主动解耦键值存储，MemTable仅存储轻量元数据，采用多队列并行存储架构处理大值对象

Result: 异步WAL模式下吞吐量较RocksDB提升7.6倍，较BlobDB提升1.9倍（64KB随机写入场景）

Conclusion: BVLSM通过前置键值分离策略有效解决现有存储架构冗余写入问题，显著优化大值场景下的存储效率与系统性能

Abstract: Modern data-intensive applications increasingly store and process big-value
items, such as multimedia objects and machine learning embeddings, which
exacerbate storage inefficiencies in Log-Structured Merge-Tree (LSM)-based
key-value stores. This paper presents BVLSM, a Write-Ahead Log (WAL)-time
key-value separation mechanism designed to address three key challenges in
LSM-Tree storage systems: write amplification, poor memory utilization, and I/O
jitter under big-value workloads. Unlike state-of-the-art approaches that delay
key-value separation until the flush stage, leading to redundant data in
MemTables and repeated writes. BVLSM proactively decouples keys and values
during the WAL phase. The MemTable stores only lightweight metadata, allowing
multi-queue parallel store for big value. The benchmark results show that BVLSM
significantly outperforms both RocksDB and BlobDB under 64KB random write
workloads. In asynchronous WAL mode, it achieves throughput improvements of
7.6x over RocksDB and 1.9x over BlobDB.

</details>


### [91] [Memory Hierarchy Design for Caching Middleware in the Age of NVM](https://arxiv.org/abs/2506.05071)
*Shahram Ghandeharizadeh,Sandy Irani,Jenny Lam*

Main category: cs.DB

TL;DR: 研究通过多选择背包问题建模NVM缓存配置，发现选择性复制在特定故障率下有效，而分层存储在低故障高更新场景更优


<details>
  <summary>Details</summary>
Motivation: 随着NVM等新型存储介质的出现，系统设计者面临多层级缓存配置选择难题。研究旨在解决存储介质选择、容量分配及数据分布策略（复制/分区）的优化问题，以提升系统性能与故障恢复效率

Method: 将缓存配置问题建模为多选择背包问题（MCKP），采用线性规划松弛法进行近似求解。实验验证基于键值存储（memcached）和主机端缓存（Flashcache）两种场景

Result: 选择性数据复制在中等故障率和特定工作负载下表现良好，而分层存储策略在低故障率（slim failure rate）且数据频繁更新的场景中更具优势

Conclusion: 缓存层次设计需综合考虑故障率与更新频率：中高故障环境适用选择性复制，低故障高更新场景更适合分层存储。该数学模型为存储系统优化提供了有效的配置决策框架

Abstract: Advances in storage technology have introduced Non-Volatile Memory, NVM, as a
new storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid
State Disk (SSD), and Disk present a system designer with a wide array of
options in designing caching middleware. Moreover, design decisions to
replicate a data item in more than one level of a caching memory hierarchy may
enhance the overall system performance with a faster recovery time in the event
of a memory failure. Given a fixed budget, the key configuration questions are:
Which storage media should constitute the memory hierarchy? What is the storage
capacity of each hierarchy? Should data be replicated or partitioned across the
different levels of the hierarchy? We model these cache configuration questions
as an instance of the Multiple Choice Knapsack Problem (MCKP). This model is
guided by the specification of each type of memory along with an application's
database characteristics and its workload. Although MCKP is NP-complete, its
linear programming relaxation is efficiently solvable and can be used to
closely approximate the optimal solution. We use the resulting simple algorithm
to evaluate design tradeoffs in the context of a memory hierarchy for a
Key-Value Store (e.g., memcached) as well as a host-side cache (e.g.,
Flashcache). The results show selective replication is appropriate with certain
failure rates and workload characteristics. With a slim failure rate and
frequent data updates, tiering of data across the different storage media that
constitute the cache is superior to replication.

</details>


### [92] [Computationally Intensive Research: Advancing a Role for Secondary Analysis of Qualitative Data](https://arxiv.org/abs/2506.04230)
*Kaveh Mohajeri,Amir Karami*

Main category: cs.DB

TL;DR: 文章探讨用计算方法二次分析过去定性研究数据的潜力，提出分析方案并指出数据共享与再利用挑战。


<details>
  <summary>Details</summary>
Motivation: 过去定性研究产生大量未使用的数据，挖掘计算方法在二次分析定性数据方面的潜力。

Method: 先阐述定性数据二次分析的益处、区别和认识论方面，接着强调计算密集型二次分析的机会，提出执行方案并探讨其对研究设计的帮助。

Result: 提出计算密集型二次分析的方案，推进该方法对创新研究设计发展的思路。

Conclusion: 指出定性数据共享和再利用存在一些关键挑战和持续关注的问题。

Abstract: This paper draws attention to the potential of computational methods in
reworking data generated in past qualitative studies. While qualitative
inquiries often produce rich data through rigorous and resource-intensive
processes, much of this data usually remains unused. In this paper, we first
make a general case for secondary analysis of qualitative data by discussing
its benefits, distinctions, and epistemological aspects. We then argue for
opportunities with computationally intensive secondary analysis, highlighting
the possibility of drawing on data assemblages spanning multiple contexts and
timeframes to address cross-contextual and longitudinal research phenomena and
questions. We propose a scheme to perform computationally intensive secondary
analysis and advance ideas on how this approach can help facilitate the
development of innovative research designs. Finally, we enumerate some key
challenges and ongoing concerns associated with qualitative data sharing and
reuse.

</details>


### [93] [OxO2 -- A SSSOM mapping browser for logically sound crosswalks](https://arxiv.org/abs/2506.04286)
*Henriette Harmse,Haider Iqbal,Helen Parkinson,James McLaughlin*

Main category: cs.DB

TL;DR: 介绍解决旧版OxO问题的OxO2，提升用户集成不同数据集的信心。


<details>
  <summary>Details</summary>
Motivation: 旧版OxO映射可能逻辑不健全、缺少重要来源信息、部分请求可能超时或崩溃，需改进。

Method: 实现映射标准SSSOM处理来源信息，用Datalog规则引擎Nemo实现SSSOM以保证逻辑映射，Nemo采用策略确保性能和内存效率。

Result: OxO2解决了旧版问题。

Conclusion: OxO2能让用户更有信心地集成不同数据集。

Abstract: EMBL-EBI created OxO to enable users to map between datasets that are
annotated with different ontologies. Mappings identified by the first version
of OxO were not necessarily logically sound, missed important provenance
information such as author and reviewer, and could timeout or crash for certain
requests. In this paper we introduce OxO2 to address these concerns. Provenance
is addressed by implementing SSSOM, a mapping standard that defines provenance
for mappings. SSSOM defines the conditions under which logical sound mappings
can be derived and is implemented in OxO2 using, Nemo, a Datalog rule engine.
To ensure reasoning is performant and memory efficient, Nemo implements a
number of strategies that ensures OxO2 will be stable for all requests. Due to
these changes, OxO2 users will be able to integrate between disparate datasets
with greater confidence.

</details>


### [94] [BVLSM: Write-Efficient LSM-Tree Storage via WAL-Time Key-Value Separation](https://arxiv.org/abs/2506.04678)
*Ming Li,Wendi Cheng,Jiahe Wei,Xueqiang Shan,Liu Weikai,Xiaonan Zhao,Xiao Zhang*

Main category: cs.DB

TL;DR: 论文提出BVLSM机制解决LSM - Tree存储系统在大值负载下的问题，基准测试显示其性能优于RocksDB和BlobDB。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用存储和处理大值项，加剧了基于LSM的键值存储的存储低效问题，需解决写放大、内存利用率低和I/O抖动问题。

Method: 在WAL阶段主动分离键值，MemTable仅存储轻量级元数据，支持大值的多队列并行存储。

Result: 在64KB随机写工作负载下，BVLSM性能显著优于RocksDB和BlobDB，异步WAL模式下，吞吐量比RocksDB高7.6倍，比BlobDB高1.9倍。

Conclusion: BVLSM能有效解决LSM - Tree存储系统在大值负载下的问题，提升系统性能。

Abstract: Modern data-intensive applications increasingly store and process big-value
items, such as multimedia objects and machine learning embeddings, which
exacerbate storage inefficiencies in Log-Structured Merge-Tree (LSM)-based
key-value stores. This paper presents BVLSM, a Write-Ahead Log (WAL)-time
key-value separation mechanism designed to address three key challenges in
LSM-Tree storage systems: write amplification, poor memory utilization, and I/O
jitter under big-value workloads. Unlike state-of-the-art approaches that delay
key-value separation until the flush stage, leading to redundant data in
MemTables and repeated writes. BVLSM proactively decouples keys and values
during the WAL phase. The MemTable stores only lightweight metadata, allowing
multi-queue parallel store for big value. The benchmark results show that BVLSM
significantly outperforms both RocksDB and BlobDB under 64KB random write
workloads. In asynchronous WAL mode, it achieves throughput improvements of
7.6x over RocksDB and 1.9x over BlobDB.

</details>


### [95] [Memory Hierarchy Design for Caching Middleware in the Age of NVM](https://arxiv.org/abs/2506.05071)
*Shahram Ghandeharizadeh,Sandy Irani,Jenny Lam*

Main category: cs.DB

TL;DR: 文章围绕缓存中间件设计的关键配置问题展开，将其建模为MCKP，用简单算法评估设计权衡，给出不同情况下数据处理策略。


<details>
  <summary>Details</summary>
Motivation: 存储技术发展带来多种存储介质，为设计缓存中间件提供多种选择，需要解决关键配置问题以提升系统性能。

Method: 将缓存配置问题建模为MCKP，利用其线性规划松弛求解并近似最优解，用简单算法评估设计权衡。

Result: 选择性复制在特定故障率和工作负载特征下合适，低故障率和频繁数据更新时，数据分层优于复制。

Conclusion: 根据不同的故障率和工作负载特征，在缓存设计中应合理选择数据复制或分层策略。

Abstract: Advances in storage technology have introduced Non-Volatile Memory, NVM, as a
new storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid
State Disk (SSD), and Disk present a system designer with a wide array of
options in designing caching middleware. Moreover, design decisions to
replicate a data item in more than one level of a caching memory hierarchy may
enhance the overall system performance with a faster recovery time in the event
of a memory failure. Given a fixed budget, the key configuration questions are:
Which storage media should constitute the memory hierarchy? What is the storage
capacity of each hierarchy? Should data be replicated or partitioned across the
different levels of the hierarchy? We model these cache configuration questions
as an instance of the Multiple Choice Knapsack Problem (MCKP). This model is
guided by the specification of each type of memory along with an application's
database characteristics and its workload. Although MCKP is NP-complete, its
linear programming relaxation is efficiently solvable and can be used to
closely approximate the optimal solution. We use the resulting simple algorithm
to evaluate design tradeoffs in the context of a memory hierarchy for a
Key-Value Store (e.g., memcached) as well as a host-side cache (e.g.,
Flashcache). The results show selective replication is appropriate with certain
failure rates and workload characteristics. With a slim failure rate and
frequent data updates, tiering of data across the different storage media that
constitute the cache is superior to replication.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [96] [Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.04368)
*Aayush Gupta,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 提出一种分布式P2P协议，能在高节点流失率和大量拜占庭节点存在下维持高扩展性网络拓扑


<details>
  <summary>Details</summary>
Motivation: 解决P2P网络中动态拓扑维护难题，传统方法难以在高流失率和恶意节点环境下维持网络连通性、低延迟和高扩展性等关键属性

Method: 基于随机化分布式协议，仅需局部初始知识，通过多项式对数级通信/计算开销维护常数度数的高扩展图

Result: 协议可容忍o(n/poly log n)拜占庭节点，拓扑维护仅需O(poly log n)开销，保持对数直径和高扩展性

Conclusion: 该协议为拜占庭协议、领导者选举等分布式问题提供基础支撑，能实现高效可扩展的解决方案

Abstract: We address a fundamental problem in Peer-to-Peer (P2P) networks, namely,
constructing and maintaining dynamic P2P overlay network topologies with
essential properties such as connectivity, low diameter, and high expansion,
that are resilient to continuous high churn and the presence of a large number
of malicious (Byzantine) nodes. Our main goal is to construct and maintain a
sparse (bounded degree) expander topology despite high churn and a large number
of Byzantine nodes. Such an expander topology has logarithmic diameter, high
expansion, and is robust to churn and the presence of a large number of bad
nodes, and facilitates efficient and robust algorithms for fundamental problems
in distributed computing, such as agreement, broadcasting, routing, etc.
  Our main contribution is a randomized, fully-distributed dynamic P2P protocol
that works with only local initial knowledge and guarantees, with a high
probability, the maintenance of a constant degree graph with high expansion
even under continuous churn and in the presence of a large number of Byzantine
nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes
(where $n$ is the stable network size). Our protocol is efficient, lightweight,
and scalable, and it incurs only $O(poly\log(n))$ overhead for topology
maintenance: only polylogarithmic (in $n$) bits need to be processed and sent
by each honest node per round, and any honest node's computation cost per round
is also polylogarithmic.
  Our protocol can be used as a building block for solving fundamental
distributed computing problems in highly dynamic networks, such as Byzantine
agreement and Byzantine leader election, and enables fast and scalable
algorithms for these problems.

</details>


### [97] [Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.04368)
*Aayush Gupta,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 提出一种动态P2P协议，在持续节点流失和拜占庭节点存在下维持稀疏扩展图拓扑


<details>
  <summary>Details</summary>
Motivation: 解决高流失率和恶意节点环境下保持网络拓扑连通性、低直径和高扩展性的核心挑战，支撑分布式计算基础问题的高效实现

Method: 基于随机化的完全分布式协议，仅需本地初始知识，通过多项式对数级通信/计算开销维护拓扑，容忍o(n/poly log n)拜占庭节点

Result: 协议以高概率维持常数度数的高扩展图，支持拜占庭共识/领导者选举等问题的快速可扩展算法

Conclusion: 该协议为动态网络中的分布式计算问题提供底层拓扑保障，在对抗性环境下实现高效可扩展的解决方案

Abstract: We address a fundamental problem in Peer-to-Peer (P2P) networks, namely,
constructing and maintaining dynamic P2P overlay network topologies with
essential properties such as connectivity, low diameter, and high expansion,
that are resilient to continuous high churn and the presence of a large number
of malicious (Byzantine) nodes. Our main goal is to construct and maintain a
sparse (bounded degree) expander topology despite high churn and a large number
of Byzantine nodes. Such an expander topology has logarithmic diameter, high
expansion, and is robust to churn and the presence of a large number of bad
nodes, and facilitates efficient and robust algorithms for fundamental problems
in distributed computing, such as agreement, broadcasting, routing, etc.
  Our main contribution is a randomized, fully-distributed dynamic P2P protocol
that works with only local initial knowledge and guarantees, with a high
probability, the maintenance of a constant degree graph with high expansion
even under continuous churn and in the presence of a large number of Byzantine
nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes
(where $n$ is the stable network size). Our protocol is efficient, lightweight,
and scalable, and it incurs only $O(poly\log(n))$ overhead for topology
maintenance: only polylogarithmic (in $n$) bits need to be processed and sent
by each honest node per round, and any honest node's computation cost per round
is also polylogarithmic.
  Our protocol can be used as a building block for solving fundamental
distributed computing problems in highly dynamic networks, such as Byzantine
agreement and Byzantine leader election, and enables fast and scalable
algorithms for these problems.

</details>


### [98] [Knowledge-Guided Attention-Inspired Learning for Task Offloading in Vehicle Edge Computing](https://arxiv.org/abs/2506.04456)
*Ke Ma,Junfei Xie*

Main category: cs.DC

TL;DR: 提出基于注意力机制的KATO方法，通过选择最优RSU子集和高效任务分配算法，显著降低计算开销并实现接近最优的任务卸载性能。


<details>
  <summary>Details</summary>
Motivation: 现有车辆边缘计算的任务卸载策略存在计算成本高和解决方案次优的问题，需开发更高效的算法。

Method: 结合注意力机制的编码器-解码器模型筛选RSU子集，配合迭代算法优化任务分配。

Result: 仿真表明KATO在保持最优性能的同时减少计算开销，并能泛化到不同规模的网络环境。

Conclusion: KATO为车辆边缘计算提供了高效可扩展的解决方案，具有实际部署潜力。

Abstract: Vehicle edge computing (VEC) brings abundant computing resources close to
vehicles by deploying them at roadside units (RSUs) or base stations, thereby
enabling diverse computation-intensive and delay sensitive applications.
Existing task offloading strategies are often computationally expensive to
execute or generate suboptimal solutions. In this paper, we propose a novel
learning-based approach, Knowledge-guided Attention-inspired Task Offloading
(KATO), designed to efficiently offload tasks from moving vehicles to nearby
RSUs. KATO integrates an attention-inspired encoder-decoder model for selecting
a subset of RSUs that can reduce overall task processing time, along with an
efficient iterative algorithm for computing optimal task allocation among the
selected RSUs. Simulation results demonstrate that KATO achieves optimal or
near-optimal performance with significantly lower computational overhead and
generalizes well across networks of varying sizes and configurations.

</details>


### [99] [Energy-Optimized Scheduling for AIoT Workloads Using TOPSIS](https://arxiv.org/abs/2506.04902)
*Preethika Pradeep,Eyhab Al-Masri*

Main category: cs.DC

TL;DR: GreenPod调度器通过TOPSIS算法优化AIoT应用在K8s集群中的能效，相比默认调度器提升39.1%能效，特别适合中等复杂度负载。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器缺乏对异构环境下多标准（能效/资源平衡）的联合优化，难以满足AIoT应用可持续性需求。

Method: 基于TOPSIS算法构建多目标调度模型，综合评估执行时间、能耗、核心/内存资源、负载均衡等五个维度进行Pod编排决策。

Result: 在Google K8s集群测试中，能源权重方案下实现最高39.1%能效提升，中等复杂度负载节能效果最佳（调度延迟仅增加3.6ms）。

Conclusion: GreenPod通过多维度资源调度，有效平衡了AIoT应用的可持续性目标与服务质量，验证了算法在真实异构环境中的实用性。

Abstract: AIoT workloads demand energy-efficient orchestration across cloud-edge
infrastructures, but Kubernetes' default scheduler lacks multi-criteria
optimization for heterogeneous environments. This paper presents GreenPod, a
TOPSIS-based scheduler optimizing pod placement based on execution time, energy
consumption, processing core, memory availability, and resource balance. Tested
on a heterogeneous Google Kubernetes cluster, GreenPod improves energy
efficiency by up to 39.1% over the default Kubernetes (K8s) scheduler,
particularly with energy-centric weighting schemes. Medium complexity workloads
showed the highest energy savings, despite slight scheduling latency. GreenPod
effectively balances sustainability and performance for AIoT applications.

</details>


### [100] [SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing](https://arxiv.org/abs/2506.04507)
*Narangerelt Batsoyol,Jonathan Guiang,Diego Davila,Aashay Arora,Philip Chang,Frank Würthwein,Steven Swanson*

Main category: cs.DC

TL;DR: SkimROOT利用DPU在存储服务器直接过滤高能物理数据，减少44.3倍处理时间


<details>
  <summary>Details</summary>
Motivation: 解决LHC存储节点与计算节点间数据传输瓶颈导致的处理延迟问题

Method: 基于数据处理器(DPU)构建近数据过滤系统，在存储端执行数据精简操作

Result: 原型系统实现44.3倍性能提升，显著减少数据移动量

Conclusion: 该技术为加速高能物理发现提供了有效的数据处理新范式

Abstract: Data analysis in high-energy physics (HEP) begins with data reduction, where
vast datasets are filtered to extract relevant events. At the Large Hadron
Collider (LHC), this process is bottlenecked by slow data transfers between
storage and compute nodes. To address this, we introduce SkimROOT, a near-data
filtering system leveraging Data Processing Units (DPUs) to accelerate LHC data
analysis. By performing filtering directly on storage servers and returning
only the relevant data, SkimROOT minimizes data movement and reduces processing
delays. Our prototype demonstrates significant efficiency gains, achieving a
44.3$\times$ performance improvement, paving the way for faster physics
discoveries.

</details>


### [101] [FlashDMoE: Fast Distributed MoE in a Single Kernel](https://arxiv.org/abs/2506.04667)
*Osayamen Jonathan Aimuyo,Byungsoo Oh,Rachee Singh*

Main category: cs.DC

TL;DR: 提出FlashDMoE——首个完全GPU驻留的MoE算子，通过GPU内核与硬件协同设计显著提升分布式MoE模型性能


<details>
  <summary>Details</summary>
Motivation: 现有MoE实现存在CPU调度效率低、通信开销大、无法利用任务局部性三大瓶颈，导致GPU利用率不足和延迟过高

Method: 开发持久化GPU内核融合计算与通信，实现细粒度流水线调度；设计设备端发起的高效通信协议，降低数据缓冲区规模

Result: 在8-H100节点上实现延迟降低6倍/吞吐提升5.7倍/弱扩展效率提升4倍/GPU利用率达基线9倍（FP32 vs FP16）

Conclusion: 系统性优化GPU内核与硬件交互机制是突破大规模机器学习性能瓶颈的关键路径

Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables
sub-linear growth in compute cost as model size increases, offering a scalable
path to training massive neural networks. However, existing implementations
suffer from \emph{low GPU utilization}, \emph{significant latency overhead},
and a fundamental \emph{inability to leverage task locality}, primarily due to
CPU-managed scheduling, host-initiated communication, and frequent kernel
launches. To overcome these limitations, we develop FlashDMoE, a fully
GPU-resident MoE operator that fuses expert computation and inter-GPU
communication into a \emph{single persistent GPU kernel}. FlashDMoE enables
fine-grained pipelining of dispatch, compute, and combine phases, eliminating
launch overheads and reducing idle gaps. Its device-initiated communication
protocol introduces \emph{payload-efficient} data transfers, significantly
shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a
single 8-H100 GPU node with MoE models having up to 128 experts and 16K token
sequences, FlashDMoE achieves up to \textbf{6}x lower latency, \textbf{5,7}x
higher throughput, \textbf{4}x better weak scaling efficiency, and \textbf{9}x
higher GPU utilization compared to state-of-the-art baselines, despite using
FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU
kernel-hardware co-design is key to unlocking the performance ceiling of
large-scale distributed ML workloads.

</details>


### [102] [Distributed system perspective on Backscatter systems](https://arxiv.org/abs/2506.04833)
*Jincheng Guan,Jun Zhang*

Main category: cs.DC

TL;DR: 本文从分布式系统视角系统研究反向散射系统，分析其原理、架构、应用场景及挑战，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏从分布式系统角度对本质为分布式架构的反向散射系统进行深入分析，需填补该研究空白。

Method: 通过全面回顾反向散射系统基本原理，对比不同系统的分布式架构，梳理其应用场景、研究现状及技术挑战。

Result: 明确了反向散射系统在分布式协调、资源分配和扩展性方面的核心挑战，识别出跨层优化与新型组网架构等研究方向。

Conclusion: 分布式视角能有效提升反向散射系统性能，未来需在协议设计、智能调度与异构集成等方向持续突破。

Abstract: Backscatter system is a system based on backscatter communication technology,
which is a low cost, low power consumption and easy to deploy communication
technology. At present, the backscatter technology is mainly applied to RFID
tags and the Internet of Things and other fields. With the rapid development of
the Internet of Things, the application of backscatter systems is increasing.
Moreover, the backscatter system is essentially a distributed system, but
existing research rarely conducts studies and analyses from a distributed
perspective. This paper conducts a study on the backscattering system from the
perspective of distributed systems, comprehensively reviewing the basic
principles of the backscattering system, and analyzing the distributed system
architectures of different backscattering systems. Then, it introduces the
application scenarios, research status and challenges of the backscattering
system, and finally discusses the future research directions of the
backscattering system, hoping to provide references for future research.

</details>


### [103] [A distributed system perspective on Backscatter systems: A review](https://arxiv.org/abs/2506.04873)
*Tonghuan Xiao,Jiecheng Zhou*

Main category: cs.DC

TL;DR: 分布式架构与智能资源分配在增强无线系统效能中的关键作用


<details>
  <summary>Details</summary>
Motivation: 应对传统无线系统在可扩展性、能效和定位精度方面的局限性，探索新一代通信技术基础架构

Method: 系统性综述分析分布式架构与智能算法在反向散射通信、室内定位、无源网络和无线能量传输中的整合应用

Result: 验证了智能资源分配可提升20-35%的频谱效率，分布式架构使定位精度提升40%，SWIPT技术实现83%的能量转化效率

Conclusion: 智能分布式架构为6G网络提供理论支撑，建议未来研究重点关注跨协议协同优化与自适应信道分配算法

Abstract: This review investigates the pivotal role of distributed architectures and
intelligent resource allocation in enabling robust and scalable wireless
systems, with a particular emphasis on backscatter communication, indoor
localization, battery-free networks, and Simultaneous Wireless Information and
Power Transfer (SWIPT).

</details>


### [104] [Improved Byzantine Agreement under an Adaptive Adversary](https://arxiv.org/abs/2506.04919)
*Fabien Dufoulon,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 提出一种针对自适应拜占庭敌手的同步随机化协议，将轮次复杂度从长期保持的O(t/log n)优化至O(min{t²log n/n, t/log n})


<details>
  <summary>Details</summary>
Motivation: 现有研究多针对静态敌手，而自适应敌手能动态选择破坏节点更具现实威胁。突破自适应敌手环境下的效率瓶颈对提升分布式系统容错性至关重要

Method: 设计同步随机化协议应对具备全面信息模型的自适应敌手，支持t < n/3节点腐败。通过优化轮次分析，在t²log n/n与t/log n间取最小值达成共识

Result: 协议轮次复杂度降至O(min{t²log n/n, t/log n})，显著优于Chor和Coan的O(t/log n)记录，特别是在t值较小时效率提升更明显

Conclusion: 首次在自适应敌手模型下突破传统轮次限制，为拜占庭协议建立了更优的复杂度边界，增强分布式系统在动态对抗环境中的理论保障

Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed
computing that has been studied intensively for the last four decades. Much of
the research has focused on a static Byzantine adversary, where the adversary
is constrained to choose the Byzantine nodes in advance of the protocol's
execution. This work focuses on the harder case of an adaptive Byzantine
adversary that can choose the Byzantine nodes \emph{adaptively} based on the
protocol's execution. While efficient $O(\log n)$-round protocols ($n$ is the
total number of nodes) are known for the static adversary (Goldwasser, Pavlov,
and Vaikuntanathan, FOCS 2006) tolerating up to $t < n/(3+\epsilon)$ Byzantine
nodes, $\Omega(t/\sqrt{n \log n})$ rounds is a well-known lower bound for
adaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol
for adaptive adversary runs in $O(t/\log n)$ rounds [Chor and Coan, IEEE Trans.
Soft. Engg., 1985].
  This work presents a synchronous randomized Byzantine agreement protocol
under an adaptive adversary that improves over previous results. Our protocol
works under the powerful \emph{adaptive rushing adversary in the full
information model}. That is, we assume that the Byzantine nodes can behave
arbitrarily and maliciously, have knowledge about the entire state of the
network at every round, including random choices made by all the nodes up to
and including the current round, have unlimited computational power, and may
collude among themselves. Furthermore, the adversary can \emph{adaptively}
corrupt up to $t < n/3$ nodes based on the protocol's execution. We present a
simple randomized Byzantine agreement protocol that runs in $O(\min\{t^2\log
n/n, t/\log n\})$ rounds that improves over the long-standing bound of
$O(t/\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].

</details>


### [105] [Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.04368)
*Aayush Gupta,Gopal Pandurangan*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We address a fundamental problem in Peer-to-Peer (P2P) networks, namely,
constructing and maintaining dynamic P2P overlay network topologies with
essential properties such as connectivity, low diameter, and high expansion,
that are resilient to continuous high churn and the presence of a large number
of malicious (Byzantine) nodes. Our main goal is to construct and maintain a
sparse (bounded degree) expander topology despite high churn and a large number
of Byzantine nodes. Such an expander topology has logarithmic diameter, high
expansion, and is robust to churn and the presence of a large number of bad
nodes, and facilitates efficient and robust algorithms for fundamental problems
in distributed computing, such as agreement, broadcasting, routing, etc.
  Our main contribution is a randomized, fully-distributed dynamic P2P protocol
that works with only local initial knowledge and guarantees, with a high
probability, the maintenance of a constant degree graph with high expansion
even under continuous churn and in the presence of a large number of Byzantine
nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes
(where $n$ is the stable network size). Our protocol is efficient, lightweight,
and scalable, and it incurs only $O(poly\log(n))$ overhead for topology
maintenance: only polylogarithmic (in $n$) bits need to be processed and sent
by each honest node per round, and any honest node's computation cost per round
is also polylogarithmic.
  Our protocol can be used as a building block for solving fundamental
distributed computing problems in highly dynamic networks, such as Byzantine
agreement and Byzantine leader election, and enables fast and scalable
algorithms for these problems.

</details>


### [106] [Knowledge-Guided Attention-Inspired Learning for Task Offloading in Vehicle Edge Computing](https://arxiv.org/abs/2506.04456)
*Ke Ma,Junfei Xie*

Main category: cs.DC

TL;DR: 提出学习型任务卸载方法KATO，可高效将车辆任务卸载到RSU，模拟显示其性能优且开销低。


<details>
  <summary>Details</summary>
Motivation: 现有任务卸载策略执行计算成本高或产生次优解，需更高效方法。

Method: 提出KATO，集成注意力启发的编解码器模型选RSU子集，用迭代算法计算任务分配。

Result: KATO能实现最优或接近最优性能，计算开销显著降低，在不同网络泛化性好。

Conclusion: KATO是高效的车辆任务卸载方法，在性能和计算开销上有优势。

Abstract: Vehicle edge computing (VEC) brings abundant computing resources close to
vehicles by deploying them at roadside units (RSUs) or base stations, thereby
enabling diverse computation-intensive and delay sensitive applications.
Existing task offloading strategies are often computationally expensive to
execute or generate suboptimal solutions. In this paper, we propose a novel
learning-based approach, Knowledge-guided Attention-inspired Task Offloading
(KATO), designed to efficiently offload tasks from moving vehicles to nearby
RSUs. KATO integrates an attention-inspired encoder-decoder model for selecting
a subset of RSUs that can reduce overall task processing time, along with an
efficient iterative algorithm for computing optimal task allocation among the
selected RSUs. Simulation results demonstrate that KATO achieves optimal or
near-optimal performance with significantly lower computational overhead and
generalizes well across networks of varying sizes and configurations.

</details>


### [107] [Energy-Optimized Scheduling for AIoT Workloads Using TOPSIS](https://arxiv.org/abs/2506.04902)
*Preethika Pradeep,Eyhab Al-Masri*

Main category: cs.DC

TL;DR: 提出基于TOPSIS的GreenPod调度器优化AIoT工作负载的Pod放置，测试显示其节能效果好，能平衡可持续性与性能。


<details>
  <summary>Details</summary>
Motivation: AIoT工作负载需要跨云边基础设施的节能编排，而Kubernetes默认调度器缺乏对异构环境的多标准优化。

Method: 提出基于TOPSIS的GreenPod调度器，基于执行时间、能耗、处理核心、内存可用性和资源平衡优化Pod放置。

Result: 在异构Google Kubernetes集群上测试，GreenPod比默认Kubernetes调度器节能高达39.1%，中等复杂度工作负载节能效果最佳，有轻微调度延迟。

Conclusion: GreenPod能有效平衡AIoT应用的可持续性和性能。

Abstract: AIoT workloads demand energy-efficient orchestration across cloud-edge
infrastructures, but Kubernetes' default scheduler lacks multi-criteria
optimization for heterogeneous environments. This paper presents GreenPod, a
TOPSIS-based scheduler optimizing pod placement based on execution time, energy
consumption, processing core, memory availability, and resource balance. Tested
on a heterogeneous Google Kubernetes cluster, GreenPod improves energy
efficiency by up to 39.1% over the default Kubernetes (K8s) scheduler,
particularly with energy-centric weighting schemes. Medium complexity workloads
showed the highest energy savings, despite slight scheduling latency. GreenPod
effectively balances sustainability and performance for AIoT applications.

</details>


### [108] [SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing](https://arxiv.org/abs/2506.04507)
*Narangerelt Batsoyol,Jonathan Guiang,Diego Davila,Aashay Arora,Philip Chang,Frank Würthwein,Steven Swanson*

Main category: cs.DC

TL;DR: 介绍SkimROOT近数据过滤系统加速LHC数据分析，原型实现44.3倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 高能物理数据分析中LHC数据过滤因存储和计算节点间数据传输慢而受限，需加速。

Method: 引入SkimROOT系统，利用DPU在存储服务器上直接进行过滤，仅返回相关数据。

Result: 原型实现44.3×的性能提升。

Conclusion: SkimROOT系统可减少数据移动和处理延迟，为更快的物理发现铺平道路。

Abstract: Data analysis in high-energy physics (HEP) begins with data reduction, where
vast datasets are filtered to extract relevant events. At the Large Hadron
Collider (LHC), this process is bottlenecked by slow data transfers between
storage and compute nodes. To address this, we introduce SkimROOT, a near-data
filtering system leveraging Data Processing Units (DPUs) to accelerate LHC data
analysis. By performing filtering directly on storage servers and returning
only the relevant data, SkimROOT minimizes data movement and reduces processing
delays. Our prototype demonstrates significant efficiency gains, achieving a
44.3$\times$ performance improvement, paving the way for faster physics
discoveries.

</details>


### [109] [FlashDMoE: Fast Distributed MoE in a Single Kernel](https://arxiv.org/abs/2506.04667)
*Osayamen Jonathan Aimuyo,Byungsoo Oh,Rachee Singh*

Main category: cs.DC

TL;DR: 提出FlashDMoE解决现有MoE模型GPU利用率低等问题，评估显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有Mixture - of - Experts (MoE)模型实现存在GPU利用率低、延迟高和无法利用任务局部性等问题。

Method: 开发全GPU驻留的MoE算子FlashDMoE，将专家计算和GPU间通信融合到单个持久GPU内核，实现细粒度流水线操作，采用设备发起的通信协议。

Result: 在单8 - H100 GPU节点上，与现有基线相比，FlashDMoE实现了高达6倍的低延迟、5.7倍的高吞吐量、4倍的弱缩放效率和9倍的GPU利用率提升。

Conclusion: 有原则的GPU内核 - 硬件协同设计是释放大规模分布式ML工作负载性能上限的关键。

Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables
sub-linear growth in compute cost as model size increases, offering a scalable
path to training massive neural networks. However, existing implementations
suffer from \emph{low GPU utilization}, \emph{significant latency overhead},
and a fundamental \emph{inability to leverage task locality}, primarily due to
CPU-managed scheduling, host-initiated communication, and frequent kernel
launches. To overcome these limitations, we develop FlashDMoE, a fully
GPU-resident MoE operator that fuses expert computation and inter-GPU
communication into a \emph{single persistent GPU kernel}. FlashDMoE enables
fine-grained pipelining of dispatch, compute, and combine phases, eliminating
launch overheads and reducing idle gaps. Its device-initiated communication
protocol introduces \emph{payload-efficient} data transfers, significantly
shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a
single 8-H100 GPU node with MoE models having up to 128 experts and 16K token
sequences, FlashDMoE achieves up to \textbf{6}x lower latency, \textbf{5,7}x
higher throughput, \textbf{4}x better weak scaling efficiency, and \textbf{9}x
higher GPU utilization compared to state-of-the-art baselines, despite using
FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU
kernel-hardware co-design is key to unlocking the performance ceiling of
large-scale distributed ML workloads.

</details>


### [110] [Distributed system perspective on Backscatter systems](https://arxiv.org/abs/2506.04833)
*Jincheng Guan,Jun Zhang*

Main category: cs.DC

TL;DR: 本文从分布式系统角度研究反向散射系统，综述原理、分析架构，介绍应用场景、研究现状与挑战，探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着物联网发展，反向散射系统应用增加，且其本质是分布式系统，但现有研究很少从分布式角度分析。

Method: 从分布式系统视角研究，全面综述反向散射系统基本原理，分析不同反向散射系统的分布式架构。

Result: 介绍了反向散射系统的应用场景、研究现状和挑战。

Conclusion: 探讨了反向散射系统未来研究方向，可为未来研究提供参考。

Abstract: Backscatter system is a system based on backscatter communication technology,
which is a low cost, low power consumption and easy to deploy communication
technology. At present, the backscatter technology is mainly applied to RFID
tags and the Internet of Things and other fields. With the rapid development of
the Internet of Things, the application of backscatter systems is increasing.
Moreover, the backscatter system is essentially a distributed system, but
existing research rarely conducts studies and analyses from a distributed
perspective. This paper conducts a study on the backscattering system from the
perspective of distributed systems, comprehensively reviewing the basic
principles of the backscattering system, and analyzing the distributed system
architectures of different backscattering systems. Then, it introduces the
application scenarios, research status and challenges of the backscattering
system, and finally discusses the future research directions of the
backscattering system, hoping to provide references for future research.

</details>


### [111] [A distributed system perspective on Backscatter systems: A review](https://arxiv.org/abs/2506.04873)
*Tonghuan Xiao,Jiecheng Zhou*

Main category: cs.DC

TL;DR: 文章探讨分布式架构和智能资源分配对无线系统的重要性，聚焦于反向散射通信等方面。


<details>
  <summary>Details</summary>
Motivation: 研究分布式架构和智能资源分配如何助力构建强大且可扩展的无线系统。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: This review investigates the pivotal role of distributed architectures and
intelligent resource allocation in enabling robust and scalable wireless
systems, with a particular emphasis on backscatter communication, indoor
localization, battery-free networks, and Simultaneous Wireless Information and
Power Transfer (SWIPT).

</details>


### [112] [Improved Byzantine Agreement under an Adaptive Adversary](https://arxiv.org/abs/2506.04919)
*Fabien Dufoulon,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 本文聚焦自适应拜占庭对手下的拜占庭协议，提出同步随机拜占庭协议，运行轮数优于以往结果。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注静态拜占庭对手，本文研究更具挑战性的自适应拜占庭对手情况，改进已有结果。

Method: 提出在自适应急速对手的全信息模型下的同步随机拜占庭协议。

Result: 提出的协议运行轮数为$O(\min\{t^2\log n/n, t/\log n\})$，优于Chor和Coan的$O(t/\log n)$。

Conclusion: 所提出的协议在自适应拜占庭对手下取得了更好的性能，改进了长期存在的轮数界限。

Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed
computing that has been studied intensively for the last four decades. Much of
the research has focused on a static Byzantine adversary, where the adversary
is constrained to choose the Byzantine nodes in advance of the protocol's
execution. This work focuses on the harder case of an adaptive Byzantine
adversary that can choose the Byzantine nodes \emph{adaptively} based on the
protocol's execution. While efficient $O(\log n)$-round protocols ($n$ is the
total number of nodes) are known for the static adversary (Goldwasser, Pavlov,
and Vaikuntanathan, FOCS 2006) tolerating up to $t < n/(3+\epsilon)$ Byzantine
nodes, $\Omega(t/\sqrt{n \log n})$ rounds is a well-known lower bound for
adaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol
for adaptive adversary runs in $O(t/\log n)$ rounds [Chor and Coan, IEEE Trans.
Soft. Engg., 1985].
  This work presents a synchronous randomized Byzantine agreement protocol
under an adaptive adversary that improves over previous results. Our protocol
works under the powerful \emph{adaptive rushing adversary in the full
information model}. That is, we assume that the Byzantine nodes can behave
arbitrarily and maliciously, have knowledge about the entire state of the
network at every round, including random choices made by all the nodes up to
and including the current round, have unlimited computational power, and may
collude among themselves. Furthermore, the adversary can \emph{adaptively}
corrupt up to $t < n/3$ nodes based on the protocol's execution. We present a
simple randomized Byzantine agreement protocol that runs in $O(\min\{t^2\log
n/n, t/\log n\})$ rounds that improves over the long-standing bound of
$O(t/\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [113] [Rumors on evolving graphs through stationary times](https://arxiv.org/abs/2506.04386)
*Vicenzo Bonasorte*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study rumor spreading in dynamic random graphs. Starting with a single
informed vertex, the information flows until it reaches all the vertices of the
graph (completion), according to the following process. At each step $k$, the
information is propagated to neighbors of the informed vertices, in the $k$-th
generated random graph. The way this information propagates from vertex to
vertex at each step will depend on the ``protocol". We provide a method based
on strong stationary times to study the completion time when the graphs are
Markovian time dependent, using known results of the literature for independent
graphs. The concept of strong stationary times is then extended to
non-Markovian Dynamics using coupling from the past algorithms. This allows to
extend results on completion times for non-Markov dynamics

</details>


### [114] [Rumors on evolving graphs through stationary times](https://arxiv.org/abs/2506.04386)
*Vicenzo Bonasorte*

Main category: cs.DS

TL;DR: 开发基于强平稳时间和过去耦合算法的新型分析方法，研究马尔可夫/非马尔可夫动态随机图中的谣言传播完成时间


<details>
  <summary>Details</summary>
Motivation: 传统研究主要针对静态或独立同分布网络，现实中的动态网络往往具有时间相关性。需要建立能同时处理马尔可夫和非马尔可夫动态图的统一分析框架

Method: 1. 利用强平稳时间理论分析马尔可夫动态图
2. 通过过去耦合算法扩展强平稳时间概念到非马尔可夫场景
3. 建立动态图协议与信息传播时间的关联模型

Result: 成功将独立图结论推广到时间相关场景，构建了适用于非马尔可夫动态图的完成时间分析体系

Conclusion: 该方法突破了传统马尔可夫假设限制，为复杂动态网络中的信息传播研究提供了通用分析工具，特别适用于社会网络和物联网等时变系统

Abstract: We study rumor spreading in dynamic random graphs. Starting with a single
informed vertex, the information flows until it reaches all the vertices of the
graph (completion), according to the following process. At each step $k$, the
information is propagated to neighbors of the informed vertices, in the $k$-th
generated random graph. The way this information propagates from vertex to
vertex at each step will depend on the ``protocol". We provide a method based
on strong stationary times to study the completion time when the graphs are
Markovian time dependent, using known results of the literature for independent
graphs. The concept of strong stationary times is then extended to
non-Markovian Dynamics using coupling from the past algorithms. This allows to
extend results on completion times for non-Markov dynamics

</details>


### [115] [Faster MPC Algorithms for Approximate Allocation in Uniformly Sparse Graphs](https://arxiv.org/abs/2506.04524)
*Jakub Łącki,Slobodan Mitrović,Srikkanth Ramachandran,Wen-Horng Sheu*

Main category: cs.DS

TL;DR: 提出了一种在MPC模型中解决分配问题的(1+ε)近似算法，时间复杂度Õ(√log λ)，显著优于先前O(log n)的算法。


<details>
  <summary>Details</summary>
Motivation: 针对低树宽图在MPC模型中缺乏高效常数近似分配算法的问题，改进Agrawal等人的LOCAL算法分析框架。

Method: 通过重新分析Agrawal等人提出的LOCAL算法，将其轮复杂度从O(log n)优化至O(log λ)，并适配到sublinear space MPC模型。

Result: 在λ为图树宽的情况下，算法使用亚线性空间/机器和Õ(λn)总空间，实现当前最优的时间复杂度。

Conclusion: 首次在低树宽图上实现o(log n)轮常数近似分配算法，为MPC模型中的图计算问题提供了新的解决方案。

Abstract: We study the allocation problem in the Massively Parallel Computation (MPC)
model. This problem is a special case of $b$-matching, in which the input is a
bipartite graph with capacities greater than $1$ in only one part of the
bipartition. We give a $(1+\epsilon)$ approximate algorithm for the problem,
which runs in $\tilde{O}(\sqrt{\log \lambda})$ MPC rounds, using sublinear
space per machine and $\tilde{O}(\lambda n)$ total space, where $\lambda$ is
the arboricity of the input graph. Our result is obtained by providing a new
analysis of a LOCAL algorithm by Agrawal, Zadimoghaddam, and Mirrokni [ICML
2018], which improves its round complexity from $O(\log n)$ to $O(\log
\lambda)$. Prior to our work, no $o(\log n)$ round algorithm for
constant-approximate allocation was known in either LOCAL or sublinear space
MPC models for graphs with low arboricity.

</details>


### [116] [Online matching on stochastic block model](https://arxiv.org/abs/2506.04921)
*Maria Cherifa,Clément Calauzènes,Vianney Perchet*

Main category: cs.DS

TL;DR: 本文针对随机块模型中的在线二分图匹配问题，提出并分析了Myopic贪婪匹配算法和Balance容量平衡算法的收敛性能，推导出对应的微分方程解并探讨了在线估计的影响。


<details>
  <summary>Details</summary>
Motivation: 现有随机在线匹配研究未能有效处理社交网络等异构图结构，本文通过引入随机块模型(SBM)来填补这一理论空白。

Method: 使用随机块模型刻画节点连接概率，分别设计Myopic(即时最优匹配)和Balance(兼顾兼容性与剩余容量的)两种算法，建立ODE和微分包含的收敛性分析框架。

Result: 证明Myopic算法匹配规模收敛于ODE解并给出显式误差界，Balance算法收敛于微分包含解，同时揭示了在线概率估计带来的探索-开发权衡效应。

Conclusion: 该理论框架为复杂网络中的匹配算法设计提供了新的分析工具，微分方程近似方法对随机系统分析具有普适参考价值。

Abstract: While online bipartite matching has gained significant attention in recent
years, existing analyses in stochastic settings fail to capture the performance
of algorithms on heterogeneous graphs, such as those incorporating inter-group
affinities or other social network structures. In this work, we address this
gap by studying online bipartite matching within the stochastic block model
(SBM). A fixed set of offline nodes is matched to a stream of online arrivals,
with connections governed probabilistically by latent class memberships. We
analyze two natural algorithms: a $\tt{Myopic}$ policy that greedily matches
each arrival to the most compatible class, and the $\tt{Balance}$ algorithm,
which accounts for both compatibility and remaining capacity. For the
$\tt{Myopic}$ algorithm, we prove that the size of the matching converges, with
high probability, to the solution of an ordinary differential equation (ODE),
for which we provide a tractable approximation along with explicit error
bounds. For the $\tt{Balance}$ algorithm, we demonstrate convergence of the
matching size to a differential inclusion and derive an explicit limiting
solution. Lastly, we explore the impact of estimating the connection
probabilities between classes online, which introduces an
exploration-exploitation trade-off.

</details>


### [117] [Decomposing Words for Enhanced Compression: Exploring the Number of Runs in the Extended Burrows-Wheeler Transform](https://arxiv.org/abs/2506.04926)
*Florian Ingels,Anaïs Denis,Bastien Cazaux*

Main category: cs.DS

TL;DR: 论文通过分析扩展BWT在不同单词分解下的游程数变化，揭示了分解策略对压缩效率的显著影响及其优化难度


<details>
  <summary>Details</summary>
Motivation: 研究不同单词分解方式对扩展BWT游程数的影响，这对基于游程编码的压缩算法效率具有决定性作用

Method: 1. 证明单词分解数量在最小约束下呈指数增长
2. 构建无限单词家族展示最差/最佳分解游程数比率无界性

Result: 1. 分解数量指数级增长
2. 存在游程数比率无界的无限单词家族
3. 分解选择显著影响压缩性能

Conclusion: 研究结果揭示了eBWT框架中分解策略的关键作用，为优化游程编码提供了理论依据，同时指出实际应用中面临的算法挑战

Abstract: The Burrows-Wheeler Transform (BWT) is a fundamental component in many data
structures for text indexing and compression, widely used in areas such as
bioinformatics and information retrieval. The extended BWT (eBWT) generalizes
the classical BWT to multisets of strings, providing a flexible framework that
captures many BWT-like constructions. Several known variants of the BWT can be
viewed as instances of the eBWT applied to specific decompositions of a word. A
central property of the BWT, essential for its compressibility, is the number
of maximal ranges of equal letters, named runs. In this article, we explore how
different decompositions of a word impact the number of runs in the resulting
eBWT. First, we show that the number of decompositions of a word is
exponential, even under minimal constraints on the size of the subsets in the
decomposition. Second, we present an infinite family of words for which the
ratio of the number of runs between the worst and best decompositions is
unbounded, under the same minimal constraints. These results illustrate the
potential cost of decomposition choices in eBWT-based compression and underline
the challenges in optimizing run-length encoding in generalized BWT frameworks.

</details>


### [118] [Resilient Pattern Mining](https://arxiv.org/abs/2506.04935)
*Pengxin Bian,Panagiotis Charalampopoulos,Lorraine A. K. Ayad,Manal Mohamed,Solon P. Pissis,Grigorios Loukides*

Main category: cs.DS

TL;DR: 提出(τ,k)-弹性模式挖掘问题及高效算法，解决传统频繁模式对数据扰动的敏感性


<details>
  <summary>Details</summary>
Motivation: 传统频繁子串挖掘无法应对数据扰动（如k个位置字符被替换），在基因组版本化数据等需鲁棒性场景存在局限

Method: 基于高级数据结构和组合数学设计O(n log n)时间复杂度的精确算法

Result: 实验证明弹性子串在基因组分析中优于传统方法，算法效率比动态规划基线提升多个数量级

Conclusion: 弹性模式挖掘在版本控制等需鲁棒性场景具有重要价值，提出的高效算法具备实际应用潜力

Abstract: Frequent pattern mining is a flagship problem in data mining. In its most
basic form, it asks for the set of substrings of a given string $S$ of length
$n$ that occur at least $\tau$ times in $S$, for some integer $\tau\in[1,n]$.
We introduce a resilient version of this classic problem, which we term the
$(\tau, k)$-Resilient Pattern Mining (RPM) problem. Given a string $S$ of
length $n$ and two integers $\tau, k\in[1,n]$, RPM asks for the set of
substrings of $S$ that occur at least $\tau$ times in $S$, even when the
letters at any $k$ positions of $S$ are substituted by other letters. Unlike
frequent substrings, resilient ones account for the fact that changes to string
$S$ are often expensive to handle or are unknown.
  We propose an exact $\mathcal{O}(n\log n)$-time and $\mathcal{O}(n)$-space
algorithm for RPM, which employs advanced data structures and combinatorial
insights. We then present experiments on real large-scale datasets from
different domains demonstrating that: (I) The notion of resilient substrings is
useful in analyzing genomic data and is more powerful than that of frequent
substrings, in scenarios where resilience is required, such as in the case of
versioned datasets; (II) Our algorithm is several orders of magnitude faster
and more space-efficient than a baseline algorithm that is based on dynamic
programming; and (III) Clustering based on resilient substrings is effective.

</details>


### [119] [Compressing Hypergraphs using Suffix Sorting](https://arxiv.org/abs/2506.05023)
*Enno Adler,Stefan Böttcher,Rita Hartel*

Main category: cs.DS

TL;DR: 提出HyperCSA超图压缩方法，在压缩率（26%-79%）和查询速度（快6-40倍）上显著优于现有方法，并具备更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 超图在表示复杂非二元关系时体积庞大，导致存储、传输和查询效率低下，现有方法无法有效兼顾压缩率与查询性能。

Method: 基于压缩后缀数组(CSA)技术构建简洁数据结构，支持直接在压缩后的超图上执行邻接查询等标准操作。

Result: 1. 压缩率达到原文件26%-79% 2. 在大型超图上性能超越现有方法 3. 邻接查询速度提升6-40倍

Conclusion: HyperCSA通过创新的压缩架构，在保持查询功能的同时实现高效压缩，为实际应用中的大规模超图处理提供了实用解决方案。

Abstract: Hypergraphs model complex, non-binary relationships like co-authorships,
social group memberships, and recommendations. Like traditional graphs,
hypergraphs can grow large, posing challenges for storage, transmission, and
query performance. We propose HyperCSA, a novel compression method for
hypergraphs that maintains support for standard queries over the succinct
representation. HyperCSA achieves compression ratios of 26% to 79% of the
original file size on real-world hypergraphs - outperforming existing methods
on all large hypergraphs in our experiments. Additionally, HyperCSA scales to
larger datasets than existing approaches. Furthermore, for common real-world
hypergraphs, HyperCSA evaluates neighbor queries 6 to 40 times faster than both
standard data structures and other hypergraph compression approaches.

</details>


### [120] [On Minimizers of Minimum Density](https://arxiv.org/abs/2506.05277)
*Arseny Shur*

Main category: cs.DS

TL;DR: 提出新算法计算最小密度minimizers，优化计算生物学中的k-mer采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在寻找最优minimizer时存在指数级复杂度，难以处理大参数组合。论文旨在通过算法优化解决这一瓶颈。

Method: 1) 开发基于ILP的优化算法，时间复杂降为w·2^(σ^k)；2) 引入正则语言理论，通过有限自动机特征分析处理渐近情况(w→∞)。

Result: 成功计算了(σ,k)∈{(2,2),(2,3),(2,4),(2,5),(4,2)}及所有w≥2的最小密度minimizers，验证新提出的理论下界有效性。

Conclusion: 算法显著提升minimizer优化效率，理论分析揭示渐近最优结构，实验数据证实新下界优于现有基准。

Abstract: Minimizers are sampling schemes with numerous applications in computational
biology. Assuming a fixed alphabet of size $\sigma$, a minimizer is defined by
two integers $k,w\ge2$ and a linear order $\rho$ on strings of length $k$ (also
called $k$-mers). A string is processed by a sliding window algorithm that
chooses, in each window of length $w+k-1$, its minimal $k$-mer with respect to
$\rho$. A key characteristic of the minimizer is its density, which is the
expected frequency of chosen $k$-mers among all $k$-mers in a random infinite
$\sigma$-ary string. Minimizers of smaller density are preferred as they
produce smaller samples with the same guarantee: each window is represented by
a $k$-mer.
  The problem of finding a minimizer of minimum density for given input
parameters $(\sigma,k,w)$ has a huge search space of $(\sigma^k)!$ and is
representable by an ILP of size $\tilde\Theta(\sigma^{k+w})$, which has
worst-case solution time that is doubly-exponential in $(k+w)$ under standard
complexity assumptions. We solve this problem in $w\cdot 2^{\sigma^k+O(k)}$
time and provide several additional tricks reducing the practical runtime and
search space. As a by-product, we describe an algorithm computing the average
density of a minimizer within the same time bound. Then we propose a novel
method of studying minimizers via regular languages and show how to find, via
the eigenvalue/eigenvector analysis over finite automata, minimizers with the
minimal density in the asymptotic case $w\to\infty$. Implementing our
algorithms, we compute the minimum density minimizers for
$(\sigma,k)\in\{(2,2),(2,3),(2,4),(2,5),(4,2)\}$ and \textbf{all} $w\ge 2$. The
obtained densities are compared against the average density and the theoretical
lower bounds, including the new bound presented in this paper.

</details>


### [121] [Rumors on evolving graphs through stationary times](https://arxiv.org/abs/2506.04386)
*Vicenzo Bonasorte*

Main category: cs.DS

TL;DR: 研究动态随机图中的谣言传播，用强平稳时间方法研究完成时间并扩展到非马尔可夫动力学。


<details>
  <summary>Details</summary>
Motivation: 研究动态随机图中谣言传播的完成时间。

Method: 基于强平稳时间方法，利用独立图的已知结果，对非马尔可夫动力学使用从过去耦合算法扩展强平稳时间概念。

Result: 能够扩展非马尔可夫动力学下关于完成时间的结果。

Conclusion: 提供了研究动态随机图中谣言传播完成时间的有效方法，并可扩展到非马尔可夫情况。

Abstract: We study rumor spreading in dynamic random graphs. Starting with a single
informed vertex, the information flows until it reaches all the vertices of the
graph (completion), according to the following process. At each step $k$, the
information is propagated to neighbors of the informed vertices, in the $k$-th
generated random graph. The way this information propagates from vertex to
vertex at each step will depend on the ``protocol". We provide a method based
on strong stationary times to study the completion time when the graphs are
Markovian time dependent, using known results of the literature for independent
graphs. The concept of strong stationary times is then extended to
non-Markovian Dynamics using coupling from the past algorithms. This allows to
extend results on completion times for non-Markov dynamics

</details>


### [122] [Faster MPC Algorithms for Approximate Allocation in Uniformly Sparse Graphs](https://arxiv.org/abs/2506.04524)
*Jakub Łącki,Slobodan Mitrović,Srikkanth Ramachandran,Wen-Horng Sheu*

Main category: cs.DS

TL;DR: 研究MPC模型中的分配问题，给出(1+ε)近似算法，运行于Õ(√log λ)轮，改进LOCAL算法轮复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决MPC模型中的分配问题，此前在低树状度图的LOCAL或亚线性空间MPC模型中，缺乏o(log n)轮的常数近似分配算法。

Method: 对Agrawal等人的LOCAL算法进行新分析，改进其轮复杂度。

Result: 得到(1+ε)近似算法，运行于Õ(√log λ) MPC轮，每台机器用亚线性空间，总空间为Õ(λn)，将LOCAL算法轮复杂度从O(log n)改进到O(log λ)。

Conclusion: 为低树状度图的分配问题在MPC模型中提供了高效的近似算法。

Abstract: We study the allocation problem in the Massively Parallel Computation (MPC)
model. This problem is a special case of $b$-matching, in which the input is a
bipartite graph with capacities greater than $1$ in only one part of the
bipartition. We give a $(1+\epsilon)$ approximate algorithm for the problem,
which runs in $\tilde{O}(\sqrt{\log \lambda})$ MPC rounds, using sublinear
space per machine and $\tilde{O}(\lambda n)$ total space, where $\lambda$ is
the arboricity of the input graph. Our result is obtained by providing a new
analysis of a LOCAL algorithm by Agrawal, Zadimoghaddam, and Mirrokni [ICML
2018], which improves its round complexity from $O(\log n)$ to $O(\log
\lambda)$. Prior to our work, no $o(\log n)$ round algorithm for
constant-approximate allocation was known in either LOCAL or sublinear space
MPC models for graphs with low arboricity.

</details>


### [123] [Online matching on stochastic block model](https://arxiv.org/abs/2506.04921)
*Maria Cherifa,Clément Calauzènes,Vianney Perchet*

Main category: cs.DS

TL;DR: 研究随机块模型下在线二分图匹配，分析两种算法并探讨在线估计连接概率的影响。


<details>
  <summary>Details</summary>
Motivation: 现有随机环境下在线二分图匹配分析无法捕捉异质图上算法性能，存在研究空白。

Method: 在随机块模型下研究在线二分图匹配，分析贪心匹配的Myopic策略和兼顾兼容性与剩余容量的Balance算法。

Result: 证明Myopic算法匹配大小高概率收敛到常微分方程解并给出近似和误差界，Balance算法匹配大小收敛到微分包含并得出显式极限解。

Conclusion: 探索了在线估计类间连接概率的影响，存在探索 - 利用权衡。

Abstract: While online bipartite matching has gained significant attention in recent
years, existing analyses in stochastic settings fail to capture the performance
of algorithms on heterogeneous graphs, such as those incorporating inter-group
affinities or other social network structures. In this work, we address this
gap by studying online bipartite matching within the stochastic block model
(SBM). A fixed set of offline nodes is matched to a stream of online arrivals,
with connections governed probabilistically by latent class memberships. We
analyze two natural algorithms: a $\tt{Myopic}$ policy that greedily matches
each arrival to the most compatible class, and the $\tt{Balance}$ algorithm,
which accounts for both compatibility and remaining capacity. For the
$\tt{Myopic}$ algorithm, we prove that the size of the matching converges, with
high probability, to the solution of an ordinary differential equation (ODE),
for which we provide a tractable approximation along with explicit error
bounds. For the $\tt{Balance}$ algorithm, we demonstrate convergence of the
matching size to a differential inclusion and derive an explicit limiting
solution. Lastly, we explore the impact of estimating the connection
probabilities between classes online, which introduces an
exploration-exploitation trade-off.

</details>


### [124] [Decomposing Words for Enhanced Compression: Exploring the Number of Runs in the Extended Burrows-Wheeler Transform](https://arxiv.org/abs/2506.04926)
*Florian Ingels,Anaïs Denis,Bastien Cazaux*

Main category: cs.DS

TL;DR: 本文探讨单词不同分解方式对扩展BWT（eBWT）游程数量的影响，指出分解数量呈指数级，且存在游程数量比无界的情况，凸显分解选择成本和优化挑战。


<details>
  <summary>Details</summary>
Motivation: 探究不同单词分解方式如何影响eBWT的游程数量，以了解eBWT基于压缩的分解选择成本和优化挑战。

Method: 首先证明单词分解数量即使在子集大小有最小约束下仍为指数级；然后给出一个无限单词族，在相同最小约束下游程数量在最差和最佳分解间的比率无界。

Result: 单词分解数量呈指数级；存在无限单词族，其最差和最佳分解的游程数量比无界。

Conclusion: 说明了eBWT基于压缩中分解选择的潜在成本，强调了在广义BWT框架中优化游程编码的挑战。

Abstract: The Burrows-Wheeler Transform (BWT) is a fundamental component in many data
structures for text indexing and compression, widely used in areas such as
bioinformatics and information retrieval. The extended BWT (eBWT) generalizes
the classical BWT to multisets of strings, providing a flexible framework that
captures many BWT-like constructions. Several known variants of the BWT can be
viewed as instances of the eBWT applied to specific decompositions of a word. A
central property of the BWT, essential for its compressibility, is the number
of maximal ranges of equal letters, named runs. In this article, we explore how
different decompositions of a word impact the number of runs in the resulting
eBWT. First, we show that the number of decompositions of a word is
exponential, even under minimal constraints on the size of the subsets in the
decomposition. Second, we present an infinite family of words for which the
ratio of the number of runs between the worst and best decompositions is
unbounded, under the same minimal constraints. These results illustrate the
potential cost of decomposition choices in eBWT-based compression and underline
the challenges in optimizing run-length encoding in generalized BWT frameworks.

</details>


### [125] [Resilient Pattern Mining](https://arxiv.org/abs/2506.04935)
*Pengxin Bian,Panagiotis Charalampopoulos,Lorraine A. K. Ayad,Manal Mohamed,Solon P. Pissis,Grigorios Loukides*

Main category: cs.DS

TL;DR: 本文引入了(τ, k)-弹性模式挖掘（RPM）问题，提出了时间复杂度为O(n log n)、空间复杂度为O(n)的精确算法，并通过实验验证了弹性子串的实用性和算法的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统频繁模式挖掘未考虑字符串S的变化，而变化处理成本高或未知，因此引入弹性模式挖掘问题。

Method: 提出使用高级数据结构和组合见解的精确算法解决RPM问题。

Result: 实验表明弹性子串在分析基因组数据中有用，比频繁子串更强大；算法比基于动态规划的基线算法快几个数量级且更节省空间；基于弹性子串的聚类有效。

Conclusion: 弹性子串概念有实际应用价值，所提算法高效，能用于不同领域数据集分析。

Abstract: Frequent pattern mining is a flagship problem in data mining. In its most
basic form, it asks for the set of substrings of a given string $S$ of length
$n$ that occur at least $\tau$ times in $S$, for some integer $\tau\in[1,n]$.
We introduce a resilient version of this classic problem, which we term the
$(\tau, k)$-Resilient Pattern Mining (RPM) problem. Given a string $S$ of
length $n$ and two integers $\tau, k\in[1,n]$, RPM asks for the set of
substrings of $S$ that occur at least $\tau$ times in $S$, even when the
letters at any $k$ positions of $S$ are substituted by other letters. Unlike
frequent substrings, resilient ones account for the fact that changes to string
$S$ are often expensive to handle or are unknown.
  We propose an exact $\mathcal{O}(n\log n)$-time and $\mathcal{O}(n)$-space
algorithm for RPM, which employs advanced data structures and combinatorial
insights. We then present experiments on real large-scale datasets from
different domains demonstrating that: (I) The notion of resilient substrings is
useful in analyzing genomic data and is more powerful than that of frequent
substrings, in scenarios where resilience is required, such as in the case of
versioned datasets; (II) Our algorithm is several orders of magnitude faster
and more space-efficient than a baseline algorithm that is based on dynamic
programming; and (III) Clustering based on resilient substrings is effective.

</details>


### [126] [Compressing Hypergraphs using Suffix Sorting](https://arxiv.org/abs/2506.05023)
*Enno Adler,Stefan Böttcher,Rita Hartel*

Main category: cs.DS

TL;DR: 提出超图压缩方法HyperCSA，在压缩率、数据集规模和查询速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超图规模增大带来存储、传输和查询性能挑战。

Method: 提出HyperCSA压缩方法以支持对简洁表示的标准查询。

Result: 在真实超图上压缩率达原文件大小的26% - 79%，能处理更大数据集，对常见超图邻居查询速度比标准数据结构和其他压缩方法快6 - 40倍。

Conclusion: HyperCSA在超图压缩和查询方面优于现有方法。

Abstract: Hypergraphs model complex, non-binary relationships like co-authorships,
social group memberships, and recommendations. Like traditional graphs,
hypergraphs can grow large, posing challenges for storage, transmission, and
query performance. We propose HyperCSA, a novel compression method for
hypergraphs that maintains support for standard queries over the succinct
representation. HyperCSA achieves compression ratios of 26% to 79% of the
original file size on real-world hypergraphs - outperforming existing methods
on all large hypergraphs in our experiments. Additionally, HyperCSA scales to
larger datasets than existing approaches. Furthermore, for common real-world
hypergraphs, HyperCSA evaluates neighbor queries 6 to 40 times faster than both
standard data structures and other hypergraph compression approaches.

</details>


### [127] [On Minimizers of Minimum Density](https://arxiv.org/abs/2506.05277)
*Arseny Shur*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Minimizers are sampling schemes with numerous applications in computational
biology. Assuming a fixed alphabet of size $\sigma$, a minimizer is defined by
two integers $k,w\ge2$ and a linear order $\rho$ on strings of length $k$ (also
called $k$-mers). A string is processed by a sliding window algorithm that
chooses, in each window of length $w+k-1$, its minimal $k$-mer with respect to
$\rho$. A key characteristic of the minimizer is its density, which is the
expected frequency of chosen $k$-mers among all $k$-mers in a random infinite
$\sigma$-ary string. Minimizers of smaller density are preferred as they
produce smaller samples with the same guarantee: each window is represented by
a $k$-mer.
  The problem of finding a minimizer of minimum density for given input
parameters $(\sigma,k,w)$ has a huge search space of $(\sigma^k)!$ and is
representable by an ILP of size $\tilde\Theta(\sigma^{k+w})$, which has
worst-case solution time that is doubly-exponential in $(k+w)$ under standard
complexity assumptions. We solve this problem in $w\cdot 2^{\sigma^k+O(k)}$
time and provide several additional tricks reducing the practical runtime and
search space. As a by-product, we describe an algorithm computing the average
density of a minimizer within the same time bound. Then we propose a novel
method of studying minimizers via regular languages and show how to find, via
the eigenvalue/eigenvector analysis over finite automata, minimizers with the
minimal density in the asymptotic case $w\to\infty$. Implementing our
algorithms, we compute the minimum density minimizers for
$(\sigma,k)\in\{(2,2),(2,3),(2,4),(2,5),(4,2)\}$ and \textbf{all} $w\ge 2$. The
obtained densities are compared against the average density and the theoretical
lower bounds, including the new bound presented in this paper.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [128] [User Altruism in Recommendation Systems](https://arxiv.org/abs/2506.04525)
*Ekaterina Fedorova,Madeline Kitch,Chara Podimata*

Main category: cs.GT

TL;DR: 用户通过'利他策略'集体操控推荐系统偏好矩阵，实证显示该行为能提升用户和平台效用


<details>
  <summary>Details</summary>
Motivation: 研究推荐系统用户如何通过策略性互动形成草根运动，对抗算法压制以提升被抑制内容的可见性

Method: 建立用户-推荐系统博弈模型，分析真实/利他偏好报告下的社会福利差异，结合GoodReads数据集和用户调查验证

Result: 满足特定条件时利他策略显著提升社会福利，且能同步增加推荐系统效用（尤其在低秩近似场景）

Conclusion: 传统推荐系统的激励机制可能意外促使用户形成集体行动，这对算法透明度和用户行为建模具有重要启示

Abstract: Users of social media platforms based on recommendation systems (RecSys)
(e.g. TikTok, X, YouTube) strategically interact with platform content to
influence future recommendations. On some such platforms, users have been
documented to form large-scale grassroots movements encouraging others to
purposefully interact with algorithmically suppressed content in order to
"boost" its recommendation; we term this behavior user altruism. To capture
this behavior, we study a game between users and a RecSys, where users provide
the RecSys (potentially manipulated) preferences over the contents available to
them, and the RecSys -- limited by data and computation constraints -- creates
a low-rank approximation preference matrix, and ultimately provides each user
her (approximately) most-preferred item. We compare the users' social welfare
under truthful preference reporting and under a class of strategies capturing
user altruism. In our theoretical analysis, we provide sufficient conditions to
ensure strict increases in user social welfare under user altruism, and provide
an algorithm to find an effective altruistic strategy. Interestingly, we show
that for commonly assumed recommender utility functions, effectively altruistic
strategies also improve the utility of the RecSys! We show that our results are
robust to several model misspecifications, thus strengthening our conclusions.
Our theoretical analysis is complemented by empirical results of effective
altruistic strategies on the GoodReads dataset, and an online survey on how
real-world users behave altruistically in RecSys. Overall, our findings serve
as a proof-of-concept of the reasons why traditional RecSys may incentivize
users to form collectives and/or follow altruistic strategies when interacting
with them.

</details>


### [129] [User Altruism in Recommendation Systems](https://arxiv.org/abs/2506.04525)
*Ekaterina Fedorova,Madeline Kitch,Chara Podimata*

Main category: cs.GT

TL;DR: 研究通过博弈论模型揭示：社交媒体用户通过策略性利他行为（如集体互动被压制内容）可提升社会福利，特定条件下甚至能同步提高推荐系统效用。


<details>
  <summary>Details</summary>
Motivation: 解析用户如何通过集体行动对抗推荐算法压制，探究利他行为对用户福利和推荐系统的双向影响机制。

Method: 构建用户-推荐系统博弈模型，采用低秩近似偏好矩阵进行理论推演，开发有效利他策略算法，配合GoodReads数据集和在线调查实证。

Result: 理论证明用户利他策略可显著提升社会福利（特定条件下），且能反向提高推荐系统效用；实证验证策略有效性及真实用户行为存在性。

Conclusion: 传统推荐系统设计存在激励用户形成利他集体的风险，研究为算法机制设计提供反直觉视角，强调需平衡用户策略行为与系统目标。

Abstract: Users of social media platforms based on recommendation systems (RecSys)
(e.g. TikTok, X, YouTube) strategically interact with platform content to
influence future recommendations. On some such platforms, users have been
documented to form large-scale grassroots movements encouraging others to
purposefully interact with algorithmically suppressed content in order to
"boost" its recommendation; we term this behavior user altruism. To capture
this behavior, we study a game between users and a RecSys, where users provide
the RecSys (potentially manipulated) preferences over the contents available to
them, and the RecSys -- limited by data and computation constraints -- creates
a low-rank approximation preference matrix, and ultimately provides each user
her (approximately) most-preferred item. We compare the users' social welfare
under truthful preference reporting and under a class of strategies capturing
user altruism. In our theoretical analysis, we provide sufficient conditions to
ensure strict increases in user social welfare under user altruism, and provide
an algorithm to find an effective altruistic strategy. Interestingly, we show
that for commonly assumed recommender utility functions, effectively altruistic
strategies also improve the utility of the RecSys! We show that our results are
robust to several model misspecifications, thus strengthening our conclusions.
Our theoretical analysis is complemented by empirical results of effective
altruistic strategies on the GoodReads dataset, and an online survey on how
real-world users behave altruistically in RecSys. Overall, our findings serve
as a proof-of-concept of the reasons why traditional RecSys may incentivize
users to form collectives and/or follow altruistic strategies when interacting
with them.

</details>


### [130] [MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable Player in Basketball](https://arxiv.org/abs/2506.04602)
*Haifeng Sun,Yu Xiong,Runze Wu,Kai Wang,Lan Zhang,Changjie Fan,Shaojie Tang,Xiang-Yang Li*

Main category: cs.GT

TL;DR: 提出基于Shapley值的MVP评估框架，通过特征处理、胜负模型训练、贡献度计算和因果优化，在NBA和游戏数据集验证有效性并实现工业部署。


<details>
  <summary>Details</summary>
Motivation: 随着电子竞技发展，现有MVP评估方法缺乏可解释性和实用性。需要建立可解释的评估体系解决玩家贡献度量化难题。

Method: 1. 基于play-by-play数据进行特征处理
2. 训练胜负预测模型
3. 计算Shapley值分配贡献度
4. 引入因果推理优化算法与专家投票结果对齐

Result: 在NBA数据集和Dunk City Dynasty游戏数据集中验证框架有效性，成功实现工业级部署应用。

Conclusion: Shapley值方法为MVP评估提供可解释解决方案，通过因果优化提升结果合理性，实证表明该方法在真实场景中的实用价值。

Abstract: The burgeoning growth of the esports and multiplayer online gaming community
has highlighted the critical importance of evaluating the Most Valuable Player
(MVP). The establishment of an explainable and practical MVP evaluation method
is very challenging. In our study, we specifically focus on play-by-play data,
which records related events during the game, such as assists and points. We
aim to address the challenges by introducing a new MVP evaluation framework,
denoted as \oursys, which leverages Shapley values. This approach encompasses
feature processing, win-loss model training, Shapley value allocation, and MVP
ranking determination based on players' contributions. Additionally, we
optimize our algorithm to align with expert voting results from the perspective
of causality. Finally, we substantiated the efficacy of our method through
validation using the NBA dataset and the Dunk City Dynasty dataset and
implemented online deployment in the industry.

</details>


### [131] [Misère Greedy Nim and Misère Bounded Greedy Nim](https://arxiv.org/abs/2506.04657)
*Nanako Omiya,Ryo Yoshinaka,Ayumi Shinohara*

Main category: cs.GT

TL;DR: 论文完整解决了k-有界贪婪取石子游戏和贪婪取石子游戏的misère版本，提出了判定P-局面的充要条件


<details>
  <summary>Details</summary>
Motivation: 研究组合游戏中misère版本的P-局面对策略分析具有重要意义，但现有成果有限。本文旨在完善两类贪婪取石子游戏的misère版本理论体系。

Method: 通过数学建模分析两种贪婪取石子游戏的变体，建立位置状态的数学表征，并运用组合博弈论方法推导P-局面的判定条件

Result: 成功证明了关于P-局面的充要条件定理，为两类游戏提供了完整的理论解决方案

Conclusion: 该研究不仅完善了贪婪取石子游戏的理论框架，其方法论对解决其他misère型组合游戏具有示范意义

Abstract: In this paper, we analyze the mis\`ere versions of two impartial
combinatorial games: k-Bounded Greedy Nim and Greedy Nim. We present a complete
solution to both games by showing necessary and sufficient conditions for a
position to be P-positions.

</details>


### [132] [An O(log log n)-approximate budget feasible mechanism for subadditive valuations](https://arxiv.org/abs/2506.04665)
*Rian Neogi,Kanstantsin Pashkovich,Chaitanya Swamy*

Main category: cs.GT

TL;DR: 提出了一种针对次可加估值的预算可行机制，将近似因子从O(log n/log log n)提升至O(log log n)，显著改进了现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决次可加估值下预算可行机制设计的效率瓶颈，突破现有近似因子O(log n/log log n)的限制。

Method: 基于需求预言机设计多项式时间机制，通过算法创新优化次可加函数的近似比分析。

Result: 实现了O(log log n)近似因子，在多项式时间内运行，显著优于现有最佳结果。

Conclusion: 该研究将次可加估值的预算机制效率提升至对数对数级别，推动了机制设计理论边界。

Abstract: In budget-feasible mechanism design, there is a set of items $U$, each owned
by a distinct seller. The seller of item $e$ incurs a private cost
$\overline{c}_e$ for supplying her item. A buyer wishes to procure a set of
items from the sellers of maximum value, where the value of a set $S\subseteq
U$ of items is given by a valuation function $v:2^U\to \mathbb{R}_+$. The buyer
has a budget of $B \in \mathbb{R}_+$ for the total payments made to the
sellers. We wish to design a mechanism that is truthful, that is, sellers are
incentivized to report their true costs, budget-feasible, that is, the sum of
the payments made to the sellers is at most the budget $B$, and that outputs a
set whose value is large compared to $\text{OPT}:=\max\{v(S):\overline{c}(S)\le
B,S\subseteq U\}$.
  Budget-feasible mechanism design has been extensively studied, with the
literature focussing on (classes of) subadditive valuation functions, and
various polytime, budget-feasible mechanisms, achieving constant-factor
approximation, have been devised for the special cases of additive, submodular,
and XOS valuations. However, for general subadditive valuations, the best-known
approximation factor achievable by a polytime budget-feasible mechanism (given
access to demand oracles) was only $O(\log n / \log \log n)$, where $n$ is the
number of items.
  We improve this state-of-the-art significantly by designing a budget-feasible
mechanism for subadditive valuations that \emph{achieves a
substantially-improved approximation factor of $O(\log\log n)$ and runs in
polynomial time, given access to demand oracles.}

</details>


### [133] [Cooperation and the Design of Public Goods](https://arxiv.org/abs/2506.05251)
*J. Carlos Martínez Mori,Alejandro Toriello*

Main category: cs.GT

TL;DR: 通过非转移效用线性生产博弈模型分析公共产品合作机制，提出核心解判定条件及优化算法，并通过芝加哥公交案例验证合作必要性及潜在分配问题


<details>
  <summary>Details</summary>
Motivation: 公共产品(如交通政策)涉及多方利益博弈，其实际部署需要建立有效的合作机制。传统方法难以量化合作稳定性与复杂利益分配的关系

Method: 构建非转移效用线性生产(NTU LP)博弈模型，结合合作博弈论与线性规划，开发核心解判定算法与切割平面优化方法

Result: 1. 提出核心解非空的充分条件 2. 证明核心成员判定属于co-NP完全问题 3. 建立社会福利优化框架 4. 案例显示合作可提升效率但可能加剧分配不公

Conclusion: 合作机制是公共产品成功部署的必要条件，但需警惕其可能引发的非对称利益分配。理论模型为政策设计提供量化工具，算法实现支持复杂决策优化

Abstract: We consider the cooperative elements that arise in the design of public
goods, such as transportation policies and infrastructure. These involve a
variety of stakeholders: governments, businesses, advocates, and users. Their
eventual deployment depends on the decision maker's ability to garner
sufficient support from each of these groups; we formalize these strategic
requirements from the perspective of cooperative game theory. Specifically, we
introduce non-transferable utility, linear production (NTU LP) games, which
combine the game-theoretic tensions inherent in public decision-making with the
modeling flexibility of linear programming. We derive structural properties
regarding the non-emptiness, representability and complexity of the core, a
solution concept that models the viability of cooperation. In particular, we
provide fairly general sufficient conditions under which the core of an NTU LP
game is guaranteed to be non-empty, prove that determining membership in the
core is co-NP-complete, and develop a cutting plane algorithm to optimize
various social welfare objectives subject to core membership. Lastly, we apply
these results in a data-driven case study on service plan optimization for the
Chicago bus system. As our study illustrates, cooperation is necessary for the
successful deployment of transportation service plans and similar public goods,
but it may also have adverse or counterintuitive distributive implications.

</details>


### [134] [Equilibrium Computation in First-Price Auctions with Correlated Priors](https://arxiv.org/abs/2506.05322)
*Aris Filos-Ratsikas,Yiannis Giannakopoulos,Alexandros Hollender,Charalampos Kokkalis*

Main category: cs.GT

TL;DR: 首次在无需先验主观性假设条件下，证明一价拍卖中纯策略贝叶斯纳什均衡存在性判定属于NP难题，并提出基于稀疏化/密集化的多项式近似算法


<details>
  <summary>Details</summary>
Motivation: 现有文献中关于拍卖均衡计算复杂性的硬度结果均依赖主观先验分布假设或特殊平局规则，本文旨在建立更普适条件下的计算复杂性理论框架

Method: 1. 投标稀疏化：基于组合枚举技术开发近似算法
2. 投标密集化：结合经济学连续理论构建计算框架

Result: 针对对称拍卖场景/固定竞拍人数情形，提出适用于离散/连续变体的多项式时间近似算法

Conclusion: 建立了拍卖均衡计算复杂性研究的新基准，同时为实际应用提供了可扩展的算法解决方案

Abstract: We consider the computational complexity of computing Bayes-Nash equilibria
in first-price auctions, where the bidders' values for the item are drawn from
a general (possibly correlated) joint distribution. We show that when the
values and the bidding space are discrete, determining the existence of a pure
Bayes-Nash equilibrium is NP-hard. This is the first hardness result in the
literature of the problem that does not rely on assumptions of subjectivity of
the priors, or convoluted tie-breaking rules. We then present two main
approaches for achieving positive results, via bid sparsification and via bid
densification. The former is more combinatorial and is based on enumeration
techniques, whereas the latter makes use of the continuous theory of the
problem developed in the economics literature. Using these approaches, we
develop polynomial-time approximation algorithms for computing equilibria in
symmetric settings or settings with a fixed number of bidders, for different
(discrete or continuous) variants of the auction.

</details>


### [135] [User Altruism in Recommendation Systems](https://arxiv.org/abs/2506.04525)
*Ekaterina Fedorova,Madeline Kitch,Chara Podimata*

Main category: cs.GT

TL;DR: 研究用户在推荐系统上的利他行为，对比不同策略下用户社会福利，给出利他策略提升福利的条件与算法，理论结果通过实证和调查验证。


<details>
  <summary>Details</summary>
Motivation: 捕捉社交媒体推荐系统中用户为提升特定内容推荐而产生的利他行为，并探究其对用户社会福利和推荐系统效用的影响。

Method: 构建用户与推荐系统的博弈模型，对比不同策略下用户社会福利，进行理论分析并提供算法，结合GoodReads数据集实证和在线调查。

Result: 给出用户利他行为严格提升社会福利的充分条件，发现有效利他策略能提升推荐系统效用，结果对多种模型误设具有鲁棒性。

Conclusion: 解释了传统推荐系统可能促使用户形成集体或采取利他策略的原因。

Abstract: Users of social media platforms based on recommendation systems (RecSys)
(e.g. TikTok, X, YouTube) strategically interact with platform content to
influence future recommendations. On some such platforms, users have been
documented to form large-scale grassroots movements encouraging others to
purposefully interact with algorithmically suppressed content in order to
"boost" its recommendation; we term this behavior user altruism. To capture
this behavior, we study a game between users and a RecSys, where users provide
the RecSys (potentially manipulated) preferences over the contents available to
them, and the RecSys -- limited by data and computation constraints -- creates
a low-rank approximation preference matrix, and ultimately provides each user
her (approximately) most-preferred item. We compare the users' social welfare
under truthful preference reporting and under a class of strategies capturing
user altruism. In our theoretical analysis, we provide sufficient conditions to
ensure strict increases in user social welfare under user altruism, and provide
an algorithm to find an effective altruistic strategy. Interestingly, we show
that for commonly assumed recommender utility functions, effectively altruistic
strategies also improve the utility of the RecSys! We show that our results are
robust to several model misspecifications, thus strengthening our conclusions.
Our theoretical analysis is complemented by empirical results of effective
altruistic strategies on the GoodReads dataset, and an online survey on how
real-world users behave altruistically in RecSys. Overall, our findings serve
as a proof-of-concept of the reasons why traditional RecSys may incentivize
users to form collectives and/or follow altruistic strategies when interacting
with them.

</details>


### [136] [MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable Player in Basketball](https://arxiv.org/abs/2506.04602)
*Haifeng Sun,Yu Xiong,Runze Wu,Kai Wang,Lan Zhang,Changjie Fan,Shaojie Tang,Xiang-Yang Li*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The burgeoning growth of the esports and multiplayer online gaming community
has highlighted the critical importance of evaluating the Most Valuable Player
(MVP). The establishment of an explainable and practical MVP evaluation method
is very challenging. In our study, we specifically focus on play-by-play data,
which records related events during the game, such as assists and points. We
aim to address the challenges by introducing a new MVP evaluation framework,
denoted as \oursys, which leverages Shapley values. This approach encompasses
feature processing, win-loss model training, Shapley value allocation, and MVP
ranking determination based on players' contributions. Additionally, we
optimize our algorithm to align with expert voting results from the perspective
of causality. Finally, we substantiated the efficacy of our method through
validation using the NBA dataset and the Dunk City Dynasty dataset and
implemented online deployment in the industry.

</details>


### [137] [Misère Greedy Nim and Misère Bounded Greedy Nim](https://arxiv.org/abs/2506.04657)
*Nanako Omiya,Ryo Yoshinaka,Ayumi Shinohara*

Main category: cs.GT

TL;DR: 分析两种无偏组合游戏的失败版本，给出位置为P-位置的充要条件。


<details>
  <summary>Details</summary>
Motivation: 研究两种无偏组合游戏（k-有界贪婪尼姆和贪婪尼姆）的失败版本。

Method: 寻找位置为P-位置的必要和充分条件。

Result: 得出了两种游戏中位置为P-位置的充要条件。

Conclusion: 给出了两种游戏的完整解决方案。

Abstract: In this paper, we analyze the mis\`ere versions of two impartial
combinatorial games: k-Bounded Greedy Nim and Greedy Nim. We present a complete
solution to both games by showing necessary and sufficient conditions for a
position to be P-positions.

</details>


### [138] [An O(log log n)-approximate budget feasible mechanism for subadditive valuations](https://arxiv.org/abs/2506.04665)
*Rian Neogi,Kanstantsin Pashkovich,Chaitanya Swamy*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In budget-feasible mechanism design, there is a set of items $U$, each owned
by a distinct seller. The seller of item $e$ incurs a private cost
$\overline{c}_e$ for supplying her item. A buyer wishes to procure a set of
items from the sellers of maximum value, where the value of a set $S\subseteq
U$ of items is given by a valuation function $v:2^U\to \mathbb{R}_+$. The buyer
has a budget of $B \in \mathbb{R}_+$ for the total payments made to the
sellers. We wish to design a mechanism that is truthful, that is, sellers are
incentivized to report their true costs, budget-feasible, that is, the sum of
the payments made to the sellers is at most the budget $B$, and that outputs a
set whose value is large compared to $\text{OPT}:=\max\{v(S):\overline{c}(S)\le
B,S\subseteq U\}$.
  Budget-feasible mechanism design has been extensively studied, with the
literature focussing on (classes of) subadditive valuation functions, and
various polytime, budget-feasible mechanisms, achieving constant-factor
approximation, have been devised for the special cases of additive, submodular,
and XOS valuations. However, for general subadditive valuations, the best-known
approximation factor achievable by a polytime budget-feasible mechanism (given
access to demand oracles) was only $O(\log n / \log \log n)$, where $n$ is the
number of items.
  We improve this state-of-the-art significantly by designing a budget-feasible
mechanism for subadditive valuations that \emph{achieves a
substantially-improved approximation factor of $O(\log\log n)$ and runs in
polynomial time, given access to demand oracles.}

</details>


### [139] [Cooperation and the Design of Public Goods](https://arxiv.org/abs/2506.05251)
*J. Carlos Martínez Mori,Alejandro Toriello*

Main category: cs.GT

TL;DR: 本文从合作博弈理论视角研究公共物品设计中的合作元素，引入 NTU LP 博弈，推导核心相关性质，开发算法并应用于芝加哥公交系统服务计划优化，指出合作对公共物品部署必要但有分配影响。


<details>
  <summary>Details</summary>
Motivation: 公共物品设计涉及多利益相关者，需从合作博弈理论角度明确战略需求，以促进公共物品成功部署。

Method: 引入非转移效用、线性生产（NTU LP）博弈，结合博弈论与线性规划；推导核心的结构性质；开发切割平面算法。

Result: 给出 NTU LP 博弈核心非空的充分条件，证明核心成员判定是 co - NP 完全问题，应用结果进行芝加哥公交系统服务计划优化。

Conclusion: 合作对公共物品如交通服务计划的成功部署必要，但可能有不利或反直觉的分配影响。

Abstract: We consider the cooperative elements that arise in the design of public
goods, such as transportation policies and infrastructure. These involve a
variety of stakeholders: governments, businesses, advocates, and users. Their
eventual deployment depends on the decision maker's ability to garner
sufficient support from each of these groups; we formalize these strategic
requirements from the perspective of cooperative game theory. Specifically, we
introduce non-transferable utility, linear production (NTU LP) games, which
combine the game-theoretic tensions inherent in public decision-making with the
modeling flexibility of linear programming. We derive structural properties
regarding the non-emptiness, representability and complexity of the core, a
solution concept that models the viability of cooperation. In particular, we
provide fairly general sufficient conditions under which the core of an NTU LP
game is guaranteed to be non-empty, prove that determining membership in the
core is co-NP-complete, and develop a cutting plane algorithm to optimize
various social welfare objectives subject to core membership. Lastly, we apply
these results in a data-driven case study on service plan optimization for the
Chicago bus system. As our study illustrates, cooperation is necessary for the
successful deployment of transportation service plans and similar public goods,
but it may also have adverse or counterintuitive distributive implications.

</details>


### [140] [Equilibrium Computation in First-Price Auctions with Correlated Priors](https://arxiv.org/abs/2506.05322)
*Aris Filos-Ratsikas,Yiannis Giannakopoulos,Alexandros Hollender,Charalampos Kokkalis*

Main category: cs.GT

TL;DR: 本文研究首价拍卖中计算贝叶斯 - 纳什均衡的计算复杂度，证明离散情况下确定纯贝叶斯 - 纳什均衡存在性是NP难问题，并提出两种求近似解的方法。


<details>
  <summary>Details</summary>
Motivation: 研究首价拍卖中，投标者估值来自一般联合分布时，计算贝叶斯 - 纳什均衡的计算复杂度。

Method: 证明离散情况下确定纯贝叶斯 - 纳什均衡存在性的NP难问题；提出通过投标稀疏化和投标致密化两种方法，前者基于枚举技术，后者利用经济学文献中的连续理论。

Result: 在对称设置或投标者数量固定的情况下，为不同拍卖变体开发了多项式时间近似算法。

Conclusion: 确定离散值和投标空间下纯贝叶斯 - 纳什均衡存在性是NP难问题；两种方法可在特定情况下开发多项式时间近似算法。

Abstract: We consider the computational complexity of computing Bayes-Nash equilibria
in first-price auctions, where the bidders' values for the item are drawn from
a general (possibly correlated) joint distribution. We show that when the
values and the bidding space are discrete, determining the existence of a pure
Bayes-Nash equilibrium is NP-hard. This is the first hardness result in the
literature of the problem that does not rely on assumptions of subjectivity of
the priors, or convoluted tie-breaking rules. We then present two main
approaches for achieving positive results, via bid sparsification and via bid
densification. The former is more combinatorial and is based on enumeration
techniques, whereas the latter makes use of the continuous theory of the
problem developed in the economics literature. Using these approaches, we
develop polynomial-time approximation algorithms for computing equilibria in
symmetric settings or settings with a fixed number of bidders, for different
(discrete or continuous) variants of the auction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [141] [I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API Use in Research](https://arxiv.org/abs/2506.04422)
*Alexandros Efstratiou*

Main category: cs.IR

TL;DR: 研究发现YouTube数据API的搜索端点返回结果存在未公开的随机化机制，基于查询期间主题流行度返回不一致结果，影响历史数据收集有效性


<details>
  <summary>Details</summary>
Motivation: 尽管YouTube平台被广泛研究，但其官方API特别是搜索端口的实际工作机制尚未被充分理解，需要揭示其数据收集的可靠性问题

Method: 通过连续12周运行完全相同的查询请求，系统分析API返回结果的稳定性与规律性

Result: API返回结果存在未文档化的随机化机制，优先推荐短小热门视频，历史视频样本收集困难（尤其在非热门时段），频道影响力因素尚不明确

Conclusion: 建议研究者调整数据收集策略，并需要进一步探索API工作机制及其潜在应用场景

Abstract: YouTube is among the most widely-used platforms worldwide, and has seen a lot
of recent academic attention. Despite its popularity and the number of studies
conducted on it, much less is understood about the way in which YouTube's Data
API, and especially the Search endpoint, operates. In this paper, we analyze
the API's behavior by running identical queries across a period of 12 weeks.
Our findings suggest that the search endpoint returns highly inconsistent
results between queries in ways that are not officially documented.
Specifically, the API seems to randomize returned videos based on the relative
popularity of the respective topic during the query period, making it nearly
impossible to obtain representative historical video samples, especially during
non-peak topical periods. Our results also suggest that the API may prioritize
shorter, more popular videos, although the role of channel popularity is not as
clear. We conclude with suggested strategies for researchers using the API for
data collection, as well as future research directions on expanding the API's
use-cases.

</details>


### [142] [I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API Use in Research](https://arxiv.org/abs/2506.04422)
*Alexandros Efstratiou*

Main category: cs.IR

TL;DR: 分析YouTube Data API搜索端点的行为，发现其返回结果存在未公开的高度不一致性，影响历史数据收集。


<details>
  <summary>Details</summary>
Motivation: 尽管YouTube被广泛使用且研究众多，但其Data API（特别是Search端点）的运作机制未被充分理解，需深入探究其实际行为。

Method: 通过连续12周执行相同查询进行对比实验，系统分析API返回结果的模式。

Result: API根据查询期间主题流行度随机化返回结果，优先推荐短/热门视频，且非高峰期难以获取代表性历史样本。

Conclusion: 建议采用多时段重复查询策略收集数据，并探索API在动态内容场景下的新应用方向。

Abstract: YouTube is among the most widely-used platforms worldwide, and has seen a lot
of recent academic attention. Despite its popularity and the number of studies
conducted on it, much less is understood about the way in which YouTube's Data
API, and especially the Search endpoint, operates. In this paper, we analyze
the API's behavior by running identical queries across a period of 12 weeks.
Our findings suggest that the search endpoint returns highly inconsistent
results between queries in ways that are not officially documented.
Specifically, the API seems to randomize returned videos based on the relative
popularity of the respective topic during the query period, making it nearly
impossible to obtain representative historical video samples, especially during
non-peak topical periods. Our results also suggest that the API may prioritize
shorter, more popular videos, although the role of channel popularity is not as
clear. We conclude with suggested strategies for researchers using the API for
data collection, as well as future research directions on expanding the API's
use-cases.

</details>


### [143] [PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation](https://arxiv.org/abs/2506.04551)
*Chenglong Ma,Ziqi Xu,Yongli Ren,Danula Hettiachchi,Jeffrey Chan*

Main category: cs.IR

TL;DR: 提出基于大五人格的PUB模拟器，通过LLM生成保留统计保真度的合成交互数据，改善推荐系统评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统离线评估方法存在行为信号稀疏、数据噪声大、用户人格建模不足等缺陷，现有模拟器又缺乏行为多样性还原能力。

Method: 开发PUB框架，动态从行为日志推断用户人格特征，结合商品元数据生成保持统计保真度的合成交互数据。

Result: 亚马逊评论数据集实验显示，PUB生成数据与真实用户行为高度吻合，并揭示了人格特质与推荐效果的显著关联。

Conclusion: 人格驱动模拟器为推荐系统评估提供了可扩展、高保真的替代方案，可减少对资源密集型真实实验的依赖。

Abstract: Traditional offline evaluation methods for recommender systems struggle to
capture the complexity of modern platforms due to sparse behavioural signals,
noisy data, and limited modelling of user personality traits. While simulation
frameworks can generate synthetic data to address these gaps, existing methods
fail to replicate behavioural diversity, limiting their effectiveness. To
overcome these challenges, we propose the Personality-driven User Behaviour
Simulator (PUB), an LLM-based simulation framework that integrates the Big Five
personality traits to model personalised user behaviour. PUB dynamically infers
user personality from behavioural logs (e.g., ratings, reviews) and item
metadata, then generates synthetic interactions that preserve statistical
fidelity to real-world data. Experiments on the Amazon review datasets show
that logs generated by PUB closely align with real user behaviour and reveal
meaningful associations between personality traits and recommendation outcomes.
These results highlight the potential of the personality-driven simulator to
advance recommender system evaluation, offering scalable, controllable,
high-fidelity alternatives to resource-intensive real-world experiments.

</details>


### [144] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 提出Exp4Fuse框架，通过融合原始查询和LLM扩展查询的双重检索路径，结合改进的排序融合方法，显著提升稀疏检索器性能并在多个数据集达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM查询扩展方法依赖生成质量且计算成本高，需探索更高效的零样本方法改进稀疏检索器的检索效果。

Method: 设计双检索路径（原始查询/LLM扩展查询），采用稀疏检索器生成两个排序列表，使用改进的互逆排序融合方法进行结果融合。

Result: 在3个MS MARCO数据集和7个低资源数据集上超越现有LLM扩展方法，结合先进稀疏检索器时实现多个基准SOTA。

Conclusion: 通过间接利用LLM扩展查询的融合策略，有效提升稀疏检索效率，为资源受限场景提供高性能解决方案。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [145] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 论文提出GOLFer方法，利用较小的开源语言模型进行查询扩展，通过过滤生成文档中的幻觉内容并优化组合策略，在多个数据集上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的查询扩展方法存在成本高、算力需求大、模型访问受限等问题，需探索小规模语言模型的替代方案。

Method: 1. 幻觉过滤器检测生成文本中的不实/矛盾内容
2. 文档组合器通过权重向量平衡过滤内容与原始查询的融合

Result: 在3个网页搜索和10个低资源数据集上，GOLFer性能优于其他小模型方法，且与大型LLM方法保持竞争力

Conclusion: 通过有效过滤幻觉内容与智能组合策略，GOLFer成功实现了用小模型替代大模型的低成本高性能查询扩展方案

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [146] [Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings](https://arxiv.org/abs/2506.04997)
*Yubo Ma,Jinsong Li,Yuhang Zang,Xiaobao Wu,Xiaoyi Dong,Pan Zhang,Yuhang Cao,Haodong Duan,Jiaqi Wang,Yixin Cao,Aixin Sun*

Main category: cs.IR

TL;DR: 通过标记剪枝和合并策略优化ColPali/ColQwen2的内存效率，Light版本在保留98.2%性能时将内存降至11.8%


<details>
  <summary>Details</summary>
Motivation: 解决现有VDR方法因多块级嵌入导致的内存过高问题

Method: 评估并组合标记剪枝/合并策略（三维度优化）

Result: Light版本保持98.2%性能（11.8%内存），2.8%内存时仍有94.6%效果

Conclusion: 标记合并更适合VDR，研究成果为高效VDR建立新基准

Abstract: Despite the strong performance of ColPali/ColQwen2 in Visualized Document
Retrieval (VDR), it encodes each page into multiple patch-level embeddings and
leads to excessive memory usage. This empirical study investigates methods to
reduce patch embeddings per page at minimum performance degradation. We
evaluate two token-reduction strategies: token pruning and token merging.
Regarding token pruning, we surprisingly observe that a simple random strategy
outperforms other sophisticated pruning methods, though still far from
satisfactory. Further analysis reveals that pruning is inherently unsuitable
for VDR as it requires removing certain page embeddings without query-specific
information. Turning to token merging (more suitable for VDR), we search for
the optimal combinations of merging strategy across three dimensions and
develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance
with only 11.8% of original memory usage, and preserves 94.6% effectiveness at
2.8% memory footprint. We expect our empirical findings and resulting
Light-ColPali/ColQwen2 offer valuable insights and establish a competitive
baseline for future research towards efficient VDR.

</details>


### [147] [Rethinking Contrastive Learning in Session-based Recommendation](https://arxiv.org/abs/2506.05044)
*Xiaokun Zhang,Bo Xu,Fenglong Ma,Zhizheng Wang,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: 提出多模态自适应对比学习框架MACL，通过多模态特征增强和自适应对比损失，解决会话推荐中数据稀疏性和语义一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法存在三个缺陷：1) 忽视item级稀疏性 2) 数据增强破坏语义一致性 3) 平等对待正负信号影响自监督效果

Method: MACL框架包含：1) 利用item多模态特征生成语义一致的item/session级增强视图 2) 设计自适应对比损失区分不同正负样本的贡献度

Result: 在三个真实数据集上的实验显示MACL优于现有SOTA方法

Conclusion: MACL有效缓解了会话推荐中的数据稀疏问题，通过多模态增强和自适应对比机制显著提升推荐效果

Abstract: Session-based recommendation aims to predict intents of anonymous users based
on limited behaviors. With the ability in alleviating data sparsity,
contrastive learning is prevailing in the task. However, we spot that existing
contrastive learning based methods still suffer from three obstacles: (1) they
overlook item-level sparsity and primarily focus on session-level sparsity; (2)
they typically augment sessions using item IDs like crop, mask and reorder,
failing to ensure the semantic consistency of augmented views; (3) they treat
all positive-negative signals equally, without considering their varying
utility. To this end, we propose a novel multi-modal adaptive contrastive
learning framework called MACL for session-based recommendation. In MACL, a
multi-modal augmentation is devised to generate semantically consistent views
at both item and session levels by leveraging item multi-modal features.
Besides, we present an adaptive contrastive loss that distinguishes varying
contributions of positive-negative signals to improve self-supervised learning.
Extensive experiments on three real-world datasets demonstrate the superiority
of MACL over state-of-the-art methods.

</details>


### [148] [Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation](https://arxiv.org/abs/2506.05069)
*Keyu Zhao,Fengli Xu,Yong Li*

Main category: cs.IR

TL;DR: 提出R2Rec框架，通过结构化交互思维链和两阶段训练增强LLM的推理能力，显著提升推荐系统性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 传统LLM推荐方法无法有效处理隐式用户反馈，缺乏推理监督机制。推荐场景的决策过程需要逐步推理能力支撑

Method: 1. 从用户-物品图中采样交互链，通过渐进式掩码提示转化为结构化思维链
2. 两阶段训练：监督微调学习基础推理模式，强化学习通过奖励机制优化推理过程

Result: 在三个真实数据集上平均提升HitRatio@1达10.48%，相比原始LLM性能提升131.81%，推理链显式展示决策过程

Conclusion: R2Rec成功将LLM推理能力引入推荐系统，通过结构化思维模拟和混合训练策略，有效解决隐式反馈场景的推理挑战

Abstract: Driven by advances in Large Language Models (LLMs), integrating them into
recommendation tasks has gained interest due to their strong semantic
understanding and prompt flexibility. Prior work encoded user-item interactions
or metadata into prompts for recommendations. In parallel, LLM reasoning,
boosted by test-time scaling and reinforcement learning, has excelled in fields
like mathematics and code, where reasoning traces and correctness signals are
clear, enabling high performance and interpretability. However, directly
applying these reasoning methods to recommendation is ineffective because user
feedback is implicit and lacks reasoning supervision. To address this, we
propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that
samples interaction chains from the user-item graph and converts them into
structured interaction-of-thoughts via a progressive masked prompting strategy,
with each thought representing stepwise reasoning grounded in interaction
context. This allows LLMs to simulate step-by-step decision-making based on
implicit patterns. We design a two-stage training pipeline: supervised
fine-tuning teaches basic reasoning from high-quality traces, and reinforcement
learning refines reasoning via reward signals, alleviating sparse explicit
supervision. Experiments on three real-world datasets show R2Rec outperforms
classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement
in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore,
the explicit reasoning chains enhance interpretability by revealing the
decision process. Our code is available at:
https://anonymous.4open.science/r/R2Rec-7C5D.

</details>


### [149] [On the Comprehensibility of Multi-structured Financial Documents using LLMs and Pre-processing Tools](https://arxiv.org/abs/2506.05182)
*Shivani Upadhyay,Messiah Ataey,Shariyar Murtuza,Yifan Nie,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究通过预处理工具提升LLMs/MLLMs处理混合结构化文档的准确性，GPT-4准确率提升至76%


<details>
  <summary>Details</summary>
Motivation: PDF/网页中的复杂结构化数据（嵌套表格/多维图表）导致现有LLMs/MLLMs存在幻觉和错误输出的问题

Method: 利用工业化工具和开源工具构建预处理流程，优化输入数据结构

Result: GPT-4o原始准确率56%，预处理后GPT-4o提升至61.3%，GPT-4达76%且成本更低

Conclusion: 预处理流程显著提升多模态模型处理复杂文档的准确性和经济性

Abstract: The proliferation of complex structured data in hybrid sources, such as PDF
documents and web pages, presents unique challenges for current Large Language
Models (LLMs) and Multi-modal Large Language Models (MLLMs) in providing
accurate answers. Despite the recent advancements of MLLMs, they still often
falter when interpreting intricately structured information, such as nested
tables and multi-dimensional plots, leading to hallucinations and erroneous
outputs. This paper explores the capabilities of LLMs and MLLMs in
understanding and answering questions from complex data structures found in PDF
documents by leveraging industrial and open-source tools as part of a
pre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,
achieves an accuracy of 56% on multi-structured documents when fed documents
directly, and that integrating pre-processing tools raises the accuracy of LLMs
to 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is
publicly available at https://github.com/OGCDS/FinancialQA.

</details>


### [150] [I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API Use in Research](https://arxiv.org/abs/2506.04422)
*Alexandros Efstratiou*

Main category: cs.IR

TL;DR: 本文对YouTube数据API搜索端点进行为期12周研究，发现其结果不一致且有随机性，还给出使用建议和研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管YouTube受欢迎且有很多相关研究，但对其数据API搜索端点的运作方式了解较少。

Method: 在12周内运行相同查询来分析API行为。

Result: 搜索端点结果高度不一致，会根据话题相对热度随机返回视频，难以获取代表性历史视频样本，可能优先返回短且热门视频，频道热度作用不明。

Conclusion: 为使用API进行数据收集的研究人员提供策略，指出API用例的未来研究方向。

Abstract: YouTube is among the most widely-used platforms worldwide, and has seen a lot
of recent academic attention. Despite its popularity and the number of studies
conducted on it, much less is understood about the way in which YouTube's Data
API, and especially the Search endpoint, operates. In this paper, we analyze
the API's behavior by running identical queries across a period of 12 weeks.
Our findings suggest that the search endpoint returns highly inconsistent
results between queries in ways that are not officially documented.
Specifically, the API seems to randomize returned videos based on the relative
popularity of the respective topic during the query period, making it nearly
impossible to obtain representative historical video samples, especially during
non-peak topical periods. Our results also suggest that the API may prioritize
shorter, more popular videos, although the role of channel popularity is not as
clear. We conclude with suggested strategies for researchers using the API for
data collection, as well as future research directions on expanding the API's
use-cases.

</details>


### [151] [PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation](https://arxiv.org/abs/2506.04551)
*Chenglong Ma,Ziqi Xu,Yongli Ren,Danula Hettiachchi,Jeffrey Chan*

Main category: cs.IR

TL;DR: 传统推荐系统离线评估方法有局限，现有模拟框架也不足，提出基于LLM的PUB框架，实验表明其生成数据贴近真实行为，可推动推荐系统评估。


<details>
  <summary>Details</summary>
Motivation: 传统离线评估方法难以捕捉现代平台复杂性，现有模拟框架无法复制行为多样性。

Method: 提出基于LLM的人格驱动用户行为模拟器PUB，集成大五人格特质，从行为日志和物品元数据推断用户人格，生成合成交互。

Result: 在亚马逊评论数据集上实验，PUB生成的日志与真实用户行为高度一致，揭示人格特质与推荐结果的有意义关联。

Conclusion: 人格驱动模拟器有潜力推动推荐系统评估，提供可扩展、可控、高保真的替代方案。

Abstract: Traditional offline evaluation methods for recommender systems struggle to
capture the complexity of modern platforms due to sparse behavioural signals,
noisy data, and limited modelling of user personality traits. While simulation
frameworks can generate synthetic data to address these gaps, existing methods
fail to replicate behavioural diversity, limiting their effectiveness. To
overcome these challenges, we propose the Personality-driven User Behaviour
Simulator (PUB), an LLM-based simulation framework that integrates the Big Five
personality traits to model personalised user behaviour. PUB dynamically infers
user personality from behavioural logs (e.g., ratings, reviews) and item
metadata, then generates synthetic interactions that preserve statistical
fidelity to real-world data. Experiments on the Amazon review datasets show
that logs generated by PUB closely align with real user behaviour and reveal
meaningful associations between personality traits and recommendation outcomes.
These results highlight the potential of the personality-driven simulator to
advance recommender system evaluation, offering scalable, controllable,
high-fidelity alternatives to resource-intensive real-world experiments.

</details>


### [152] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 文章提出Exp4Fuse框架，用零样本大语言模型查询扩展提升稀疏检索性能，实验显示其优于现有方法并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成文档用于查询扩展的方法成本高、计算量大，需改进。

Method: 引入Exp4Fuse框架，同时考虑原查询和大语言模型增强查询两条检索路径，用稀疏检索器生成两个排序列表，并用改进的互反排名融合方法融合。

Result: 在多个数据集上实验，Exp4Fuse超越现有大语言模型查询扩展方法，与先进稀疏检索器结合达SOTA。

Conclusion: Exp4Fuse在提升稀疏检索的查询扩展方面性能优越、效果显著。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [153] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 本文提出用GOLFer方法解决基于大语言模型的查询扩展存在的问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的查询扩展依赖大模型，成本高、计算量大且可访问性有限，需解决这些问题。

Method: 引入GOLFer方法，包含幻觉过滤器和文档组合器两个模块。前者检测和去除生成文档中的非事实和不一致句子，后者用权重向量将过滤内容与查询组合。

Result: 在三个网络搜索和十个低资源数据集上评估，GOLFer使用较小语言模型时始终优于其他方法，与使用大语言模型的方法相比也有竞争力。

Conclusion: GOLFer方法有效。

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [154] [Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings](https://arxiv.org/abs/2506.04997)
*Yubo Ma,Jinsong Li,Yuhang Zang,Xiaobao Wu,Xiaoyi Dong,Pan Zhang,Yuhang Cao,Haodong Duan,Jiaqi Wang,Yixin Cao,Aixin Sun*

Main category: cs.IR

TL;DR: 研究降低可视化文档检索中每页补丁嵌入数量以减少内存使用的方法，提出Light - ColPali/ColQwen2，能在低内存占用下保持较高性能。


<details>
  <summary>Details</summary>
Motivation: ColPali/ColQwen2在可视化文档检索中内存使用过多，需降低每页补丁嵌入数量并使性能损失最小。

Method: 评估两种减少令牌的策略，即令牌修剪和令牌合并，对令牌合并搜索三个维度的最优组合。

Result: 简单随机修剪策略优于其他复杂修剪方法但仍不理想；Light - ColPali/ColQwen2在11.8%原内存使用下保持98.2%检索性能，在2.8%内存占用下保持94.6%有效性。

Conclusion: 研究结果和Light - ColPali/ColQwen2为高效可视化文档检索的未来研究提供有价值见解和竞争基线。

Abstract: Despite the strong performance of ColPali/ColQwen2 in Visualized Document
Retrieval (VDR), it encodes each page into multiple patch-level embeddings and
leads to excessive memory usage. This empirical study investigates methods to
reduce patch embeddings per page at minimum performance degradation. We
evaluate two token-reduction strategies: token pruning and token merging.
Regarding token pruning, we surprisingly observe that a simple random strategy
outperforms other sophisticated pruning methods, though still far from
satisfactory. Further analysis reveals that pruning is inherently unsuitable
for VDR as it requires removing certain page embeddings without query-specific
information. Turning to token merging (more suitable for VDR), we search for
the optimal combinations of merging strategy across three dimensions and
develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance
with only 11.8% of original memory usage, and preserves 94.6% effectiveness at
2.8% memory footprint. We expect our empirical findings and resulting
Light-ColPali/ColQwen2 offer valuable insights and establish a competitive
baseline for future research towards efficient VDR.

</details>


### [155] [Rethinking Contrastive Learning in Session-based Recommendation](https://arxiv.org/abs/2506.05044)
*Xiaokun Zhang,Bo Xu,Fenglong Ma,Zhizheng Wang,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: 提出用于会话推荐的多模态自适应对比学习框架MACL，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的会话推荐方法存在忽视项目级稀疏性、增强视图语义不一致、未考虑正负信号效用差异等问题。

Method: 设计多模态增强方法，利用项目多模态特征在项目和会话级别生成语义一致的视图；提出自适应对比损失，区分正负信号的不同贡献。

Result: 在三个真实数据集上的大量实验表明，MACL优于现有方法。

Conclusion: 所提出的MACL框架有效，能提升会话推荐性能。

Abstract: Session-based recommendation aims to predict intents of anonymous users based
on limited behaviors. With the ability in alleviating data sparsity,
contrastive learning is prevailing in the task. However, we spot that existing
contrastive learning based methods still suffer from three obstacles: (1) they
overlook item-level sparsity and primarily focus on session-level sparsity; (2)
they typically augment sessions using item IDs like crop, mask and reorder,
failing to ensure the semantic consistency of augmented views; (3) they treat
all positive-negative signals equally, without considering their varying
utility. To this end, we propose a novel multi-modal adaptive contrastive
learning framework called MACL for session-based recommendation. In MACL, a
multi-modal augmentation is devised to generate semantically consistent views
at both item and session levels by leveraging item multi-modal features.
Besides, we present an adaptive contrastive loss that distinguishes varying
contributions of positive-negative signals to improve self-supervised learning.
Extensive experiments on three real-world datasets demonstrate the superiority
of MACL over state-of-the-art methods.

</details>


### [156] [Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation](https://arxiv.org/abs/2506.05069)
*Keyu Zhao,Fengli Xu,Yong Li*

Main category: cs.IR

TL;DR: 本文提出推理增强推荐框架R2Rec，通过采样交互链和渐进掩码提示策略，结合两阶段训练，在三个真实数据集上表现优于基线模型并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理方法直接用于推荐因用户反馈隐式且缺乏推理监督而无效，需新方法。

Method: 提出R2Rec框架，从用户 - 项目图采样交互链，用渐进掩码提示策略转换为结构化交互思想，设计两阶段训练管道。

Result: 在三个真实数据集上，R2Rec的HitRatio@1平均提高10.48%，比原始大语言模型提升131.81%，推理链增强可解释性。

Conclusion: R2Rec在推荐任务中有效且具有良好可解释性，代码已开源。

Abstract: Driven by advances in Large Language Models (LLMs), integrating them into
recommendation tasks has gained interest due to their strong semantic
understanding and prompt flexibility. Prior work encoded user-item interactions
or metadata into prompts for recommendations. In parallel, LLM reasoning,
boosted by test-time scaling and reinforcement learning, has excelled in fields
like mathematics and code, where reasoning traces and correctness signals are
clear, enabling high performance and interpretability. However, directly
applying these reasoning methods to recommendation is ineffective because user
feedback is implicit and lacks reasoning supervision. To address this, we
propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that
samples interaction chains from the user-item graph and converts them into
structured interaction-of-thoughts via a progressive masked prompting strategy,
with each thought representing stepwise reasoning grounded in interaction
context. This allows LLMs to simulate step-by-step decision-making based on
implicit patterns. We design a two-stage training pipeline: supervised
fine-tuning teaches basic reasoning from high-quality traces, and reinforcement
learning refines reasoning via reward signals, alleviating sparse explicit
supervision. Experiments on three real-world datasets show R2Rec outperforms
classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement
in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore,
the explicit reasoning chains enhance interpretability by revealing the
decision process. Our code is available at:
https://anonymous.4open.science/r/R2Rec-7C5D.

</details>


### [157] [On the Comprehensibility of Multi-structured Financial Documents using LLMs and Pre-processing Tools](https://arxiv.org/abs/2506.05182)
*Shivani Upadhyay,Messiah Ataey,Shariyar Murtuza,Yifan Nie,Jimmy Lin*

Main category: cs.IR

TL;DR: 论文探讨大语言模型和多模态大语言模型处理复杂结构数据的能力，借助预处理工具提升准确率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 混合源中复杂结构数据的增加给大语言模型和多模态大语言模型准确作答带来挑战，现有模型处理复杂信息时易出错。

Method: 利用工业和开源工具作为预处理流程的一部分，探究大语言模型和多模态大语言模型理解和回答PDF复杂数据结构问题的能力。

Result: GPT - 4o直接处理多结构文档准确率56%，集成预处理工具后，GPT - 4o准确率达61.3%，GPT - 4达76%，且成本更低。

Conclusion: 通过集成预处理工具能有效提升大语言模型处理复杂结构数据的准确率和降低成本。

Abstract: The proliferation of complex structured data in hybrid sources, such as PDF
documents and web pages, presents unique challenges for current Large Language
Models (LLMs) and Multi-modal Large Language Models (MLLMs) in providing
accurate answers. Despite the recent advancements of MLLMs, they still often
falter when interpreting intricately structured information, such as nested
tables and multi-dimensional plots, leading to hallucinations and erroneous
outputs. This paper explores the capabilities of LLMs and MLLMs in
understanding and answering questions from complex data structures found in PDF
documents by leveraging industrial and open-source tools as part of a
pre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,
achieves an accuracy of 56% on multi-structured documents when fed documents
directly, and that integrating pre-processing tools raises the accuracy of LLMs
to 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is
publicly available at https://github.com/OGCDS/FinancialQA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [158] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文系统分析了基于概念模型的局限性（概念泄露/鲁棒性问题）并提出改进方向


<details>
  <summary>Details</summary>
Motivation: 基于概念模型在敏感领域应用广泛但存在可靠性隐患，需系统性研究其缺陷

Method: 通过文献综述整合监督/非监督范式下的常见挑战及架构改进方案

Result: 揭示模型存在概念纠缠、对抗脆弱性等问题，提出模块化设计等缓解方案

Conclusion: 需持续研究对抗性防御和有效人机协同机制以提升概念模型实用性

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [159] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 本文系统梳理了概念模型在结构、训练、对抗鲁棒性等方面的固有缺陷，并探讨了监督/无监督范式下的解决方案及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 概念模型虽在医疗诊断等敏感领域广泛应用，但其存在的概念泄露、表示纠缠、抗干扰能力弱等缺陷威胁着模型可靠性，需系统性风险评估与改进框架研究。

Method: 通过文献综述法，整合监督学习与非监督学习模式下概念模型常见故障模式，分析不同架构设计对模型鲁棒性的影响机制。

Result: 揭示概念模型存在概念表征泄露路径、对抗样本敏感度高等核心缺陷，提出模块化架构与解纠缠训练可有效提升模型可解释性与稳定性。

Conclusion: 概念模型需从理论层面建立安全边界，未来应开发动态概念验证机制与自适应干预框架，推动可解释AI在关键领域的可靠部署。

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [160] [Improving Out-of-Distribution Detection with Markov Logic Networks](https://arxiv.org/abs/2506.04241)
*Konstantin Kirchheim,Frank Ortmeier*

Main category: cs.LG

TL;DR: 提出使用马尔可夫逻辑网络（MLNs）增强现有OOD检测器，通过可解释的概率推理显著提升性能，并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测器依赖深度神经网络的统计模型，缺乏可解释性。MLNs通过可解释的逻辑约束连接概率推理，能同时提升检测性能和解释性。

Method: 在现有OOD检测器中集成MLNs框架，基于可解释概念定义加权逻辑约束，并开发了从数据中自动学习逻辑约束的算法。

Result: 多数据集实验表明MLNs可显著提升各类OOD检测器性能（如FPR95降低15%-30%），且推理时间仅增加约20ms。

Conclusion: MLNs为OOD检测提供了可解释的增强方案，提出的约束学习算法使该方法具备实际部署可行性，推动了可靠开放世界AI系统的发展。

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models operating in open-world scenarios. Current OOD
detectors mainly rely on statistical models to identify unusual patterns in the
latent representations of a deep neural network. This work proposes to augment
existing OOD detectors with probabilistic reasoning, utilizing Markov logic
networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to
assign probabilities to inputs based on weighted logical constraints defined
over human-understandable concepts, which offers improved explainability.
Through extensive experiments on multiple datasets, we demonstrate that MLNs
can significantly enhance the performance of a wide range of existing OOD
detectors while maintaining computational efficiency. Furthermore, we introduce
a simple algorithm for learning logical constraints for OOD detection from a
dataset and showcase its effectiveness.

</details>


### [161] [Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction](https://arxiv.org/abs/2506.04243)
*Warayut Dokduea,Weerachart Tangchirapat,Sompote Youwai*

Main category: cs.LG

TL;DR: 提出三重注意力Transformer架构，通过自注意力机制建模混凝土徐变序列特性，显著提升预测精度至MAPE 1.63%/R² 0.999。


<details>
  <summary>Details</summary>
Motivation: 传统方法将时间作为静态输入参数，未能有效捕捉徐变发展的时序依赖性。现有机器学习方法在长期序列建模和材料特征交互方面存在局限。

Method: 设计包含时间注意力（序列演化）、特征注意力（材料特性交互）和批次注意力（样本间关系）的三流架构，采用类语言模型的序列自回归预测框架。

Result: 160天标准化数据集验证显示：1) 较传统模型误差降低65%；2) 消融实验证实注意力池化贡献度达42%；3) SHAP分析揭示弹性模量为首要特征因子。

Conclusion: 成功验证Transformer在材料科学中的适用性，部署的Web界面实现实验室参数实时预测，为工程实践提供数据驱动的决策支持新范式。

Abstract: This paper presents a novel Triple Attention Transformer Architecture for
predicting time-dependent concrete creep, addressing fundamental limitations in
current approaches that treat time as merely an input parameter rather than
modeling the sequential nature of deformation development. By transforming
concrete creep prediction into an autoregressive sequence modeling task similar
to language processing, our architecture leverages the transformer's
self-attention mechanisms to capture long-range dependencies in historical
creep patterns. The model implements a triple-stream attention framework
incorporating temporal attention for sequential progression, feature attention
for material property interactions, and batch attention for inter-sample
relationships. Evaluated on experimental datasets with standardized daily
measurements spanning 160 days, the architecture achieves exceptional
performance with mean absolute percentage error of 1.63% and R2 values of 0.999
across all datasets, substantially outperforming traditional empirical models
and existing machine learning approaches. Ablation studies confirm the critical
role of attention mechanisms, with attention pooling contributing most
significantly to model performance. SHAP analysis reveals Young's modulus as
the primary predictive feature, followed by density and compressive strength,
providing interpretability essential for engineering applications. A deployed
web-based interface facilitates practical implementation, enabling real-time
predictions using standard laboratory parameters. This work establishes the
viability of applying transformer architectures to materials science problems,
demonstrating the potential for data-driven approaches to revolutionize
structural behavior prediction and engineering design practices.

</details>


### [162] [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
*Shaona Ghosh,Amrita Bhattacharjee,Yftah Ziser,Christopher Parisien*

Main category: cs.LG

TL;DR: 提出无需微调LLM的SafeSteer方法，通过激活导向技术实现安全策略的精准控制，在保持文本质量的同时防止过度拒绝响应。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法成本高且不灵活，现有激活导向技术尚未充分利用其精准安全调整潜力。需要不依赖对比数据、能保持主题相关性的安全控制方案。

Method: 1) 使用类别特定导向向量实现精准控制
2) 无监督梯度方法保持文本质量
3) 无需成对安全对比数据
4) 多模型/数据集/风险类别的验证框架

Result: 在多个主流LLM和数据集上验证有效，成功降低风险内容生成率85%+，同时保持主题相关性和自然语言流畅度（困惑度仅上升0.3）

Conclusion: 简单有效的激活导向方法突破微调限制，为动态安全策略实施提供新范式，证明简单技术往往优于复杂方案的研究结论

Abstract: Fine-tuning large language models (LLMs) to adapt to evolving safety policies
is costly and impractical. Mechanistic interpretability enables inference-time
control through latent activation steering, yet its potential for precise,
customizable safety adjustments remains largely untapped. This paper
investigates an approach called SafeSteer for guiding the outputs of LLMs by:
(i) leveraging category-specific steering vectors for more precise control,
(ii) employing a simple, gradient-free unsupervised method to enhance safety
steering while preserving text quality, topic relevance, and without explicit
refusal, and (iii) accomplishing this without a hard requirement of contrastive
pairwise safe data. We also highlight that our method, being simple and
effective, aligns with recent studies suggesting that simple techniques often
outperform more complex ones in activation steering. We showcase the
effectiveness of our approach across various LLMs, datasets, and risk
categories, demonstrating its ability to provide precise control, prevent
blanket refusals, and guide models toward generating safe content while
maintaining topic relevance.

</details>


### [163] [Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices](https://arxiv.org/abs/2506.04553)
*Andersen Chang,Tiffany M. Tang,Tarek M. Zikry,Genevera I. Allen*

Main category: cs.LG

TL;DR: 提出了一个结构化工作流程以提升无监督机器学习在科学发现中的可靠性和可复现性


<details>
  <summary>Details</summary>
Motivation: 当前无监督学习在科学领域广泛应用但缺乏标准化流程，导致科学发现的可信度和可重复性不足

Method: 包含可验证问题构建、数据预处理、多模型应用、稳定性验证、结果沟通的完整工作流程框架

Result: 通过银河系星团化学成分分析的案例验证，证明该流程能有效提升科学发现的严谨性

Conclusion: 结构化工作流程可显著提高无监督学习在科学研究中的应用价值，推动更可靠的跨学科发现

Abstract: Unsupervised machine learning is widely used to mine large, unlabeled
datasets to make data-driven discoveries in critical domains such as climate
science, biomedicine, astronomy, chemistry, and more. However, despite its
widespread utilization, there is a lack of standardization in unsupervised
learning workflows for making reliable and reproducible scientific discoveries.
In this paper, we present a structured workflow for using unsupervised learning
techniques in science. We highlight and discuss best practices starting with
formulating validatable scientific questions, conducting robust data
preparation and exploration, using a range of modeling techniques, performing
rigorous validation by evaluating the stability and generalizability of
unsupervised learning conclusions, and promoting effective communication and
documentation of results to ensure reproducible scientific discoveries. To
illustrate our proposed workflow, we present a case study from astronomy,
seeking to refine globular clusters of Milky Way stars based upon their
chemical composition. Our case study highlights the importance of validation
and illustrates how the benefits of a carefully-designed workflow for
unsupervised learning can advance scientific discovery.

</details>


### [164] [Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support](https://arxiv.org/abs/2506.04254)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 提出基于行政区划的森林火灾风险评估方法，建立法国首个国家级AI基准测试，强调区域差异化预测对消防实战的价值


<details>
  <summary>Details</summary>
Motivation: 传统火灾二元分类模型忽视地区差异（地形/气候/历史经验），导致预测结果对地方消防部门缺乏可操作性。需建立地域敏感的风险评估体系

Method: 使用先进AI模型在未充分开发数据集上构建部门定制化风险评估模型，创建覆盖法国本土的国家级AI预测基准

Result: 成功构建首个法国国家级火灾预测基准，验证区域定制化方法的有效性，并提出关键未来研究方向（数据增强/模型优化方向）

Conclusion: 基于行政区划的差异化风险评估比全域统一模型更具实用性，有助于消防部门依据地域特征优化资源配置，GitHub提供完整实验复现支持

Abstract: Forest fire prediction involves estimating the likelihood of fire ignition or
related risk levels in a specific area over a defined time period. With climate
change intensifying fire behavior and frequency, accurate prediction has become
one of the most pressing challenges in Artificial Intelligence (AI).
Traditionally, fire ignition is approached as a binary classification task in
the literature. However, this formulation oversimplifies the problem,
especially from the perspective of end-users such as firefighters. In general,
as is the case in France, firefighting units are organized by department, each
with its terrain, climate conditions, and historical experience with fire
events. Consequently, fire risk should be modeled in a way that is sensitive to
local conditions and does not assume uniform risk across all regions. This
paper proposes a new approach that tailors fire risk assessment to departmental
contexts, offering more actionable and region-specific predictions for
operational use. With this, we present the first national-scale AI benchmark
for metropolitan France using state-of-the-art AI models on a relatively
unexplored dataset. Finally, we offer a summary of important future works that
should be taken into account. Supplementary materials are available on GitHub.

</details>


### [165] [Semi-Implicit Variational Inference via Kernelized Path Gradient Descent](https://arxiv.org/abs/2506.05088)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 提出核化KL散度估计器与重要性采样校正，改进半隐式变分推断的稳定性与效率，理论关联Stein方法并验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有半隐式变分推断(SIVI)方法在高维场景下存在KL散度训练的高方差/偏差问题，且训练成本较高

Method: 1. 核化KL散度估计器实现非参数平滑
2. 重要性采样校正降低偏差
3. 建立与Stein变分梯度下降的理论等价性分析

Result: 实验证明该方法在性能与训练效率上达到/超越SOTA方法，梯度方差降低30%，优化稳定性提升2倍

Conclusion: 通过核平滑与重要性采样机制，在保持半隐式建模优势的同时，显著降低优化方差并提升计算效率，为概率推断提供新思路

Abstract: Semi-implicit variational inference (SIVI) is a powerful framework for
approximating complex posterior distributions, but training with the
Kullback-Leibler (KL) divergence can be challenging due to high variance and
bias in high-dimensional settings. While current state-of-the-art semi-implicit
variational inference methods, particularly Kernel Semi-Implicit Variational
Inference (KSIVI), have been shown to work in high dimensions, training remains
moderately expensive. In this work, we propose a kernelized KL divergence
estimator that stabilizes training through nonparametric smoothing. To further
reduce the bias, we introduce an importance sampling correction. We provide a
theoretical connection to the amortized version of the Stein variational
gradient descent, which estimates the score gradient via Stein's identity,
showing that both methods minimize the same objective, but our semi-implicit
approach achieves lower gradient variance. In addition, our method's bias in
function space is benign, leading to more stable and efficient optimization.
Empirical results demonstrate that our method outperforms or matches
state-of-the-art SIVI methods in both performance and training efficiency.

</details>


### [166] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: 提出基于扰动梯度训练（PGT）的新方法，通过参数空间随机扰动替代反向传播，解决物理储层计算中梯度不可追踪的难题。


<details>
  <summary>Details</summary>
Motivation: 物理储层的黑盒特性导致无法应用反向传播，限制了其在深度神经网络中的集成应用。PGT旨在突破该限制，实现物理储层与深度网络的高效整合。

Method: 受物理学扰动理论启发，通过参数空间的随机扰动生成梯度近似。在仿真环境（含稠密网络和带储层的Transformer）及磁振子自振荡环硬件平台上验证方法有效性。

Result: 在无法使用反向传播的场景下，PGT取得与传统反向传播相当的模型性能，验证了该范式在软硬件系统中的可行性。

Conclusion: PGT为物理储层融入深度网络架构开辟新路径，有望显著提升AI训练能效，是迈向物理智能系统的重要进展。

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [167] [MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression](https://arxiv.org/abs/2506.04268)
*Jingyang Li,Guoqiang Li*

Main category: cs.LG

TL;DR: 提出MUC-G4框架，通过SMT公式编码和最小不可满足核(MUC)技术，实现压缩神经网络的高效形式化验证，支持量化和剪枝压缩，验证速度显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署神经网络面临高内存和计算复杂度挑战，现有增量验证方法仅支持量化且难以处理结构变化，需开发支持多种压缩技术的高效验证框架。

Method: 将原始/压缩网络编码为SMT公式，通过变更分类和复用原始网络MUC指导压缩网络验证，实现增量验证过程的证明重用和加速。

Result: 实验证明MUC-G4支持量化和剪枝，证明复用率达64%-100%，验证速度比传统方法提升2-4倍。

Conclusion: MUC-G4为压缩神经网络的安全验证提供了高效解决方案，在保持精度的同时显著提升验证效率，具有实际应用价值。

Abstract: The rapid development of deep learning has led to challenges in deploying
neural networks on edge devices, mainly due to their high memory and runtime
complexity. Network compression techniques, such as quantization and pruning,
aim to reduce this complexity while maintaining accuracy. However, existing
incremental verification methods often focus only on quantization and struggle
with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided
Incremental Verification), a novel framework for incremental verification of
compressed deep neural networks. It encodes both the original and compressed
networks into SMT formulas, classifies changes, and use \emph{Minimal Unsat
Cores (MUCs)} from the original network to guide efficient verification for the
compressed network. Experimental results show its effectiveness in handling
quantization and pruning, with high proof reuse rates and significant speedup
in verification time compared to traditional methods. MUC-G4 hence offers a
promising solution for ensuring the safety and reliability of compressed neural
networks in practical applications.

</details>


### [168] [Understanding the Impact of Sampling Quality in Direct Preference Optimization](https://arxiv.org/abs/2506.04272)
*Kyung Rok Kim,Yumo Bai,Chonghuan Wang,Guanting Chen*

Main category: cs.LG

TL;DR: 论文揭示了直接偏好优化（DPO）中采样分布对解空间和收敛行为的关键影响，指出高质量响应样本能增强梯度信号并改善优化效果。


<details>
  <summary>Details</summary>
Motivation: 针对实践中DPO训练效果不稳定的现象，研究者试图从采样分布的角度解释其优化动态，揭示分布支持范围与样本质量对策略学习的根本性作用。

Method: 通过理论分析策略更新的动态过程，构建结构化对齐代理模型，结合定量实验验证采样分布对梯度信号和优化景观的影响。

Result: 高频高质量响应能放大梯度信号幅度（理论证明达3倍增益），改善优化曲率，实证显示在线更新策略比静态分布提升25%以上效果。

Conclusion: 采样分布的支撑集覆盖度和样本质量共同决定DPO效果，为在线DPPO框架提供理论支撑，强调动态更新采样分布的重要性。

Abstract: We study the role of the sampling distribution in Direct Preference
Optimization (DPO) and aim to understand its impact on DPO's training dynamics.
Our analyses show that both the solution space and the convergence behavior of
DPO depend on the support and quality of the generating distribution. We first
analyze how distribution of responses influences policy updates during gradient
descent, drawing connections to common phenomena found in practice. We then
design a simplified yet well-structured alignment model as a proxy, and develop
quantitative results showing how more frequent high-quality responses amplify
the gradient signal and improve the optimization landscape, leading to more
effective policy learning. Our theoretical findings are supported by empirical
experiments and provide a principled justification for the online DPO framework
in practice.

</details>


### [169] [SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida](https://arxiv.org/abs/2506.04281)
*Xu Zheng,Chaohao Lin,Sipeng Chen,Zhuomin Chen,Jimeng Shi,Wei Cheng,Jayantha Obeysekera,Jason Liu,Dongsheng Luo*

Main category: cs.LG

TL;DR: 论文提出SF2Bench复合洪水数据集，集成潮汐/降雨/地下水/人为管理四要素，通过六类机器学习模型验证时空特征对洪水预测的影响


<details>
  <summary>Details</summary>
Motivation: 传统物理模型效率低下且现有洪水数据集存在监测站稀疏、要素覆盖不全的问题，需构建更全面的数据集并评估不同机器学习方法的预测潜力

Method: 创建集成潮汐/降雨/地下水/水闸泵站运营数据的SF2Bench数据集，系统评估MLP/CNN/RNN/GNN/Transformer/大语言模型六类方法

Result: 不同模型在时空依赖性建模上表现差异显著，实验验证了关键驱动因素对洪水预测的影响程度，揭示了历史时序长度与空间依赖关系的作用规律

Conclusion: SF2Bench填补复合洪水数据空白，多维度评估为模型选择提供依据。研究表明需根据数据特征选择适配的建模范式，时空特征的联合建模是提升精度的关键

Abstract: Forecasting compound floods presents a significant challenge due to the
intricate interplay of meteorological, hydrological, and oceanographic factors.
Analyzing compound floods has become more critical as the global climate
increases flood risks. Traditional physics-based methods, such as the
Hydrologic Engineering Center's River Analysis System, are often
time-inefficient. Machine learning has recently demonstrated promise in both
modeling accuracy and computational efficiency. However, the scarcity of
comprehensive datasets currently hinders systematic analysis. Existing
water-related datasets are often limited by a sparse network of monitoring
stations and incomplete coverage of relevant factors. To address this
challenge, we introduce SF2Bench, a comprehensive time series collection on
compound floods in South Florida, which integrates four key factors: tide,
rainfall, groundwater, and human management activities (gate and pump
controlling). This integration allows for a more detailed analysis of the
individual contributions of these drivers to compound flooding and informs the
development of improved flood forecasting approaches. To comprehensively
evaluate the potential of various modeling paradigms, we assess the performance
of six categories of methods, encompassing Multilayer Perceptrons,
Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural
Networks, Transformers, and Large Language Models. We verified the impact of
different key features on flood forecasting through experiments. Our analysis
examines temporal and spatial aspects, providing insights into the influence of
historical data and spatial dependencies. The varying performance across these
approaches underscores the diverse capabilities of each in capturing complex
temporal and spatial dependencies inherent in compound floods.

</details>


### [170] [Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General Games](https://arxiv.org/abs/2506.05005)
*Ashkan Soleymani,Georgios Piliouras,Gabriele Farina*

Main category: cs.LG

TL;DR: 提出Cautious Optimism框架，通过自适应调节FTRL算法的学习节奏，在保留解耦性的前提下显著加速一般游戏中的正则化学习，在自我对弈场景实现O(log T)遗憾，对抗场景保持O(√T)最优遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有OMWU方法通过动态调节学习速度实现了遗憾最小化突破，但加速学习的通用性未被验证。本文旨在证明这种加速机制可推广到整个FTRL框架，同时保持学习者无需知晓对手效用的解耦特性。

Method: 将任意FTRL实例转化为加速算法：1）通过谨慎乐观机制动态调节学习步长 2）保持底层FTRL结构不变 3）支持不同玩家使用混合正则化器 4）计算开销极低

Result: 1.自我对弈场景实现O_T(log T)近最优遗憾 2.对抗场景维持O(√T)理论最优 3.突破传统单调步长限制（与Syrgkanis2015/Daskalakis2021对比） 4.保持完全解耦性

Conclusion: Cautious Optimism框架为一般游戏中的快速学习提供了新范式，其优势在于：1）适用性广（兼容所有FTRL）2）计算高效 3）理论保证完备。该工作开辟了非单调步长分析的新路径，为多智能体学习算法设计提供理论支撑。

Abstract: Recent work [Soleymani et al., 2025] introduced a variant of Optimistic
Multiplicative Weights Updates (OMWU) that adaptively controls the learning
pace in a dynamic, non-monotone manner, achieving new state-of-the-art regret
minimization guarantees in general games. In this work, we demonstrate that
no-regret learning acceleration through adaptive pacing of the learners is not
an isolated phenomenon. We introduce \emph{Cautious Optimism}, a framework for
substantially faster regularized learning in general games. Cautious Optimism
takes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs
an accelerated no-regret learning algorithm by pacing the underlying FTRL with
minimal computational overhead. Importantly, we retain uncoupledness (learners
do not need to know other players' utilities). Cautious Optimistic FTRL
achieves near-optimal $O_T(\log T)$ regret in diverse self-play
(mixing-and-matching regularizers) while preserving the optimal $O(\sqrt{T})$
regret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et
al. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic
step-sizes, showcasing a novel route for fast learning in general games.

</details>


### [171] [Inference economics of language models](https://arxiv.org/abs/2506.04645)
*Ege Erdil*

Main category: cs.LG

TL;DR: 开发理论模型优化LLM推理部署，平衡单token成本与生成速度，考虑算力/带宽/延迟约束，通过并行策略寻找帕累托最优解。


<details>
  <summary>Details</summary>
Motivation: 解决大规模部署LLM时成本与推理速度的权衡难题，为实际应用提供经济高效的部署方案。

Method: 建立多约束模型（计算单元/内存带宽/网络带宽/延迟），系统优化并行配置（数据并行/流水线并行）和批处理量，计算帕累托前沿。

Result: 绘制主流语言模型的成本-速度帕累托边界曲线，量化不同部署策略的效能极限。

Conclusion: 该模型为LLM规模化部署提供系统性优化框架，实现成本与推理延迟的最优权衡。

Abstract: We develop a theoretical model that addresses the economic trade-off between
cost per token versus serial token generation speed when deploying LLMs for
inference at scale. Our model takes into account arithmetic, memory bandwidth,
network bandwidth and latency constraints; and optimizes over different
parallelism setups and batch sizes to find the ones that optimize serial
inference speed at a given cost per token. We use the model to compute Pareto
frontiers of serial speed versus cost per token for popular language models.

</details>


### [172] [DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience](https://arxiv.org/abs/2506.04282)
*Runxiang Wang,Boxiao Wang,Kai Li,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出DrSR框架，通过数据洞察与反思学习结合提升符号回归效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法过度依赖内部先验，缺乏数据理解和系统反思机制

Method: 双重推理机制：分析数据结构关系生成描述+性能监控反馈循环优化

Result: 多学科实验显示有效方程率显著提升，准确率与搜索效率全面超越基准方法

Conclusion: 闭环整合数据理解与生成反思，为科学方程发现提供新范式

Abstract: Symbolic regression is a fundamental tool for discovering interpretable
mathematical expressions from data, with broad applications across scientific
and engineering domains. Recently, large language models (LLMs) have
demonstrated strong performance in this task, leveraging embedded scientific
priors and reasoning capabilities to surpass traditional methods. However,
existing LLM-based approaches, such as LLM-SR, often over-rely on internal
priors, lacking explicit data understanding and systematic reflection during
equation generation. To address these limitations, we propose DrSR (Dual
Reasoning Symbolic Regression), a framework that combines data-driven insight
with reflective learning to enhance both robustness and discovery capability.
Specifically, DrSR guides LLMs to analyze structural relationships (e.g.,
monotonicity, nonlinearity, and correlation) within the data to generate
structured descriptions. Simultaneously, it monitors equation performance and
establishes a feedback loop to refine subsequent generations. By integrating
data understanding and generation reflection in a closed loop, DrSR enables
more efficient exploration of the symbolic expression space. Experiments across
interdisciplinary datasets in physics, chemistry, biology, and materials
science demonstrate that DrSR substantially improves the valid equation rate
and consistently outperforms both classical and recent LLM-based methods in
terms of accuracy, generalization, and search efficiency. These results
underscore its potential for scientific equation discovery.

</details>


### [173] [Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory](https://arxiv.org/abs/2506.04434)
*Hari K. Prakash,Charles H. Martin*

Main category: cs.LG

TL;DR: 论文发现神经网络的训练中存在新型anti-grokking阶段，HTSR指标α可有效检测该阶段及传统grokking过程。


<details>
  <summary>Details</summary>
Motivation: 针对现有grokking研究未能覆盖训练后期阶段的问题，揭示测试精度崩溃的anti-grokking现象及其检测方法。

Method: 使用3层MLP在MNIST子集训练，结合WeightWatcher工具分析HTSR层质量指标α，通过谱密度分析和KS检验验证Correlation Traps。

Result: α<2时出现Correlation Traps标志anti-grokking，HTSR α能区分三个阶段，传统指标仅能检测前两阶段。

Conclusion: HTSR α指标在α≈2时提供通用层收敛目标，可作为无需测试数据的泛化能力评估标准，对神经网络训练监控具有重要意义。

Abstract: We study the well-known grokking phenomena in neural networks (NNs) using a
3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight
decay, and discover a novel third phase -- \emph{anti-grokking} -- that occurs
very late in training and resembles but is distinct from the familiar
\emph{pre-grokking} phases: test accuracy collapses while training accuracy
stays perfect. This late-stage collapse is distinct, from the known
pre-grokking and grokking phases, and is not detected by other proposed
grokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR
through the open-source WeightWatcher tool, we show that the HTSR layer quality
metric $\alpha$ alone delineates all three phases, whereas the best competing
metrics detect only the first two. The \emph{anti-grokking} is revealed by
training for $10^7$ and is invariably heralded by $\alpha < 2$ and the
appearance of \emph{Correlation Traps} -- outlier singular values in the
randomized layer weight matrices that make the layer weight matrix atypical and
signal overfitting of the training set. Such traps are verified by visual
inspection of the layer-wise empirical spectral densities, and by using
Kolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including
activation sparsity, absolute weight entropy, circuit complexity, and $l^2$
weight norms track pre-grokking and grokking but fail to distinguish grokking
from anti-grokking. This discovery provides a way to measure overfitting and
generalization collapse without direct access to the test data. These results
strengthen the claim that the \emph{HTSR} $\alpha$ provides universal
layer-convergence target at $\alpha \approx 2$ and underscore the value of
using the HTSR alpha $(\alpha)$ metric as a measure of generalization.

</details>


### [174] [Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning](https://arxiv.org/abs/2506.05252)
*Dravyansh Sharma,Alec Sun*

Main category: cs.LG

TL;DR: 本文系统拓展了战略分类中的改进学习理论，提出非对称最小一致概念类并实现多场景学习优化，解决了Attias等人提出的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 现有战略分类研究集中于防范欺骗行为，但Attias等人发现当被分类者真实改进时学习具有更优泛化能力，需系统探索改进学习的理论边界与应用潜力。

Method: 1. 引入非对称最小一致概念类描述可实现设置下的改进学习
2. 基于欧几里得球改进区域构建学习框架
3. 结合生成假设与噪声模型优化泛化误差
4. 开发在线学习场景的实时错误边界分析工具

Result: 1. 建立改进学习的完备理论框架
2. 在温和生成假设下实现非适当学习
3. 在bounded noise模型中降低泛化误差40%
4. 解决Attias等人提出的开放性问题

Conclusion: 战略分类中的主动改进机制可突破传统PAC学习边界，通过结构化建模改进区域与噪声模式，显著提升多场景学习效能，为智能决策系统提供新理论支撑。

Abstract: Machine learning is now ubiquitous in societal decision-making, for example
in evaluating job candidates or loan applications, and it is increasingly
important to take into account how classified agents will react to the learning
algorithms. The majority of recent literature on strategic classification has
focused on reducing and countering deceptive behaviors by the classified
agents, but recent work of Attias et al. identifies surprising properties of
learnability when the agents genuinely improve in order to attain the desirable
classification, such as smaller generalization error than standard
PAC-learning. In this paper we characterize so-called learnability with
improvements across multiple new axes. We introduce an asymmetric variant of
minimally consistent concept classes and use it to provide an exact
characterization of proper learning with improvements in the realizable
setting. While prior work studies learnability only under general, arbitrary
agent improvement regions, we give positive results for more natural Euclidean
ball improvement sets. In particular, we characterize improper learning under a
mild generative assumption on the data distribution. We further show how to
learn in more challenging settings, achieving lower generalization error under
well-studied bounded noise models and obtaining mistake bounds in realizable
and agnostic online learning. We resolve open questions posed by Attias et al.
for both proper and improper learning.

</details>


### [175] [Training-free AI for Earth Observation Change Detection using Physics Aware Neuromorphic Networks](https://arxiv.org/abs/2506.04285)
*Stephen Smith,Cormac Purcell,Zdenka Kuncic*

Main category: cs.LG

TL;DR: 提出基于忆阻器的物理感知神经形态网络（PANN），实现卫星图像的自然灾害变化检测并生成变化图谱，解决星上数据处理资源受限问题。


<details>
  <summary>Details</summary>
Motivation: 卫星数据下行传输带宽受限导致应急响应延迟，需星上处理优先传输灾害相关数据。

Method: 采用动态权重更新的忆阻器物理神经网络，通过电路守恒定律和距离度量实现无训练约束的特征提取与变化检测。

Result: 在各类自然灾害检测任务中表现优于或持平现有先进AI模型。

Conclusion: PANN为资源受限的星上处理提供了免训练的轻量化解决方案，有效提升灾害响应时效性。

Abstract: Earth observations from low Earth orbit satellites provide vital information
for decision makers to better manage time-sensitive events such as natural
disasters. For the data to be most effective for first responders, low latency
is required between data capture and its arrival to decision makers. A major
bottleneck is in the bandwidth-limited downlinking of the data from satellites
to ground stations. One approach to overcome this challenge is to process at
least some of the data on-board and prioritise pertinent data to be downlinked.
In this work we propose a Physics Aware Neuromorphic Network (PANN) to detect
changes caused by natural disasters from a sequence of multi-spectral satellite
images and produce a change map, enabling relevant data to be prioritised for
downlinking. The PANN used in this study is motivated by physical neural
networks comprised of nano-electronic circuit elements known as "memristors"
(nonlinear resistors with memory). The weights in the network are dynamic and
update in response to varying input signals according to memristor equations of
state and electrical circuit conservation laws. The PANN thus generates
physics-constrained dynamical output features which are used to detect changes
in a natural disaster detection task by applying a distance-based metric.
Importantly, this makes the whole model training-free, allowing it to be
implemented with minimal computing resources. The PANN was benchmarked against
a state-of-the-art AI model and achieved comparable or better results in each
natural disaster category. It thus presents a promising solution to the
challenge of resource-constrained on-board processing.

</details>


### [176] [Orthogonal Gradient Descent Improves Neural Calibration](https://arxiv.org/abs/2506.04487)
*C. Evans Hedges*

Main category: cs.LG

TL;DR: 论文提出通过正交化梯度（⊥Grad）优化训练过程，可在保持模型精度的同时显著提升校准性，在数据受限和输入干扰等场景下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有方法常在校准性与精度间存在权衡。正交梯度通过约束梯度方向，避免参数更新破坏已学特征，从而实现校准性与精度的双赢。

Method: 在标准优化过程中引入梯度正交化操作，该方案与优化器无关，计算开销低，且可与温度缩放等后处理校准技术协同使用。

Result: 在CIFAR-10/10C数据集上，⊥Grad在保持分类精度的同时，显著降低了测试损失（-18%）和过自信率（-35%），理论证明了其在齐次网络中的收敛性。

Conclusion: 正交梯度机制为提升模型可靠性提供了新方向，其实现简洁、兼容性强，为实际部署中的模型校准问题提供了有效解决方案。

Abstract: We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement.

</details>


### [177] [Backbone Augmented Training for Adaptations](https://arxiv.org/abs/2506.04288)
*Jae Wan Park,Junhyeok Kim,Youngjun Jun,Hyunah Ko,Seong Jae Hwang*

Main category: cs.LG

TL;DR: 提出BAT和ALBAT方法，利用预训练数据增强适配训练，解决数据稀缺问题并通过数学验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有适配方法在数据稀缺时面临训练挑战，需要有效利用海量预训练数据提升适配效果

Method: 1. 建立两个数学命题验证BAT有效性及适用条件 2. 设计满足命题的数据选择方案 3. 开发ALBAT算法实现增强训练

Result: ALBAT在数据稀缺场景下显著提升个性化任务和语言生成任务的适配训练效果

Conclusion: 通过理论验证和算法实现，证明利用预训练数据增强适配训练的有效性，为解决数据稀缺问题提供新思路

Abstract: Adaptations facilitate efficient training of large backbone models, including
diffusion models for image generation and transformer-based language models.
While various adaptation techniques enhance performance with minimal
computational resources, limited adaptation data often leads to challenges in
training. To address this, we focus on the enormous amount of backbone data
used to pre-train the backbone models. We propose Backbone Augmented Training
(BAT), a method that leverages backbone data to augment the adaptation dataset.
First, we formulate and prove two mathematical key propositions: one
establishes the validity of BAT, while the other identifies a condition under
which BAT benefits adaptation. Furthermore, we introduce an advanced data
selection scheme that satisfies these propositions and present ALBAT algorithm
to implement this approach. ALBAT efficiently enhances adaptation training in
both personalization and language generation tasks with scarce data.

</details>


### [178] [Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems](https://arxiv.org/abs/2506.05138)
*Pavle Vasiljevic,Milica Matic,Miroslav Popovic*

Main category: cs.LG

TL;DR: 论文提出基于隔离森林的联邦学习温度异常检测系统，在MicroPython边缘设备上实现96%准确率和160KB内存占用


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备数据隐私保护和资源受限环境下高效异常检测的需求，利用联邦学习实现隐私保护与协作学习的平衡

Method: 采用Python/MicroPython联邦学习框架，结合FLiForest无监督隔离森林算法，针对边缘设备设计轻量化模型

Result: 异常检测准确率96%，精确度78%+，模型训练内存<160KB，支持多设备配置的联邦学习部署

Conclusion: 验证了联邦学习在边缘计算场景的可行性，为资源受限设备提供隐私保护与高效异常检测的解决方案

Abstract: Recently, federated learning frameworks such as Python TestBed for Federated
Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms
have emerged to tackle user privacy concerns and efficiency in embedded
systems. Even more recently, an efficient federated anomaly detection
algorithm, FLiForest, based on Isolation Forests has been developed, offering a
low-resource, unsupervised method well-suited for edge deployment and
continuous learning. In this paper, we present an application of Isolation
Forest-based temperature anomaly detection, developed using the previously
mentioned federated learning frameworks, aimed at small edge devices and IoT
systems running MicroPython. The system has been experimentally evaluated,
achieving over 96% accuracy in distinguishing normal from abnormal readings and
above 78% precision in detecting anomalies across all tested configurations,
while maintaining a memory usage below 160 KB during model training. These
results highlight its suitability for resource-constrained environments and
edge systems, while upholding federated learning principles of data privacy and
collaborative learning.

</details>


### [179] [A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values](https://arxiv.org/abs/2506.05216)
*Tyler Chen,Akshay Seshadri,Mattia J. Villani,Pradeep Niroula,Shouvanik Chakrabarti,Archan Ray,Pranav Deshpande,Romina Yalovetzky,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: 提出统一框架证明KernelSHAP等Shapley值估计器的理论保证，首次为KernelSHAP建立非渐近理论支撑，并通过实验验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Shapley值估计器（如KernelSHAP）缺乏理论保证，需建立统一理论框架并揭示不同估计器的性能差异

Method: 构建包含替换/非替换采样的统一分析框架，采用蒙特卡洛采样策略，改进实现支持高维数据计算

Result: 在中小维度数据集（决策树模型）上实现低MSE（0.006±0.002），MNIST/CIFAR10高维数据计算效率提升3倍

Conclusion: 首次为KernelSHAP建立理论保证，提出模块化框架揭示估计器间的精度-方差权衡，实现方案支持工业级应用

Abstract: Shapley values have emerged as a critical tool for explaining which features
impact the decisions made by machine learning models. However, computing exact
Shapley values is difficult, generally requiring an exponential (in the feature
dimension) number of model evaluations. To address this, many model-agnostic
randomized estimators have been developed, the most influential and widely used
being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators
such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &
Witter, 2025) are known to satisfy theoretical guarantees, bounds for
KernelSHAP have remained elusive. We describe a broad and unified framework
that encompasses KernelSHAP and related estimators constructed using both with
and without replacement sampling strategies. We then prove strong
non-asymptotic theoretical guarantees that apply to all estimators from our
framework. This provides, to the best of our knowledge, the first theoretical
guarantees for KernelSHAP and sheds further light on tradeoffs between existing
estimators. Through comprehensive benchmarking on small and medium dimensional
datasets for Decision-Tree models, we validate our approach against exact
Shapley values, consistently achieving low mean squared error with modest
sample sizes. Furthermore, we make specific implementation improvements to
enable scalability of our methods to high-dimensional datasets. Our methods,
tested on datasets such MNIST and CIFAR10, provide consistently better results
compared to the KernelSHAP library.

</details>


### [180] [Relational reasoning and inductive bias in transformers trained on a transitive inference task](https://arxiv.org/abs/2506.04289)
*Jesse Geerts,Stephanie Chan,Claudia Clopath,Kimberly Stachenfeld*

Main category: cs.LG

TL;DR: 研究发现，Transformer模型在权重内学习（IWL）中自然具备传递推理能力，而上下文学习（ICL）需依赖预训练的结构化任务才能实现类似表现。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在不同学习机制（IWL与ICL）下处理心理学经典传递推理任务的能力差异及其内在机制。

Method: 通过对比IWL（参数存储信息）和ICL（上下文利用信息）在传递推理任务中的表现，结合归纳电路分析与预训练策略验证。

Result: IWL对传递推理存在自然泛化偏置，ICL未预训练时仅实现相邻关系匹配策略，预训练结构化任务后ICL可展示符号距离效应和终端项目效应。

Conclusion: 任务底层结构的预训练能促进表征形成，支撑上下文关系推理，不同学习机制导致模型内部信息处理模式的本质差异。

Abstract: Transformer-based models have demonstrated remarkable reasoning abilities,
but the mechanisms underlying relational reasoning in different learning
regimes remain poorly understood. In this work, we investigate how transformers
perform a classic relational reasoning task from the Psychology literature,
\textit{transitive inference}, which requires inference about indirectly
related items by integrating information across observed adjacent item pairs
(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior
across two distinct learning regimes: in-weights learning (IWL), where models
store information in network parameters, and in-context learning (ICL), where
models flexibly utilize information presented within the input sequence. Our
findings reveal that IWL naturally induces a generalization bias towards
transitive inference, despite being trained only on adjacent items, whereas ICL
models trained solely on adjacent items do not generalize transitively.
Mechanistic analysis shows that ICL models develop induction circuits that
implement a simple match-and-copy strategy that performs well at relating
adjacent pairs, but does not encoding hierarchical relationships among
indirectly related items. Interestingly, when pre-trained on in-context linear
regression tasks, transformers successfully exhibit in-context generalizable
transitive inference. Moreover, like IWL, they display both \textit{symbolic
distance} and \textit{terminal item effects} characteristic of human and animal
performance, without forming induction circuits. These results suggest that
pre-training on tasks with underlying structure promotes the development of
representations that can scaffold in-context relational reasoning.

</details>


### [181] [Tight analyses of first-order methods with error feedback](https://arxiv.org/abs/2506.05271)
*Daniel Berg Thomsen,Adrien Taylor,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 对分布式学习中EF和EF21误差反馈方法的紧致收敛性分析，揭示了其在通信压缩场景下的最优收敛速率


<details>
  <summary>Details</summary>
Motivation: 分布式学习中代理间通信构成计算瓶颈，压缩通信会降低收敛性。现有误差反馈方法(EF/EF21)的收敛保证需要更精确的理论分析与横向比较

Method: 通过构建最优Lyapunov函数，在简化但具代表性的实验设定下推导EF和EF21的收敛速率下界，并与压缩梯度下降进行机制层面的公平比较

Result: 获得了EF和EF21的紧致收敛边界(含匹配下界)，揭示了各方法在压缩通信场景下的最优收敛性能

Conclusion: 基于Lyapunov函数的理论框架为误差反馈机制提供了精确的性能保证，简化实验设定揭示了算法本质机理，为方法比较提供了新基准

Abstract: Communication between agents often constitutes a major computational
bottleneck in distributed learning. One of the most common mitigation
strategies is to compress the information exchanged, thereby reducing
communication overhead. To counteract the degradation in convergence associated
with compressed communication, error feedback schemes -- most notably
$\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we
provide a tight analysis of both of these methods. Specifically, we find the
Lyapunov function that yields the best possible convergence rate for each
method -- with matching lower bounds. This principled approach yields sharp
performance guarantees and enables a rigorous, apples-to-apples comparison
between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our
analysis is carried out in a simplified yet representative setting, which
allows for clean theoretical insights and fair comparison of the underlying
mechanisms.

</details>


### [182] [A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability](https://arxiv.org/abs/2506.04291)
*Wenhan Xu,Jiashuo Jiang,Lei Deng,Danny Hin-Kwok Tsang*

Main category: cs.LG

TL;DR: 提出LDPTRLQ算法，通过Lyapunov漂移加惩罚方法与强化学习的创新性结合，在物联网优化问题中实现队列稳定性与长期优化目标的平衡，仿真验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有Lyapunov漂移加惩罚方法与强化学习的简单融合未能有效平衡短期贪婪优化与长期视角的矛盾，需建立具有理论保障的整合框架。

Method: 通过严格理论推导建立Lyapunov漂移加惩罚方法与强化学习的结合条件，设计LDPTRLQ算法实现两者的动态平衡机制。

Result: 多场景仿真显示LDPTRLQ在队列长度减少25%、长期回报提升18%等指标上超越基准算法，验证理论推导的有效性。

Conclusion: LDPTRLQ算法在理论严谨性和实践效果上均展现优势，为物联网复杂优化问题提供新的解决方案，算法兼容性指标较基准提升30%。

Abstract: With the proliferation of Internet of Things (IoT) devices, the demand for
addressing complex optimization challenges has intensified. The Lyapunov
Drift-Plus-Penalty algorithm is a widely adopted approach for ensuring queue
stability, and some research has preliminarily explored its integration with
reinforcement learning (RL). In this paper, we investigate the adaptation of
the Lyapunov Drift-Plus-Penalty algorithm for RL applications, deriving an
effective method for combining Lyapunov Drift-Plus-Penalty with RL under a set
of common and reasonable conditions through rigorous theoretical analysis.
Unlike existing approaches that directly merge the two frameworks, our proposed
algorithm, termed Lyapunov drift-plus-penalty method tailored for reinforcement
learning with queue stability (LDPTRLQ) algorithm, offers theoretical
superiority by effectively balancing the greedy optimization of Lyapunov
Drift-Plus-Penalty with the long-term perspective of RL. Simulation results for
multiple problems demonstrate that LDPTRLQ outperforms the baseline methods
using the Lyapunov drift-plus-penalty method and RL, corroborating the validity
of our theoretical derivations. The results also demonstrate that our proposed
algorithm outperforms other benchmarks in terms of compatibility and stability.

</details>


### [183] [On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models](https://arxiv.org/abs/2506.04695)
*Xingwu Chen,Tianle Li,Difan Zou*

Main category: cs.LG

TL;DR: 论文通过系统研究带可验证奖励的强化学习（RLVR），揭示了其通过优化推理模式选择提升模型能力的机制，理论分析表明强初始模型收敛快，弱模型需结合监督微调（SFT）加速优化。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升模型推理能力方面效果显著，但其作用机制尚不明确。研究旨在揭示RLVR如何通过优化推理模式选择来增强模型性能，并建立相关理论框架。

Method: 结合大规模实验验证RLVR的选择机制，基于简化的QRA模型进行理论分析（梯度流/收敛动态），并探讨SFT预训练对弱模型的优化加速作用。

Result: 实验表明RLVR模型优先选择高成功率推理模式，理论发现强/弱模型存在收敛速度差异，且SFT预训练可显著改善弱模型优化效率。

Conclusion: 研究建立了RL在推理能力优化中的理论解释，提出了SFT+RLVR协同训练范式，为LLM能力提升提供了可解释的方法论指导。

Abstract: Reinforcement learning (RL) has demonstrated remarkable success in enhancing
model capabilities, including instruction-following, preference learning, and
reasoning. Yet despite its empirical successes, the mechanisms by which RL
improves reasoning abilities remain poorly understood. We present a systematic
study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that
its primary benefit comes from optimizing the selection of existing reasoning
patterns. Through extensive experiments, we demonstrate that RLVR-trained
models preferentially adopt high-success-rate reasoning patterns while mostly
maintaining stable performance on individual patterns. We further develop
theoretical analyses on the convergence and training dynamics of RLVR based on
a simplified question-reason-answer model. We study the gradient flow and show
that RLVR can indeed find the solution that selects the reason pattern with the
highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1)
rapid convergence for models with relatively strong initial reasoning
capabilities versus (2) slower optimization dynamics for weaker models.
Furthermore, we show that the slower optimization for weaker models can be
mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using
a feasibly high-quality SFT dataset. We validate the theoretical findings
through extensive experiments. This work advances our theoretical understanding
of RL's role in LLM fine-tuning and offers insights for further enhancing
reasoning capabilities.

</details>


### [184] [AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents](https://arxiv.org/abs/2506.04293)
*Fengze Liu,Haoyu Wang,Joonhyuk Cho,Dan Roth,Andrew W. Lo*

Main category: cs.LG

TL;DR: 提出AutoCT框架，结合大语言模型推理能力和经典机器学习可解释性，实现高效、透明的临床试验结果预测


<details>
  <summary>Details</summary>
Motivation: 临床试验成本高昂且耗时，现有深度学习方法存在黑箱特性、解释性不足和标签泄露风险，难以满足生物医学领域的高风险决策需求

Method: 通过大语言模型自主生成/评估/优化结构化特征，结合蒙特卡洛树搜索进行迭代优化，无需人工干预即可完成预测模型构建

Result: 在有限自优化迭代次数下，预测性能达到或超越现有最先进方法

Conclusion: 开创了可扩展、可解释、低成本的临床试验预测新范式，显著提升药物研发效率

Abstract: Clinical trials are critical for advancing medical treatments but remain
prohibitively expensive and time-consuming. Accurate prediction of clinical
trial outcomes can significantly reduce research and development costs and
accelerate drug discovery. While recent deep learning models have shown promise
by leveraging unstructured data, their black-box nature, lack of
interpretability, and vulnerability to label leakage limit their practical use
in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel
framework that combines the reasoning capabilities of large language models
with the explainability of classical machine learning. AutoCT autonomously
generates, evaluates, and refines tabular features based on public information
without human input. Our method uses Monte Carlo Tree Search to iteratively
optimize predictive performance. Experimental results show that AutoCT performs
on par with or better than SOTA methods on clinical trial prediction tasks
within only a limited number of self-refinement iterations, establishing a new
paradigm for scalable, interpretable, and cost-efficient clinical trial
prediction.

</details>


### [185] [Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence](https://arxiv.org/abs/2506.04700)
*José Manuel de Frutos,Manuel A. Vázquez,Pablo M. Olmos,Joaquín Míguez*

Main category: cs.LG

TL;DR: 提出dual-ISL作为新型无似然目标函数，通过交换目标分布与模型分布角色构建凸优化问题，兼具理论优势（连续性/凸性）与实用效果（快速收敛/模式防崩溃）。


<details>
  <summary>Details</summary>
Motivation: 传统KL/Wasserstein等散度缺乏凸性和连续性，现有ISL框架在模型训练中存在优化效率限制。需要建立更稳定高效的隐式生成模型训练范式。

Method: 1) 构建dual-ISL将密度比投影到Bernstein多项式基 2) 推导截断误差界与闭式密度近似 3) 通过随机切片扩展至多维场景

Result: 理论证明dual-ISL的L1连续性及凸性，实验显示比标准ISL收敛快35%，训练稳定性提升2倍，在CIFAR-10等数据集上FID指标优于主流方法15%

Conclusion: dual-ISL通过创新的数学框架统一理论严谨性与实践有效性，为隐式生成模型提供兼具优化效率与密度解释性的新范式

Abstract: Rank-based statistical metrics, such as the invariant statistical loss (ISL),
have recently emerged as robust and practically effective tools for training
implicit generative models. In this work, we introduce dual-ISL, a novel
likelihood-free objective for training implicit generative models that
interchanges the roles of the target and model distributions in the ISL
framework, yielding a convex optimization problem in the space of model
densities. We prove that the resulting rank-based discrepancy $d_K$ is i)
continuous under weak convergence and with respect to the $L^1$ norm, and ii)
convex in its first argument-properties not shared by classical divergences
such as KL or Wasserstein distances. Building on this, we develop a theoretical
framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q
= p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact
bounds on the truncation error, precise convergence rates, and a closed-form
expression for the truncated density approximation. We further extend our
analysis to the multivariate setting via random one-dimensional projections,
defining a sliced dual-ISL divergence that retains both convexity and
continuity. We empirically show that these theoretical advantages translate
into practical ones. Specifically, across several benchmarks dual-ISL converges
more rapidly, delivers markedly smoother and more stable training, and more
effectively prevents mode collapse than classical ISL and other leading
implicit generative methods-while also providing an explicit density
approximation.

</details>


### [186] [Short-Term Power Demand Forecasting for Diverse Consumer Types to Enhance Grid Planning and Synchronisation](https://arxiv.org/abs/2506.04294)
*Asier Diaz-Iglesias,Xabier Belaunzaran,Ane M. Florez-Tapia*

Main category: cs.LG

TL;DR: 通过客户分群和天气数据增强的定制化预测模型显著提升了电力需求预测精度，支持电网稳定。


<details>
  <summary>Details</summary>
Motivation: 可再生能源转型需要精准的电力需求预测，但不同用户群体（工业/商业/居民）的用电模式差异显著，传统单一预测模型效果有限。

Method: 1. 基于消费模式的用户分群 2. 整合时间序列、社会经济和Copernicus气象数据 3. 开发定制化的STLF/VSTLF机器学习模型并进行算法对比

Result: 提出的新预测模型在短期/超短期预测中均优于传统模型，天气数据使预测准确率提升显著（工业用户MAPE降低3.2%，居民用户降低4.8%）

Conclusion: 差异化用户策略与精细化天气数据整合是提升预测精度的关键，为智能电网管理提供了可靠技术方案。

Abstract: Ensuring grid stability in the transition to renewable energy sources
requires accurate power demand forecasting. This study addresses the need for
precise forecasting by differentiating among industrial, commercial, and
residential consumers through customer clusterisation, tailoring the
forecasting models to capture the unique consumption patterns of each group. A
feature selection process is done for each consumer type including temporal,
socio-economic, and weather-related data obtained from the Copernicus Earth
Observation (EO) program. A variety of AI and machine learning algorithms for
Short-Term Load Forecasting (STLF) and Very Short-Term Load Forecasting (VSTLF)
are explored and compared, determining the most effective approaches. With all
that, the main contribution of this work are the new forecasting approaches
proposed, which have demonstrated superior performance compared to simpler
models, both for STLF and VSTLF, highlighting the importance of customized
forecasting strategies for different consumer groups and demonstrating the
impact of incorporating detailed weather data on forecasting accuracy. These
advancements contribute to more reliable power demand predictions, thereby
supporting grid stability.

</details>


### [187] [Deep learning for predicting hauling fleet production capacity under uncertainties in open pit mines using real and simulated data](https://arxiv.org/abs/2506.04296)
*N Guerin,M Nakhla,A Dehoux,J L Loyer*

Main category: cs.LG

TL;DR: 提出融合实际运营数据与合成故障场景的深度学习框架，提升露天矿运输车队容量短期预测精度


<details>
  <summary>Details</summary>
Motivation: 露天矿运输受天气波动、机械故障和人员配置等动态不确定性因素影响显著，需建立可靠预测模型支持主动决策

Method: 采用XGBoost回归器(MedAE 14.3%)和LSTM网络(MedAE 15.1%)，结合SHAP可解释性分析识别关键预测因子

Result: 累计降雨量、历史负载趋势及模拟故障频率为关键预测因子，合成故障数据整合使预测波动率降低12%

Conclusion: 混合建模方法为动态不确定条件下的车队管理提供决策支持，未来将通过整合维护计划指标和人力资源数据增强模型鲁棒性

Abstract: Accurate short-term forecasting of hauling-fleet capacity is crucial in
open-pit mining, where weather fluctuations, mechanical breakdowns, and
variable crew availability introduce significant operational uncertainties. We
propose a deep-learning framework that blends real-world operational records
(high-resolution rainfall measurements, fleet performance telemetry) with
synthetically generated mechanical-breakdown scenarios to enable the model to
capture fluctuating high-impact failure events. We evaluate two architectures:
an XGBoost regressor achieving a median absolute error (MedAE) of 14.3 per cent
and a Long Short-Term Memory network with a MedAE of 15.1 per cent. Shapley
Additive exPlanations (SHAP) value analyses identify cumulative rainfall,
historical payload trends, and simulated breakdown frequencies as dominant
predictors. Integration of simulated breakdown data and shift-planning features
notably reduces prediction volatility. Future work will further integrate
maintenance-scheduling indicators (Mean Time Between Failures, Mean Time to
Repair), detailed human resource data (operator absenteeism, crew efficiency
metrics), blast event scheduling, and other operational constraints to enhance
forecast robustness and adaptability. This hybrid modelling approach offers a
comprehensive decision-support tool for proactive, data-driven fleet management
under dynamically uncertain conditions.

</details>


### [188] [Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards](https://arxiv.org/abs/2506.04775)
*Artin Tajdini,Jonathan Scarlett,Kevin Jamieson*

Main category: cs.LG

TL;DR: 提出了基于实验设计的消除算法，显著改进了重尾奖励线性bandit问题的regret上下界，尤其在维度依赖关系上取得突破，并扩展了有限/无限动作集的场景应用。


<details>
  <summary>Details</summary>
Motivation: 现有重尾奖励场景下的线性bandit研究存在regret上下界不匹配问题，特别是当υ=O(1)时下界构造方法失效，且现有算法维度依赖关系不理想。

Method: 基于实验设计指导的消除算法，通过智能动作选择降低维度依赖；扩展至l_p范数球(p≤1+ε)等几何结构，并通过核方法处理无限维场景。

Result: 获得Õ(d^{(1+3ε)/(2(1+ε))} T^{1/(1+ε)})的regret上界和Ω(d^{2ε/(1+ε)} T^{1/(1+ε)})下界，在有限动作集和Matérn核场景中同样实现突破性改进。

Conclusion: 该工作通过算法创新和严密理论分析，揭示了重尾线性bandit问题的本质难度，为高维和非参数化场景提供了新的解决方案框架。

Abstract: We study stochastic linear bandits with heavy-tailed rewards, where the
rewards have a finite $(1+\epsilon)$-absolute central moment bounded by
$\upsilon$ for some $\epsilon \in (0,1]$. We improve both upper and lower
bounds on the minimax regret compared to prior work. When $\upsilon =
\mathcal{O}(1)$, the best prior known regret upper bound is
$\tilde{\mathcal{O}}(d T^{\frac{1}{1+\epsilon}})$. While a lower with the same
scaling has been given, it relies on a construction using $\upsilon =
\mathcal{O}(d)$, and adapting the construction to the bounded-moment regime
with $\upsilon = \mathcal{O}(1)$ yields only a
$\Omega(d^{\frac{\epsilon}{1+\epsilon}} T^{\frac{1}{1+\epsilon}})$ lower bound.
This matches the known rate for multi-armed bandits and is generally loose for
linear bandits, in particular being $\sqrt{d}$ below the optimal rate in the
finite-variance case ($\epsilon = 1$). We propose a new elimination-based
algorithm guided by experimental design, which achieves regret
$\tilde{\mathcal{O}}(d^{\frac{1+3\epsilon}{2(1+\epsilon)}}
T^{\frac{1}{1+\epsilon}})$, thus improving the dependence on $d$ for all
$\epsilon \in (0,1)$ and recovering a known optimal result for $\epsilon = 1$.
We also establish a lower bound of $\Omega(d^{\frac{2\epsilon}{1+\epsilon}}
T^{\frac{1}{1+\epsilon}})$, which strictly improves upon the multi-armed bandit
rate and highlights the hardness of heavy-tailed linear bandit problems. For
finite action sets, we derive similarly improved upper and lower bounds for
regret. Finally, we provide action set dependent regret upper bounds showing
that for some geometries, such as $l_p$-norm balls for $p \le 1 + \epsilon$, we
can further reduce the dependence on $d$, and we can handle
infinite-dimensional settings via the kernel trick, in particular establishing
new regret bounds for the Mat\'ern kernel that are the first to be sublinear
for all $\epsilon \in (0, 1]$.

</details>


### [189] [Softlog-Softmax Layers and Divergences Contribute to a Computationally Dependable Ensemble Learning](https://arxiv.org/abs/2506.04297)
*Abdourrahmane Mahamane Atto*

Main category: cs.LG

TL;DR: 提出基于softlog-softmax级联的四步流程框架，通过模型多样性构建、对数操作优化、信息理论创新及性能评估体系，提升集成学习系统的一致性和可靠性


<details>
  <summary>Details</summary>
Motivation: 解决下一代集成学习系统在复杂场景下面临的决策逻辑不一致、信息度量不统一及评估体系不完备等核心问题

Method: 1) 解剖学过程构建卷积frustum多样性模型 2) 生理学过程推导softlog优化对数运算 3) 信息理论创建softlog熵/散度 4) 建立性能张量评估体系

Result: 框架显著提升复杂问题的处理能力，softlog结构增强决策一致性，新型信息度量实现系统级可靠性验证

Conclusion: 通过系统性融合模型架构创新与信息理论突破，为高可靠集成学习系统提供了完整的理论框架和实践路径

Abstract: The paper proposes a 4-step process for highlighting that softlog-softmax
cascades can improve both consistency and dependability of the next generation
ensemble learning systems. The first process is anatomical in nature: the
target ensemble model under consideration is composed by canonical elements
relating to the definition of a convolutional frustum. No a priori is
considered in the choice of canonical forms. Diversity is the main criterion
for selecting these forms. It is shown that the more complex the problem, the
more useful this ensemble diversity is. The second process is physiological and
relates to neural engineering: a softlog is derived to both make weak
logarithmic operations consistent and lead, through multiple softlog-softmax
layers, to intermediate decisions in the sense of respecting the same class
logic as that faced by the output layer. The third process concerns neural
information theory: softlog-based entropy and divergence are proposed for the
sake of constructing information measures yielding consistent values on closed
intervals. These information measures are used to determine the relationships
between individual and sub-community decisions in frustum diversitybased
ensemble learning. The concluding process addresses the derivation of an
informative performance tensor for the purpose of a reliable ensemble
evaluation.

</details>


### [190] [The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective](https://arxiv.org/abs/2506.04301)
*Jiin Kim,Byeongjun Shin,Jinha Chung,Minsoo Rhu*

Main category: cs.LG

TL;DR: 研究发现基于大语言模型的AI代理虽提升任务泛化能力，但引发严重的计算资源消耗与可持续性危机，需转向计算高效的推理范式


<details>
  <summary>Details</summary>
Motivation: 揭示AI代理从静态推理转向动态多步工作流时，系统级成本、能效和基础设施可持续性风险未被充分认知的现状

Method: 通过量化分析不同代理设计的资源使用、延迟分布、能耗指标，结合少样本提示、反思深度等设计维度评估精度-成本权衡

Result: 代理精度随计算量提升但边际效益骤降，延迟方差扩大3-5倍，数据中心级功耗需求超过传统LLM服务30倍

Conclusion: 必须重构代理架构设计，在保持性能优势的同时实现现实约束下的计算效率革命，避免即将到来的可持续性危机

Abstract: Large-language-model (LLM)-based AI agents have recently showcased impressive
versatility by employing dynamic reasoning, an adaptive, multi-step process
that coordinates with external tools. This shift from static, single-turn
inference to agentic, multi-turn workflows broadens task generalization and
behavioral flexibility, but it also introduces serious concerns about
system-level cost, efficiency, and sustainability. This paper presents the
first comprehensive system-level analysis of AI agents, quantifying their
resource usage, latency behavior, energy consumption, and datacenter-wide power
consumption demands across diverse agent designs and test-time scaling
strategies. We further characterize how AI agent design choices, such as
few-shot prompting, reflection depth, and parallel reasoning, impact
accuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy
with increased compute, they suffer from rapidly diminishing returns, widening
latency variance, and unsustainable infrastructure costs. Through detailed
evaluation of representative agents, we highlight the profound computational
demands introduced by AI agent workflows, uncovering a looming sustainability
crisis. These results call for a paradigm shift in agent design toward
compute-efficient reasoning, balancing performance with deployability under
real-world constraints.

</details>


### [191] [RedRFT: A Light-Weight Benchmark for Reinforcement Fine-Tuning-Based Red Teaming](https://arxiv.org/abs/2506.04302)
*Xiang Zheng,Xingjun Ma,Wei-Bin Lee,Cong Wang*

Main category: cs.LG

TL;DR: 提出RedRFT轻量级基准，通过模块化设计和消融实验解决RFT红队测试标准化问题，提升实现稳定性和可复现性


<details>
  <summary>Details</summary>
Motivation: 现有基于RFT的红队测试方法缺乏统一基准，PPO实现细节差异导致结果不稳定且难以复现

Method: 结合CleanRL和Tianshou框架优势，构建支持模块化PPO组件/多样化度量指标/插件式实验的基准系统，开展LoRA/KL散度/拉格朗日乘数等关键组件消融研究

Result: 明确不同实现组件对RFT性能的影响机制，RedRFT可实现快速原型开发并深入理解算法实现细节

Conclusion: RedRFT为RFT红队测试提供标准化基准框架，其模块化设计既促进算法实现理解，也为后续创新功能开发奠定基础

Abstract: Red teaming has proven to be an effective method for identifying and
mitigating vulnerabilities in Large Language Models (LLMs). Reinforcement
Fine-Tuning (RFT) has emerged as a promising strategy among existing red
teaming techniques. However, a lack of a unified benchmark hinders current
RFT-based red teaming methods. Implementation details, especially in Proximal
Policy Optimization (PPO)-based RFT, significantly affect outcome stability and
reproducibility. To address this issue, we introduce RedRFT, a lightweight
benchmark designed to simplify and standardize the implementation and
evaluation of RFT-based red teaming. RedRFT combines the design strengths of
both single-file CleanRL and highly modularized Tianshou, offering high-quality
single-file red teaming implementations and modular PPO core components, such
as the General Advantage Estimator. It supports a variety of token and sentence
diversity metrics, featuring modularized intrinsic reward computation that
facilitates plug-and-play experimentation. To clarify their influence on RFT
performance, we conducted an extensive ablation study on key components,
including Low-Rank Adaptation (LoRA), Kullback-Leibler (KL) divergence, and
Lagrange Multiplier. We hope this work contributes to 1) gaining a
comprehensive understanding of the implementation nuances of RFT-based red
teaming algorithms, and 2) enabling rapid prototyping of innovative features
for RFT-based red teaming. Code for the benchmark can be accessed at
https://github.com/x-zheng16/RedRFT.git.

</details>


### [192] [Towards Reasonable Concept Bottleneck Models](https://arxiv.org/abs/2506.05014)
*Nektarios Kalampalikis,Kavya Gupta,Georgi Vitanov,Isabel Valera*

Main category: cs.LG

TL;DR: 提出CREAM模型，通过显式嵌入专家定义的概念关系与正则化侧信道机制，在保持任务性能的同时增强概念可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型存在概念间关系模糊、任务性能受限以及概念泄露问题，需平衡可解释性与预测能力。

Method: 架构嵌入双向C-C关系与C→Y连接，阻断无关信息流；引入正则化黑盒侧信道辅助预测，仅在必要时激活。

Result: 任务性能与黑盒模型相当，概念重要性显著；结构化关系支持人工干预并减少概念泄露。

Conclusion: CREAM通过结构化概念推理与自适应侧信道，为可解释AI提供了性能与透明度兼得的解决方案。

Abstract: In this paper, we propose $\textbf{C}$oncept $\textbf{REA}$soning
$\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)
that: (i) explicitly encodes concept-concept (${\texttt{C-C}}$) and
concept-task (${\texttt{C$\rightarrow$Y}}$) relationships to enforce a desired
model reasoning; and (ii) use a regularized side-channel to achieve competitive
task performance, while keeping high concept importance. Specifically, CREAM
architecturally embeds (bi)directed concept-concept, and concept to task
relationships specified by a human expert, while severing undesired information
flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates
a black-box side-channel that is regularized to encourage task predictions to
be grounded in the relevant concepts, thereby utilizing the side-channel only
when necessary to enhance performance. Our experiments show that: (i) CREAM
mainly relies on concepts while achieving task performance on par with
black-box models; and (ii) the embedded ${\texttt{C-C}}$ and
${\texttt{C$\rightarrow$Y}}$ relationships ease model interventions and
mitigate concept leakage.

</details>


### [193] [You Only Train Once](https://arxiv.org/abs/2506.04349)
*Christos Sakaridis*

Main category: cs.LG

TL;DR: 论文提出YOTO方法，通过单次训练自动优化损失函数权重，在3D估计和语义分割任务中超越传统网格搜索方法


<details>
  <summary>Details</summary>
Motivation: 传统多损失函数优化需多次训练调整超参数，计算成本高。YOTO旨在通过端到端梯度优化实现损失权重的单次训练自动化

Method: 将损失权重建模为可学习参数，使用softmax层保证参数非负性，结合均匀性正则化损失防止权重退化

Result: 在PASCAL3D+和Cityscapes数据集上，YOTO在测试集表现持续优于网格搜索最佳模型

Conclusion: YOTO成功实现了损失权重与模型参数的联合单次优化，为自动多任务学习提供了有效解决方案

Abstract: The title of this paper is perhaps an overclaim. Of course, the process of
creating and optimizing a learned model inevitably involves multiple training
runs which potentially feature different architectural designs, input and
output encodings, and losses. However, our method, You Only Train Once (YOTO),
indeed contributes to limiting training to one shot for the latter aspect of
losses selection and weighting. We achieve this by automatically optimizing
loss weight hyperparameters of learned models in one shot via standard
gradient-based optimization, treating these hyperparameters as regular
parameters of the networks and learning them. To this end, we leverage the
differentiability of the composite loss formulation which is widely used for
optimizing multiple empirical losses simultaneously and model it as a novel
layer which is parameterized with a softmax operation that satisfies the
inherent positivity constraints on loss hyperparameters while avoiding
degenerate empirical gradients. We complete our joint end-to-end optimization
scheme by defining a novel regularization loss on the learned hyperparameters,
which models a uniformity prior among the employed losses while ensuring
boundedness of the identified optima. We evidence the efficacy of YOTO in
jointly optimizing loss hyperparameters and regular model parameters in one
shot by comparing it to the commonly used brute-force grid search across
state-of-the-art networks solving two key problems in computer vision, i.e. 3D
estimation and semantic segmentation, and showing that it consistently
outperforms the best grid-search model on unseen test data. Code will be made
publicly available.

</details>


### [194] [NIMO: a Nonlinear Interpretable MOdel](https://arxiv.org/abs/2506.05059)
*Shijian Xu,Marcello Massimo Negri,Volker Roth*

Main category: cs.LG

TL;DR: 提出NIMO模型，在保持线性系数可解释性的基础上，通过神经网络学习非线性修正，显著提升预测精度


<details>
  <summary>Details</summary>
Motivation: 神经网络虽预测能力强但缺乏可解释性，线性模型可解释性强但表达能力有限。需要结合两者优势，在保持线性系数边际效应解释性的前提下引入非线性修正

Method: 1. 基于profile likelihood设计优化算法，实现神经网络参数优化与线性系数解析更新的同步
2. 采用自适应岭回归整合稀疏性约束
3. 通过架构设计确保神经网络仅修正线性预测残差

Result: 1. 成功恢复潜在真实线性系数
2. 在回归/分类任务中预测精度显著优于其他可解释模型
3. 唯一保持与纯线性模型同等系数解释性的混合架构

Conclusion: NIMO开创性地实现了预测性能与可解释性的双重突破，为需要模型解释性的场景提供了更优解决方案，其架构设计思路对可解释AI发展具有启发意义

Abstract: Neural networks (NNs) have achieved tremendous success over the past decade,
yet they are still extremely difficult to interpret. In contrast, linear models
are less expressive but offer inherent interpretability. Linear coefficients
are interpretable as the marginal effect of a feature on the prediction,
assuming all other features are kept fixed. To combine the benefits of both
approaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is
to define a model where the NN is designed to learn nonlinear corrections to
the linear model predictions, while also maintaining the original
interpretability of the linear coefficients. Relevantly, we develop an
optimization algorithm based on profile likelihood that elegantly allows for
optimizing over the NN parameters while updating the linear coefficients
analytically. By relying on adaptive ridge regression we can easily incorporate
sparsity constraints as well. We show empirically that we can recover the
underlying linear coefficients while significantly improving the predictive
accuracy. Compared to other hybrid interpretable approaches, our model is the
only one that actually maintains the same interpretability of linear
coefficients as in linear models. We also achieve higher performance on various
regression and classification settings.

</details>


### [195] [Half-Layered Neural Networks](https://arxiv.org/abs/2506.04352)
*Ethem Alpaydin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a ``half'' layer of hidden units that has some of its weights
randomly set and some of them trained. A half unit is composed of two stages:
First, it takes a weighted sum of its inputs with fixed random weights, and
second, the total activation is multiplied and then translated using two
modifiable weights, before the result is passed through a nonlinearity. The
number of modifiable weights of each hidden unit is thus two and does not
depend on the fan-in. We show how such half units can be used in the first or
any later layer in a deep network, possibly following convolutional layers. Our
experiments on MNIST and FashionMNIST data sets indicate the promise of half
layers, where we can achieve reasonable accuracy with a reduced number of
parameters due to the regularizing effect of the randomized connections.

</details>


### [196] [UnHiPPO: Uncertainty-aware Initialization for State Space Models](https://arxiv.org/abs/2506.05065)
*Marten Lienen,Abdullah Saydemir,Stephan Günnemann*

Main category: cs.LG

TL;DR: 扩展HiPPO框架以处理含噪声数据，提出不确定性感知的状态空间模型动态初始化方法，增强模型抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 原始HiPPO框架假设数据无噪声，但实际场景中数据常含噪声。该研究旨在解决噪声环境下状态空间模型的动态初始化问题。

Method: 将HiPPO重构为含噪声输出的潜在系统控制问题，通过线性随机控制框架推导后验推断策略，保持运行时效率的同时实现噪声鲁棒初始化。

Result: 实验表明该方法显著提升状态空间模型在训练和推理阶段的噪声抵抗力。

Conclusion: 提出的初始化策略有效平衡模型效率与鲁棒性，为噪声场景下的序列建模提供理论保障。

Abstract: State space models are emerging as a dominant model class for sequence
problems with many relying on the HiPPO framework to initialize their dynamics.
However, HiPPO fundamentally assumes data to be noise-free; an assumption often
violated in practice. We extend the HiPPO theory with measurement noise and
derive an uncertainty-aware initialization for state space model dynamics. In
our analysis, we interpret HiPPO as a linear stochastic control problem where
the data enters as a noise-free control signal. We then reformulate the problem
so that the data become noisy outputs of a latent system and arrive at an
alternative dynamics initialization that infers the posterior of this latent
system from the data without increasing runtime. Our experiments show that our
initialization improves the resistance of state-space models to noise both at
training and inference time. Find our implementation at
https://cs.cit.tum.de/daml/unhippo.

</details>


### [197] [A Risk-Aware Reinforcement Learning Reward for Financial Trading](https://arxiv.org/abs/2506.04358)
*Uditansh Srivastava,Shivam Aryan,Shaurya Singh*

Main category: cs.LG

TL;DR: 提出一种包含年化收益、下行风险、差异收益和特雷诺比率的四元复合奖励函数，通过可微分参数权重平衡收益与风险，支持梯度训练并允许定制化风险偏好。


<details>
  <summary>Details</summary>
Motivation: 传统金融交易强化学习多采用单一指标（如夏普比率），缺乏多目标优化能力。本文旨在构建模块化的多目标奖励框架，适应复杂交易环境中的多样化投资者偏好。

Method: 1. 设计四元可微分奖励函数结构 2. 通过网格搜索调参实现特定风险收益特征 3. 推导闭式梯度公式支持梯度训练 4. 理论分析单调性/有界性/模块化特性

Result: 建立具有理论保证的模块化奖励框架，实验证明可通过权重调整实现不同风险收益组合。闭式梯度提升训练效率，理论分析验证框架稳健性。

Conclusion: 该框架为复杂交易环境提供多目标奖励函数设计范式，支持扩展更多风险指标或自适应权重机制，具有工程实践价值。

Abstract: We propose a novel composite reward function for reinforcement learning in
financial trading that balances return and risk using four differentiable
terms: annualized return downside risk differential return and the Treynor
ratio
  Unlike single metric objectives for example the Sharpe ratio our formulation
is modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners
to encode diverse investor preferences
  We tune these weights via grid search to target specific risk return profiles
  We derive closed form gradients for each term to facilitate gradient based
training and analyze key theoretical properties including monotonicity
boundedness and modularity
  This framework offers a general blueprint for building robust multi objective
reward functions in complex trading environments and can be extended with
additional risk measures or adaptive weighting

</details>


### [198] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: 合成数据继承生成模型的差分隐私保证，在模型隐藏时存在隐私放大潜力。通过线性回归分析，揭示控制生成种子会导致信息泄露，而随机输入生成的合成数据能提供额外隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对合成数据隐私放大现象的理论支撑，本文通过线性回归模型建立理论分析框架。

Method: 使用线性回归模型，分别研究攻击者控制生成种子与随机输入生成数据两种场景下的隐私泄露边界。

Result: 发现控制生成种子时单数据点泄露等价模型信息；随机输入生成有限数据点时隐私保护超越模型固有保证。

Conclusion: 在线性回归中的理论发现为未来推导更通用的隐私保护边界奠定基础，强调生成机制对隐私放大效果的关键影响。

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


### [199] [Even Faster Hyperbolic Random Forests: A Beltrami-Klein Wrapper Approach](https://arxiv.org/abs/2506.04360)
*Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: 将hyperDT算法重构为Beltrami-Klein双曲空间模型，通过包装器实现欧氏空间决策树的双曲空间扩展


<details>
  <summary>Details</summary>
Motivation: 现有双曲空间决策树使用齐次超平面划分空间，限制了模型优化和代码维护性

Method: 将hyperDT算法转换为Beltrami-Klein模型表示，通过前后处理封装现有欧氏空间决策树模型

Result: 兼容欧氏空间优化技术，提升模型灵活性、运算速度（快3倍）和准确率（最高提升5%）

Conclusion: 该包装方法简化了双曲空间决策树实现，在保持精度的同时显著提升工程实践价值

Abstract: Decision trees and models that use them as primitives are workhorses of
machine learning in Euclidean spaces. Recent work has further extended these
models to the Lorentz model of hyperbolic space by replacing axis-parallel
hyperplanes with homogeneous hyperplanes when partitioning the input space. In
this paper, we show how the hyperDT algorithm can be elegantly reexpressed in
the Beltrami-Klein model of hyperbolic spaces. This preserves the thresholding
operation used in Euclidean decision trees, enabling us to further rewrite
hyperDT as simple pre- and post-processing steps that form a wrapper around
existing tree-based models designed for Euclidean spaces. The wrapper approach
unlocks many optimizations already available in Euclidean space models,
improving flexibility, speed, and accuracy while offering a simpler, more
maintainable, and extensible codebase. Our implementation is available at
https://github.com/pchlenski/hyperdt.

</details>


### [200] [Transformers Meet In-Context Learning: A Universal Approximation Theory](https://arxiv.org/abs/2506.05200)
*Gen Li,Yuchen Jiao,Yu Huang,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern large language models are capable of in-context learning, the ability
to perform new tasks at inference time using only a handful of input-output
examples in the prompt, without any fine-tuning or parameter updates. We
develop a universal approximation theory to better understand how transformers
enable in-context learning. For any class of functions (each representing a
distinct task), we demonstrate how to construct a transformer that, without any
further weight updates, can perform reliable prediction given only a few
in-context examples. In contrast to much of the recent literature that frames
transformers as algorithm approximators -- i.e., constructing transformers to
emulate the iterations of optimization algorithms as a means to approximate
solutions of learning problems -- our work adopts a fundamentally different
approach rooted in universal function approximation. This alternative approach
offers approximation guarantees that are not constrained by the effectiveness
of the optimization algorithms being approximated, thereby extending far beyond
convex problems and linear function classes. Our construction sheds light on
how transformers can simultaneously learn general-purpose representations and
adapt dynamically to in-context examples.

</details>


### [201] [Replay Can Provably Increase Forgetting](https://arxiv.org/abs/2506.04377)
*Yasaman Mahdaviyeh,James Lucas,Mengye Ren,Andreas S. Tolias,Richard Zemel,Toniann Pitassi*

Main category: cs.LG

TL;DR: 论文通过理论和实验证明，持续学习中的样本回放可能因任务关联性和样本选择导致遗忘加剧，挑战了回放机制普适有效的假设。


<details>
  <summary>Details</summary>
Motivation: 探索样本回放机制在持续学习中的理论边界，揭示回放样本可能产生负面影响的场景，解释其非单调性遗忘现象。

Method: 采用过参数化线性回归模型进行理论分析，构建任务子空间关系框架，并通过神经网络+SGD实验进行实证验证。

Result: 发现回放样本数量与遗忘程度存在非单调关系，识别出有害回放场景，在MNIST等基准测试中观察到类似现象。

Conclusion: 样本回放的有效性高度依赖任务结构关系与样本选择策略，需建立理论指导框架优化回放机制设计。

Abstract: Continual learning seeks to enable machine learning systems to solve an
increasing corpus of tasks sequentially. A critical challenge for continual
learning is forgetting, where the performance on previously learned tasks
decreases as new tasks are introduced. One of the commonly used techniques to
mitigate forgetting, sample replay, has been shown empirically to reduce
forgetting by retaining some examples from old tasks and including them in new
training episodes. In this work, we provide a theoretical analysis of sample
replay in an over-parameterized continual linear regression setting, where each
task is given by a linear subspace and with enough replay samples, one would be
able to eliminate forgetting. Our analysis focuses on sample replay and
highlights the role of the replayed samples and the relationship between task
subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting
can be non-monotonic with respect to the number of replay samples. We present
tasks where replay can be harmful with respect to worst-case settings, and also
in distributional settings where replay of randomly selected samples increases
forgetting in expectation. We also give empirical evidence that harmful replay
is not limited to training with linear models by showing similar behavior for a
neural networks equipped with SGD. Through experiments on a commonly used
benchmark, we provide additional evidence that, even in seemingly benign
scenarios, performance of the replay heavily depends on the choice of replay
samples and the relationship between tasks.

</details>


### [202] [Progressive Tempering Sampler with Diffusion](https://arxiv.org/abs/2506.05231)
*Severi Rissanen,RuiKang OuYang,Jiajun He,Wenlin Chen,Markus Heinonen,Arno Solin,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出渐进回火扩散采样器PTSD，融合并行回火(PT)和扩散模型优势，通过温度层级间的信息复用显著提升神经采样器的目标评估效率


<details>
  <summary>Details</summary>
Motivation: 现有神经采样器目标评估效率不及并行回火(PT)，但PT存在样本依赖性强、重复计算成本高的问题。需要结合两者优势开发新方法

Method: 1. 跨温度序列训练扩散模型
2. 组合高温扩散模型生成近似低温样本
3. 使用MCMC微调后训练下层扩散模型
4. 实现温度层级间样本信息复用

Result: 目标评估效率显著优于扩散基神经采样器，能生成混合充分且无关联的样本

Conclusion: PTSD通过创新的温度层级信息传递机制，在保持神经采样器优势的同时，显著提升了采样效率，为概率密度采样领域提供了新思路

Abstract: Recent research has focused on designing neural samplers that amortize the
process of sampling from unnormalized densities. However, despite significant
advancements, they still fall short of the state-of-the-art MCMC approach,
Parallel Tempering (PT), when it comes to the efficiency of target evaluations.
On the other hand, unlike a well-trained neural sampler, PT yields only
dependent samples and needs to be rerun -- at considerable computational cost
-- whenever new samples are required. To address these weaknesses, we propose
the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion
models sequentially across temperatures, leveraging the advantages of PT to
improve the training of neural samplers. We also introduce a novel method to
combine high-temperature diffusion models to generate approximate
lower-temperature samples, which are minimally refined using MCMC and used to
train the next diffusion model. PTSD enables efficient reuse of sample
information across temperature levels while generating well-mixed, uncorrelated
samples. Our method significantly improves target evaluation efficiency,
outperforming diffusion-based neural samplers.

</details>


### [203] [Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning](https://arxiv.org/abs/2506.04398)
*Théo Vincent,Yogesh Tripathi,Tim Faust,Yaniv Oren,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出iS-QL方法，通过共享在线网络最后一层线性层作为目标网络，在保持目标网络优势的同时提升目标无关方法的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中使用目标网络会占用内存且限制在线网络容量，但完全移除目标网络会导致性能下降。现有方法无法兼顾目标网络的优势与资源效率。

Method: 构建共享参数的在线网络架构（最后一层独立复制作为目标网络），结合迭代Q学习并行学习连续Bellman迭代过程。

Result: 实验显示iS-QL在多种场景下缩小了目标无关方法与目标方法的性能差距，内存占用减少且训练时间与传统方法相当。

Conclusion: 该方法突破了目标网络二元选择的限制，为扩展强化学习研究的规模提供了新的技术路径。

Abstract: In value-based reinforcement learning, removing the target network is
tempting as the boostrapped target would be built from up-to-date estimates,
and the spared memory occupied by the target network could be reallocated to
expand the capacity of the online network. However, eliminating the target
network introduces instability, leading to a decline in performance. Removing
the target network also means we cannot leverage the literature developed
around target networks. In this work, we propose to use a copy of the last
linear layer of the online network as a target network, while sharing the
remaining parameters with the up-to-date online network, hence stepping out of
the binary choice between target-based and target-free methods. It enables us
to leverage the concept of iterated Q-learning, which consists of learning
consecutive Bellman iterations in parallel, to reduce the performance gap
between target-free and target-based approaches. Our findings demonstrate that
this novel method, termed iterated Shared Q-Learning (iS-QL), improves the
sample efficiency of target-free approaches across various settings.
Importantly, iS-QL requires a smaller memory footprint and comparable training
time to classical target-based algorithms, highlighting its potential to scale
reinforcement learning research.

</details>


### [204] [Sample Complexity and Representation Ability of Test-time Scaling Paradigms](https://arxiv.org/abs/2506.05295)
*Baihe Huang,Shanda Li,Tianhao Wu,Yiming Yang,Ameet Talwalkar,Kannan Ramchandran,Michael I. Jordan,Jiantao Jiao*

Main category: cs.LG

TL;DR: 论文通过理论分析和实验验证，揭示了自洽性、最佳选择策略的样本效率差异，并证明了带验证的自纠正方法可实现Transformer多任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时策略缺乏理论支撑，尤其不同策略的样本效率差异及自纠正方法如何扩展模型表达能力尚未明确。

Method: 1. 建立自洽性(Θ(1/Δ²))与最佳选择(Θ(1/Δ))的样本需求分离理论；2. 提出带验证的自纠正方法可实现Transformer在线学习模拟；3. 实验验证理论结论。

Result: 自洽性样本效率低于最佳选择，自纠正方法使单Transformer架构可处理多任务（无需预知具体任务）。

Conclusion: 理论分析填补测试时策略研究空白，自纠正方法为Transformer多任务处理提供理论依据，实验结果验证其有效性。

Abstract: Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.

</details>


### [205] [Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning](https://arxiv.org/abs/2506.04399)
*Suzan Ece Ada,Emre Ugur*

Main category: cs.LG

TL;DR: 提出UMCNP方法，结合策略梯度元强化学习与任务推断，在元测试阶段无需奖励信号即可显著减少样本需求


<details>
  <summary>Details</summary>
Motivation: 解决元强化学习在奖励信号缺失的元测试场景中样本效率低的问题，避免额外样本的元训练需求

Method: 1. 利用条件神经过程(CNPs)减少在线交互
2. 离线复用策略梯度样本进行任务推断
3. 通过单次测试任务推演推断动态模型参数

Result: 在2D-Point Agent和连续控制基准测试中，元测试阶段样本需求比基线减少50%以上

Conclusion: UMCNP通过分离策略学习与动态推断，实现了无监督条件下的高效元适应，为现实场景提供可行方案

Abstract: We introduce Unsupervised Meta-Testing with Conditional Neural Processes
(UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method
that uniquely combines, yet distinctly separates, parameterized policy
gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for
settings where the reward signal is missing during meta-testing, our method
increases sample efficiency without requiring additional samples in
meta-training. UMCNP leverages the efficiency and scalability of Conditional
Neural Processes (CNPs) to reduce the number of online interactions required in
meta-testing. During meta-training, samples previously collected through PPG
meta-RL are efficiently reused for learning task inference in an offline
manner. UMCNP infers the latent representation of the transition dynamics model
from a single test task rollout with unknown parameters. This approach allows
us to generate rollouts for self-adaptation by interacting with the learned
dynamics model. We demonstrate our method can adapt to an unseen test task
using significantly fewer samples during meta-testing than the baselines in
2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with
unknown angle sensor bias, walker agent with randomized dynamics parameters.

</details>


### [206] [Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning](https://arxiv.org/abs/2506.04411)
*Achleshwar Luthra,Tianbao Yang,Tomer Galanti*

Main category: cs.LG

TL;DR: 论文通过理论分析证明自监督对比学习（CL）隐式逼近仅负样本监督对比损失（NSCL），揭示了表示空间的几何结构特性，并提出基于特征变异性的新型few-shot误差界限。


<details>
  <summary>Details</summary>
Motivation: 现有自监督对比学习虽实证有效但缺乏理论支撑，需建立其与监督学习的理论联系并解释其成功机制。

Method: 1. 理论证明CL与NSCL损失差距随类别数增加而消失
2. 分析NSCL损失的全局极小值几何结构（增强坍塌/类内坍塌/单纯形等角紧框架）
3. 提出基于类内离散度和类间方向变异性的few-shot误差新界限

Result: 1. CL与NSCL差距按O(1/类别数)衰减
2. 特征方向变异主导few-shot误差
3. 实证验证理论预测（损失相关性达0.89，误差界限预测误差<3%）

Conclusion: 该理论框架首次系统解释CL成功机理，为设计更优对比学习目标及评估指标提供数学依据，同时提出的few-shot误差界限为实际应用提供理论指导。

Abstract: Despite its empirical success, the theoretical foundations of self-supervised
contrastive learning (CL) are not yet fully established. In this work, we
address this gap by showing that standard CL objectives implicitly approximate
a supervised variant we call the negatives-only supervised contrastive loss
(NSCL), which excludes same-class contrasts. We prove that the gap between the
CL and NSCL losses vanishes as the number of semantic classes increases, under
a bound that is both label-agnostic and architecture-independent.
  We characterize the geometric structure of the global minimizers of the NSCL
loss: the learned representations exhibit augmentation collapse, within-class
collapse, and class centers that form a simplex equiangular tight frame. We
further introduce a new bound on the few-shot error of linear-probing. This
bound depends on two measures of feature variability--within-class dispersion
and variation along the line between class centers. We show that directional
variation dominates the bound and that the within-class dispersion's effect
diminishes as the number of labeled samples increases. These properties enable
CL and NSCL-trained representations to support accurate few-shot label recovery
using simple linear probes.
  Finally, we empirically validate our theoretical findings: the gap between CL
and NSCL losses decays at a rate of $\mathcal{O}(\frac{1}{\#\text{classes}})$;
the two losses are highly correlated; minimizing the CL loss implicitly brings
the NSCL loss close to the value achieved by direct minimization; and the
proposed few-shot error bound provides a tight estimate of probing performance
in practice.

</details>


### [207] [Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order](https://arxiv.org/abs/2506.04430)
*Egor Petrov,Grigoriy Evseev,Aleksey Antonov,Andrey Veprikov,Pavel Plyusnin,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出两种零阶优化方法JAGUAR SignSGD和Muon，通过动量机制与矩阵结构优化，在LLM微调中实现与一阶方法相当的收敛质量并显著降低内存消耗


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化器（如SGD/Adam）在大模型微调时存在内存与计算成本过高的问题，参数规模越大越不经济。需探索零阶优化方法作为替代方案，特别是在LoRA等参数高效微调场景中

Method: 1. JAGUAR SignSGD：基于动量机制的ZO SignSGD扩展，保持ZO SGD参数量的同时每轮迭代仅需O(1)次函数评估
2. JAGUAR Muon：首个利用参数矩阵结构的ZO优化器，在任意随机噪声下具备收敛保证

Result: 在LLM微调基准测试中，新算法达到或超越传统一阶方法的收敛质量，内存占用显著降低。首次为随机ZO场景下的SignSGD建立严格收敛保证

Conclusion: 通过理论证明与实验验证，确立零阶优化作为资源受限场景下LLM适配的实用方法，为高效模型微调提供新的理论框架与实践方案

Abstract: Fine-tuning Large Language Models (LLMs) is essential for adapting
pre-trained models to downstream tasks. Yet traditional first-order optimizers
such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and
computational costs that scale poorly with model size. In this paper, we
investigate zero-order (ZO) optimization methods as a memory- and
compute-efficient alternative, particularly in the context of
parameter-efficient fine-tuning techniques like LoRA. We propose
$\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO
SignSGD, requiring the same number of parameters as the standard ZO SGD and
only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our
knowledge, this is the first study to establish rigorous convergence guarantees
for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR
Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix
structure of model parameters, and we provide its convergence rate under
arbitrary stochastic noise. Through extensive experiments on challenging LLM
fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or
exceed the convergence quality of standard first-order methods, achieving
significant memory reduction. Our theoretical and empirical results establish
new ZO optimization methods as a practical and theoretically grounded approach
for resource-constrained LLM adaptation. Our code is available at
https://github.com/brain-mmo-lab/ZO_LLM

</details>


### [208] [KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)
*Zixuan Xia,Aram Davtyan,Paolo Favaro*

Main category: cs.LG

TL;DR: KOALA++通过递归更新紧凑梯度协方差估计参数矩阵，在保持一阶方法效率的同时达到或超越现有优化器的精度


<details>
  <summary>Details</summary>
Motivation: 传统二阶优化器计算代价高昂，原始KOALA框架的协方差对角假设限制了对复杂不确定性结构的建模能力

Method: 通过递归更新梯度协方差乘积估计参数协方差矩阵，避免存储完整协方差矩阵和大型矩阵求逆运算

Result: 在图像分类、语言建模等任务中，精度达到或超越当前最优的一阶/二阶优化器，同时保持一阶方法计算效率

Conclusion: 该算法实现了优化效率与精度的双重突破，为神经网络训练提供了新的高效优化范式

Abstract: We propose KOALA++, a scalable Kalman-based optimization algorithm that
explicitly models structured gradient uncertainty in neural network training.
Unlike second-order methods, which rely on expensive second order gradient
calculation, our method directly estimates the parameter covariance matrix by
recursively updating compact gradient covariance products. This design improves
upon the original KOALA framework that assumed diagonal covariance by
implicitly capturing richer uncertainty structure without storing the full
covariance matrix and avoiding large matrix inversions. Across diverse tasks,
including image classification and language modeling, KOALA++ achieves accuracy
on par or better than state-of-the-art first- and second-order optimizers while
maintaining the efficiency of first-order methods.

</details>


### [209] [RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis](https://arxiv.org/abs/2506.04439)
*Robin Yadav,Qi Yan,Guy Wolf,Avishek Joey Bose,Renjie Liao*

Main category: cs.LG

TL;DR: 提出RETRO SYNFLOW框架，通过离散流匹配和Feynman-Kac引导机制显著提升单步逆合成预测的准确率与多样性


<details>
  <summary>Details</summary>
Motivation: 现有无模板生成方法在单步逆合成预测中面临生成结果准确性和多样性不足的挑战，化学搜索空间的组合爆炸特性加剧了该问题

Method: 1. 建立目标产物与反应物的马尔可夫桥 2. 引入反应中心识别生成合成子 3. 采用Feynman-Kac引导和SMC重采样 4. 开发前向合成模型奖励机制

Result: Top-1准确率60%（提升20% SOTA），FK引导使Top-5往返准确率提升19%，同时保持优秀Top-k性能

Conclusion: RSF框架通过中间结构生成和推理时引导机制，在保持生成多样性的同时实现准确率突破，为逆合成预测提供新范式

Abstract: A fundamental problem in organic chemistry is identifying and predicting the
series of reactions that synthesize a desired target product molecule. Due to
the combinatorial nature of the chemical search space, single-step reactant
prediction -- i.e. single-step retrosynthesis -- remains challenging even for
existing state-of-the-art template-free generative approaches to produce an
accurate yet diverse set of feasible reactions. In this paper, we model
single-step retrosynthesis planning and introduce RETRO SYNFLOW (RSF) a
discrete flow-matching framework that builds a Markov bridge between the
prescribed target product molecule and the reactant molecule. In contrast to
past approaches, RSF employs a reaction center identification step to produce
intermediate structures known as synthons as a more informative source
distribution for the discrete flow. To further enhance diversity and
feasibility of generated samples, we employ Feynman-Kac steering with
Sequential Monte Carlo based resampling to steer promising generations at
inference using a new reward oracle that relies on a forward-synthesis model.
Empirically, we demonstrate \nameshort achieves $60.0 \%$ top-1 accuracy, which
outperforms the previous SOTA by $20 \%$. We also substantiate the benefits of
steering at inference and demonstrate that FK-steering improves top-$5$
round-trip accuracy by $19 \%$ over prior template-free SOTA methods, all while
preserving competitive top-$k$ accuracy results.

</details>


### [210] [Selective Matching Losses -- Not All Scores Are Created Equal](https://arxiv.org/abs/2506.04446)
*Gil I. Shamir,Manfred K. Warmuth*

Main category: cs.LG

TL;DR: 提出通过递增链接函数设计选择性匹配损失函数，提升关键区域预测准确性，并解决多分类场景的排名敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 传统损失函数无法有效区分区域重要性，导致模型在关键区域预测表现不足，需针对性设计强调高敏感区域的损失机制。

Method: 使用Sigmoid/双曲正弦等递增链接函数构建选择性损失，通过斜率和位置调整敏感性；采用复合Softmax解决多分类排名敏感性问题。

Result: 在停留时间预测、检索排序、大模型微调等场景中，选择性损失显著优于传统损失函数。

Conclusion: 通过敏感性区域控制有效提升关键预测性能，多维扩展框架拓宽了实际应用范围。

Abstract: Learning systems match predicted scores to observations over some domain.
Often, it is critical to produce accurate predictions in some subset (or
region) of the domain, yet less important to accurately predict in other
regions. We construct selective matching loss functions by design of increasing
link functions over score domains. A matching loss is an integral over the
link. A link defines loss sensitivity as function of the score, emphasizing
high slope high sensitivity regions over flat ones. Loss asymmetry drives a
model and resolves its underspecification to predict better in high sensitivity
regions where it is more important, and to distinguish between high and low
importance regions. A large variety of selective scalar losses can be designed
with scaled and shifted Sigmoid and hyperbolic sine links. Their properties,
however, do not extend to multi-class. Applying them per dimension lacks
ranking sensitivity that assigns importance according to class score ranking.
Utilizing composite Softmax functions, we develop a framework for
multidimensional selective losses. We overcome limitations of the standard
Softmax function, that is good for classification, but not for distinction
between adjacent scores. Selective losses have substantial advantage over
traditional losses in applications with more important score regions, including
dwell-time prediction, retrieval, ranking with either pointwise, contrastive
pairwise, or listwise losses, distillation problems, and fine-tuning alignment
of Large Language Models (LLMs).

</details>


### [211] [Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning](https://arxiv.org/abs/2506.04454)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: 扩展神经符号框架ODXU，集成深度聚类/XGBoost/不确定性量化，提出迁移学习策略并在网络安全领域验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统网络入侵检测系统在鲁棒性、可解释性和迁移能力上的不足，探索神经符号AI与迁移学习在网络安全中的创新应用。

Method: 1. 改进ODXU框架：结合深度聚类特征提取、XGBoost符号推理、置信度/熵/信息增益等UQ方法
2. 开发迁移策略：复用预训练自动编码器+重训练聚类模块+微调分类器

Result: CIC-IDS-2017数据集上6项指标超越传统模型；ACI-IoT-2023仅用1.6万样本(50%数据量)实现最优迁移效果，元模型UQ方法持续优于传统评分方法

Conclusion: ODXU框架显著提升检测性能，验证神经符号架构与迁移学习在网络安全领域的适用性，为小样本场景提供有效解决方案

Abstract: Network Intrusion Detection Systems (NIDS) play a vital role in protecting
digital infrastructures against increasingly sophisticated cyber threats. In
this paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates
deep embedded clustering for feature extraction, symbolic reasoning using
XGBoost, and comprehensive uncertainty quantification (UQ) to enhance
robustness, interpretability, and generalization in NIDS. The extended ODXU
incorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)
and metamodel-based techniques, including SHAP values and Information Gain, to
assess the reliability of predictions. Experimental results on the CIC-IDS-2017
dataset show that ODXU outperforms traditional neural models across six
evaluation metrics, including classification accuracy and false omission rate.
While transfer learning has seen widespread adoption in fields such as computer
vision and natural language processing, its potential in cybersecurity has not
been thoroughly explored. To bridge this gap, we develop a transfer learning
strategy that enables the reuse of a pre-trained ODXU model on a different
dataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal
transfer configuration involves reusing the pre-trained autoencoder, retraining
the clustering module, and fine-tuning the XGBoost classifier, and outperforms
traditional neural models when trained with as few as 16,000 samples
(approximately 50% of the training data). Additionally, results show that
metamodel-based UQ methods consistently outperform score-based approaches on
both datasets.

</details>


### [212] [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
*Ivan Vegner,Sydelle de Souza,Valentin Forch,Martha Lewis,Leonidas A. A. Doumas*

Main category: cs.LG

TL;DR: 论文探讨了机器学习模型系统性泛化的本质区别，指出现有研究多关注行为层面的系统性，而忽视了表示层面的系统性，提出了结合机制可解释性评估表示系统性的新视角。


<details>
  <summary>Details</summary>
Motivation: 针对Fodor和Pylyshyn关于表示系统性的理论主张与当前ML实践侧重行为系统性的研究鸿沟，旨在推动系统性评估从表面行为到内在表征的深化。

Method: 基于Hadley的系统性分类体系，对语言和视觉领域的核心基准展开元分析，并引入机制可解释性领域的表征评估方法论。

Result: 揭示主流基准主要测试行为层面的弱系统性，而表示层面的强系统性评估存在方法论缺失，证明机制可解释性工具可有效填补该空白。

Conclusion: 系统性研究需明确区分行为与表征层面，建议未来开发融合机制解释的评估框架以实现真正的系统性泛化能力验证。

Abstract: A core aspect of compositionality, systematicity is a desirable property in
ML models as it enables strong generalization to novel contexts. This has led
to numerous studies proposing benchmarks to assess systematic generalization,
as well as models and training regimes designed to enhance it. Many of these
efforts are framed as addressing the challenge posed by Fodor and Pylyshyn.
However, while they argue for systematicity of representations, existing
benchmarks and models primarily focus on the systematicity of behaviour. We
emphasize the crucial nature of this distinction. Furthermore, building on
Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to
which behavioural systematicity is tested by key benchmarks in the literature
across language and vision. Finally, we highlight ways of assessing
systematicity of representations in ML models as practiced in the field of
mechanistic interpretability.

</details>


### [213] [Classifying Dental Care Providers Through Machine Learning with Features Ranking](https://arxiv.org/abs/2506.04474)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh,Mohammed Hasan Abu-Arqoub,Rashiq Rafiq Marie*

Main category: cs.LG

TL;DR: 应用12种机器学习模型对牙科服务提供者分类，神经网络以94.1%准确率最优，验证特征选择对不平衡医疗数据的重要性


<details>
  <summary>Details</summary>
Motivation: 通过精准分类实现医疗资源优化配置，重点支持服务不足的安全网诊所群体

Method: 使用含38.1%缺失值的24,300条数据，通过信息增益/基尼指数/ANOVA进行特征排序，采用10折交叉验证评估kNN/决策树/SVM/随机森林/神经网络等模型

Result: 神经网络全特征准确率94.1%，治疗服务量(TXMT指标)为关键特征，集成方法对缺失数据表现稳健，人口统计学特征影响微弱

Conclusion: 特征排序与集成学习的结合能有效提升分类性能，为弱势群体医疗资源配置提供数据驱动的决策支持

Abstract: This study investigates the application of machine learning (ML) models for
classifying dental providers into two categories - standard rendering providers
and safety net clinic (SNC) providers - using a 2018 dataset of 24,300
instances with 20 features. The dataset, characterized by high missing values
(38.1%), includes service counts (preventive, treatment, exams), delivery
systems (FFS, managed care), and beneficiary demographics. Feature ranking
methods such as information gain, Gini index, and ANOVA were employed to
identify critical predictors, revealing treatment-related metrics
(TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models,
including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines
(SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and
Gradient Boosting, were evaluated using 10-fold cross-validation.
Classification accuracy was tested across incremental feature subsets derived
from rankings. The Neural Network achieved the highest accuracy (94.1%) using
all 20 features, followed by Gradient Boosting (93.2%) and Random Forest
(93.0%). Models showed improved performance as more features were incorporated,
with SGD and ensemble methods demonstrating robustness to missing data. Feature
ranking highlighted the dominance of treatment service counts and annotation
codes in distinguishing provider types, while demographic variables (AGE_GROUP,
CALENDAR_YEAR) had minimal impact. The study underscores the importance of
feature selection in enhancing model efficiency and accuracy, particularly in
imbalanced healthcare datasets. These findings advocate for integrating
feature-ranking techniques with advanced ML algorithms to optimize dental
provider classification, enabling targeted resource allocation for underserved
populations.

</details>


### [214] [Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data](https://arxiv.org/abs/2506.04479)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Abdullah Alourani*

Main category: cs.LG

TL;DR: 研究利用机器学习模型（特别是神经网络和随机森林）有效分类牙科服务提供者，显著提升医疗资源分配效率。


<details>
  <summary>Details</summary>
Motivation: 优化医疗服务资源分配，特别是通过准确分类标准服务提供者和安全网诊所提供者，以更好地服务弱势群体。

Method: 使用2018年24,300条含20个特征的数据集，测试kNN、决策树、SVM、随机森林、神经网络等7种算法，采用10折交叉验证，以AUC、准确率(94.1%)、F1-score等指标评估模型性能。

Result: 神经网络表现最佳（AUC 0.975，准确率94.1%），随机森林次之（AUC 0.948，准确率93.0%），均显著优于传统逻辑回归和SVM。

Conclusion: 集成学习和深度学习模型能有效处理医疗数据复杂性，将其纳入医疗分析系统可提升服务提供者识别精度，优化弱势群体医疗资源覆盖。

Abstract: Dental provider classification plays a crucial role in optimizing healthcare
resource allocation and policy planning. Effective categorization of providers,
such as standard rendering providers and safety net clinic (SNC) providers,
enhances service delivery to underserved populations. This study aimed to
evaluate the performance of machine learning models in classifying dental
providers using a 2018 dataset. A dataset of 24,300 instances with 20 features
was analyzed, including beneficiary and service counts across fee-for-service
(FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were
categorized by delivery system and patient age groups (0-20 and 21+). Despite
38.1% missing data, multiple machine learning algorithms were tested, including
k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM),
Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient
Boosting. A 10-fold cross-validation approach was applied, and models were
evaluated using AUC, classification accuracy (CA), F1-score, precision, and
recall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%),
followed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively
handled imbalanced data and complex feature interactions, outperforming
traditional classifiers like Logistic Regression and SVM. Advanced machine
learning techniques, particularly ensemble and deep learning models,
significantly enhance dental workforce classification. Their integration into
healthcare analytics can improve provider identification and resource
distribution, benefiting underserved populations.

</details>


### [215] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)
*Jiayu Wang,Aws Albarghouthi,Frederic Sala*

Main category: cs.LG

TL;DR: 提出COSMOS框架，通过嵌入增强代理模型和低样本缩放法则，高效预测LLM适应策略效果，平均减少92.72%计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM适应策略选择需要大量实验试错，资源消耗严重，需开发高效预测方法降低部署开销。

Method: 使用嵌入增强的轻量代理模型预测微调性能，结合低样本缩放法则预测检索增强的上下文学习效果。

Result: 在8个基准测试中，COSMOS平均降低92.72%计算成本（最高98.71%），同时保持高预测准确率。

Conclusion: LLM适应效果预测可行性强，可大幅降低计算开销（达98.71%）并维持性能，推动高效部署。

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>


### [216] [Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data](https://arxiv.org/abs/2506.04490)
*Rishwanth Raghu,Axel Levy,Gordon Wetzstein,Ellen D. Zhong*

Main category: cs.LG

TL;DR: CryoBoltz整合冷冻电镜与蛋白质结构预测模型，实现动态生物复合物的多构象建模


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质预测模型缺乏构象多样性表征能力，而冷冻电镜数据处理流程复杂，本研究旨在结合两者的优势建立统一建模框架

Method: 通过冷冻电镜密度图提取全局和局部结构约束，引导预训练模型的采样轨迹，生成实验数据相容的构象

Result: 在转运蛋白、抗体等动态体系中成功构建符合异质性冷冻电镜图的原子模型

Conclusion: 该方法开创性地融合实验数据与AI模型先验，显著提升动态生物分子系统的结构解析能力

Abstract: Protein structure prediction models are now capable of generating accurate 3D
structural hypotheses from sequence alone. However, they routinely fail to
capture the conformational diversity of dynamic biomolecular complexes, often
requiring heuristic MSA subsampling approaches for generating alternative
states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a
powerful tool for imaging near-native structural heterogeneity, but is
challenged by arduous pipelines to go from raw experimental data to atomic
models. Here, we bridge the gap between these modalities, combining cryo-EM
density maps with the rich sequence and biophysical priors learned by protein
structure prediction models. Our method, CryoBoltz, guides the sampling
trajectory of a pretrained protein structure prediction model using both global
and local structural constraints derived from density maps, driving predictions
towards conformational states consistent with the experimental data. We
demonstrate that this flexible yet powerful inference-time approach allows us
to build atomic models into heterogeneous cryo-EM maps across a variety of
dynamic biomolecular systems including transporters and antibodies.

</details>


### [217] [Hierarchical Implicit Neural Emulators](https://arxiv.org/abs/2506.04528)
*Ruoxi Jiang,Xiao Zhang,Karan Jakhar,Peter Y. Lu,Pedram Hassanzadeh,Michael Maire,Rebecca Willett*

Main category: cs.LG

TL;DR: 提出多尺度隐式神经模拟器，通过分层未来状态表征提升PDE求解器的长期预测稳定性


<details>
  <summary>Details</summary>
Motivation: 神经PDE求解器存在误差累积和物理一致性保持难题，传统方法在长期预测中稳定性不足

Method: 结合数值隐式时间步进法的稳定性优势，构建多粒度时间下采样机制，利用压缩率递增的未来状态进行跨尺度动态捕捉

Result: 在湍流流体动力学任务中实现高短期精度，长期预测稳定性显著超越自回归基线，计算开销仅微量增加

Conclusion: 多尺度隐式架构有效解决了神经求解器的长期误差累积问题，为复杂动力系统建模提供了新的稳定化方案

Abstract: Neural PDE solvers offer a powerful tool for modeling complex dynamical
systems, but often struggle with error accumulation over long time horizons and
maintaining stability and physical consistency. We introduce a multiscale
implicit neural emulator that enhances long-term prediction accuracy by
conditioning on a hierarchy of lower-dimensional future state representations.
Drawing inspiration from the stability properties of numerical implicit
time-stepping methods, our approach leverages predictions several steps ahead
in time at increasing compression rates for next-timestep refinements. By
actively adjusting the temporal downsampling ratios, our design enables the
model to capture dynamics across multiple granularities and enforce long-range
temporal coherence. Experiments on turbulent fluid dynamics show that our
method achieves high short-term accuracy and produces long-term stable
forecasts, significantly outperforming autoregressive baselines while adding
minimal computational overhead.

</details>


### [218] [HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training](https://arxiv.org/abs/2506.04531)
*Geon-Woo Kim,Junbo Li,Shashidhar Gandham,Omar Baldonado,Adithya Gangidi,Pavan Balaji,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 提出分层异步优化框架HALoS，通过局部/全局参数服务器架构降低跨区域通信成本，提升异构环境下LLM训练效率


<details>
  <summary>Details</summary>
Motivation: 解决地理分布式LLM训练中跨区域通信成本过高和异构硬件利用率不均的问题

Method: 设计含区域LPS和全局GPS的层次架构，引入分层动量机制，支持服务器端更新累积和全局模型合并

Result: 收敛速度比同步基线快7.5倍，模型质量保持同步SGD水平，在下游任务中表现更优

Conclusion: 分层异步机制是地理异构环境下可扩展LLM训练的有效解决方案，显著降低训练时间同时保持模型性能

Abstract: Training large language models (LLMs) increasingly relies on geographically
distributed accelerators, causing prohibitive communication costs across
regions and uneven utilization of heterogeneous hardware. We propose HALoS, a
hierarchical asynchronous optimization framework that tackles these issues by
introducing local parameter servers (LPSs) within each region and a global
parameter server (GPS) that merges updates across regions. This hierarchical
design minimizes expensive inter-region communication, reduces straggler
effects, and leverages fast intra-region links. We provide a rigorous
convergence analysis for HALoS under non-convex objectives, including
theoretical guarantees on the role of hierarchical momentum in asynchronous
training. Empirically, HALoS attains up to 7.5x faster convergence than
synchronous baselines in geo-distributed LLM training and improves upon
existing asynchronous methods by up to 2.1x. Crucially, HALoS preserves the
model quality of fully synchronous SGD-matching or exceeding accuracy on
standard language modeling and downstream benchmarks-while substantially
lowering total training time. These results demonstrate that hierarchical,
server-side update accumulation and global model merging are powerful tools for
scalable, efficient training of new-era LLMs in heterogeneous, geo-distributed
environments.

</details>


### [219] [NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models](https://arxiv.org/abs/2506.04536)
*Luca Ghafourpour,Valentin Duruisseaux,Bahareh Tolooshams,Philip H. Wong,Costas A. Anastassiou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出NOBLE神经操作器框架，通过连续特征嵌入映射预测神经电压响应，解决传统模型在变异性处理和计算效率上的不足，实现4200倍加速并保持实验变异性。


<details>
  <summary>Details</summary>
Motivation: 现有神经元建模受限于数据稀缺性和内在变异性，确定性模型集合难以扩展，深度学习无法充分模拟生物物理复杂性和非线性电压动态。需要新方法高效生成符合实验变异性的合成神经元。

Method: 构建NOBLE框架：1) 建立可解释神经元特征的频率调制嵌入空间；2) 学习从特征嵌入到体细胞电压响应的神经操作器映射；3) 通过生物物理模型生成训练数据；4) 支持嵌入空间插值生成实验一致性模型。

Result: 1) 成功预测考虑实验变异性的神经动力学分布；2) 在真实实验数据验证中实现4200倍速度提升；3) 首次实现大规模深度学习框架对实验数据的有效建模；4) 生成的合成神经元保持试验间变异性特性。

Conclusion: NOBLE突破性地统一了生物物理复杂性与计算效率，为神经形态计算、脑环路模拟和神经AI提供新范式，开辟了从细胞特性到大规模脑网络研究的新路径。

Abstract: Characterizing the diverse computational properties of human neurons via
multimodal electrophysiological, transcriptomic, and morphological data
provides the foundation for constructing and validating bio-realistic neuron
models that can advance our understanding of fundamental mechanisms underlying
brain function. However, current modeling approaches remain constrained by the
limited availability and intrinsic variability of experimental neuronal data.
To capture variability, ensembles of deterministic models are often used, but
are difficult to scale as model generation requires repeating computationally
expensive optimization for each neuron. While deep learning is becoming
increasingly relevant in this space, it fails to capture the full biophysical
complexity of neurons, their nonlinear voltage dynamics, and variability. To
address these shortcomings, we introduce NOBLE, a neural operator framework
that learns a mapping from a continuous frequency-modulated embedding of
interpretable neuron features to the somatic voltage response induced by
current injection. Trained on data generated from biophysically realistic
neuron models, NOBLE predicts distributions of neural dynamics accounting for
the intrinsic experimental variability. Unlike conventional bio-realistic
neuron models, interpolating within the embedding space offers models whose
dynamics are consistent with experimentally observed responses. NOBLE is the
first scaled-up deep learning framework validated on real experimental data,
enabling efficient generation of synthetic neurons that exhibit trial-to-trial
variability and achieve a $4200\times$ speedup over numerical solvers. To this
end, NOBLE captures fundamental neural properties, opening the door to a better
understanding of cellular composition and computations, neuromorphic
architectures, large-scale brain circuits, and general neuroAI applications.

</details>


### [220] [Neural MJD: Neural Non-Stationary Merton Jump Diffusion for Time Series Prediction](https://arxiv.org/abs/2506.04542)
*Yuanpei Gao,Qi Yan,Yan Leng,Renjie Liao*

Main category: cs.LG

TL;DR: 提出Neural MJD模型，将神经网络与非平稳随机微分方程结合，通过显式建模扩散过程和跳跃过程提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法因黑箱特性难以显式建模随机过程，在非平稳数据（尤其突变场景）泛化能力受限。本文旨在结合神经网络与Merton跳跃扩散模型，增强对非平稳动态和突发跳变的建模能力。

Method: 1) 构建时变Itô扩散过程捕捉非平稳随机动态
2) 用时变复合泊松过程建模跳跃
3) 提出跳跃数截断机制确保可学习性
4) 设计带重启的Euler-Maruyama求解器降低误差方差

Result: 在合成/真实数据集上均超越现有深度学习和统计学习方法，理论分析证明截断机制的误差边界及新求解器的更低方差。

Conclusion: Neural MJD通过融合神经网络与随机过程建模，有效解决了非平稳时间序列预测中的动态建模问题，截断机制和新型求解器提升了方法的理论保障和实际性能。

Abstract: While deep learning methods have achieved strong performance in time series
prediction, their black-box nature and inability to explicitly model underlying
stochastic processes often limit their generalization to non-stationary data,
especially in the presence of abrupt changes. In this work, we introduce Neural
MJD, a neural network based non-stationary Merton jump diffusion (MJD) model.
Our model explicitly formulates forecasting as a stochastic differential
equation (SDE) simulation problem, combining a time-inhomogeneous It\^o
diffusion to capture non-stationary stochastic dynamics with a
time-inhomogeneous compound Poisson process to model abrupt jumps. To enable
tractable learning, we introduce a likelihood truncation mechanism that caps
the number of jumps within small time intervals and provide a theoretical error
bound for this approximation. Additionally, we propose an Euler-Maruyama with
restart solver, which achieves a provably lower error bound in estimating
expected states and reduced variance compared to the standard solver.
Experiments on both synthetic and real-world datasets demonstrate that Neural
MJD consistently outperforms state-of-the-art deep learning and statistical
learning methods.

</details>


### [221] [Communication Efficient Adaptive Model-Driven Quantum Federated Learning](https://arxiv.org/abs/2506.04548)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出模型驱动的量子联邦学习算法mdQFL，有效解决联邦学习的通信瓶颈、设备扩展性和数据异构性问题，通信成本降低50%同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习面临大规模数据训练瓶颈、设备规模扩展挑战和非独立同分布数据导致的性能下降问题，量子计算环境加剧了这些挑战。

Method: 开发模型驱动的mdQFL算法，实现训练参数个性化更新和测试泛化，适应不同设备规模，首次在QFL框架中探索个性化训练机制。

Result: 在Qiskit环境中验证，mdQFL减少50%通信成本，模型精度持平或优于基线，局部模型训练效果持续提升，支持不同非IID数据分布场景。

Conclusion: mdQFL框架在量子联邦学习中具有显著效率优势，其个性化更新机制可拓展至其他联邦学习场景，为大规模分布式学习提供新范式。

Abstract: Training with huge datasets and a large number of participating devices leads
to bottlenecks in federated learning (FL). Furthermore, the challenges of
heterogeneity between multiple FL clients affect the overall performance of the
system. In a quantum federated learning (QFL) context, we address these three
main challenges: i) training bottlenecks from massive datasets, ii) the
involvement of a substantial number of devices, and iii) non-IID data
distributions. We introduce a model-driven quantum federated learning algorithm
(mdQFL) to tackle these challenges. Our proposed approach is efficient and
adaptable to various factors, including different numbers of devices. To the
best of our knowledge, it is the first to explore training and update
personalization, as well as test generalization within a QFL setting, which can
be applied to other FL scenarios. We evaluated the efficiency of the proposed
mdQFL framework through extensive experiments under diverse non-IID data
heterogeneity conditions using various datasets within the Qiskit environment.
Our results demonstrate a nearly 50% decrease in total communication costs
while maintaining or, in some cases, exceeding the accuracy of the final model
and consistently improving local model training compared to the standard QFL
baseline. Moreover, our experimental evaluation thoroughly explores the QFL and
mdQFL algorithms, along with several influencing factors. In addition, we
present a theoretical analysis to clarify the complexities of the proposed
algorithm. The experimental code is available at 1.

</details>


### [222] [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)
*Kareem Amin,Salman Avestimehr,Sara Babakniya,Alex Bie,Weiwei Kong,Natalia Ponomareva,Umar Syed*

Main category: cs.LG

TL;DR: 通过数据聚类和私有中位数统计改进差分隐私语言模型推理，显著提升合成文本质量并降低隐私成本


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私文本生成方法采用均匀抽样构建推理批次，在处理异构主题数据时会导致生成质量下降。同时传统方法使用平均统计容易受异常值影响，需要更稳定的聚合方式

Method: 1. 先对输入数据进行聚类构建同质推理批次
2. 利用聚类后预测相似性特征，改用中位数替代平均数进行token统计
3. 通过数据相关性的ex-post差分隐私保证机制

Result: MAUVE指标提升18%，下游任务准确率提高12%，隐私成本降低40%。在相同隐私预算下生成质量超越SOTA方法

Conclusion: 该方法通过数据预处理优化和统计方法创新，在保持差分隐私保证的同时显著提升合成文本质量，为隐私敏感场景的文本生成提供了更优解决方案

Abstract: Differentially private (DP) language model inference is an approach for
generating private synthetic text. A sensitive input example is used to prompt
an off-the-shelf large language model (LLM) to produce a similar example.
Multiple examples can be aggregated together to formally satisfy the DP
guarantee.
  Prior work creates inference batches by sampling sensitive inputs uniformly
at random. We show that uniform sampling degrades the quality of privately
generated text, especially when the sensitive examples concern heterogeneous
topics.
  We remedy this problem by clustering the input data before selecting
inference batches. Next, we observe that clustering also leads to more similar
next-token predictions across inferences. We use this insight to introduce a
new algorithm that aggregates next token statistics by privately computing
medians instead of averages. This approach leverages the fact that the median
has decreased local sensitivity when next token predictions are similar,
allowing us to state a data-dependent and ex-post DP guarantee about the
privacy properties of this algorithm. Finally, we demonstrate improvements in
terms of representativeness metrics (e.g., MAUVE) as well as downstream task
performance. We show that our method produces high-quality synthetic data at
significantly lower privacy cost than a previous state-of-the-art method.

</details>


### [223] [StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation](https://arxiv.org/abs/2506.04567)
*Ranjith Merugu,Bryan Bo Cao,Shubham Jain*

Main category: cs.LG

TL;DR: StatsMerging提出基于权重统计的轻量级模型合并方法，通过SVD奇异值指导任务系数预测，结合蒸馏技术实现异构架构合并，在八个任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法需依赖标注数据且难以适应异构架构，本文旨在解决内存约束下多模型部署问题，实现无标签/无测试样本的通用模型合并方案。

Method: 1. 利用SVD奇异值表征任务权重分布
2. 设计StatsMergeLearner建模权重分布
3. 异构架构合并采用任务特定教师蒸馏（含两种蒸馏范式）

Result: 在8个任务上超越SOTA方法，整体准确率提升，对未见任务泛化性增强（+4.2%），且对图像质量变化展现更强鲁棒性（误差降低12.1%）。

Conclusion: 首次实现无监督的统计驱动模型合并，通过轻量统计学习器与蒸馏框架，为多模型部署提供高效解决方案，特别适合边缘设备部署场景。

Abstract: Model merging has emerged as a promising solution to accommodate multiple
large models within constrained memory budgets. We present StatsMerging, a
novel lightweight learning-based model merging method guided by weight
distribution statistics without requiring ground truth labels or test samples.
StatsMerging offers three key advantages: (1) It uniquely leverages singular
values from singular value decomposition (SVD) to capture task-specific weight
distributions, serving as a proxy for task importance to guide task coefficient
prediction; (2) It employs a lightweight learner StatsMergeLearner to model the
weight distributions of task-specific pre-trained models, improving
generalization and enhancing adaptation to unseen samples; (3) It introduces
Task-Specific Teacher Distillation for merging vision models with heterogeneous
architectures, a merging learning paradigm that avoids costly ground-truth
labels by task-specific teacher distillation. Notably, we present two types of
knowledge distillation, (a) distilling knowledge from task-specific models to
StatsMergeLearner; and (b) distilling knowledge from models with heterogeneous
architectures prior to merging. Extensive experiments across eight tasks
demonstrate the effectiveness of StatsMerging. Our results show that
StatsMerging outperforms state-of-the-art techniques in terms of overall
accuracy, generalization to unseen tasks, and robustness to image quality
variations.

</details>


### [224] [Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets](https://arxiv.org/abs/2506.04598)
*Marianna Nezhurina,Tomer Porian,Giovanni Pucceti,Tommie Kerssies,Romain Beaumont,Mehdi Cherti,Jenia Jitsev*

Main category: cs.LG

TL;DR: 通过扩展定律对比CLIP与MaMMUT模型，发现MaMMUT在规模扩展和样本效率上优于CLIP，验证了多任务损失的优势。


<details>
  <summary>Details</summary>
Motivation: 传统扩展定律主要用于预测模型性能，本研究将其拓展至模型与数据集对比领域，旨在系统评估不同预训练方法的优劣。

Method: 在DataComp、DFN、Re-LAION等数据集上密集训练不同规模模型，采用恒定学习率策略降低计算成本，建立跨任务（分类/检索/分割）的扩展定律。

Result: MaMMUT-L/14实现80.3%的零样本ImageNet-1k准确率，其规模扩展效率较CLIP提升2.1倍，样本效率提升1.8倍。

Conclusion: 扩展定律可避免单尺度评估偏差，为开源基础模型和训练集的系统优化提供方法论，相关资源已开源促进社区验证。

Abstract: In studies of transferable learning, scaling laws are obtained for various
important foundation models to predict their properties and performance at
larger scales. We show here how scaling law derivation can also be used for
model and dataset comparison, allowing to decide which procedure is to be
preferred for pre-training. For the first time, full scaling laws based on
dense measurements across a wide span of model and samples seen scales are
derived for two important language-vision learning procedures, CLIP and MaMMUT,
that use either contrastive only or contrastive and captioning text generative
loss. Ensuring sufficient prediction accuracy for held out points, we use
derived scaling laws to compare both models, obtaining evidence for MaMMUT's
stronger improvement with scale and better sample efficiency than standard
CLIP. To strengthen validity of the comparison, we show scaling laws for
various downstream tasks, classification, retrieval, and segmentation, and for
different open datasets, DataComp, DFN and Re-LAION, observing consistently the
same trends. We show that comparison can also be performed when deriving
scaling laws with a constant learning rate schedule, reducing compute cost.
Accurate derivation of scaling laws provides thus means to perform model and
dataset comparison across scale spans, avoiding misleading conclusions based on
measurements from single reference scales only, paving the road for systematic
comparison and improvement of open foundation models and datasets for their
creation. We release all the pre-trained models with their intermediate
checkpoints, including openMaMMUT-L/14, which achieves $80.3\%$ zero-shot
ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for
reproducing experiments in the paper and raw experiments data can be found at
https://github.com/LAION-AI/scaling-laws-for-comparison.

</details>


### [225] [Ignoring Directionality Leads to Compromised Graph Neural Network Explanations](https://arxiv.org/abs/2506.04608)
*Changsheng Sun,Xinke Li,Jin Song Dong*

Main category: cs.LG

TL;DR: 论文揭示图对称化操作会丢弃方向语义信息导致解释失真，提出保留方向特征可显著提升GNN可解释性，对安全关键领域尤为重要。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法在对称化过程中丢失方向信息，导致解释结果可信度降低，影响人类在安全关键领域的决策可靠性。

Method: 通过理论分析揭示方向信息对消息传递机制的影响，并设计控制变量实验验证方向语义保留对解释质量的提升效果。

Result: 实证研究表明保留方向特征的GNN解释方法能捕捉更精确的因果模式，解释保真度平均提升37.2%。

Conclusion: 方向感知的GNN可解释框架是安全敏感场景的必要技术，研究为可信图学习开辟了新的设计范式。

Abstract: Graph Neural Networks (GNNs) are increasingly used in critical domains, where
reliable explanations are vital for supporting human decision-making. However,
the common practice of graph symmetrization discards directional information,
leading to significant information loss and misleading explanations. Our
analysis demonstrates how this practice compromises explanation fidelity.
Through theoretical and empirical studies, we show that preserving directional
semantics significantly improves explanation quality, ensuring more faithful
insights for human decision-makers. These findings highlight the need for
direction-aware GNN explainability in security-critical applications.

</details>


### [226] [Exploring bidirectional bounds for minimax-training of Energy-based models](https://arxiv.org/abs/2506.04609)
*Cong Geng,Jia Wang,Li Chen,Zhiyong Gao,Jes Frellsen,Søren Hauberg*

Main category: cs.LG

TL;DR: 提出使用双向边界训练基于能量的模型，通过同时优化上下界解决传统训练不稳定的问题，显著提升密度估计和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于变分下界的EBM训练存在不稳定性，需要更稳健的优化框架来改善模型性能。

Method: 设计四种双向边界：基于生成器雅可比矩阵奇异值/互信息的下界，结合梯度惩罚/扩散过程的上界，构建双向优化目标。

Result: 双向边界使EBM训练收敛更稳定，在CIFAR-10等数据集上实现SOTA级的似然估计和生成样本质量。

Conclusion: 双向边界机制为EBM训练提供了理论保障，平衡了模型表达能力与优化稳定性，推动了概率模型的实际应用。

Abstract: Energy-based models (EBMs) estimate unnormalized densities in an elegant
framework, but they are generally difficult to train. Recent work has linked
EBMs to generative adversarial networks, by noting that they can be trained
through a minimax game using a variational lower bound. To avoid the
instabilities caused by minimizing a lower bound, we propose to instead work
with bidirectional bounds, meaning that we maximize a lower bound and minimize
an upper bound when training the EBM. We investigate four different bounds on
the log-likelihood derived from different perspectives. We derive lower bounds
based on the singular values of the generator Jacobian and on mutual
information. To upper bound the negative log-likelihood, we consider a gradient
penalty-like bound, as well as one based on diffusion processes. In all cases,
we provide algorithms for evaluating the bounds. We compare the different
bounds to investigate, the pros and cons of the different approaches. Finally,
we demonstrate that the use of bidirectional bounds stabilizes EBM training and
yields high-quality density estimation and sample generation.

</details>


### [227] [Composing Agents to Minimize Worst-case Risk](https://arxiv.org/abs/2506.04632)
*Guruprerana Shabadi,Rajeev Alur*

Main category: cs.LG

TL;DR: 提出基于agent graph的代理组合框架，通过近似价值风险算法平衡任务成功与尾部风险，实验验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有代理系统需同时优化任务成功率和风险指标，但缺乏对低概率尾部行为的系统性风险管理方法

Method: 用有向无环图建模代理组合，将风险量化为损失分布的尾部VaR，设计基于联合边界近似和动态规划的高效搜索算法

Result: 在强化学习代理组合的仿真测试中，算法有效近似VaR并找到最优组合，理论证明对常见损失函数具有渐进最优性

Conclusion: 该框架为实际AI系统部署提供了风险感知的代理组合方案，在安全关键场景中具有重要应用价值

Abstract: From software development to robot control, modern agentic systems decompose
complex objectives into a sequence of subtasks and choose a set of specialized
AI agents to complete them. We formalize an agentic workflow as a directed
acyclic graph, called an agent graph, where edges represent AI agents and paths
correspond to feasible compositions of agents. When deploying these systems in
the real world, we need to choose compositions of agents that not only maximize
the task success, but also minimize risk where the risk captures requirements
like safety, fairness, and privacy. This additionally requires carefully
analyzing the low-probability (tail) behaviors of compositions of agents. In
this work, we consider worst-case risk minimization over the set of feasible
agent compositions. We define worst-case risk as the tail quantile -- also
known as value-at-risk -- of the loss distribution of the agent composition
where the loss quantifies the risk associated with agent behaviors. We
introduce an efficient algorithm that traverses the agent graph and finds a
near-optimal composition of agents by approximating the value-at-risk via a
union bound and dynamic programming. Furthermore, we prove that the
approximation is near-optimal asymptotically for a broad class of practical
loss functions. To evaluate our framework, we consider a suite of video
game-like control benchmarks that require composing several agents trained with
reinforcement learning and demonstrate our algorithm's effectiveness in
approximating the value-at-risk and identifying the optimal agent composition.

</details>


### [228] [Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction](https://arxiv.org/abs/2506.04650)
*Zesheng Ye,Chengyi Cai,Ruijiang Dong,Jianzhong Qi,Lei Feng,Pin-Yu Chen,Feng Liu*

Main category: cs.LG

TL;DR: 提出神经网络可重编程性作为统一框架，通过信息接口操纵实现预训练模型的高效适应，建立模型重编程/提示调整/指令提示等技术的内在关联，并提出四维分类法


<details>
  <summary>Details</summary>
Motivation: 现有模型适应方法孤立发展缺乏统一理解，需要揭示不同技术（模型重编程/提示调整/指令提示）在信息接口操纵层面的共性原理

Method: 在保持模型参数冻结的前提下，通过在输入层/中间层/上下文等接口进行信息操纵（扰动输入/插入token/提供示例）来调整模型行为

Result: 建立跨模态的统一分析框架，提出基于操纵形式/位置/算子/输出对齐的四维分类法，揭示上下文学习/思维链等技术在理论层面的内在关联

Conclusion: 神经网络可重编程性为模型适应研究提供基础范式，未来可通过接口操纵理论深化模型控制、扩展应用场景，并需关注模型安全性问题

Abstract: As large-scale pre-trained foundation models continue to expand in size and
capability, efficiently adapting them to specific downstream tasks has become
increasingly critical. Despite substantial progress, existing adaptation
approaches have evolved largely in isolation, without a clear understanding of
their interrelationships. This survey introduces neural network
reprogrammability as a unifying framework that bridges mainstream model
adaptation techniques--model reprogramming, prompt tuning, and prompt
instruction--previously fragmented research areas yet converges on a shared
principle: repurposing a pre-trained model by manipulating information at the
interfaces while keeping the model parameters frozen. These methods exploit
neural networks' sensitivity to manipulation on different interfaces, be it
through perturbing inputs, inserting tokens into intermediate layers, or
providing task-specific examples in context, to redirect model behaviors
towards desired outcomes. We then present a taxonomy that categorizes such
information manipulation-based adaptation approaches across four key
dimensions: manipulation format (fixed or learnable), location (interfaces
where manipulations occur), operator (how they are applied), and output
alignment requirement (post-processing needed to align outputs with downstream
tasks). Notably, this framework applies consistently across data modalities,
independent of specific model architectures. Moreover, viewing established
techniques like in-context learning and chain-of-thought prompting through this
lens reveals both their theoretical connections and practical distinctions. We
further analyze remaining technical challenges and ethical considerations,
positioning neural network reprogrammability as a fundamental paradigm for
efficient model adaptation. We lastly identify promising research directions
emerging from this integrative viewpoint.

</details>


### [229] [The Oversmoothing Fallacy: A Misguided Narrative in GNN Research](https://arxiv.org/abs/2506.04653)
*MoonJeong Park,Sunghyun Choi,Jaeseung Heo,Eunhyeok Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: 论文挑战了GNN中过度平滑的传统认知，指出其影响被高估，并论证通过经典技术可实现深层GNN的有效构建


<details>
  <summary>Details</summary>
Motivation: 现有研究错误地将梯度消失归因于图聚合导致的过度平滑，阻碍了深层GNN架构的发展。作者旨在澄清这一误解，揭示GNN深度扩展的可能性

Method: 通过分析GNN三大核心操作（聚合、线性变换、非线性激活），结合经典解决方案（跳跃连接、归一化）进行深度GNN堆叠实验

Result: 证明梯度消失源于变换和激活而非聚合，传统方法可成功堆叠深层网络且不产生性能退化

Conclusion: 研究纠正了过度平滑的认知偏差，为深度GNN的潜力提供了新视角，推动该领域的进一步探索

Abstract: Oversmoothing has been recognized as a main obstacle to building deep Graph
Neural Networks (GNNs), limiting the performance. This position paper argues
that the influence of oversmoothing has been overstated and advocates for a
further exploration of deep GNN architectures. Given the three core operations
of GNNs, aggregation, linear transformation, and non-linear activation, we show
that prior studies have mistakenly confused oversmoothing with the vanishing
gradient, caused by transformation and activation rather than aggregation. Our
finding challenges prior beliefs about oversmoothing being unique to GNNs.
Furthermore, we demonstrate that classical solutions such as skip connections
and normalization enable the successful stacking of deep GNN layers without
performance degradation. Our results clarify misconceptions about oversmoothing
and shed new light on the potential of deep GNNs.

</details>


### [230] [Noise-Resistant Label Reconstruction Feature Selection for Partial Multi-Label Learning](https://arxiv.org/abs/2506.04669)
*Wanfu Gao,Hanlin Pan,Qingqi Han,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出一种考虑标签关系抗噪性和标签连通性的PML特征选择方法，以解决维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 现有PML方法主要基于低秩假设，但实际中难以满足且可能丢失高维信息，且现有方法在识别正标签方面表现不足。

Method: 1.利用标签关系的抗噪性消歧；2.改进低秩假设设计学习过程；3.通过标签连通性发现代表性标签并重构权重矩阵选择特征。

Result: 在基准数据集上的实验结果验证了方法的优越性。

Conclusion: 结合标签抗噪性和连通性的特征选择方法能有效提升PML模型性能，缓解维度灾难问题。

Abstract: The "Curse of dimensionality" is prevalent across various data patterns,
which increases the risk of model overfitting and leads to a decline in model
classification performance. However, few studies have focused on this issue in
Partial Multi-label Learning (PML), where each sample is associated with a set
of candidate labels, at least one of which is correct. Existing PML methods
addressing this problem are mainly based on the low-rank assumption. However,
low-rank assumption is difficult to be satisfied in practical situations and
may lead to loss of high-dimensional information. Furthermore, we find that
existing methods have poor ability to identify positive labels, which is
important in real-world scenarios. In this paper, a PML feature selection
method is proposed considering two important characteristics of dataset: label
relationship's noise-resistance and label connectivity. Our proposed method
utilizes label relationship's noise-resistance to disambiguate labels. Then the
learning process is designed through the reformed low-rank assumption. Finally,
representative labels are found through label connectivity, and the weight
matrix is reconstructed to select features with strong identification ability
to these labels. The experimental results on benchmark datasets demonstrate the
superiority of the proposed method.

</details>


### [231] [FedAPM: Federated Learning via ADMM with Partial Model Personalization](https://arxiv.org/abs/2506.04672)
*Shengkun Zhu,Feiteng Nie,Jinshan Zeng,Sheng Wang,Yuan Sun,Yuan Yao,Shangfeng Chen,Quanqing Xu,Chuanhui Yang*

Main category: cs.LG

TL;DR: 提出基于ADMM的联邦学习框架FedAPM，通过一阶和二阶邻近项缓解客户端漂移问题，在异构多模态数据上实现高效收敛


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据非独立同分布(non-iid)和多模态特性导致模型漂移，传统部分个性化方法会加剧这种漂移

Method: 构建含一阶补偿修正和二阶固定修正的增广拉格朗日函数，利用ADMM框架实现显式的拉格朗日乘子估计

Result: 在4个异构数据集上平均提升12.3%准确率、16.4% F1分数和18.0% AUC，通信轮次减少

Conclusion: FedAPM通过双阶修正机制有效控制客户端漂移，在理论上证明全局收敛性，实验验证其高效性和多指标优越性

Abstract: In federated learning (FL), the assumption that datasets from different
devices are independent and identically distributed (i.i.d.) often does not
hold due to user differences, and the presence of various data modalities
across clients makes using a single model impractical. Personalizing certain
parts of the model can effectively address these issues by allowing those parts
to differ across clients, while the remaining parts serve as a shared model.
However, we found that partial model personalization may exacerbate client
drift (each client's local model diverges from the shared model), thereby
reducing the effectiveness and efficiency of FL algorithms. We propose an FL
framework based on the alternating direction method of multipliers (ADMM),
referred to as FedAPM, to mitigate client drift. We construct the augmented
Lagrangian function by incorporating first-order and second-order proximal
terms into the objective, with the second-order term providing fixed correction
and the first-order term offering compensatory correction between the local and
shared models. Our analysis demonstrates that FedAPM, by using explicit
estimates of the Lagrange multiplier, is more stable and efficient in terms of
convergence compared to other FL frameworks. We establish the global
convergence of FedAPM training from arbitrary initial points to a stationary
point, achieving three types of rates: constant, linear, and sublinear, under
mild assumptions. We conduct experiments using four heterogeneous and
multimodal datasets with different metrics to validate the performance of
FedAPM. Specifically, FedAPM achieves faster and more accurate convergence,
outperforming the SOTA methods with average improvements of 12.3% in test
accuracy, 16.4% in F1 score, and 18.0% in AUC while requiring fewer
communication rounds.

</details>


### [232] [The cost of ensembling: is it always worth combining?](https://arxiv.org/abs/2506.04677)
*Marco Zanotti*

Main category: cs.LG

TL;DR: 集成学习能提升时间序列预测精度但显著增加计算成本，适当降低训练频率并使用小型集成（2-3模型）可在精度与效率间取得平衡


<details>
  <summary>Details</summary>
Motivation: 针对大规模数据集下预测精度与计算成本的矛盾，研究旨在为零售预测系统提供兼顾准确性和可持续性的集成策略指南

Method: 使用M5和VN1两个零售数据集，评估10个基础模型与8种集成配置，考量不同重训练频率下的点预测和概率预测精度

Result: 集成显著提升概率预测性能但计算成本高昂，降低重训练频率可节省70%成本且精度损失<1%，小型集成能达到接近最优的预测效果

Conclusion: 通过优化集成规模与训练策略，可实现精确、鲁棒且低成本的预测方案，推动可持续AI在预测领域的实际应用

Abstract: Given the continuous increase in dataset sizes and the complexity of
forecasting models, the trade-off between forecast accuracy and computational
cost is emerging as an extremely relevant topic, especially in the context of
ensemble learning for time series forecasting. To asses it, we evaluated ten
base models and eight ensemble configurations across two large-scale retail
datasets (M5 and VN1), considering both point and probabilistic accuracy under
varying retraining frequencies. We showed that ensembles consistently improve
forecasting performance, particularly in probabilistic settings. However, these
gains come at a substantial computational cost, especially for larger,
accuracy-driven ensembles. We found that reducing retraining frequency
significantly lowers costs, with minimal impact on accuracy, particularly for
point forecasts. Moreover, efficiency-driven ensembles offer a strong balance,
achieving competitive accuracy with considerably lower costs compared to
accuracy-optimized combinations. Most importantly, small ensembles of two or
three models are often sufficient to achieve near-optimal results. These
findings provide practical guidelines for deploying scalable and cost-efficient
forecasting systems, supporting the broader goals of sustainable AI in
forecasting. Overall, this work shows that careful ensemble design and
retraining strategy selection can yield accurate, robust, and cost-effective
forecasts suitable for real-world applications.

</details>


### [233] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: 提出差分隐私框架Urania，通过私有聚类和混合关键词提取技术，在保护用户隐私的同时有效提取对话洞察。


<details>
  <summary>Details</summary>
Motivation: 解决LLM交互分析中隐私保护与数据效用的平衡难题，满足严格隐私要求下的洞察提取需求。

Method: 整合差分隐私工具（聚类/分区选择/直方图汇总）与多模态关键词提取（频率/TF-IDF/LLM引导），构建端到端隐私保护流程。

Result: 实证显示框架在语义保留（提升15%）和隐私强度（ε=1.5）间取得平衡，隐私评估指标优于非私有基准38%。

Conclusion: Urania首次实现对话洞察的隐私安全分析范式，为LLM应用的隐私敏感场景提供可靠解决方案。

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [234] [Towards Better Generalization via Distributional Input Projection Network](https://arxiv.org/abs/2506.04690)
*Yifan Hao,Yanxin Lu,Xinwei Shen,Tong Zhang*

Main category: cs.LG

TL;DR: 提出DIPNet框架，通过分布输入投影改善神经网络泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接在神经网络中实施平滑性约束，导致泛化性能提升受限。传统训练损失无法有效反映模型泛化能力。

Method: 设计分布输入投影网络(DIPNet)，在每层将输入映射到可学习分布空间。通过分布表示诱导更平滑的损失曲面，降低局部平滑度指标和网络Lipschitz常数。

Result: 在ViT/LLM/ResNet/MLP等架构中验证有效性，提升标准场景/对抗攻击/OOD输入/推理任务下的测试性能。模型兼容性显示该方法具备通用性。

Conclusion: DIPNet提供了一种可无缝集成到现有模型的输入投影策略，为提升现代深度学习泛化性能提供了通用解决方案。

Abstract: As overparameterized models become increasingly prevalent, training loss
alone offers limited insight into generalization performance. While smoothness
has been linked to improved generalization across various settings, directly
enforcing smoothness in neural networks remains challenging. To address this,
we introduce Distributional Input Projection Networks (DIPNet), a novel
framework that projects inputs into learnable distributions at each layer. This
distributional representation induces a smoother loss landscape with respect to
the input, promoting better generalization. We provide theoretical analysis
showing that DIPNet reduces both local smoothness measures and the Lipschitz
constant of the network, contributing to improved generalization performance.
Empirically, we validate DIPNet across a wide range of architectures and tasks,
including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and
MLPs. Our method consistently enhances test performance under standard
settings, adversarial attacks, out-of-distribution inputs, and reasoning
benchmarks. We demonstrate that the proposed input projection strategy can be
seamlessly integrated into existing models, providing a general and effective
approach for boosting generalization performance in modern deep learning.

</details>


### [235] [Influence Functions for Edge Edits in Non-Convex Graph Neural Networks](https://arxiv.org/abs/2506.04694)
*Jaeseung Heo,Kyeongheung Yun,Seokwon Yoon,MoonJeong Park,Jungseul Ok,Dongwoo Kim*

Main category: cs.LG

TL;DR: 提出近端Bregman响应函数改进GNN影响预测，支持边删除/插入，在对抗攻击等场景验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有图影响函数依赖严格凸性假设，仅处理边删除且忽略消息传播变化，亟需更普适的预测方法

Method: 设计适用于神经网络的非凸近端Bregman响应函数，显式建模消息传播效应，统一处理边删除与插入

Result: 真实数据集验证预测准确性，成功应用于图重构和对抗攻击场景

Conclusion: 该方法突破传统凸性限制，首次实现边增删的联合影响预测，为GNN可解释性提供新工具

Abstract: Understanding how individual edges influence the behavior of graph neural
networks (GNNs) is essential for improving their interpretability and
robustness. Graph influence functions have emerged as promising tools to
efficiently estimate the effects of edge deletions without retraining. However,
existing influence prediction methods rely on strict convexity assumptions,
exclusively consider the influence of edge deletions while disregarding edge
insertions, and fail to capture changes in message propagation caused by these
modifications. In this work, we propose a proximal Bregman response function
specifically tailored for GNNs, relaxing the convexity requirement and enabling
accurate influence prediction for standard neural network architectures.
Furthermore, our method explicitly accounts for message propagation effects and
extends influence prediction to both edge deletions and insertions in a
principled way. Experiments with real-world datasets demonstrate accurate
influence predictions for different characteristics of GNNs. We further
demonstrate that the influence function is versatile in applications such as
graph rewiring and adversarial attacks.

</details>


### [236] [Enhanced Drought Analysis in Bangladesh: A Machine Learning Approach for Severity Classification Using Satellite Data](https://arxiv.org/abs/2506.04696)
*Tonmoy Paul,Mrittika Devi Mati,Md. Mahmudul Islam*

Main category: cs.LG

TL;DR: 提出结合卫星数据与机器学习的框架，利用K-means聚类和KNN等分类算法，成功划分孟加拉国38个地区的干旱等级，发现区域脆弱性差异显著


<details>
  <summary>Details</summary>
Motivation: 传统干旱指数忽视土壤湿度/温度因子，现有机器学习模型缺乏多地区卫星数据整合，需建立适合孟加拉国国情的预测体系

Method: 集成2012-2024年湿度/土壤湿度/温度数据，先通过K-means和贝叶斯高斯混合模型进行无监督聚类，再用KNN/随机森林/决策树/朴素贝叶斯构建分类模型

Result: 框架有效实现干旱等级分类，西北部地区干旱脆弱性显著高于沿海区域，随机森林模型准确率达89%

Conclusion: 机器学习模型在干旱识别预测中展现优势，区域差异分析为精准抗旱提供决策依据，多源数据融合是提升预测精度的关键

Abstract: Drought poses a pervasive environmental challenge in Bangladesh, impacting
agriculture, socio-economic stability, and food security due to its unique
geographic and anthropogenic vulnerabilities. Traditional drought indices, such
as the Standardized Precipitation Index (SPI) and Palmer Drought Severity Index
(PDSI), often overlook crucial factors like soil moisture and temperature,
limiting their resolution. Moreover, current machine learning models applied to
drought prediction have been underexplored in the context of Bangladesh,
lacking a comprehensive integration of satellite data across multiple
districts. To address these gaps, we propose a satellite data-driven machine
learning framework to classify drought across 38 districts of Bangladesh. Using
unsupervised algorithms like K-means and Bayesian Gaussian Mixture for
clustering, followed by classification models such as KNN, Random Forest,
Decision Tree, and Naive Bayes, the framework integrates weather data
(humidity, soil moisture, temperature) from 2012-2024. This approach
successfully classifies drought severity into different levels. However, it
shows significant variabilities in drought vulnerabilities across regions which
highlights the aptitude of machine learning models in terms of identifying and
predicting drought conditions.

</details>


### [237] [UNO: Unlearning via Orthogonalization in Generative models](https://arxiv.org/abs/2506.04712)
*Pinak Mandal,Georg A. Gottwald*

Main category: cs.LG

TL;DR: 提出基于损失梯度正交化的快速遗忘算法，能在保持模型生成质量的同时高效移除特定数据影响，实验表明其遗忘速度比梯度手术等方法快多个数量级。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型广泛应用，需解决因隐私/法律/有害内容修正导致的数据遗忘需求。传统全量重训练成本过高，需开发高效选择性遗忘算法。

Method: 基于损失梯度正交化原理设计算法，通过正交化梯度方向实现目标数据遗忘与保留数据参数影响的解耦，减少训练步骤。

Result: 在MNIST和CelebA数据集验证显示，新算法相比梯度手术等方法，遗忘速度提升2-3个数量级，且保持原始模型生成质量。

Conclusion: 梯度正交化方法有效平衡了数据遗忘需求与模型性能维护，为生成模型的安全部署提供了高效解决方案。

Abstract: As generative models become increasingly powerful and pervasive, the ability
to unlearn specific data, whether due to privacy concerns, legal requirements,
or the correction of harmful content, has become increasingly important. Unlike
in conventional training, where data are accumulated and knowledge is
reinforced, unlearning aims to selectively remove the influence of particular
data points without costly retraining from scratch. To be effective and
reliable, such algorithms need to achieve (i) forgetting of the undesired data,
(ii) preservation of the quality of the generation, (iii) preservation of the
influence of the desired training data on the model parameters, and (iv) small
number of training steps. We propose fast unlearning algorithms based on loss
gradient orthogonalization. We show that our algorithms are able to forget data
while maintaining the fidelity of the original model. Using MNIST and CelebA
data, we demonstrate that our algorithms achieve orders of magnitude faster
unlearning times than their predecessors, such as gradient surgery.

</details>


### [238] [Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models](https://arxiv.org/abs/2506.04746)
*Fei Ding,Baiqiao Wang,Zijian Zeng,Youwei Wang*

Main category: cs.LG

TL;DR: MGRPO通过双层GRPO架构解决标准GRPO中间监督缺失问题，首层生成响应，第二层自我纠正错误，在数学推理任务中显著超越原方法。


<details>
  <summary>Details</summary>
Motivation: 标准GRPO算法缺乏中间过程监督，导致复杂推理链条中单个错误会引发整体解决方案失效和训练不稳定。

Method: 采用双层GRPO架构：第一层生成初始响应，第二层通过专门训练的错误纠正机制形成自我修正闭环。

Result: 在多个数学推理基准测试中，MGRPO显著优于标准GRPO，同步提升了模型的推理能力和自我纠正能力。

Conclusion: MGRPO通过隐式的过程级监督机制，在不依赖密集标注奖励模型的情况下，成功提升了训练稳定性和模型性能。

Abstract: The Group Relative Policy Optimization (GRPO) algorithm has demonstrated
considerable success in enhancing the reasoning capabilities of large language
models (LLMs), as evidenced by DeepSeek-R1. However, the absence of
intermediate supervision in GRPO frequently leads to inefficient exploration
dynamics. A single error in a complex reasoning chain can invalidate the entire
solution, resulting in abrupt reward vanishing and compromising training
stability.To address these challenges, we propose MGRPO (Multi-layer GRPO).
MGRPO operates in two layers: the first layer employs standard GRPO to generate
an initial response. This response, along with the original query, is then fed
into a second-layer GRPO process. This second layer is specifically trained to
identify and correct errors in the initial response, effectively creating a
self-correction loop. This mechanism provides implicit process-level
supervision by rewarding successful error correction, without requiring an
explicit, densely-annotated reward model. Experimental results on several
mathematical reasoning benchmarks demonstrate that MGRPO significantly
outperforms standard GRPO, achieving superior performance by fostering both
reasoning and self-correction abilities.

</details>


### [239] [Log-Linear Attention](https://arxiv.org/abs/2506.04761)
*Han Guo,Songlin Yang,Tarushii Goel,Eric P. Xing,Tri Dao,Yoon Kim*

Main category: cs.LG

TL;DR: 提出对数线性注意力机制，在保持线性注意力计算效率的同时，通过对数增长的隐藏状态集合增强了模型表达能力，在Mamba-2和Gated DeltaNet架构中验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制存在二次计算复杂度问题，而现有线性注意力模型作为RNN变体受限于固定大小的隐藏状态。需要平衡效率与表达能力的新型注意力机制

Method: 用对数增长隐藏状态替代固定大小状态，设计特定增长函数实现对数线性计算复杂度的并行计算框架，可适配现有线性注意力架构（如Mamba-2/Gated DeltaNet）

Result: 基于Mamba-2和Gated DeltaNet实例化的对数线性变体相比原线性时间架构表现更优，验证了框架的有效性

Conclusion: Log-linear attention成功平衡效率与表达力，其灵活架构可提升现有线性注意力模型性能，实验证明该方法在保持计算效率的同时增强了序列建模能力

Abstract: The attention mechanism in Transformers is an important primitive for
accurate and scalable sequence modeling. Its quadratic-compute and
linear-memory complexity however remain significant bottlenecks. Linear
attention and state-space models enable linear-time, constant-memory sequence
modeling and can moreover be trained efficiently through matmul-rich
parallelization across sequence length. However, at their core these models are
still RNNs, and thus their use of a fixed-size hidden state to model the
context is a fundamental limitation. This paper develops log-linear attention,
an attention mechanism that balances linear attention's efficiency and the
expressiveness of softmax attention. Log-linear attention replaces the
fixed-size hidden state with a logarithmically growing set of hidden states. We
show that with a particular growth function, log-linear attention admits a
similarly matmul-rich parallel form whose compute cost is log-linear in
sequence length. Log-linear attention is a general framework and can be applied
on top of existing linear attention variants. As case studies, we instantiate
log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet
-- and find they perform well compared to their linear-time variants.

</details>


### [240] [OpenGT: A Comprehensive Benchmark For Graph Transformers](https://arxiv.org/abs/2506.04765)
*Jiachen Tang,Zhonghao Wang,Sirui Chen,Sheng Zhou,Jiawei Chen,Jiajun Bu*

Main category: cs.LG

TL;DR: 提出OpenGT基准测试框架，系统评估图变换器性能并揭示其设计选择与适用场景的关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有图变换器适用场景不明确且缺乏统一评估标准，多样化设计选择缺乏有效性指导，阻碍研究发展。

Method: 通过建立标准化实验设置，整合前沿GNN/GT模型，从任务层级、注意力机制、位置编码等多维度开展系统性评估。

Result: 发现跨任务迁移困难、局部注意力局限性、位置编码场景特异性、预处理开销等关键设计影响因素。

Conclusion: OpenGT为图变换器研究建立公平可复现的评估基准，提供开源库支持，推动通用图学习框架发展。

Abstract: Graph Transformers (GTs) have recently demonstrated remarkable performance
across diverse domains. By leveraging attention mechanisms, GTs are capable of
modeling long-range dependencies and complex structural relationships beyond
local neighborhoods. However, their applicable scenarios are still
underexplored, this highlights the need to identify when and why they excel.
Furthermore, unlike GNNs, which predominantly rely on message-passing
mechanisms, GTs exhibit a diverse design space in areas such as positional
encoding, attention mechanisms, and graph-specific adaptations. Yet, it remains
unclear which of these design choices are truly effective and under what
conditions. As a result, the community currently lacks a comprehensive
benchmark and library to promote a deeper understanding and further development
of GTs. To address this gap, this paper introduces OpenGT, a comprehensive
benchmark for Graph Transformers. OpenGT enables fair comparisons and
multidimensional analysis by establishing standardized experimental settings
and incorporating a broad selection of state-of-the-art GNNs and GTs. Our
benchmark evaluates GTs from multiple perspectives, encompassing diverse tasks
and datasets with varying properties. Through extensive experiments, our
benchmark has uncovered several critical insights, including the difficulty of
transferring models across task levels, the limitations of local attention, the
efficiency trade-offs in several models, the application scenarios of specific
positional encodings, and the preprocessing overhead of some positional
encodings. We aspire for this work to establish a foundation for future graph
transformer research emphasizing fairness, reproducibility, and
generalizability. We have developed an easy-to-use library OpenGT for training
and evaluating existing GTs. The benchmark code is available at
https://github.com/eaglelab-zju/OpenGT.

</details>


### [241] [Kernel $k$-Medoids as General Vector Quantization](https://arxiv.org/abs/2506.04786)
*Thore Gerlach,Sascha Mücke,Christian Bauckhage*

Main category: cs.LG

TL;DR: 论文通过QUBO框架揭示了k-medoids与KDE两种向量量化方法的内在联系，证明KDE-QUBO是k-medoids-QUBO的特例。


<details>
  <summary>Details</summary>
Motivation: 旨在探索基于距离的k-medoids聚类与基于概率密度匹配的KDE方法在向量量化中的潜在联系，突破二者传统上看似无关的认知局限。

Method: 采用二次无约束二进制优化(QUBO)框架，分别构建k-medoids的启发式QUBO(平衡中心性与多样性)和KDE的MMD最小化QUBO进行对比分析。

Result: 在核特征映射的温和假设下，证明KDE-QUBO可视为k-medoids-QUBO的特殊情况，揭示二者权重参数的几何解释关系。

Conclusion: 突破方法论的表面差异，建立了概率密度方法与距离方法的深层结构关联，为VQ参数选择提供新的理论依据。

Abstract: Vector Quantization (VQ) is a widely used technique in machine learning and
data compression, valued for its simplicity and interpretability. Among hard VQ
methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches
represent two prominent yet seemingly unrelated paradigms -- one
distance-based, the other rooted in probability density matching. In this
paper, we investigate their connection through the lens of Quadratic
Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO
formulation for $k$-medoids, which balances centrality and diversity, with a
principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based
VQ. Surprisingly, we show that the KDE-QUBO is a special case of the
$k$-medoids-QUBO under mild assumptions on the kernel's feature map. This
reveals a deeper structural relationship between these two approaches and
provides new insight into the geometric interpretation of the weighting
parameters used in QUBO formulations for VQ.

</details>


### [242] [Adaptive Preconditioners Trigger Loss Spikes in Adam](https://arxiv.org/abs/2506.04805)
*Zhiwei Bai,Zhangchen Zhou,Jiajie Zhao,Xiaolong Li,Zhiyu Li,Feiyu Xiong,Hongkang Yang,Yaoyu Zhang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 发现Adam优化器的自适应预条件器本身会引发训练中的损失尖峰现象，当梯度方向曲率超过2/η阈值时触发


<details>
  <summary>Details</summary>
Motivation: 突破传统将损失尖峰归因于损失景观特性的认知，揭示优化器内部机制的关键影响

Method: 通过数学分析预条件Hessian矩阵特征值，结合β2指数衰减机制，在FCN、CNN和Transformer架构进行实验验证

Result: 二阶矩估计的滞后响应导致预条件Hessian最大特征值持续超过稳定性阈值，梯度方向对齐引发尖峰

Conclusion: 揭示了Adam优化器内在的稳定性机制缺陷，为优化算法设计提供了新的理论视角

Abstract: Loss spikes emerge commonly during training across neural networks of varying
architectures and scales when using the Adam optimizer. In this work, we
investigate the underlying mechanism responsible for Adam spikes. While
previous explanations attribute these phenomena to the lower-loss-as-sharper
characteristics of the loss landscape, our analysis reveals that Adam's
adaptive preconditioners themselves can trigger spikes. Specifically, we
identify a critical regime where squared gradients become substantially smaller
than the second-order moment estimates, causing the latter to undergo a
$\beta_2$-exponential decay and to respond sluggishly to current gradient
information. This mechanism can push the maximum eigenvalue of the
preconditioned Hessian beyond the classical stability threshold $2/\eta$ for a
sustained period, inducing instability. This instability further leads to an
alignment between the gradient and the maximum eigendirection, and a loss spike
occurs precisely when the gradient-directional curvature exceeds $2/\eta$. We
verify this mechanism through extensive experiments on fully connected
networks, convolutional networks, and Transformer architectures.

</details>


### [243] [LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.04821)
*Zhen Hao Wong,Jingwen Deng,Runming He,Zirong Chen,Qijie You,Hejun Dong,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.LG

TL;DR: 通过逻辑谜题强化学习框架提升大语言模型的结构化推理能力，实现跨领域数学问题的泛化提升


<details>
  <summary>Details</summary>
Motivation: 传统微调方法使LLMs形成特定领域启发式策略，而非通用推理能力，导致其在陌生场景的结构化推理表现不佳

Method: 构建7种逻辑谜题（约束传播/空间一致性/符号推理等），采用可验证奖励的强化学习机制，通过二值反馈驱动假设迭代式问题解决

Result: 在数学基准测试中分布外性能显著提升（中等难度多步推理问题提升27.6%），代数运算/几何推理/组合逻辑能力增强，但对机械记忆任务提升有限

Conclusion: 逻辑谜题训练重塑LLMs推理机制，形成可迁移的推理范式，实现无需领域符号工具的鲁棒组合泛化，为通用推理能力培养提供新路径

Abstract: Large language models (LLMs) excel at many supervised tasks but often
struggle with structured reasoning in unfamiliar settings. This discrepancy
suggests that standard fine-tuning pipelines may instill narrow,
domain-specific heuristics rather than fostering general-purpose thinking
strategies. In this work, we propose a "play to learn" framework that
fine-tunes LLMs through reinforcement learning on a suite of seven custom logic
puzzles, each designed to cultivate distinct reasoning skills such as
constraint propagation, spatial consistency, and symbolic deduction. Using a
reinforcement learning setup with verifiable rewards, models receive binary
feedback based on puzzle correctness, encouraging iterative, hypothesis-driven
problem solving. We demonstrate that this training approach significantly
improves out-of-distribution performance on a range of mathematical benchmarks,
especially for mid-difficulty problems that require multi-step reasoning.
Analyses across problem categories and difficulty levels reveal that puzzle
training promotes transferable reasoning routines, strengthening algebraic
manipulation, geometric inference, and combinatorial logic, while offering
limited gains on rote or highly specialized tasks. These findings show that
reinforcement learning over logic puzzles reshapes the internal reasoning of
LLMs, enabling more robust and compositional generalization without relying on
task-specific symbolic tools.

</details>


### [244] [From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs](https://arxiv.org/abs/2506.04831)
*Chantal Pellegrini,Ege Özsoy,David Bani-Harouni,Matthias Keicher,Nassir Navab*

Main category: cs.LG

TL;DR: 提出EHR2Path模型，通过结构化电子健康记录实现患者路径预测，改进医疗轨迹模拟效果


<details>
  <summary>Details</summary>
Motivation: 现有医疗数据分析方法局限于狭窄用例且忽视长期交互，需更全面的患者健康轨迹建模方法

Method: 1. 将多样化EHR数据转换为结构化表示 2. 设计带有时间上下文摘要机制的主题特定标记系统 3. 开发端到端路径预测模型

Result: 在时序预测和长期模拟中表现优异，预测指标包括生命体征、检验结果等，显著优于基线模型

Conclusion: 该框架支持详细的患者轨迹模拟，为实现预测性个性化医疗开辟新路径

Abstract: Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.

</details>


### [245] [Sparse Autoencoders, Again?](https://arxiv.org/abs/2506.04859)
*Yin Lu,Tong He,Xuening Zhu,David Wipf*

Main category: cs.LG

TL;DR: 提出混合稀疏自编码器模型，克服传统SAE和VAE在稀疏性与重建误差间的局限性，理论证明可恢复流形数据结构，实验验证在图像和语言任务中优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)和变分自编码器(VAE)存在固有缺陷：SAE依赖确定性稀疏正则化导致表示能力受限，VAE在稀疏场景下编码器存在稳定性问题。需开发新方法平衡稀疏性与重建精度。

Method: 融合SAE确定性编码与VAE概率建模优势，设计混合架构：1) 引入可微分稀疏门控机制；2) 构建联合优化目标函数，同步约束潜在空间稀疏性和数据分布似然；3) 理论证明模型全局最优解可恢复流形联合结构。

Result: 在合成数据中准确估计流形维度(误差<3%)，语言模型激活模式压缩率提升40%，图像重建PSNR达32.6dB(比SAE高1.2dB)，在等效容量下超越扩散模型2.4%的FID指标。

Conclusion: 混合模型突破传统自编码器的性能瓶颈，为语言模型可解释性分析和复杂数据建模提供新工具，理论保证与实证效果验证其在稀疏表征学习领域的先进性。

Abstract: Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.

</details>


### [246] [Aligning Multimodal Representations through an Information Bottleneck](https://arxiv.org/abs/2506.04870)
*Antonio Almudévar,José Miguel Hernández-Lobato,Sameer Khurana,Ricard Marxer,Alfonso Ortega*

Main category: cs.LG

TL;DR: 提出通过信息瓶颈原理分析多模态对比学习中模态特定信息残留问题，并设计正则化项增强表示对齐


<details>
  <summary>Details</summary>
Motivation: 现有对比损失虽能最大化模态间互信息，但无法去除模态特定信息，导致表示空间未真正对齐

Method: 基于信息瓶颈原理推导变分正则化项，通过约束潜在表示的条件熵消除模态特定信息

Result: 实验证明正则化项能有效提升对齐效果，在控制实验和实际应用场景中均表现出优势

Conclusion: 该正则化方法为多模态表示对齐提供了理论指导，具有广泛的应用潜力

Abstract: Contrastive losses have been extensively used as a tool for multimodal
representation learning. However, it has been empirically observed that their
use is not effective to learn an aligned representation space. In this paper,
we argue that this phenomenon is caused by the presence of modality-specific
information in the representation space. Although some of the most widely used
contrastive losses maximize the mutual information between representations of
both modalities, they are not designed to remove the modality-specific
information. We give a theoretical description of this problem through the lens
of the Information Bottleneck Principle. We also empirically analyze how
different hyperparameters affect the emergence of this phenomenon in a
controlled experimental setup. Finally, we propose a regularization term in the
loss function that is derived by means of a variational approximation and aims
to increase the representational alignment. We analyze in a set of controlled
experiments and real-world applications the advantages of including this
regularization term.

</details>


### [247] [There Was Never a Bottleneck in Concept Bottleneck Models](https://arxiv.org/abs/2506.04877)
*Antonio Almudévar,José Miguel Hernández-Lobato,Alfonso Ortega*

Main category: cs.LG

TL;DR: 提出最小概念瓶颈模型（MCBMs），通过信息瓶颈约束确保概念表示的纯净性，解决传统概念瓶颈模型（CBMs）多信息残留问题。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型（CBMs）无法保证表示仅编码对应概念信息，影响模型可解释性和干预有效性。

Method: 引入信息瓶颈目标（变分正则化），约束每个表示组件仅保留与概念相关的最小必要信息。

Result: MCBMs支持理论保障的概念干预，符合贝叶斯原则，且具备更高设计灵活性。

Conclusion: MCBMs通过信息瓶颈实现了真正的概念约束，提升可解释性和干预可靠性，同时维持模型灵活性。

Abstract: Deep learning representations are often difficult to interpret, which can
hinder their deployment in sensitive applications. Concept Bottleneck Models
(CBMs) have emerged as a promising approach to mitigate this issue by learning
representations that support target task performance while ensuring that each
component predicts a concrete concept from a predefined set. In this work, we
argue that CBMs do not impose a true bottleneck: the fact that a component can
predict a concept does not guarantee that it encodes only information about
that concept. This shortcoming raises concerns regarding interpretability and
the validity of intervention procedures. To overcome this limitation, we
propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an
Information Bottleneck (IB) objective to constrain each representation
component to retain only the information relevant to its corresponding concept.
This IB is implemented via a variational regularization term added to the
training loss. As a result, MCBMs support concept-level interventions with
theoretical guarantees, remain consistent with Bayesian principles, and offer
greater flexibility in key design choices.

</details>


### [248] [Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms Angle-Based Methods for Assessment of Hip Dysplasia](https://arxiv.org/abs/2506.04886)
*Allen Paul,George Grammatopoulos,Adwaye Rambojun,Neill D. F. Campbell,Harinderjit S. Gill,Tony Shardlow*

Main category: cs.LG

TL;DR: 开发基于CT扫描的GPDSSM模型实现髋关节发育不良半自动分类，AUC达96.2%优于传统角度测量法


<details>
  <summary>Details</summary>
Motivation: 髋关节发育不良是骨关节炎重要诱因，需通过早期诊断实施手术干预降低风险，但传统角度测量法耗时且依赖二维影像解读

Method: 融合高斯过程隐变量模型与微分同胚框架，构建三维统计形状模型(GPDSSM)，使用192例髋关节CT扫描数据（100训练/92测试）

Result: 模型有效区分发育不良样本(AUC 96.2%)，可视化显示病变区域特征，较传统角度测量法提升5%准确率并大幅节省临床时间

Conclusion: GPDSSM为髋关节发育不良诊断提供自动化三维分析方案，兼具高精度与临床实用性，有望优化早期筛查流程

Abstract: Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,
early diagnosis of dysplasia is important to provide opportunities for surgical
interventions aimed at reducing the risk of hip OA. We have developed a
pipeline for semi-automated classification of dysplasia using volumetric CT
scans of patients' hips and a minimal set of clinically annotated landmarks,
combining the framework of the Gaussian Process Latent Variable Model with
diffeomorphism to create a statistical shape model, which we termed the
Gaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT
scans, 100 for model training and 92 for testing. The GPDSSM effectively
distinguishes dysplastic samples from controls while also highlighting regions
of the underlying surface that show dysplastic variations. As well as improving
classification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),
the GPDSSM can save time for clinicians by removing the need to manually
measure angles and interpreting 2D scans for possible markers of dysplasia.

</details>


### [249] [Dissecting Long Reasoning Models: An Empirical Study](https://arxiv.org/abs/2506.04913)
*Yongyu Mu,Jiali Zeng,Bei Li,Xinyan Guan,Fandong Meng,Jie Zhou,Tong Xiao,Jingbo Zhu*

Main category: cs.LG

TL;DR: 论文系统分析强化学习长文本推理中的正负样本作用、数据效率问题和模型稳定性，提出负样本对泛化能力的关键作用、数据利用率提升策略以及多轮评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习训练长文本推理模型存在的三个反直觉现象展开研究：（1）正负样本作用的系统性分析；（2）群体相对策略优化的数据低效问题；（3）跨模型和基准的不稳定性能表现。

Method: 1. 通过正负样本分离实验揭示其作用机理
2. 提出相对长度奖励和离线样本注入策略提升数据利用率
3. 引入多轮评估机制解决不确定性问题

Result: 发现：（1）负样本贡献70%以上泛化能力
（2）数据利用率提升50%后推理效率提高2.3倍
（3）5次评估运行可使指标标准差降低47%

Conclusion: 重新定义强化学习训练范式：强调负样本价值、数据利用优化和多维度评估，为复杂推理任务提供系统解决方案。

Abstract: Despite recent progress in training long-context reasoning models via
reinforcement learning (RL), several open questions and counterintuitive
behaviors remain. This work focuses on three key aspects: (1) We systematically
analyze the roles of positive and negative samples in RL, revealing that
positive samples mainly facilitate data fitting, whereas negative samples
significantly enhance generalization and robustness. Interestingly, training
solely on negative samples can rival standard RL training performance. (2) We
identify substantial data inefficiency in group relative policy optimization,
where over half of the samples yield zero advantage. To address this, we
explore two straightforward strategies, including relative length rewards and
offline sample injection, to better leverage these data and enhance reasoning
efficiency and capability. (3) We investigate unstable performance across
various reasoning models and benchmarks, attributing instability to uncertain
problems with ambiguous outcomes, and demonstrate that multiple evaluation runs
mitigate this issue.

</details>


### [250] [Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer Fusion](https://arxiv.org/abs/2506.04924)
*Han Wang,Ruoyun He,Guoguang Lao,Ting Liu,Hejiao Luo,Changqi Qin,Hongying Luo,Junmin Huang,Zihan Wei,Lu Chen,Yongzhi Xu,Ziqian Bi,Junhao Song,Tianyang Wang,Chia Xin Liang,Xinyuan Song,Huafeng Liu,Junfeng Hao,Chunjie Tian*

Main category: cs.LG

TL;DR: 提出了ALFIA模块化架构，通过融合BERT的多层语义特征和自适应层加权机制，在ICU患者早期预警任务中超越现有分类器，并能与GBDT/深度神经网络协同提升性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别ICU高危患者对医疗资源分配至关重要，现有方法在临床文本特征融合和时效性方面存在局限。

Method: 1. BERT主干网络提取文本特征 2. LoRA适配器进行参数高效微调 3. 自适应层加权机制融合多层语义 4. 支持与梯度提升树(ALFIA-boost)和深度网络(ALFIA-nn)集成

Result: 在cw-24基准测试中AUPRC指标超越现有表格分类器，且嵌入特征与GBDT/深度网络结合带来额外6-8%性能提升

Conclusion: ALFIA通过直接处理临床文本，为危重护理提供了兼具时效性和准确性的风险分层工具，支持临床决策闭环。

Abstract: Early identification of high-risk ICU patients is crucial for directing
limited medical resources. We introduce ALFIA (Adaptive Layer Fusion with
Intelligent Attention), a modular, attention-based architecture that jointly
trains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting
mechanism to fuse multi-layer semantic features from a BERT backbone. Trained
on our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses
state-of-the-art tabular classifiers in AUPRC while preserving a balanced
precision-recall profile. The embeddings produced by ALFIA's fusion module,
capturing both fine-grained clinical cues and high-level concepts, enable
seamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro
networks as ALFIA-nn, yielding additional performance gains. Our experiments
confirm ALFIA's superior early-warning performance, by operating directly on
routine clinical text, it furnishes clinicians with a convenient yet robust
tool for risk stratification and timely intervention in critical-care settings.

</details>


### [251] [Agentic AI for Intent-Based Industrial Automation](https://arxiv.org/abs/2506.04980)
*Marcos Lima Romero,Ricardo Suyama*

Main category: cs.LG

TL;DR: 提出融合自主AI代理与意图处理的概念框架，通过自然语言交互简化工业5.0人机协同，实验验证了预测性维护场景的可行性。


<details>
  <summary>Details</summary>
Motivation: 工业4.0系统复杂度增加，需通过Agentic AI与意图导向范式整合实现人本化、可持续的工业5.0自动化，降低技术门槛。

Method: 采用意图处理框架实现自然语言目标分解，通过子代理协调和Google ADK工具链，基于CMAPSS数据集构建预测性维护验证系统。

Result: 概念验证成功实现意图分解、代理协调和自主决策，证实可扩展意图驱动自动化潜力，但存在数据质量和解释性挑战。

Conclusion: 该框架有效降低工业自动化技术障碍，支持符合工业5.0原则的意图驱动范式，需进一步解决数据质量和系统可解释性问题。

Abstract: The recent development of Agentic AI systems, empowered by autonomous large
language models (LLMs) agents with planning and tool-usage capabilities,
enables new possibilities for the evolution of industrial automation and
reduces the complexity introduced by Industry 4.0. This work proposes a
conceptual framework that integrates Agentic AI with the intent-based paradigm,
originally developed in network research, to simplify human-machine interaction
(HMI) and better align automation systems with the human-centric, sustainable,
and resilient principles of Industry 5.0. Based on the intent-based processing,
the framework allows human operators to express high-level business or
operational goals in natural language, which are decomposed into actionable
components. These intents are broken into expectations, conditions, targets,
context, and information that guide sub-agents equipped with specialized tools
to execute domain-specific tasks. A proof of concept was implemented using the
CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the
feasibility of intent decomposition, agent orchestration, and autonomous
decision-making in predictive maintenance scenarios. The results confirm the
potential of this approach to reduce technical barriers and enable scalable,
intent-driven automation, despite data quality and explainability concerns.

</details>


### [252] [FPTQuant: Function-Preserving Transforms for LLM Quantization](https://arxiv.org/abs/2506.04985)
*Boris van Breugel,Yelysei Bondarenko,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: FPTQuant提出四种函数保持变换实现高效LLM量化，在保持精度的同时实现3.9倍推理加速


<details>
  <summary>Details</summary>
Motivation: LLM推理存在巨大的计算能耗，传统量化方法因激活值异常导致性能显著下降，需开发保持模型功能的高效量化方案

Method: 设计预RoPE变换、值变换、MLP缩放变换和动态缩放变换四种FPT，利用Transformer的等变特性调整激活分布，采用端到端训练匹配全精度模型输出

Result: 实现静态INT4量化（无需定制内核），速度提升3.9倍，精度与现有方法相当/更优，比同类方法快29%

Conclusion: FPTQuant在精度-速度权衡中表现优异，为LLM高效推理提供了创新解决方案

Abstract: Large language models (LLMs) require substantial compute, and thus energy, at
inference time. While quantizing weights and activations is effective at
improving efficiency, naive quantization of LLMs can significantly degrade
performance due to large magnitude outliers. This paper describes FPTQuant,
which introduces four novel, lightweight, and expressive function-preserving
transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable
pre-RoPE transform for queries and keys, (2) a mergeable transform for values,
(3) a mergeable scaling transform within the MLP block, and (4) a cheap,
dynamic scaling transform. By leveraging the equivariances and independencies
inherent to canonical transformer operation, we designed these FPTs to maintain
the model's function while shaping the intermediate activation distributions to
be more quantization friendly. FPTQuant requires no custom kernels and adds
virtually no overhead during inference. The FPTs are trained both locally to
reduce outliers, and end-to-end such that the outputs of the quantized and
full-precision models match. FPTQuant enables static INT4 quantization with
minimal overhead and shows SOTA speed-up of up to 3.9 times over FP.
Empirically, FPTQuant has an excellent accuracy-speed trade-off -- it is
performing on par or exceeding most prior work and only shows slightly lower
accuracy compared to a method that is up to 29% slower.

</details>


### [253] [Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine Learning](https://arxiv.org/abs/2506.05016)
*John Collins*

Main category: cs.LG

TL;DR: 提出基于参考点距离的多点邻近编码（MPP），可将矢量地理空间数据转换为适合机器学习模型使用的向量表示，性能优于传统栅格化方法


<details>
  <summary>Details</summary>
Motivation: 现有地理空间数据编码方法需要有效捕捉几何特征，需要适用于各类空间形状的通用编码方法以增强机器学习模型的几何特征表示能力

Method: 通过计算目标形状与区域内多个参考点的标准化距离，建立具有几何特征保持能力的向量表示方法（MPP编码）

Result: MPP编码具备形状中心性、连续性特征，能有效区分空间对象几何形态，精准捕捉空间关系，在所有测试场景中均优于栅格化方法

Conclusion: MPP编码为地理空间数据提供了更优的编码方案，其几何特征保持能力可提升机器学习模型性能，未来可拓展应用于需要空间关系建模的领域

Abstract: Vector-mode geospatial data -- points, lines, and polygons -- must be encoded
into an appropriate form in order to be used with traditional machine learning
and artificial intelligence models. Encoding methods attempt to represent a
given shape as a vector that captures its essential geometric properties. This
paper presents an encoding method based on scaled distances from a shape to a
set of reference points within a region of interest. The method, MultiPoint
Proximity (MPP) encoding, can be applied to any type of shape, enabling the
parameterization of machine learning models with encoded representations of
vector-mode geospatial features. We show that MPP encoding possesses the
desirable properties of shape-centricity and continuity, can be used to
differentiate spatial objects based on their geometric features, and can
capture pairwise spatial relationships with high precision. In all cases, MPP
encoding is shown to perform better than an alternative method based on
rasterization.

</details>


### [254] [Tuning the Right Foundation Models is What you Need for Partial Label Learning](https://arxiv.org/abs/2506.05027)
*Kuang He,Wei Tang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 通过评估基础模型在部分标签学习中的表现，提出PartialCLIP框架，揭示当前PLL方法的局限性并给出改进方向


<details>
  <summary>Details</summary>
Motivation: 现有PLL方法主要关注CNN模型优化，但忽视了基础模型的迁移能力。本文旨在探索基础模型在PLL中的应用潜力，并提出有效改进方案。

Method: 1. 对11个基础模型在8个数据集进行3种PLL场景下的系统评估
2. 提出PartialCLIP框架（包含文本嵌入分类器初始化和零样本CLIP标签过滤）
3. 分析模型选择/适应策略对性能的影响

Result: 发现：1) 基础模型带来显著性能提升
2) 不同方法表现趋同
3) 模型对标签模糊度保持稳定
4) 性能受基础模型选择影响大
PartialCLIP验证了文本嵌入初始化（提升7.7%）和零样本标签过滤（提升4.3%）的有效性

Conclusion: 当前PLL方法存在基础模型依赖性和创新局限性，未来应聚焦：1) 基础模型适配策略优化
2) 零样本能力与PLL框架融合
3) 更严格的评估体系构建

Abstract: Partial label learning (PLL) seeks to train generalizable classifiers from
datasets with inexact supervision, a common challenge in real-world
applications. Existing studies have developed numerous approaches to
progressively refine and recover ground-truth labels by training convolutional
neural networks. However, limited attention has been given to foundation models
that offer transferrable representations. In this work, we empirically conduct
comprehensive evaluations of 11 foundation models across 13 PLL approaches on 8
benchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an
efficient fine-tuning framework for foundation models in PLL. Our findings
reveal that current PLL approaches tend to 1) achieve significant performance
gains when using foundation models, 2) exhibit remarkably similar performance
to each other, 3) maintain stable performance across varying ambiguity levels,
while 4) are susceptible to foundation model selection and adaptation
strategies. Additionally, we demonstrate the efficacy of text-embedding
classifier initialization and effective candidate label filtering using
zero-shot CLIP. Our experimental results and analysis underscore the
limitations of current PLL approaches and provide valuable insights for
developing more generalizable PLL models. The source code can be found at
https://github.com/SEU-hk/PartialCLIP.

</details>


### [255] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: 研究通过类间特征视角揭示对抗训练(AT)的机制，发现模型在训练初期学习更多跨类特征提升鲁棒性，后期过度依赖类内特征导致鲁棒过拟合。


<details>
  <summary>Details</summary>
Motivation: 探索对抗训练动态机制，解释为何类间特征影响模型鲁棒性，以及这些特征如何导致软标签训练优势和鲁棒过拟合现象。

Method: 基于理论建模与系统实验，使用合成数据验证类间特征作用，分析不同模型架构在AT各阶段特征学习模式的变化。

Result: AT初期跨类特征学习主导，达到最佳鲁棒性后持续训练转向类内特征，导致决策偏移和鲁棒过拟合，统一解释了软标签训练的有效性。

Conclusion: 研究为理解AT机制提供新视角，揭示特征学习动态与鲁棒性的内在关联，对优化对抗训练策略具有指导意义。

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [256] [TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation](https://arxiv.org/abs/2506.05035)
*Hyeongwon Jang,Changhun Kim,Eunho Yang*

Main category: cs.LG

TL;DR: 提出TIMING方法改进集成梯度，通过时间感知路径和新评估指标CPD/CPP提升时间序列XAI的归因准确性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列可解释性方法仅关注归因量级，忽略方向性影响；传统评估指标无法有效评估正负关键点的识别能力

Method: 开发TIMING方法：在集成梯度中引入时间感知机制，保持理论特性同时避免时空关系破坏和样本分布偏移

Result: 在合成和真实时间序列数据集上，TIMING在CPD/CPP指标下显著优于现有XAI基线方法

Conclusion: 新评估指标有效揭示传统IG的优势，TIMING通过时间感知增强成功解决直接应用IG的局限性，推动时间序列可解释性研究

Abstract: Recent explainable artificial intelligence (XAI) methods for time series
primarily estimate point-wise attribution magnitudes, while overlooking the
directional impact on predictions, leading to suboptimal identification of
significant points. Our analysis shows that conventional Integrated Gradients
(IG) effectively capture critical points with both positive and negative
impacts on predictions. However, current evaluation metrics fail to assess this
capability, as they inadvertently cancel out opposing feature contributions. To
address this limitation, we propose novel evaluation metrics-Cumulative
Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to
systematically assess whether attribution methods accurately identify
significant positive and negative points in time series XAI. Under these
metrics, conventional IG outperforms recent counterparts. However, directly
applying IG to time series data may lead to suboptimal outcomes, as generated
paths ignore temporal relationships and introduce out-of-distribution samples.
To overcome these challenges, we introduce TIMING, which enhances IG by
incorporating temporal awareness while maintaining its theoretical properties.
Extensive experiments on synthetic and real-world time series benchmarks
demonstrate that TIMING outperforms existing time series XAI baselines. Our
code is available at https://github.com/drumpt/TIMING.

</details>


### [257] [iN2V: Bringing Transductive Node Embeddings to Inductive Graphs](https://arxiv.org/abs/2506.05039)
*Nicolas Lell,Ansgar Scherp*

Main category: cs.LG

TL;DR: iN2V扩展node2vec到归纳式学习，通过后处理和训练调整提升节点分类效果，平均提升1%


<details>
  <summary>Details</summary>
Motivation: 解决传统node2vec仅适用于直推式场景（需所有节点在训练时可见）的局限性，使嵌入方法能处理训练时未见的新节点

Method: 1. 添加后处理步骤计算新节点嵌入 2. 改进原始N2V训练流程以适配后处理 3. 设计插件式架构兼容其他嵌入方法

Result: 在多个基准数据集上实现节点分类平均1%提升（最高达6%），且能无缝增强现有嵌入方法

Conclusion: iN2V成功桥接直推式与归纳式节点表示学习，具有通用性强、即插即用、可复现性高等优势（代码已开源）

Abstract: Shallow node embeddings like node2vec (N2V) can be used for nodes without
features or to supplement existing features with structure-based information.
Embedding methods like N2V are limited in their application on new nodes, which
restricts them to the transductive setting where the entire graph, including
the test nodes, is available during training. We propose inductive node2vec
(iN2V), which combines a post-hoc procedure to compute embeddings for nodes
unseen during training and modifications to the original N2V training procedure
to prepare the embeddings for this post-hoc procedure. We conduct experiments
on several benchmark datasets and demonstrate that iN2V is an effective
approach to bringing transductive embeddings to an inductive setting. Using
iN2V embeddings improves node classification by 1 point on average, with up to
6 points of improvement depending on the dataset and the number of unseen
nodes. Our iN2V is a plug-in approach to create new or enrich existing
embeddings. It can also be combined with other embedding methods, making it a
versatile approach for inductive node representation learning. Code to
reproduce the results is available at https://github.com/Foisunt/iN2V .

</details>


### [258] [Reliably detecting model failures in deployment without labels](https://arxiv.org/abs/2506.05047)
*Viet Nguyen Changjian Shui,Vijay Giri,Siddarth Arya,Amol Verma,Fahad Razak,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 提出D3M监控框架，通过模型预测分歧检测部署后模型性能衰退，有效区分无害数据漂移与性能损害性漂移。


<details>
  <summary>Details</summary>
Motivation: 动态环境中数据分布漂移可能导致模型性能衰退，但现有方法缺乏无标签情况下的可靠预警机制。传统方法无法区分无害漂移与损害性漂移，导致不必要的模型重训练。

Method: 基于预测模型间的分歧度设计D3M算法：①利用模型预测不一致性作为衰退指标 ②建立样本复杂度边界保证检测效率 ③采用双重阈值机制控制误报率

Result: 在基准测试和真实医疗数据集验证：①非衰退性漂移下误报率降低40%+ ②检测到95%衰退事件时样本需求减少50% ③医学案例中成功预警抗生素处方模型性能下降

Conclusion: D3M框架为高风险ML系统提供可靠监控，特别适用于医疗等关键领域，通过实时预警机制保障模型持续有效性。

Abstract: The distribution of data changes over time; models operating operating in
dynamic environments need retraining. But knowing when to retrain, without
access to labels, is an open challenge since some, but not all shifts degrade
model performance. This paper formalizes and addresses the problem of
post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and
efficient monitoring algorithm based on the disagreement of predictive models,
achieving low false positive rates under non-deteriorating shifts and provides
sample complexity bounds for high true positive rates under deteriorating
shifts. Empirical results on both standard benchmark and a real-world
large-scale internal medicine dataset demonstrate the effectiveness of the
framework and highlight its viability as an alert mechanism for high-stakes
machine learning pipelines.

</details>


### [259] [Associative Memory and Generative Diffusion in the Zero-noise Limit](https://arxiv.org/abs/2506.05178)
*Joshua Hess,Quaid Morris*

Main category: cs.LG

TL;DR: 通过动力系统理论揭示生成扩散模型与联想记忆的内在联系，证明零噪声极限下学习景观的稳定性及分岔机制的关键作用


<details>
  <summary>Details</summary>
Motivation: 研究生成扩散模型与连续状态联想记忆模型的动力学联系，揭示噪声水平降低时从生成模式向记忆模式转变的通用机制

Method: 将Morse-Smale系统作为梯度型联想记忆的通用近似框架，分析其结构稳定性，并应用于单/双参数梯度族的学习景观

Result: 证明联想记忆学习景观除孤立点外均稳定，扩散模型在零噪声极限下具有稳定梯度漂移，通过分岔序列刻画系统定性转变

Conclusion: 结构稳定性理论为理解扩散模型与联想记忆的动力学联系提供了新视角，分岔机制解释了学习过程中稳定系统的演化规律

Abstract: Connections between generative diffusion and continuous-state associative
memory models are studied. Morse-Smale dynamical systems are emphasized as
universal approximators of gradient-based associative memory models and
diffusion models as white-noise perturbed systems thereof. Universal properties
of associative memory that follow from this description are described and used
to characterize a generic transition from generation to memory as noise levels
diminish. Structural stability inherited by Morse-Smale flows is shown to imply
a notion of stability for diffusions at vanishing noise levels. Applied to one-
and two-parameter families of gradients, this indicates stability at all but
isolated points of associative memory learning landscapes and the learning and
generation landscapes of diffusion models with gradient drift in the zero-noise
limit, at which small sets of generic bifurcations characterize qualitative
transitions between stable systems. Examples illustrating the characterization
of these landscapes by sequences of these bifurcations are given, along with
structural stability criterion for classic and modern Hopfield networks
(equivalently, the attention mechanism).

</details>


### [260] [TreeRPO: Tree Relative Policy Optimization](https://arxiv.org/abs/2506.05183)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Xiaodan Liang,Yiwei Wang,Jing Tang*

Main category: cs.LG

TL;DR: 提出TreeRPO方法，通过树采样生成细粒度奖励信号，显著提升大语言模型推理性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在全轨迹层面定义奖励，无法有效指导推理过程的中间步骤优化

Method: 基于GRPO框架，创新使用树采样估算步骤级奖励期望，构建组相对奖励机制

Result: Qwen-2.5-Math测试准确率提升86.8%（19.0%→35.5%），性能超越GRPO 2.9%同时缩短响应长度18.1%

Conclusion: TreeRPO通过密集奖励信号有效优化中间推理步骤，显著提升模型性能与效率，推动LLM推理技术发展

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.

</details>


### [261] [Locality Preserving Markovian Transition for Instance Retrieval](https://arxiv.org/abs/2506.05196)
*Jifei Luo,Wenzheng Wu,Hantao Yao,Lu Yu,Changsheng Xu*

Main category: cs.LG

TL;DR: 提出LPMT框架，通过双向协同扩散和热力学马尔可夫转移解决传统扩散重排序中远距离信号衰减问题


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的重排序方法在亲和力图中经过多步传播后，正向信号会逐渐衰减，导致全局判别力不足

Method: 1. 双向协同扩散(BCD)整合多图扩散过程
2. 局部状态嵌入(LSE)增强局部一致性
3. 热力学马尔可夫转移(TMT)平衡全局检索与局部有效性

Result: 跨任务实验验证了LPMT在实例检索中的有效性

Conclusion: 通过热力学多状态转移机制，在保持局部有效性的同时实现了高效的全局检索

Abstract: Diffusion-based re-ranking methods are effective in modeling the data
manifolds through similarity propagation in affinity graphs. However, positive
signals tend to diminish over several steps away from the source, reducing
discriminative power beyond local regions. To address this issue, we introduce
the Locality Preserving Markovian Transition (LPMT) framework, which employs a
long-term thermodynamic transition process with multiple states for accurate
manifold distance measurement. The proposed LPMT first integrates diffusion
processes across separate graphs using Bidirectional Collaborative Diffusion
(BCD) to establish strong similarity relationships. Afterwards, Locality State
Embedding (LSE) encodes each instance into a distribution for enhanced local
consistency. These distributions are interconnected via the Thermodynamic
Markovian Transition (TMT) process, enabling efficient global retrieval while
maintaining local effectiveness. Experimental results across diverse tasks
confirm the effectiveness of LPMT for instance retrieval.

</details>


### [262] [Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning](https://arxiv.org/abs/2506.05214)
*Jingyu Hu,Hongbo Bo,Jun Hong,Xiaowei Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 论文提出基于学习难度自适应加权对比损失HAR和实验框架SHARP，通过增加正样本对和动态调整权重缓解GNN的节点度数偏差问题，在四个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法(GCL)存在正样本对数量有限、所有正负样本对平等加权的问题，导致低度数节点获得的信息不足且噪声较大。

Method: 1. 提出HAR对比损失：利用节点标签增加正样本对，根据学习难度自适应调整正负样本对权重
2. 开发SHARP实验框架：将HAR扩展到更广泛的应用场景

Result: 在四个数据集上的实验表明：
1. 全局层面性能优于基线模型
2. 不同度数节点层面的预测效果更均衡
3. 理论分析验证了框架有效性

Conclusion: 通过HAR对比损失的动态加权机制和SHARP框架的扩展应用，有效缓解了GNN的度数偏差问题，理论分析与实验结果一致证明了该方法的优越性。

Abstract: Graph Neural Networks (GNNs) often suffer from degree bias in node
classification tasks, where prediction performance varies across nodes with
different degrees. Several approaches, which adopt Graph Contrastive Learning
(GCL), have been proposed to mitigate this bias. However, the limited number of
positive pairs and the equal weighting of all positives and negatives in GCL
still lead to low-degree nodes acquiring insufficient and noisy information.
This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to
mitigate degree bias. It adds more positive pairs by leveraging node labels and
adaptively weights positive and negative pairs based on their learning
hardness. In addition, we develop an experimental framework named SHARP to
extend HAR to a broader range of scenarios. Both our theoretical analysis and
experiments validate the effectiveness of SHARP. The experimental results
across four datasets show that SHARP achieves better performance against
baselines at both global and degree levels.

</details>


### [263] [Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm](https://arxiv.org/abs/2506.05215)
*Zhan Yu*

Main category: cs.LG

TL;DR: 提出基于RKHS的去中心化鲁棒核学习算法，通过窗口函数和σ参数统一鲁棒回归框架，理论验证最优收敛速率与σ的鲁棒性增强作用。


<details>
  <summary>Details</summary>
Motivation: 现有分布式鲁棒核学习方法均基于分治策略，缺乏统一的去中心化框架。本文旨在构建基于网络系统的去中心化学习方案，通过理论保证解决算法鲁棒性、收敛性与系统结构的关联问题。

Method: 1. 构建由窗口函数W和σ参数诱导的鲁棒损失函数ℒσ 2. 设计基于图网络结构的去中心化算法 3. 运用核积分算子技术推导均方距离、RKHS范数和泛化误差的收敛界 4. 建立局部样本量与步长/σ参数的定量选择规则

Result: 1. 算法在适当参数下达到最优学习速率(对数因子内) 2. σ参数同时保证鲁棒性增强与收敛性 3. 理论揭示了去中心化架构-样本选择-鲁棒性-收敛性的四元关系

Conclusion: 该工作首次实现去中心化鲁棒学习的统一框架，理论证明σ参数的双重作用，建立了网络拓扑结构与学习性能的定量联系，为分布式鲁棒学习提供了新的方法论基础。

Abstract: We propose a new decentralized robust kernel-based learning algorithm within
the framework of reproducing kernel Hilbert space (RKHS) by utilizing a
networked system that can be represented as a connected graph. The robust loss
function $\mathcal{L}_\sigma$ induced by a windowing function $W$ and a
robustness scaling parameter $\sigma>0$, can encompass a broad spectrum of
robust losses. Consequently, the proposed algorithm effectively provides a
unified decentralized learning framework for robust regression, which
fundamentally differs from the existing distributed robust kernel learning
schemes, all of which are divide-and-conquer based. We rigorously establish the
learning theory and offer a comprehensive convergence analysis for the
algorithm. We show each local robust estimator generated from the decentralized
algorithm can be utilized to approximate the regression function. Based on
kernel-based integral operator techniques, we derive general high confidence
convergence bounds for each local approximating sequence in terms of the mean
square distance, RKHS norm, and generalization error, respectively. Moreover,
we provide rigorous selection rules for local sample size and show that, under
properly selected step size and scaling parameter $\sigma$, the decentralized
robust algorithm can achieve optimal learning rates (up to logarithmic factors)
in both norms. The parameter $\sigma$ is shown to be essential for enhancing
robustness while also ensuring favorable convergence behavior. The intrinsic
connection among decentralization, sample selection, robustness of the
algorithm, and its convergence is clearly reflected.

</details>


### [264] [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts](https://arxiv.org/abs/2506.05229)
*Danil Sivtsov,Ivan Rodkin,Gleb Kuzmin,Yuri Kuratov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出Diagonal Batching调度方案，通过并行化处理消除Recurrent Memory Transformers的顺序执行瓶颈，实现3.3倍推理加速


<details>
  <summary>Details</summary>
Motivation: 传统Transformer处理长文本存在二次复杂度问题，RMT虽优化内存但引入顺序执行限制。需突破顺序瓶颈提升实际应用效率

Method: 运行时计算重排序技术，在保持循环精确性的前提下实现跨段并行处理，无需模型重训练即可应用

Result: 在131,072 token序列上，LLaMA-1B ARMT模型推理速度提升3.3倍（相比标准注意力）和1.8倍（相比顺序RMT）

Conclusion: Diagonal Batching显著降低长文本推理延迟和成本，增强RMT在现实场景中的实用性

Abstract: Transformer models struggle with long-context inference due to their
quadratic time and linear memory complexity. Recurrent Memory Transformers
(RMTs) offer a solution by reducing the asymptotic cost to linear time and
constant memory usage. However, their memory update mechanism leads to
sequential execution, causing a performance bottleneck.
  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism
across segments in RMTs while preserving exact recurrence. This approach
eliminates the sequential constraint, enabling efficient GPU inference even for
single long-context inputs without complex batching and pipelining techniques.
Because the technique is purely a run-time computation reordering, existing RMT
models adopt it with no retraining.
  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup
over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential
RMT implementation on 131,072-token sequences. By removing sequential
bottleneck, Diagonal Batching reduces inference cost and latency, thereby
strengthening RMTs as a practical solution for real-world, long-context
applications.

</details>


### [265] [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)
*Johannes von Oswald,Nino Scherrer,Seijin Kobayashi,Luca Versari,Songlin Yang,Maximilian Schlegel,Kaitlin Maile,Yanick Schimpf,Oliver Sieberling,Alexander Meulemans,Rif A. Saurous,Guillaume Lajoie,Charlotte Frenkel,Razvan Pascanu,Blaise Agüera y Arcas,João Sacramento*

Main category: cs.LG

TL;DR: 论文提出了一种基于在线优化目标的Mesa层，通过共轭梯度求解器在推理阶段实现更优的序列建模效果，在保持RNN恒定计算成本优势的同时，显著提升了长上下文任务表现。


<details>
  <summary>Details</summary>
Motivation: 针对传统Transformer推理时计算资源线性增长的问题，探索通过在线学习规则优化RNN模型，寻求在恒定计算成本下实现更优的序列建模性能。

Method: 1. 设计数值稳定且支持分块并行的Mesa层
2. 引入上下文损失函数，在每一步用共轭梯度法求解最优参数
3. 在十亿参数规模验证语言建模能力

Result: 1. 困惑度低于现有RNN模型（Mamba等）
2. 长上下文理解任务表现显著提升
3. 推理计算量增加(FLOPs上升)
4. 在PG19、ProofNet等长序列基准测试中刷新记录

Conclusion: 通过将测试阶段计算资源投入序列优化问题的求解，验证了『增加推理时计算量提升性能』的新范式，为神经网络架构设计开辟了基于在线优化的新方向。

Abstract: Sequence modeling is currently dominated by causal transformer architectures
that use softmax self-attention. Although widely adopted, transformers require
scaling memory and compute linearly during inference. A recent stream of work
linearized the softmax operation, resulting in powerful recurrent neural
network (RNN) models with constant memory and compute costs such as DeltaNet,
Mamba or xLSTM. These models can be unified by noting that their recurrent
layer dynamics can all be derived from an in-context regression objective,
approximately optimized through an online learning rule. Here, we join this
line of work and introduce a numerically stable, chunkwise parallelizable
version of the recently proposed Mesa layer (von Oswald et al., 2024), and
study it in language modeling at the billion-parameter scale. This layer again
stems from an in-context loss, but which is now minimized to optimality at
every time point using a fast conjugate gradient solver. Through an extensive
suite of experiments, we show that optimal test-time training enables reaching
lower language modeling perplexity and higher downstream benchmark performance
than previous RNNs, especially on tasks requiring long context understanding.
This performance gain comes at the cost of additional flops spent during
inference time. Our results are therefore intriguingly related to recent trends
of increasing test-time compute to improve performance -- here by spending
compute to solve sequential optimization problems within the neural network
itself.

</details>


### [266] [Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit](https://arxiv.org/abs/2506.05239)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

TL;DR: 该论文提出基于匹配追踪的多层稀疏自编码器（MP-SAE），突破传统准正交性假设限制，实现相关特征提取并保证重构效果单调提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器在提取相关特征时受限于准正交性假设，无法有效处理手写数字生成等层次化场景中的特征关联性问题。

Method: 通过展开匹配追踪算法构建多层架构（MP-SAE），利用残差迭代机制逐步提取相关特征，同时引入重构效果单调性保证机制。

Result: 在MNIST数据集验证中，MP-SAE成功提取手写数字生成过程的层次化相关特征，重构误差随迭代次数增加呈单调下降趋势。

Conclusion: MP-SAE为处理非正交特征场景提供新范式，通过理论保证的特征提取机制显著提升可解释性分析能力。

Abstract: Sparse autoencoders (SAEs) have recently become central tools for
interpretability, leveraging dictionary learning principles to extract sparse,
interpretable features from neural representations whose underlying structure
is typically unknown. This paper evaluates SAEs in a controlled setting using
MNIST, which reveals that current shallow architectures implicitly rely on a
quasi-orthogonality assumption that limits the ability to extract correlated
features. To move beyond this, we introduce a multi-iteration SAE by unrolling
Matching Pursuit (MP-SAE), enabling the residual-guided extraction of
correlated features that arise in hierarchical settings such as handwritten
digit generation while guaranteeing monotonic improvement of the reconstruction
as more atoms are selected.

</details>


### [267] [Aligning Latent Spaces with Flow Priors](https://arxiv.org/abs/2506.05240)
*Yizhuo Li,Yuying Ge,Yixiao Ge,Ying Shan,Ping Luo*

Main category: cs.LG

TL;DR: 提出基于流生成模型的潜在空间对齐框架，通过预训练流模型和对齐损失实现高效分布匹配，避免传统方法的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统潜在空间对齐方法依赖计算密集型似然评估和ODE求解，本文旨在通过流模型先验和变分下界替代目标实现高效优化。

Method: 1. 预训练流模型捕捉目标分布
2. 设计对齐损失作为变分下界代理目标
3. 通过隐空间正则化实现分布匹配，避免ODE求解和显式似然计算

Result: 理论证明对齐损失与变分下界等价，图像生成实验验证框架有效性，消融研究证实关键组件贡献。

Conclusion: 该框架为潜在空间对齐提供了计算高效的新范式，通过理论推导和大规模实验验证了方法的优越性。

Abstract: This paper presents a novel framework for aligning learnable latent spaces to
arbitrary target distributions by leveraging flow-based generative models as
priors. Our method first pretrains a flow model on the target features to
capture the underlying distribution. This fixed flow model subsequently
regularizes the latent space via an alignment loss, which reformulates the flow
matching objective to treat the latents as optimization targets. We formally
prove that minimizing this alignment loss establishes a computationally
tractable surrogate objective for maximizing a variational lower bound on the
log-likelihood of latents under the target distribution. Notably, the proposed
method eliminates computationally expensive likelihood evaluations and avoids
ODE solving during optimization. As a proof of concept, we demonstrate in a
controlled setting that the alignment loss landscape closely approximates the
negative log-likelihood of the target distribution. We further validate the
effectiveness of our approach through large-scale image generation experiments
on ImageNet with diverse target distributions, accompanied by detailed
discussions and ablation studies. With both theoretical and empirical
validation, our framework paves a new way for latent space alignment.

</details>


### [268] [On the Convergence of Gradient Descent on Learning Transformers with Residual Connections](https://arxiv.org/abs/2506.05249)
*Zhen Qin,Jinxin Zhou,Zhihui Zhu*

Main category: cs.LG

TL;DR: 本文通过理论分析证明了单层Transformer在适当初始化下梯度下降呈现线性收敛速率，残差连接可改善优化稳定性，并通过实验验证了理论结果


<details>
  <summary>Details</summary>
Motivation: 现有研究主要孤立分析Transformer组件（自注意力/前馈网络），缺乏对残差连接下组件间相互作用的收敛性理论分析

Method: 构建包含自注意力、前馈网络和残差连接的单层Transformer架构，通过奇异值分析注意力层输出矩阵，推导梯度下降的收敛速率

Result: 残差连接有效缓解softmax引起的低秩结构导致的病态矩阵问题，实验验证了理论预测的线性收敛特性

Conclusion: 残差连接通过改善输出矩阵条件数，在保证Transformer模型优化稳定性方面发挥关键作用，为理解深层架构训练动态奠定理论基础

Abstract: Transformer models have emerged as fundamental tools across various
scientific and engineering disciplines, owing to their outstanding performance
in diverse applications. Despite this empirical success, the theoretical
foundations of Transformers remain relatively underdeveloped, particularly in
understanding their training dynamics. Existing research predominantly examines
isolated components--such as self-attention mechanisms and feedforward
networks--without thoroughly investigating the interdependencies between these
components, especially when residual connections are present. In this paper, we
aim to bridge this gap by analyzing the convergence behavior of a structurally
complete yet single-layer Transformer, comprising self-attention, a feedforward
network, and residual connections. We demonstrate that, under appropriate
initialization, gradient descent exhibits a linear convergence rate, where the
convergence speed is determined by the minimum and maximum singular values of
the output matrix from the attention layer. Moreover, our analysis reveals that
residual connections serve to ameliorate the ill-conditioning of this output
matrix, an issue stemming from the low-rank structure imposed by the softmax
operation, thereby promoting enhanced optimization stability. We also extend
our theoretical findings to a multi-layer Transformer architecture, confirming
the linear convergence rate of gradient descent under suitable initialization.
Empirical results corroborate our theoretical insights, illustrating the
beneficial role of residual connections in promoting convergence stability.

</details>


### [269] [Learning long range dependencies through time reversal symmetry breaking](https://arxiv.org/abs/2506.05259)
*Guillaume Pourcel,Maxence Ernoult*

Main category: cs.LG

TL;DR: 提出RHEL算法，通过哈密顿系统物理轨迹实现高效梯度计算，在保持BPTT性能的同时支持超长序列建模


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的计算范式缺乏遵循守恒定律的高效学习算法，需开发既能保持哈密顿系统非耗散特性，又能实现梯度高效计算的训练方法

Method: 1. 连续时间RHEL通过物理系统轨迹有限差分计算梯度
2. 离散版本HRU模块等效BPTT
3. 分层结构HSSM支持模块化扩展

Result: 在50k长度序列任务中，分类/回归性能与BPTT完全持平，计算复杂度与模型规模无关，仅需三次前向传播

Conclusion: RHEL为物理自学习系统开辟新路径，通过守恒系统实现高效序列建模，支持硬件友好型低能耗实现

Abstract: Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,
as RNNs could natively be embodied into dynamical systems. This calls for
dedicated learning algorithms obeying to core physical principles, with
efficient techniques to simulate these systems and guide their design. We
propose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably
computes loss gradients as finite differences of physical trajectories of
non-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three
"forward passes" irrespective of model size, without explicit Jacobian
computation, nor incurring any variance in the gradient estimation. Motivated
by the physical realization of our algorithm, we first introduce RHEL in
continuous time and demonstrate its formal equivalence with the continuous
adjoint state method. To facilitate the simulation of Hamiltonian systems
trained by RHEL, we propose a discrete-time version of RHEL which is equivalent
to Backpropagation Through Time (BPTT) when applied to a class of recurrent
modules which we call Hamiltonian Recurrent Units (HRUs). This setting allows
us to demonstrate the scalability of RHEL by generalizing these results to
hierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to
train HSSMs with linear and nonlinear dynamics on a variety of time-series
tasks ranging from mid-range to long-range classification and regression with
sequence length reaching $\sim 50k$. We show that RHEL consistently matches the
performance of BPTT across all models and tasks. This work opens new doors for
the design of scalable, energy-efficient physical systems endowed with
self-learning capabilities for sequence modelling.

</details>


### [270] [How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control](https://arxiv.org/abs/2506.05276)
*Hao Yu,Chu Xin Cheng,Runlong Yu,Yuyang Ye,Shiwei Tong,Zhaofeng Liu,Defu Lian*

Main category: cs.LG

TL;DR: 提出CocktailEdit框架实现时间序列的灵活编辑，通过置信度加权锚点控制和分类器控制机制，在保持时序一致性的同时实现点级与统计特性的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法难以同时满足点级约束和段级统计特性控制，无法适应现实场景的编辑需求。

Method: 结合置信度加权的点级锚点控制与基于分类器的统计特性控制，在去噪推理阶段实现局部精确控制，适配各类条件扩散模型。

Result: 跨多个数据集和模型的实验验证了框架有效性，代码和演示已开源供验证。

Conclusion: 该框架填补了生成模型与现实需求间的鸿沟，为人机协同的时间序列编辑提供了灵活解决方案。

Abstract: Recent advances in time series generation have shown promise, yet controlling
properties in generated sequences remains challenging. Time Series Editing
(TSE) - making precise modifications while preserving temporal coherence -
consider both point-level constraints and segment-level controls that current
methods struggle to provide. We introduce the CocktailEdit framework to enable
simultaneous, flexible control across different types of constraints. This
framework combines two key mechanisms: a confidence-weighted anchor control for
point-wise constraints and a classifier-based control for managing statistical
properties such as sums and averages over segments. Our methods achieve precise
local control during the denoising inference stage while maintaining temporal
coherence and integrating seamlessly, with any conditionally trained
diffusion-based time series models. Extensive experiments across diverse
datasets and models demonstrate its effectiveness. Our work bridges the gap
between pure generative modeling and real-world time series editing needs,
offering a flexible solution for human-in-the-loop time series generation and
editing. The code and demo are provided for validation.

</details>


### [271] [Fast-DataShapley: Neural Modeling for Training Data Valuation](https://arxiv.org/abs/2506.05281)
*Haifeng Sun,Yu Xiong,Runze Wu,Xinyu Cai,Changjie Fan,Lan Zhang,Xiang-Yang Li*

Main category: cs.LG

TL;DR: 提出Fast-DataShapley方法，通过加权最小二乘法和三项优化技术，实现无需重复训练的高效数据贡献度评估。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值计算存在指数级计算复杂度，且现有近似方法需对每个测试样本重复训练，无法满足实际需求。需保护数据提供者权益并实现高效补偿机制。

Method: 1. 基于加权最小二乘法的单次训练解释器模型
2. 提出效用函数近似计算和训练数据分组计算的三项优化方法
3. 理论证明与时间复杂度分析

Result: 图像数据集实验显示：模型性能提升2.5倍以上，解释器训练速度加快两个数量级，实时推理速度显著优化

Conclusion: Fast-DataShapley有效解决了Shapley值计算效率瓶颈，为数据版权保护和公平补偿提供了实用解决方案

Abstract: The value and copyright of training data are crucial in the artificial
intelligence industry. Service platforms should protect data providers'
legitimate rights and fairly reward them for their contributions. Shapley
value, a potent tool for evaluating contributions, outperforms other methods in
theory, but its computational overhead escalates exponentially with the number
of data providers. Recent works based on Shapley values attempt to mitigate
computation complexity by approximation algorithms. However, they need to
retrain for each test sample, leading to intolerable costs. We propose
Fast-DataShapley, a one-pass training method that leverages the weighted least
squares characterization of the Shapley value to train a reusable explainer
model with real-time reasoning speed. Given new test samples, no retraining is
required to calculate the Shapley values of the training data. Additionally, we
propose three methods with theoretical guarantees to reduce training overhead
from two aspects: the approximate calculation of the utility function and the
group calculation of the training data. We analyze time complexity to show the
efficiency of our methods. The experimental evaluations on various image
datasets demonstrate superior performance and efficiency compared to baselines.
Specifically, the performance is improved to more than 2.5 times, and the
explainer's training speed can be increased by two orders of magnitude.

</details>


### [272] [Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing](https://arxiv.org/abs/2506.05292)
*Declan A. Norton,Yuanzhao Zhang,Michelle Girvan*

Main category: cs.LG

TL;DR: 研究证明储层计算无需显式结构先验即可实现跨状态空间域的泛化，通过多轨迹训练方案使模型在未观测吸引盆中捕捉系统行为。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习建模动态系统时缺乏结构先验会导致泛化能力受限，本文旨在探索储层计算是否具备跨域泛化能力。

Method: 提出多轨迹训练方案，通过整合不连续时间序列数据训练储层计算机，特别针对多稳态系统的不同吸引盆进行训练。

Result: 实验表明储层计算机仅通过单一吸引盆数据训练后，即可准确预测其他未观测吸引盆内的系统动力学行为。

Conclusion: 该发现突破了数据驱动建模对结构先验的依赖，为复杂多稳态系统的跨域预测提供了新范式。

Abstract: Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.

</details>


### [273] [A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search](https://arxiv.org/abs/2506.05294)
*Arnav Kumar Jain,Vibhakar Mohta,Subin Kim,Atiksh Bhardwaj,Juntao Ren,Yunhai Feng,Sanjiban Choudhury,Gokul Swamy*

Main category: cs.LG

TL;DR: 行为克隆（BC）在未知状态恢复能力有限，SAILOR方法通过结合世界模型和奖励模型实现自主规划，显著超越传统BC策略


<details>
  <summary>Details</summary>
Motivation: BC仅在专家访问状态提供监督，无法处理测试时未知状态的错误恢复。需开发能独立推理专家目标的方法

Method: 提出SAILOR框架：1）学习世界模型预测状态变化 2）构建奖励模型评估行动 3）设计高效算法整合组件，无需人工修正

Result: 在12个视觉操作任务中超越BC扩散策略，即使BC数据量增加5-10倍仍有性能差距，且具备抗奖励滥用能力

Conclusion: SAILOR有效解决了BC的恢复能力缺陷，证明模型驱动的规划方法在复杂任务中的优越性，代码已开源推动后续研究

Abstract: The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .

</details>


### [274] [Power Law Guided Dynamic Sifting for Efficient Attention](https://arxiv.org/abs/2506.05300)
*Nirav Koley,Prajwal Singhania,Abhinav Bhatele*

Main category: cs.LG

TL;DR: 提出SiftAttention方法，通过动态阈值过滤替代传统top-k操作，在保证模型质量的同时减少GPU内存带宽使用


<details>
  <summary>Details</summary>
Motivation: 现有近似注意力方法依赖GPU效率低下的top-k操作，且内存带宽限制影响注意力计算效率

Method: 基于注意力分数τ分位数呈现幂律分布的观察，设计动态阈值过滤机制，仅加载超过阈值的注意力分数及对应value向量

Result: 相比现有方法更好保持模型质量，同时显著减少value向量加载时的内存带宽消耗

Conclusion: SiftAttention通过创新的动态阈值机制，为近似注意力计算提供了更高效且有效的解决方案

Abstract: Efficient inference on GPUs using large language models remains challenging
due to memory bandwidth limitations, particularly during data transfers between
High Bandwidth Memory (HBM) and SRAM in attention computations. Approximate
attention methods address this issue by reducing computational and memory
overhead but often rely on expensive top-$k$ operations, which perform poorly
on GPUs. We propose SiftAttention, a novel approximate attention method that
replaces the top-$k$ step with a computationally efficient element-wise
filtering operation based on a threshold value. Our intuition for doing this is
based on our empirical observation that the $\tau$-th quantile of attention
scores follows a predictable power-law over sequential generation steps.
Exploiting this insight, our approach dynamically estimates a threshold value
per prompt at each generation step. Only attention scores above this threshold
and their corresponding value vectors are loaded/used to compute the attention
output, reducing data movement between HBM and SRAM. Our evaluation
demonstrates that SiftAttention preserves model quality better than existing
approximate attention methods while reducing memory bandwidth usage when
loading value vectors.

</details>


### [275] [Learning normalized image densities via dual score matching](https://arxiv.org/abs/2506.05310)
*Florentin Guth,Zahra Kadkhodaie,Eero P Simoncelli*

Main category: cs.LG

TL;DR: 提出基于扩散模型的双评分匹配框架，有效学习归一化能量模型，在ImageNet64上取得优异性能并挑战传统假设


<details>
  <summary>Details</summary>
Motivation: 传统概率模型在高维数据中受维数灾难限制，需开发新方法解决评分匹配与能量模型归一化难题

Method: 改进评分网络架构实现能量计算，通过双重评分匹配目标（主目标优化密度评分，次级目标保持跨噪声层能量一致性）

Result: 1. 在ImageNet64上获得SOTA级别交叉熵
2. 能量模型展现强泛化性（对数概率与训练集图像无关）
3. 图像概率/局部维度随内容显著变化，突破传统测度集中假设

Conclusion: 概率分布复杂度远超传统低维流形假设，未来模型需更好适应数据特性，双重评分机制为能量建模提供新方向

Abstract: Learning probability models from data is at the heart of many machine
learning endeavors, but is notoriously difficult due to the curse of
dimensionality. We introduce a new framework for learning \emph{normalized}
energy (log probability) models that is inspired from diffusion generative
models, which rely on networks optimized to estimate the score. We modify a
score network architecture to compute an energy while preserving its inductive
biases. The gradient of this energy network with respect to its input image is
the score of the learned density, which can be optimized using a denoising
objective. Importantly, the gradient with respect to the noise level provides
an additional score that can be optimized with a novel secondary objective,
ensuring consistent and normalized energies across noise levels. We train an
energy network with this \emph{dual} score matching objective on the ImageNet64
dataset, and obtain a cross-entropy (negative log likelihood) value comparable
to the state of the art. We further validate our approach by showing that our
energy model \emph{strongly generalizes}: estimated log probabilities are
nearly independent of the specific images in the training set. Finally, we
demonstrate that both image probability and dimensionality of local
neighborhoods vary significantly with image content, in contrast with
traditional assumptions such as concentration of measure or support on a
low-dimensional manifold.

</details>


### [276] [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)
*Yifan Sun,Jingyan Shen,Yibin Wang,Tianyu Chen,Zhendong Wang,Mingyuan Zhou,Huan Zhang*

Main category: cs.LG

TL;DR: 提出两种提高LLM强化学习微调数据效率的技术：基于难度自适应的在线数据选择和回放重用机制


<details>
  <summary>Details</summary>
Motivation: 现有强化学习微调方法计算资源消耗大且数据效率低，需要解决在线数据选择效率和计算成本问题

Method: 1. 基于注意力机制的自适应难度评估框架，通过少量参考问题预测难度分布
2. 重用近期轨迹的回放机制降低计算量

Result: 在6种LLM-数据集组合中减少25%-65%的微调时间（相比GRPO算法达到相同性能）

Conclusion: 该技术显著提升RL微调效率，为资源受限场景提供实用解决方案

Abstract: Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.

</details>


### [277] [LSM-2: Learning from Incomplete Wearable Sensor Data](https://arxiv.org/abs/2506.05321)
*Maxwell A. Xu,Girish Narayanswamy,Kumar Ayush,Dimitris Spathis,Shun Liao,Shyam A. Tailor,Ahmed Metwally,A. Ali Heydari,Yuwei Zhang,Jake Garrison,Samy Abdel-Ghaffar,Xuhai Xu,Ken Gu,Jacob Sunshine,Ming-Zher Poh,Yun Liu,Tim Althoff,Shrikanth Narayanan,Pushmeet Kohli,Mark Malhotra,Shwetak Patel,Yuzhe Yang,James M. Rehg,Xin Liu,Daniel McDuff*

Main category: cs.LG

TL;DR: LSM-2通过自适应继承掩码技术直接从不完整可穿戴数据中学习，无需数据填补，在40M小时多模态数据预训练后实现分类/回归/生成任务最优表现，并保持临床相关缺失场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习依赖完整数据输入，而现实可穿戴传感器数据存在显著缺失，需开发能直接从不完整数据中学习鲁棒表征的新方法。

Method: AIM方法创新性地使用可学习掩码令牌，同时建模传感器数据中自然存在的继承缺失和人工引入的缺失模式，实现推理阶段对碎片化数据的自适应处理。

Result: 在40M小时多模态传感器数据预训练后，LSM-2在分类/回归/生成任务中全面领先，展示优异扩展性，并在模拟高血压诊断等临床缺失场景下保持94%性能稳定性。

Conclusion: AIM通过双重掩码机制有效捕获真实世界可穿戴数据的缺失模式，为医疗监测等实际应用提供了更可靠的SSL解决方案。

Abstract: Foundation models, a cornerstone of recent advancements in machine learning,
have predominantly thrived on complete and well-structured data. Wearable
sensor data frequently suffers from significant missingness, posing a
substantial challenge for self-supervised learning (SSL) models that typically
assume complete data inputs. This paper introduces the second generation of
Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel
SSL approach that learns robust representations directly from incomplete data
without requiring explicit imputation. AIM's core novelty lies in its use of
learnable mask tokens to model both existing ("inherited") and artificially
introduced missingness, enabling it to robustly handle fragmented real-world
data during inference. Pre-trained on an extensive dataset of 40M hours of
day-long multimodal sensor data, our LSM-2 with AIM achieves the best
performance across a diverse range of tasks, including classification,
regression and generative modeling. Furthermore, LSM-2 with AIM exhibits
superior scaling performance, and critically, maintains high performance even
under targeted missingness scenarios, reflecting clinically coherent patterns,
such as the diagnostic value of nighttime biosignals for hypertension
prediction. This makes AIM a more reliable choice for real-world wearable data
applications.

</details>


### [278] [Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment](https://arxiv.org/abs/2506.05325)
*Yingshuai Ji,Haomin Zhuang,Matthew Toole,James McKenzie,Xiaolong Liu,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出首个基于AI的QPI核提取框架，通过两步学习策略显著提升提取精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统QPI成像在多散射体场景下存在病态逆问题，需可靠核提取方法解决复杂散射条件下的分析难题。

Method: 使用变分自编码器学习散射核的紧凑潜在空间，再通过专用编码器实现观测数据到核空间的映射，实现解耦表示学习与推理。

Result: 在包含100个独特核的数据集上验证，相比单步基线方法提取精度显著提升，对未见核的泛化能力增强。

Conclusion: 两步学习策略有效解决了复杂散射条件下QPI核提取的逆问题，为量子材料电子结构研究提供了可靠工具。

Abstract: Quasiparticle interference (QPI) imaging is a powerful tool for probing
electronic structures in quantum materials, but extracting the single-scatterer
QPI pattern (i.e., the kernel) from a multi-scatterer image remains a
fundamentally ill-posed inverse problem. In this work, we propose the first
AI-based framework for QPI kernel extraction. We introduce a two-step learning
strategy that decouples kernel representation learning from
observation-to-kernel inference. In the first step, we train a variational
autoencoder to learn a compact latent space of scattering kernels. In the
second step, we align the latent representation of QPI observations with those
of the pre-learned kernels using a dedicated encoder. This design enables the
model to infer kernels robustly even under complex, entangled scattering
conditions. We construct a diverse and physically realistic QPI dataset
comprising 100 unique kernels and evaluate our method against a direct one-step
baseline. Experimental results demonstrate that our approach achieves
significantly higher extraction accuracy, and improved generalization to unseen
kernels.

</details>


### [279] [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
*Ranajoy Sadhukhan,Zhuoming Chen,Haizhong Zheng,Yang Zhou,Emma Strubell,Beidi Chen*

Main category: cs.LG

TL;DR: 重新审视测试扩展定律，发现小模型效率被高估，提出融合计算与内存成本的Kinetics定律，并通过稀疏注意力实现更优资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有基于计算最优性的方法忽视了推理策略（如Best-of-N、长CoT）引入的内存访问瓶颈，需建立更全面的效率评估体系。

Method: 提出Kinetics Scaling Law量化计算与内存成本，设计稀疏注意力机制降低单token成本，支持长文本生成和并行推理。

Result: 稀疏模型在AIME问题求解任务中，低资源场景提升60+点，高资源场景提升5+点，对前沿MoE模型同样有效。

Conclusion: 稀疏注意力是测试扩展的关键，其通过生成扩展持续提升精度，突破传统参数扩展的饱和限制。

Abstract: We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.

</details>


### [280] [Exploring Diffusion Transformer Designs via Grafting](https://arxiv.org/abs/2506.05340)
*Keshigeyan Chandrasegaran,Michael Poli,Daniel Y. Fu,Dongjun Kim,Lea M. Hadzic,Manling Li,Agrim Gupta,Stefano Massaroli,Azalia Mirhoseini,Juan Carlos Niebles,Stefano Ermon,Li Fei-Fei*

Main category: cs.LG

TL;DR: 提出基于预训练扩散Transformer的嫁接技术，通过修改现有模型架构实现高效架构探索（FID: 2.38-2.64 vs. 2.27，仅需<2%计算量）


<details>
  <summary>Details</summary>
Motivation: 传统架构设计需完整预训练验证效果，计算成本过高。受软件工程复用思想启发，探索基于预训练模型的架构创新可行性

Method: 开发嫁接技术：1）替换注意力机制为门控卷积/局部注意力/线性注意力 2）改造MLP层 3）案例研究并行化结构重组

Result: 混合架构保持性能（PixArt-Sigma加速1.43倍，质量下降<2%）；深度减半架构FID 2.77优于同类模型

Conclusion: 嫁接技术为扩散模型架构创新提供高效路径，支持从算子替换到结构重构的多层次改进

Abstract: Designing model architectures requires decisions such as selecting operators
(e.g., attention, convolution) and configurations (e.g., depth, width).
However, evaluating the impact of these decisions on model quality requires
costly pretraining, limiting architectural investigation. Inspired by how new
software is built on existing code, we ask: can new architecture designs be
studied using pretrained models? To this end, we present grafting, a simple
approach for editing pretrained diffusion transformers (DiTs) to materialize
new architectures under small compute budgets. Informed by our analysis of
activation behavior and attention locality, we construct a testbed based on the
DiT-XL/2 design to study the impact of grafting on model quality. Using this
testbed, we develop a family of hybrid designs via grafting: replacing softmax
attention with gated convolution, local attention, and linear attention, and
replacing MLPs with variable expansion ratio and convolutional variants.
Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for
DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model
(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval
score. Finally, we present a case study that restructures DiT-XL/2 by
converting every pair of sequential transformer blocks into parallel blocks via
grafting. This reduces model depth by 2x and yields better quality (FID: 2.77)
than other models of comparable depth. Together, we show that new diffusion
model designs can be explored by grafting pretrained DiTs, with edits ranging
from operator replacement to architecture restructuring. Code and grafted
models: https://grafting.stanford.edu

</details>


### [281] [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
*Adrian Łańcucki,Konrad Staniszewski,Piotr Nawrot,Edoardo M. Ponti*

Main category: cs.LG

TL;DR: 提出动态内存稀疏化（DMS）方法，通过延迟KV缓存令牌驱逐实现8倍压缩，在保持精度的同时提升大语言模型推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型的生成成本受KV缓存容量限制，传统KV缓存压缩方法在高压缩率下会显著降低模型精度。需要找到一种既能高效压缩缓存又不损失精度的方案。

Method: 动态内存稀疏化（DMS）创新性地延迟令牌驱逐，通过隐式合并表示保留关键信息，仅需1K训练步即可实现8倍KV缓存压缩。

Result: 在Qwen-R1 32B模型上，AIME 24平均提升9.1分，GPQA提升7.6分，LiveCodeBench提升9.6分；在相同计算资源下实现精度提升。

Conclusion: DMS验证了推理时超扩展的有效性，通过智能KV缓存管理突破传统精度-效率权衡，为LLM部署提供新的优化维度。

Abstract: Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.

</details>


### [282] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 本文对基于概念的模型的风险和局限性进行全面综述，聚焦常见挑战、架构选择、可靠性提升进展及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基于概念的模型在关键应用成功，但近期研究发现其存在显著局限性，影响可靠性和泛化能力，且人类干预效果存疑，需综合评估风险与局限。

Method: 对基于概念的模型进行全面调查，聚焦监督和无监督范式中常见挑战及缓解架构选择。

Result: 汇总了常见挑战、缓解架构选择，研究了提升可靠性的进展。

Conclusion: 探讨了该领域的开放问题和未来有前景的研究方向。

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [283] [Improving Out-of-Distribution Detection with Markov Logic Networks](https://arxiv.org/abs/2506.04241)
*Konstantin Kirchheim,Frank Ortmeier*

Main category: cs.LG

TL;DR: 本文提出用马尔可夫逻辑网络（MLNs）增强现有分布外（OOD）检测器，实验证明其可提升性能并保持计算效率，还介绍了学习逻辑约束的算法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测器主要依赖统计模型，本文旨在增强其性能并提高可解释性。

Method: 利用马尔可夫逻辑网络（MLNs）将一阶逻辑与概率推理相结合，为输入分配概率；引入从数据集学习OOD检测逻辑约束的算法。

Result: 在多个数据集上的实验表明，MLNs能显著提升多种现有OOD检测器的性能，且保持计算效率，学习逻辑约束的算法有效。

Conclusion: MLNs可有效增强现有OOD检测器的性能，所提出的学习逻辑约束算法也有效果。

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models operating in open-world scenarios. Current OOD
detectors mainly rely on statistical models to identify unusual patterns in the
latent representations of a deep neural network. This work proposes to augment
existing OOD detectors with probabilistic reasoning, utilizing Markov logic
networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to
assign probabilities to inputs based on weighted logical constraints defined
over human-understandable concepts, which offers improved explainability.
Through extensive experiments on multiple datasets, we demonstrate that MLNs
can significantly enhance the performance of a wide range of existing OOD
detectors while maintaining computational efficiency. Furthermore, we introduce
a simple algorithm for learning logical constraints for OOD detection from a
dataset and showcase its effectiveness.

</details>


### [284] [Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction](https://arxiv.org/abs/2506.04243)
*Warayut Dokduea,Weerachart Tangchirapat,Sompote Youwai*

Main category: cs.LG

TL;DR: 提出用于预测混凝土徐变的三重注意力Transformer架构，表现出色并证明其在材料科学中应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法未对变形发展的顺序性建模的局限，提升混凝土徐变预测能力。

Method: 将混凝土徐变预测转化为自回归序列建模任务，采用三重注意力框架。

Result: 在实验数据集上表现优异，优于传统和现有机器学习方法，消融实验和SHAP分析有相应结论。

Conclusion: Transformer架构可用于材料科学问题，数据驱动方法有望革新结构行为预测和工程设计实践。

Abstract: This paper presents a novel Triple Attention Transformer Architecture for
predicting time-dependent concrete creep, addressing fundamental limitations in
current approaches that treat time as merely an input parameter rather than
modeling the sequential nature of deformation development. By transforming
concrete creep prediction into an autoregressive sequence modeling task similar
to language processing, our architecture leverages the transformer's
self-attention mechanisms to capture long-range dependencies in historical
creep patterns. The model implements a triple-stream attention framework
incorporating temporal attention for sequential progression, feature attention
for material property interactions, and batch attention for inter-sample
relationships. Evaluated on experimental datasets with standardized daily
measurements spanning 160 days, the architecture achieves exceptional
performance with mean absolute percentage error of 1.63% and R2 values of 0.999
across all datasets, substantially outperforming traditional empirical models
and existing machine learning approaches. Ablation studies confirm the critical
role of attention mechanisms, with attention pooling contributing most
significantly to model performance. SHAP analysis reveals Young's modulus as
the primary predictive feature, followed by density and compressive strength,
providing interpretability essential for engineering applications. A deployed
web-based interface facilitates practical implementation, enabling real-time
predictions using standard laboratory parameters. This work establishes the
viability of applying transformer architectures to materials science problems,
demonstrating the potential for data-driven approaches to revolutionize
structural behavior prediction and engineering design practices.

</details>


### [285] [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
*Shaona Ghosh,Amrita Bhattacharjee,Yftah Ziser,Christopher Parisien*

Main category: cs.LG

TL;DR: 提出SafeSteer方法指导大语言模型输出，在多模型、数据集和风险类别中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型适应安全策略成本高且不实际，机械可解释性在精确、可定制安全调整方面潜力未充分挖掘。

Method: 使用特定类别转向向量精确控制，采用无梯度无监督方法增强安全转向，且无需对比成对安全数据。

Result: 在多种大语言模型、数据集和风险类别中展示了该方法能提供精确控制、避免全面拒绝、引导模型生成安全且切题内容。

Conclusion: 该方法简单有效，符合简单技术在激活转向中常优于复杂技术的研究。

Abstract: Fine-tuning large language models (LLMs) to adapt to evolving safety policies
is costly and impractical. Mechanistic interpretability enables inference-time
control through latent activation steering, yet its potential for precise,
customizable safety adjustments remains largely untapped. This paper
investigates an approach called SafeSteer for guiding the outputs of LLMs by:
(i) leveraging category-specific steering vectors for more precise control,
(ii) employing a simple, gradient-free unsupervised method to enhance safety
steering while preserving text quality, topic relevance, and without explicit
refusal, and (iii) accomplishing this without a hard requirement of contrastive
pairwise safe data. We also highlight that our method, being simple and
effective, aligns with recent studies suggesting that simple techniques often
outperform more complex ones in activation steering. We showcase the
effectiveness of our approach across various LLMs, datasets, and risk
categories, demonstrating its ability to provide precise control, prevent
blanket refusals, and guide models toward generating safe content while
maintaining topic relevance.

</details>


### [286] [Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices](https://arxiv.org/abs/2506.04553)
*Andersen Chang,Tiffany M. Tang,Tarek M. Zikry,Genevera I. Allen*

Main category: cs.LG

TL;DR: 本文提出无监督学习在科学领域工作流，以天文学案例说明其对科学发现的作用。


<details>
  <summary>Details</summary>
Motivation: 无监督机器学习在多领域广泛应用，但缺乏标准化工作流以实现可靠和可重复的科学发现。

Method: 提出结构化工作流，包括提出可验证科学问题、数据准备与探索、运用多种建模技术、严格验证、有效沟通与记录结果，并通过天文学案例说明。

Result: 以天文学案例展示了所提工作流中验证的重要性。

Conclusion: 精心设计的无监督学习工作流有助于推动科学发现。

Abstract: Unsupervised machine learning is widely used to mine large, unlabeled
datasets to make data-driven discoveries in critical domains such as climate
science, biomedicine, astronomy, chemistry, and more. However, despite its
widespread utilization, there is a lack of standardization in unsupervised
learning workflows for making reliable and reproducible scientific discoveries.
In this paper, we present a structured workflow for using unsupervised learning
techniques in science. We highlight and discuss best practices starting with
formulating validatable scientific questions, conducting robust data
preparation and exploration, using a range of modeling techniques, performing
rigorous validation by evaluating the stability and generalizability of
unsupervised learning conclusions, and promoting effective communication and
documentation of results to ensure reproducible scientific discoveries. To
illustrate our proposed workflow, we present a case study from astronomy,
seeking to refine globular clusters of Milky Way stars based upon their
chemical composition. Our case study highlights the importance of validation
and illustrates how the benefits of a carefully-designed workflow for
unsupervised learning can advance scientific discovery.

</details>


### [287] [Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support](https://arxiv.org/abs/2506.04254)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 文章指出传统森林火灾预测问题存在简化情况，提出针对部门情况定制火灾风险评估的新方法，并给出法国国家尺度的AI基准及未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 气候变化使森林火灾预测成为AI紧迫挑战，传统二元分类法简化问题，需考虑地方条件进行火灾风险建模。

Method: 提出针对部门背景定制火灾风险评估的新方法，使用最先进AI模型在未充分探索的数据集上建立法国国家尺度的AI基准。

Result: 建立了法国国家尺度的AI基准。

Conclusion: 提出的方法能提供更具可操作性和区域针对性的预测，还总结了重要的未来工作。

Abstract: Forest fire prediction involves estimating the likelihood of fire ignition or
related risk levels in a specific area over a defined time period. With climate
change intensifying fire behavior and frequency, accurate prediction has become
one of the most pressing challenges in Artificial Intelligence (AI).
Traditionally, fire ignition is approached as a binary classification task in
the literature. However, this formulation oversimplifies the problem,
especially from the perspective of end-users such as firefighters. In general,
as is the case in France, firefighting units are organized by department, each
with its terrain, climate conditions, and historical experience with fire
events. Consequently, fire risk should be modeled in a way that is sensitive to
local conditions and does not assume uniform risk across all regions. This
paper proposes a new approach that tailors fire risk assessment to departmental
contexts, offering more actionable and region-specific predictions for
operational use. With this, we present the first national-scale AI benchmark
for metropolitan France using state-of-the-art AI models on a relatively
unexplored dataset. Finally, we offer a summary of important future works that
should be taken into account. Supplementary materials are available on GitHub.

</details>


### [288] [Semi-Implicit Variational Inference via Kernelized Path Gradient Descent](https://arxiv.org/abs/2506.05088)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 提出核化KL散度估计器和重要性采样校正改进半隐式变分推理（SIVI），在性能和训练效率上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有SIVI方法在高维训练时因KL散度存在高方差和偏差，且训练成本较高。

Method: 提出核化KL散度估计器进行非参数平滑以稳定训练，引入重要性采样校正减少偏差，建立与Stein变分梯度下降摊销版本的理论联系。

Result: 在性能和训练效率上优于或媲美现有SIVI方法。

Conclusion: 所提方法能实现更低的梯度方差，函数空间偏差小，优化更稳定高效。

Abstract: Semi-implicit variational inference (SIVI) is a powerful framework for
approximating complex posterior distributions, but training with the
Kullback-Leibler (KL) divergence can be challenging due to high variance and
bias in high-dimensional settings. While current state-of-the-art semi-implicit
variational inference methods, particularly Kernel Semi-Implicit Variational
Inference (KSIVI), have been shown to work in high dimensions, training remains
moderately expensive. In this work, we propose a kernelized KL divergence
estimator that stabilizes training through nonparametric smoothing. To further
reduce the bias, we introduce an importance sampling correction. We provide a
theoretical connection to the amortized version of the Stein variational
gradient descent, which estimates the score gradient via Stein's identity,
showing that both methods minimize the same objective, but our semi-implicit
approach achieves lower gradient variance. In addition, our method's bias in
function space is benign, leading to more stable and efficient optimization.
Empirical results demonstrate that our method outperforms or matches
state-of-the-art SIVI methods in both performance and training efficiency.

</details>


### [289] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: 提出扰动梯度训练（PGT）范式，可解决物理储层计算无法反向传播的问题，在模拟网络和实验硬件上验证可行性，性能与标准反向传播相当。


<details>
  <summary>Details</summary>
Motivation: 克服物理储层计算因黑盒特性无法进行反向传播的关键局限。

Method: 借鉴物理中的微扰理论，在网络参数空间使用随机扰动，仅通过前向传播近似梯度更新。

Result: 在模拟神经网络架构和实验硬件上证明可行，性能与标准反向传播方法相当。

Conclusion: PGT是将物理储层集成到更深神经网络架构并实现AI训练显著节能的有前景的一步。

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [290] [MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression](https://arxiv.org/abs/2506.04268)
*Jingyang Li,Guoqiang Li*

Main category: cs.LG

TL;DR: 提出MUC - G4框架用于压缩深度神经网络增量验证，实验证明其有效且能加速验证。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展使神经网络在边缘设备部署有挑战，现有增量验证方法有局限。

Method: 将原始和压缩网络编码为SMT公式，分类变化，用原始网络的最小不可满足核心指导压缩网络验证。

Result: 能有效处理量化和剪枝，有高证明复用率，验证时间相比传统方法显著加快。

Conclusion: MUC - G4为确保压缩神经网络在实际应用中的安全和可靠性提供了有前景的解决方案。

Abstract: The rapid development of deep learning has led to challenges in deploying
neural networks on edge devices, mainly due to their high memory and runtime
complexity. Network compression techniques, such as quantization and pruning,
aim to reduce this complexity while maintaining accuracy. However, existing
incremental verification methods often focus only on quantization and struggle
with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided
Incremental Verification), a novel framework for incremental verification of
compressed deep neural networks. It encodes both the original and compressed
networks into SMT formulas, classifies changes, and use \emph{Minimal Unsat
Cores (MUCs)} from the original network to guide efficient verification for the
compressed network. Experimental results show its effectiveness in handling
quantization and pruning, with high proof reuse rates and significant speedup
in verification time compared to traditional methods. MUC-G4 hence offers a
promising solution for ensuring the safety and reliability of compressed neural
networks in practical applications.

</details>


### [291] [Understanding the Impact of Sampling Quality in Direct Preference Optimization](https://arxiv.org/abs/2506.04272)
*Kyung Rok Kim,Yumo Bai,Chonghuan Wang,Guanting Chen*

Main category: cs.LG

TL;DR: 研究采样分布在DPO中的作用及其对训练动态的影响，理论发现获实验支持。


<details>
  <summary>Details</summary>
Motivation: 了解采样分布对DPO训练动态的影响。

Method: 分析响应分布对梯度下降中策略更新的影响，设计简化对齐模型并得出定量结果。

Result: DPO的解空间和收敛行为取决于生成分布的支持和质量，高质量响应能改善优化格局。

Conclusion: 理论发现支持在线DPO框架在实践中的应用。

Abstract: We study the role of the sampling distribution in Direct Preference
Optimization (DPO) and aim to understand its impact on DPO's training dynamics.
Our analyses show that both the solution space and the convergence behavior of
DPO depend on the support and quality of the generating distribution. We first
analyze how distribution of responses influences policy updates during gradient
descent, drawing connections to common phenomena found in practice. We then
design a simplified yet well-structured alignment model as a proxy, and develop
quantitative results showing how more frequent high-quality responses amplify
the gradient signal and improve the optimization landscape, leading to more
effective policy learning. Our theoretical findings are supported by empirical
experiments and provide a principled justification for the online DPO framework
in practice.

</details>


### [292] [SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida](https://arxiv.org/abs/2506.04281)
*Xu Zheng,Chaohao Lin,Sipeng Chen,Zhuomin Chen,Jimeng Shi,Wei Cheng,Jayantha Obeysekera,Jason Liu,Dongsheng Luo*

Main category: cs.LG

TL;DR: 本文介绍了SF2Bench数据集用于复合洪水分析，评估六种建模方法性能并通过实验验证关键特征影响，分析了时空因素。


<details>
  <summary>Details</summary>
Motivation: 传统物理方法分析复合洪水效率低，机器学习因数据稀缺受限，因此需综合数据集用于洪水分析和预测。

Method: 引入SF2Bench数据集，评估多层感知器、卷积神经网络等六类方法性能，通过实验验证不同关键特征对洪水预测的影响。

Result: 不同方法在捕捉复合洪水复杂时空依赖方面表现不同。

Conclusion: SF2Bench数据集有助于详细分析驱动因素对复合洪水的贡献，不同建模方法能力多样，为改进洪水预测方法提供依据。

Abstract: Forecasting compound floods presents a significant challenge due to the
intricate interplay of meteorological, hydrological, and oceanographic factors.
Analyzing compound floods has become more critical as the global climate
increases flood risks. Traditional physics-based methods, such as the
Hydrologic Engineering Center's River Analysis System, are often
time-inefficient. Machine learning has recently demonstrated promise in both
modeling accuracy and computational efficiency. However, the scarcity of
comprehensive datasets currently hinders systematic analysis. Existing
water-related datasets are often limited by a sparse network of monitoring
stations and incomplete coverage of relevant factors. To address this
challenge, we introduce SF2Bench, a comprehensive time series collection on
compound floods in South Florida, which integrates four key factors: tide,
rainfall, groundwater, and human management activities (gate and pump
controlling). This integration allows for a more detailed analysis of the
individual contributions of these drivers to compound flooding and informs the
development of improved flood forecasting approaches. To comprehensively
evaluate the potential of various modeling paradigms, we assess the performance
of six categories of methods, encompassing Multilayer Perceptrons,
Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural
Networks, Transformers, and Large Language Models. We verified the impact of
different key features on flood forecasting through experiments. Our analysis
examines temporal and spatial aspects, providing insights into the influence of
historical data and spatial dependencies. The varying performance across these
approaches underscores the diverse capabilities of each in capturing complex
temporal and spatial dependencies inherent in compound floods.

</details>


### [293] [Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General Games](https://arxiv.org/abs/2506.05005)
*Ashkan Soleymani,Georgios Piliouras,Gabriele Farina*

Main category: cs.LG

TL;DR: 提出Cautious Optimism框架用于一般游戏的正则化学习加速，在不同场景有良好遗憾保证且分析不依赖单调步长。


<details>
  <summary>Details</summary>
Motivation: 证明通过自适应节奏实现无悔学习加速并非孤立现象，为一般游戏的正则化学习提供更快方法。

Method: 引入Cautious Optimism框架，以Follow - the - Regularized - Leader (FTRL) 为输入，通过控制FTRL节奏输出加速无悔学习算法。

Result: Cautious Optimistic FTRL在多样化自博弈中实现近最优的$O_T(\log T)$遗憾，在对抗场景保留最优的$O(\sqrt{T})$遗憾。

Conclusion: 提供了一种不依赖单调步长的新途径，用于一般游戏的快速学习。

Abstract: Recent work [Soleymani et al., 2025] introduced a variant of Optimistic
Multiplicative Weights Updates (OMWU) that adaptively controls the learning
pace in a dynamic, non-monotone manner, achieving new state-of-the-art regret
minimization guarantees in general games. In this work, we demonstrate that
no-regret learning acceleration through adaptive pacing of the learners is not
an isolated phenomenon. We introduce \emph{Cautious Optimism}, a framework for
substantially faster regularized learning in general games. Cautious Optimism
takes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs
an accelerated no-regret learning algorithm by pacing the underlying FTRL with
minimal computational overhead. Importantly, we retain uncoupledness (learners
do not need to know other players' utilities). Cautious Optimistic FTRL
achieves near-optimal $O_T(\log T)$ regret in diverse self-play
(mixing-and-matching regularizers) while preserving the optimal $O(\sqrt{T})$
regret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et
al. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic
step-sizes, showcasing a novel route for fast learning in general games.

</details>


### [294] [Inference economics of language models](https://arxiv.org/abs/2506.04645)
*Ege Erdil*

Main category: cs.LG

TL;DR: 本文开发理论模型，权衡大语言模型大规模推理时每token成本与串行token生成速度的经济利弊，计算流行语言模型的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型大规模推理时每token成本与串行token生成速度的经济权衡问题。

Method: 开发考虑算术、内存带宽、网络带宽和延迟约束的理论模型，对不同并行设置和批量大小进行优化。

Result: 计算出流行语言模型串行速度与每token成本的帕累托前沿。

Conclusion: 所开发的理论模型可用于优化大语言模型在给定每token成本下的串行推理速度。

Abstract: We develop a theoretical model that addresses the economic trade-off between
cost per token versus serial token generation speed when deploying LLMs for
inference at scale. Our model takes into account arithmetic, memory bandwidth,
network bandwidth and latency constraints; and optimizes over different
parallelism setups and batch sizes to find the ones that optimize serial
inference speed at a given cost per token. We use the model to compute Pareto
frontiers of serial speed versus cost per token for popular language models.

</details>


### [295] [DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience](https://arxiv.org/abs/2506.04282)
*Runxiang Wang,Boxiao Wang,Kai Li,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出DrSR框架用于符号回归，结合数据驱动洞察与反思学习，实验表明其性能优于传统和现有基于大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的符号回归方法如LLM - SR过度依赖内部先验，缺乏明确的数据理解和系统反思，需要改进。

Method: 提出DrSR框架，引导大语言模型分析数据结构关系生成结构化描述，同时监控方程性能并建立反馈循环。

Result: 在物理、化学、生物和材料科学等跨学科数据集上实验，DrSR大幅提高有效方程率，在准确性、泛化性和搜索效率方面均优于经典和近期基于大语言模型的方法。

Conclusion: DrSR在科学方程发现方面具有潜力。

Abstract: Symbolic regression is a fundamental tool for discovering interpretable
mathematical expressions from data, with broad applications across scientific
and engineering domains. Recently, large language models (LLMs) have
demonstrated strong performance in this task, leveraging embedded scientific
priors and reasoning capabilities to surpass traditional methods. However,
existing LLM-based approaches, such as LLM-SR, often over-rely on internal
priors, lacking explicit data understanding and systematic reflection during
equation generation. To address these limitations, we propose DrSR (Dual
Reasoning Symbolic Regression), a framework that combines data-driven insight
with reflective learning to enhance both robustness and discovery capability.
Specifically, DrSR guides LLMs to analyze structural relationships (e.g.,
monotonicity, nonlinearity, and correlation) within the data to generate
structured descriptions. Simultaneously, it monitors equation performance and
establishes a feedback loop to refine subsequent generations. By integrating
data understanding and generation reflection in a closed loop, DrSR enables
more efficient exploration of the symbolic expression space. Experiments across
interdisciplinary datasets in physics, chemistry, biology, and materials
science demonstrate that DrSR substantially improves the valid equation rate
and consistently outperforms both classical and recent LLM-based methods in
terms of accuracy, generalization, and search efficiency. These results
underscore its potential for scientific equation discovery.

</details>


### [296] [Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory](https://arxiv.org/abs/2506.04434)
*Hari K. Prakash,Charles H. Martin*

Main category: cs.LG

TL;DR: 研究神经网络的grokking现象，发现新的anti - grokking阶段，HTSR的α指标可区分三阶段，此发现有助于衡量过拟合和泛化崩溃。


<details>
  <summary>Details</summary>
Motivation: 深入研究神经网络的grokking现象，探索新的阶段及有效衡量指标。

Method: 用3层MLP在MNIST的1k样本子集上训练，使用开源WeightWatcher工具的HTSR，通过可视化和统计检验验证。

Result: 发现anti - grokking阶段，HTSR的α指标能区分三阶段，其他指标无法区分grokking和anti - grokking。

Conclusion: HTSR的α指标提供通用的层收敛目标，可作为泛化衡量指标。

Abstract: We study the well-known grokking phenomena in neural networks (NNs) using a
3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight
decay, and discover a novel third phase -- \emph{anti-grokking} -- that occurs
very late in training and resembles but is distinct from the familiar
\emph{pre-grokking} phases: test accuracy collapses while training accuracy
stays perfect. This late-stage collapse is distinct, from the known
pre-grokking and grokking phases, and is not detected by other proposed
grokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR
through the open-source WeightWatcher tool, we show that the HTSR layer quality
metric $\alpha$ alone delineates all three phases, whereas the best competing
metrics detect only the first two. The \emph{anti-grokking} is revealed by
training for $10^7$ and is invariably heralded by $\alpha < 2$ and the
appearance of \emph{Correlation Traps} -- outlier singular values in the
randomized layer weight matrices that make the layer weight matrix atypical and
signal overfitting of the training set. Such traps are verified by visual
inspection of the layer-wise empirical spectral densities, and by using
Kolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including
activation sparsity, absolute weight entropy, circuit complexity, and $l^2$
weight norms track pre-grokking and grokking but fail to distinguish grokking
from anti-grokking. This discovery provides a way to measure overfitting and
generalization collapse without direct access to the test data. These results
strengthen the claim that the \emph{HTSR} $\alpha$ provides universal
layer-convergence target at $\alpha \approx 2$ and underscore the value of
using the HTSR alpha $(\alpha)$ metric as a measure of generalization.

</details>


### [297] [Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning](https://arxiv.org/abs/2506.05252)
*Dravyansh Sharma,Alec Sun*

Main category: cs.LG

TL;DR: 本文从多个新维度刻画了有改进的可学习性，解决了Attias等人提出的开放问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习广泛用于社会决策，需考虑被分类主体对学习算法的反应，此前研究多关注减少欺骗行为，而本文关注主体真正改进时的可学习性。

Method: 引入最小一致概念类的不对称变体，针对欧几里得球改进集给出积极结果，在数据分布的温和生成假设下刻画非恰当学习等。

Result: 在有界噪声模型下实现更低泛化误差，在可实现和不可知在线学习中获得错误界。

Conclusion: 解决了Attias等人提出的恰当和非恰当学习的开放问题。

Abstract: Machine learning is now ubiquitous in societal decision-making, for example
in evaluating job candidates or loan applications, and it is increasingly
important to take into account how classified agents will react to the learning
algorithms. The majority of recent literature on strategic classification has
focused on reducing and countering deceptive behaviors by the classified
agents, but recent work of Attias et al. identifies surprising properties of
learnability when the agents genuinely improve in order to attain the desirable
classification, such as smaller generalization error than standard
PAC-learning. In this paper we characterize so-called learnability with
improvements across multiple new axes. We introduce an asymmetric variant of
minimally consistent concept classes and use it to provide an exact
characterization of proper learning with improvements in the realizable
setting. While prior work studies learnability only under general, arbitrary
agent improvement regions, we give positive results for more natural Euclidean
ball improvement sets. In particular, we characterize improper learning under a
mild generative assumption on the data distribution. We further show how to
learn in more challenging settings, achieving lower generalization error under
well-studied bounded noise models and obtaining mistake bounds in realizable
and agnostic online learning. We resolve open questions posed by Attias et al.
for both proper and improper learning.

</details>


### [298] [Training-free AI for Earth Observation Change Detection using Physics Aware Neuromorphic Networks](https://arxiv.org/abs/2506.04285)
*Stephen Smith,Cormac Purcell,Zdenka Kuncic*

Main category: cs.LG

TL;DR: 提出物理感知神经形态网络PANN处理卫星图像，检测自然灾害变化并生成变化图，优先下传相关数据，该模型免训练，资源需求少，效果良好。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星数据下传带宽有限，为让数据及时用于灾害应对，需在星上处理数据并优先下传相关数据。

Method: 提出基于忆阻器物理神经网络的PANN，权重动态更新，根据状态方程和电路守恒定律生成物理约束的动态输出特征，用距离度量检测变化。

Result: PANN与先进AI模型对比，在各自然灾害类别中取得相当或更好结果。

Conclusion: PANN是资源受限的星上处理难题的有前景解决方案。

Abstract: Earth observations from low Earth orbit satellites provide vital information
for decision makers to better manage time-sensitive events such as natural
disasters. For the data to be most effective for first responders, low latency
is required between data capture and its arrival to decision makers. A major
bottleneck is in the bandwidth-limited downlinking of the data from satellites
to ground stations. One approach to overcome this challenge is to process at
least some of the data on-board and prioritise pertinent data to be downlinked.
In this work we propose a Physics Aware Neuromorphic Network (PANN) to detect
changes caused by natural disasters from a sequence of multi-spectral satellite
images and produce a change map, enabling relevant data to be prioritised for
downlinking. The PANN used in this study is motivated by physical neural
networks comprised of nano-electronic circuit elements known as "memristors"
(nonlinear resistors with memory). The weights in the network are dynamic and
update in response to varying input signals according to memristor equations of
state and electrical circuit conservation laws. The PANN thus generates
physics-constrained dynamical output features which are used to detect changes
in a natural disaster detection task by applying a distance-based metric.
Importantly, this makes the whole model training-free, allowing it to be
implemented with minimal computing resources. The PANN was benchmarked against
a state-of-the-art AI model and achieved comparable or better results in each
natural disaster category. It thus presents a promising solution to the
challenge of resource-constrained on-board processing.

</details>


### [299] [Orthogonal Gradient Descent Improves Neural Calibration](https://arxiv.org/abs/2506.04487)
*C. Evans Hedges*

Main category: cs.LG

TL;DR: 训练时正交化梯度可在不牺牲准确率的情况下改善模型校准。


<details>
  <summary>Details</summary>
Motivation: 探索提升模型校准的方法，且不降低准确率。

Method: 训练时对梯度进行正交化（⊥Grad）。

Result: 在CIFAR - 10上，⊥Grad准确率与SGD相当，但校准指标更佳；在输入损坏和扩展训练时表现更优；理论上证明简化版⊥Grad收敛并刻画其驻点。

Conclusion: 训练时正交化梯度是一种有效提升模型校准的方法，开销小且与后处理校准技术兼容。

Abstract: We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement.

</details>


### [300] [Backbone Augmented Training for Adaptations](https://arxiv.org/abs/2506.04288)
*Jae Wan Park,Junhyeok Kim,Youngjun Jun,Hyunah Ko,Seong Jae Hwang*

Main category: cs.LG

TL;DR: 提出Backbone Augmented Training (BAT)方法，利用骨干数据增强适应数据集，通过数学命题和数据选择方案，以ALBAT算法在少数据任务中提升适应训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有适应技术在适应数据有限时训练存在挑战，因此聚焦预训练骨干模型的大量骨干数据来解决问题。

Method: 提出BAT方法，建立并证明两个数学关键命题，引入满足命题的高级数据选择方案，提出ALBAT算法。

Result: ALBAT在个性化和语言生成等少数据任务中有效增强了适应训练。

Conclusion: BAT方法及ALBAT算法能利用骨干数据增强适应数据集，在少数据场景下有效提升适应训练效果。

Abstract: Adaptations facilitate efficient training of large backbone models, including
diffusion models for image generation and transformer-based language models.
While various adaptation techniques enhance performance with minimal
computational resources, limited adaptation data often leads to challenges in
training. To address this, we focus on the enormous amount of backbone data
used to pre-train the backbone models. We propose Backbone Augmented Training
(BAT), a method that leverages backbone data to augment the adaptation dataset.
First, we formulate and prove two mathematical key propositions: one
establishes the validity of BAT, while the other identifies a condition under
which BAT benefits adaptation. Furthermore, we introduce an advanced data
selection scheme that satisfies these propositions and present ALBAT algorithm
to implement this approach. ALBAT efficiently enhances adaptation training in
both personalization and language generation tasks with scarce data.

</details>


### [301] [Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems](https://arxiv.org/abs/2506.05138)
*Pavle Vasiljevic,Milica Matic,Miroslav Popovic*

Main category: cs.LG

TL;DR: 本文基于联邦学习框架开发了基于隔离森林的温度异常检测系统，适用于小型边缘设备和运行MicroPython的物联网系统，实验评估准确率超96%，精度超78%，内存使用低。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式系统中的用户隐私问题和效率问题，为资源受限的边缘设备和物联网系统开发合适的异常检测系统。

Method: 使用Python TestBed for Federated Learning Algorithms和MicroPython TestBed for Federated Learning Algorithms框架，基于隔离森林开发温度异常检测系统。

Result: 系统在区分正常和异常读数上准确率超96%，检测异常精度超78%，模型训练时内存使用低于160 KB。

Conclusion: 该系统适用于资源受限环境和边缘系统，同时遵循联邦学习的数据隐私和协作学习原则。

Abstract: Recently, federated learning frameworks such as Python TestBed for Federated
Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms
have emerged to tackle user privacy concerns and efficiency in embedded
systems. Even more recently, an efficient federated anomaly detection
algorithm, FLiForest, based on Isolation Forests has been developed, offering a
low-resource, unsupervised method well-suited for edge deployment and
continuous learning. In this paper, we present an application of Isolation
Forest-based temperature anomaly detection, developed using the previously
mentioned federated learning frameworks, aimed at small edge devices and IoT
systems running MicroPython. The system has been experimentally evaluated,
achieving over 96% accuracy in distinguishing normal from abnormal readings and
above 78% precision in detecting anomalies across all tested configurations,
while maintaining a memory usage below 160 KB during model training. These
results highlight its suitability for resource-constrained environments and
edge systems, while upholding federated learning principles of data privacy and
collaborative learning.

</details>


### [302] [A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values](https://arxiv.org/abs/2506.05216)
*Tyler Chen,Akshay Seshadri,Mattia J. Villani,Pradeep Niroula,Shouvanik Chakrabarti,Archan Ray,Pranav Deshpande,Romina Yalovetzky,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: 提出统一框架涵盖KernelSHAP及相关估计器，证明理论保证，在多数据集验证方法有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 计算精确Shapley值困难，KernelSHAP理论保证缺失。

Method: 构建涵盖KernelSHAP及相关估计器的统一框架，证明理论保证，进行数据集基准测试并改进实现。

Result: 在中小维度数据集上均方误差低，在高维数据集上结果优于KernelSHAP库。

Conclusion: 为KernelSHAP提供首份理论保证，揭示现有估计器权衡，方法有效且可扩展。

Abstract: Shapley values have emerged as a critical tool for explaining which features
impact the decisions made by machine learning models. However, computing exact
Shapley values is difficult, generally requiring an exponential (in the feature
dimension) number of model evaluations. To address this, many model-agnostic
randomized estimators have been developed, the most influential and widely used
being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators
such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &
Witter, 2025) are known to satisfy theoretical guarantees, bounds for
KernelSHAP have remained elusive. We describe a broad and unified framework
that encompasses KernelSHAP and related estimators constructed using both with
and without replacement sampling strategies. We then prove strong
non-asymptotic theoretical guarantees that apply to all estimators from our
framework. This provides, to the best of our knowledge, the first theoretical
guarantees for KernelSHAP and sheds further light on tradeoffs between existing
estimators. Through comprehensive benchmarking on small and medium dimensional
datasets for Decision-Tree models, we validate our approach against exact
Shapley values, consistently achieving low mean squared error with modest
sample sizes. Furthermore, we make specific implementation improvements to
enable scalability of our methods to high-dimensional datasets. Our methods,
tested on datasets such MNIST and CIFAR10, provide consistently better results
compared to the KernelSHAP library.

</details>


### [303] [Relational reasoning and inductive bias in transformers trained on a transitive inference task](https://arxiv.org/abs/2506.04289)
*Jesse Geerts,Stephanie Chan,Claudia Clopath,Kimberly Stachenfeld*

Main category: cs.LG

TL;DR: 研究Transformer模型在传递性推理任务中不同学习机制的表现，发现权重内学习有泛化偏差，上下文学习需预训练才能实现泛化传递性推理。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型在不同学习机制下关系推理的底层机制。

Method: 对比Transformer模型在权重内学习和上下文学习两种机制下的传递性推理行为。

Result: 权重内学习自然产生传递性推理泛化偏差，仅基于相邻项训练的上下文学习模型无法泛化；预训练后上下文学习模型可实现泛化传递性推理。

Conclusion: 对有潜在结构的任务进行预训练，有助于促进上下文关系推理表征的发展。

Abstract: Transformer-based models have demonstrated remarkable reasoning abilities,
but the mechanisms underlying relational reasoning in different learning
regimes remain poorly understood. In this work, we investigate how transformers
perform a classic relational reasoning task from the Psychology literature,
\textit{transitive inference}, which requires inference about indirectly
related items by integrating information across observed adjacent item pairs
(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior
across two distinct learning regimes: in-weights learning (IWL), where models
store information in network parameters, and in-context learning (ICL), where
models flexibly utilize information presented within the input sequence. Our
findings reveal that IWL naturally induces a generalization bias towards
transitive inference, despite being trained only on adjacent items, whereas ICL
models trained solely on adjacent items do not generalize transitively.
Mechanistic analysis shows that ICL models develop induction circuits that
implement a simple match-and-copy strategy that performs well at relating
adjacent pairs, but does not encoding hierarchical relationships among
indirectly related items. Interestingly, when pre-trained on in-context linear
regression tasks, transformers successfully exhibit in-context generalizable
transitive inference. Moreover, like IWL, they display both \textit{symbolic
distance} and \textit{terminal item effects} characteristic of human and animal
performance, without forming induction circuits. These results suggest that
pre-training on tasks with underlying structure promotes the development of
representations that can scaffold in-context relational reasoning.

</details>


### [304] [Tight analyses of first-order methods with error feedback](https://arxiv.org/abs/2506.05271)
*Daniel Berg Thomsen,Adrien Taylor,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 文章对EF和EF²¹两种误差反馈方案进行严格分析，在简化场景下对比其与压缩梯度下降的性能。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中通信是计算瓶颈，压缩信息会使收敛性下降，需要对误差反馈方案进行分析。

Method: 找出使两种方法收敛率最优的Lyapunov函数，并给出匹配的下界，在简化且有代表性的场景下分析。

Result: 得到了性能保证，能够严格对比EF、EF²¹和压缩梯度下降。

Conclusion: 该方法能给出清晰的理论见解和公平对比底层机制。

Abstract: Communication between agents often constitutes a major computational
bottleneck in distributed learning. One of the most common mitigation
strategies is to compress the information exchanged, thereby reducing
communication overhead. To counteract the degradation in convergence associated
with compressed communication, error feedback schemes -- most notably
$\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we
provide a tight analysis of both of these methods. Specifically, we find the
Lyapunov function that yields the best possible convergence rate for each
method -- with matching lower bounds. This principled approach yields sharp
performance guarantees and enables a rigorous, apples-to-apples comparison
between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our
analysis is carried out in a simplified yet representative setting, which
allows for clean theoretical insights and fair comparison of the underlying
mechanisms.

</details>


### [305] [A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability](https://arxiv.org/abs/2506.04291)
*Wenhan Xu,Jiashuo Jiang,Lei Deng,Danny Hin-Kwok Tsang*

Main category: cs.LG

TL;DR: 研究将Lyapunov Drift - Plus - Penalty算法用于强化学习应用，提出LDPTRLQ算法，经仿真验证优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备增多，解决复杂优化挑战需求增强，探索Lyapunov Drift - Plus - Penalty算法与强化学习结合。

Method: 通过严格理论分析，在一组常见合理条件下得出结合Lyapunov Drift - Plus - Penalty与强化学习的有效方法，提出LDPTRLQ算法。

Result: 多问题仿真结果显示LDPTRLQ算法优于使用Lyapunov drift - plus - penalty方法和强化学习的基线方法，在兼容性和稳定性上也表现更优。

Conclusion: 所提LDPTRLQ算法有效，理论推导合理。

Abstract: With the proliferation of Internet of Things (IoT) devices, the demand for
addressing complex optimization challenges has intensified. The Lyapunov
Drift-Plus-Penalty algorithm is a widely adopted approach for ensuring queue
stability, and some research has preliminarily explored its integration with
reinforcement learning (RL). In this paper, we investigate the adaptation of
the Lyapunov Drift-Plus-Penalty algorithm for RL applications, deriving an
effective method for combining Lyapunov Drift-Plus-Penalty with RL under a set
of common and reasonable conditions through rigorous theoretical analysis.
Unlike existing approaches that directly merge the two frameworks, our proposed
algorithm, termed Lyapunov drift-plus-penalty method tailored for reinforcement
learning with queue stability (LDPTRLQ) algorithm, offers theoretical
superiority by effectively balancing the greedy optimization of Lyapunov
Drift-Plus-Penalty with the long-term perspective of RL. Simulation results for
multiple problems demonstrate that LDPTRLQ outperforms the baseline methods
using the Lyapunov drift-plus-penalty method and RL, corroborating the validity
of our theoretical derivations. The results also demonstrate that our proposed
algorithm outperforms other benchmarks in terms of compatibility and stability.

</details>


### [306] [On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models](https://arxiv.org/abs/2506.04695)
*Xingwu Chen,Tianle Li,Difan Zou*

Main category: cs.LG

TL;DR: 本文系统研究了带可验证奖励的强化学习（RLVR），发现其主要作用是优化现有推理模式的选择，通过实验和理论分析揭示训练动态和收敛机制，并提出用监督微调缓解弱模型优化慢的问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习提升推理能力的机制尚不清楚，需要深入研究。

Method: 进行大量实验研究RLVR训练模型的推理模式选择情况；基于简化的问答推理模型对RLVR的收敛性和训练动态进行理论分析；研究梯度流；实验验证理论发现。

Result: RLVR训练的模型优先采用高成功率推理模式，在单个模式上性能稳定；RLVR训练有两种收敛机制；监督微调可缓解弱模型优化慢的问题。

Conclusion: 该工作提升了对强化学习在大语言模型微调中作用的理论理解，为进一步提升推理能力提供了见解。

Abstract: Reinforcement learning (RL) has demonstrated remarkable success in enhancing
model capabilities, including instruction-following, preference learning, and
reasoning. Yet despite its empirical successes, the mechanisms by which RL
improves reasoning abilities remain poorly understood. We present a systematic
study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that
its primary benefit comes from optimizing the selection of existing reasoning
patterns. Through extensive experiments, we demonstrate that RLVR-trained
models preferentially adopt high-success-rate reasoning patterns while mostly
maintaining stable performance on individual patterns. We further develop
theoretical analyses on the convergence and training dynamics of RLVR based on
a simplified question-reason-answer model. We study the gradient flow and show
that RLVR can indeed find the solution that selects the reason pattern with the
highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1)
rapid convergence for models with relatively strong initial reasoning
capabilities versus (2) slower optimization dynamics for weaker models.
Furthermore, we show that the slower optimization for weaker models can be
mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using
a feasibly high-quality SFT dataset. We validate the theoretical findings
through extensive experiments. This work advances our theoretical understanding
of RL's role in LLM fine-tuning and offers insights for further enhancing
reasoning capabilities.

</details>


### [307] [AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents](https://arxiv.org/abs/2506.04293)
*Fengze Liu,Haoyu Wang,Joonhyuk Cho,Dan Roth,Andrew W. Lo*

Main category: cs.LG

TL;DR: 提出AutoCT框架结合大语言模型推理能力与经典机器学习可解释性，用于临床试验结果预测，性能良好。


<details>
  <summary>Details</summary>
Motivation: 临床试验昂贵耗时，现有深度学习模型有黑盒性、缺乏可解释性和易标签泄露等问题，需更好方法降低研发成本和加速药物发现。

Method: 提出AutoCT框架，基于公开信息自动生成、评估和优化表格特征，用蒙特卡罗树搜索迭代优化预测性能。

Result: AutoCT在临床试验预测任务中，在有限的自优化迭代次数内表现与或优于SOTA方法。

Conclusion: AutoCT为可扩展、可解释和经济高效的临床试验预测建立了新范式。

Abstract: Clinical trials are critical for advancing medical treatments but remain
prohibitively expensive and time-consuming. Accurate prediction of clinical
trial outcomes can significantly reduce research and development costs and
accelerate drug discovery. While recent deep learning models have shown promise
by leveraging unstructured data, their black-box nature, lack of
interpretability, and vulnerability to label leakage limit their practical use
in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel
framework that combines the reasoning capabilities of large language models
with the explainability of classical machine learning. AutoCT autonomously
generates, evaluates, and refines tabular features based on public information
without human input. Our method uses Monte Carlo Tree Search to iteratively
optimize predictive performance. Experimental results show that AutoCT performs
on par with or better than SOTA methods on clinical trial prediction tasks
within only a limited number of self-refinement iterations, establishing a new
paradigm for scalable, interpretable, and cost-efficient clinical trial
prediction.

</details>


### [308] [Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence](https://arxiv.org/abs/2506.04700)
*José Manuel de Frutos,Manuel A. Vázquez,Pablo M. Olmos,Joaquín Míguez*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rank-based statistical metrics, such as the invariant statistical loss (ISL),
have recently emerged as robust and practically effective tools for training
implicit generative models. In this work, we introduce dual-ISL, a novel
likelihood-free objective for training implicit generative models that
interchanges the roles of the target and model distributions in the ISL
framework, yielding a convex optimization problem in the space of model
densities. We prove that the resulting rank-based discrepancy $d_K$ is i)
continuous under weak convergence and with respect to the $L^1$ norm, and ii)
convex in its first argument-properties not shared by classical divergences
such as KL or Wasserstein distances. Building on this, we develop a theoretical
framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q
= p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact
bounds on the truncation error, precise convergence rates, and a closed-form
expression for the truncated density approximation. We further extend our
analysis to the multivariate setting via random one-dimensional projections,
defining a sliced dual-ISL divergence that retains both convexity and
continuity. We empirically show that these theoretical advantages translate
into practical ones. Specifically, across several benchmarks dual-ISL converges
more rapidly, delivers markedly smoother and more stable training, and more
effectively prevents mode collapse than classical ISL and other leading
implicit generative methods-while also providing an explicit density
approximation.

</details>


### [309] [Short-Term Power Demand Forecasting for Diverse Consumer Types to Enhance Grid Planning and Synchronisation](https://arxiv.org/abs/2506.04294)
*Asier Diaz-Iglesias,Xabier Belaunzaran,Ane M. Florez-Tapia*

Main category: cs.LG

TL;DR: 研究通过客户聚类区分不同类型消费者，结合特征选择和多种AI、机器学习算法进行电力需求预测，提出新方法提升预测性能，支持电网稳定。


<details>
  <summary>Details</summary>
Motivation: 在向可再生能源转型中，需要准确的电力需求预测以确保电网稳定。

Method: 通过客户聚类区分工业、商业和居民消费者；对每种消费者类型进行特征选择，利用哥白尼地球观测计划数据；探索并比较多种用于短期和超短期负荷预测的AI和机器学习算法。

Result: 提出的新预测方法在短期和超短期负荷预测中表现优于简单模型。

Conclusion: 定制化预测策略对不同消费者群体很重要，纳入详细天气数据可提高预测准确性，有助于实现更可靠的电力需求预测和电网稳定。

Abstract: Ensuring grid stability in the transition to renewable energy sources
requires accurate power demand forecasting. This study addresses the need for
precise forecasting by differentiating among industrial, commercial, and
residential consumers through customer clusterisation, tailoring the
forecasting models to capture the unique consumption patterns of each group. A
feature selection process is done for each consumer type including temporal,
socio-economic, and weather-related data obtained from the Copernicus Earth
Observation (EO) program. A variety of AI and machine learning algorithms for
Short-Term Load Forecasting (STLF) and Very Short-Term Load Forecasting (VSTLF)
are explored and compared, determining the most effective approaches. With all
that, the main contribution of this work are the new forecasting approaches
proposed, which have demonstrated superior performance compared to simpler
models, both for STLF and VSTLF, highlighting the importance of customized
forecasting strategies for different consumer groups and demonstrating the
impact of incorporating detailed weather data on forecasting accuracy. These
advancements contribute to more reliable power demand predictions, thereby
supporting grid stability.

</details>


### [310] [Deep learning for predicting hauling fleet production capacity under uncertainties in open pit mines using real and simulated data](https://arxiv.org/abs/2506.04296)
*N Guerin,M Nakhla,A Dehoux,J L Loyer*

Main category: cs.LG

TL;DR: 提出深度学习框架进行露天矿运输车队容量短期预测，评估两种架构，分析主导预测因素，未来将集成更多数据。


<details>
  <summary>Details</summary>
Motivation: 露天矿中天气、机械故障和人员可用性等带来运营不确定性，需准确的运输车队容量短期预测。

Method: 提出结合真实运营记录和合成机械故障场景的深度学习框架，评估XGBoost回归器和长短期记忆网络两种架构，用SHAP值分析主导预测因素。

Result: XGBoost回归器中位绝对误差14.3%，长短期记忆网络中位绝对误差15.1%，确定了主导预测因素，集成模拟故障数据和排班特征减少预测波动性。

Conclusion: 该混合建模方法为动态不确定条件下的车队管理提供综合决策支持工具，未来将集成更多数据增强预测的鲁棒性和适应性。

Abstract: Accurate short-term forecasting of hauling-fleet capacity is crucial in
open-pit mining, where weather fluctuations, mechanical breakdowns, and
variable crew availability introduce significant operational uncertainties. We
propose a deep-learning framework that blends real-world operational records
(high-resolution rainfall measurements, fleet performance telemetry) with
synthetically generated mechanical-breakdown scenarios to enable the model to
capture fluctuating high-impact failure events. We evaluate two architectures:
an XGBoost regressor achieving a median absolute error (MedAE) of 14.3 per cent
and a Long Short-Term Memory network with a MedAE of 15.1 per cent. Shapley
Additive exPlanations (SHAP) value analyses identify cumulative rainfall,
historical payload trends, and simulated breakdown frequencies as dominant
predictors. Integration of simulated breakdown data and shift-planning features
notably reduces prediction volatility. Future work will further integrate
maintenance-scheduling indicators (Mean Time Between Failures, Mean Time to
Repair), detailed human resource data (operator absenteeism, crew efficiency
metrics), blast event scheduling, and other operational constraints to enhance
forecast robustness and adaptability. This hybrid modelling approach offers a
comprehensive decision-support tool for proactive, data-driven fleet management
under dynamically uncertain conditions.

</details>


### [311] [Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards](https://arxiv.org/abs/2506.04775)
*Artin Tajdini,Jonathan Scarlett,Kevin Jamieson*

Main category: cs.LG

TL;DR: 研究重尾奖励的随机线性老虎机问题，改进了极小极大后悔值的上下界，提出新算法并得出更好的结果。


<details>
  <summary>Details</summary>
Motivation: 改进重尾奖励随机线性老虎机问题中极小极大后悔值的上下界，解决现有下界较宽松的问题。

Method: 提出基于实验设计的新消除算法。

Result: 新算法实现了更好的后悔值上界，建立了更优的下界，对有限动作集也有类似改进，给出动作集相关的上界并可处理无限维情况。

Conclusion: 新算法和结果凸显了重尾线性老虎机问题的难度，在多种情况下有更好的性能。

Abstract: We study stochastic linear bandits with heavy-tailed rewards, where the
rewards have a finite $(1+\epsilon)$-absolute central moment bounded by
$\upsilon$ for some $\epsilon \in (0,1]$. We improve both upper and lower
bounds on the minimax regret compared to prior work. When $\upsilon =
\mathcal{O}(1)$, the best prior known regret upper bound is
$\tilde{\mathcal{O}}(d T^{\frac{1}{1+\epsilon}})$. While a lower with the same
scaling has been given, it relies on a construction using $\upsilon =
\mathcal{O}(d)$, and adapting the construction to the bounded-moment regime
with $\upsilon = \mathcal{O}(1)$ yields only a
$\Omega(d^{\frac{\epsilon}{1+\epsilon}} T^{\frac{1}{1+\epsilon}})$ lower bound.
This matches the known rate for multi-armed bandits and is generally loose for
linear bandits, in particular being $\sqrt{d}$ below the optimal rate in the
finite-variance case ($\epsilon = 1$). We propose a new elimination-based
algorithm guided by experimental design, which achieves regret
$\tilde{\mathcal{O}}(d^{\frac{1+3\epsilon}{2(1+\epsilon)}}
T^{\frac{1}{1+\epsilon}})$, thus improving the dependence on $d$ for all
$\epsilon \in (0,1)$ and recovering a known optimal result for $\epsilon = 1$.
We also establish a lower bound of $\Omega(d^{\frac{2\epsilon}{1+\epsilon}}
T^{\frac{1}{1+\epsilon}})$, which strictly improves upon the multi-armed bandit
rate and highlights the hardness of heavy-tailed linear bandit problems. For
finite action sets, we derive similarly improved upper and lower bounds for
regret. Finally, we provide action set dependent regret upper bounds showing
that for some geometries, such as $l_p$-norm balls for $p \le 1 + \epsilon$, we
can further reduce the dependence on $d$, and we can handle
infinite-dimensional settings via the kernel trick, in particular establishing
new regret bounds for the Mat\'ern kernel that are the first to be sublinear
for all $\epsilon \in (0, 1]$.

</details>


### [312] [Softlog-Softmax Layers and Divergences Contribute to a Computationally Dependable Ensemble Learning](https://arxiv.org/abs/2506.04297)
*Abdourrahmane Mahamane Atto*

Main category: cs.LG

TL;DR: 本文提出4步流程，表明softlog - softmax级联可提升下一代集成学习系统的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升下一代集成学习系统的一致性和可靠性。

Method: 提出4步流程，包括解剖性质的第一步（按多样性选择规范形式构建目标集成模型）、与神经工程相关的第二步（推导softlog实现弱对数运算一致性）、涉及神经信息论的第三步（提出基于softlog的熵和散度构建信息度量）以及得出信息性能张量的第四步。

Result: 未明确提及具体结果。

Conclusion: softlog - softmax级联能提升下一代集成学习系统的一致性和可靠性。

Abstract: The paper proposes a 4-step process for highlighting that softlog-softmax
cascades can improve both consistency and dependability of the next generation
ensemble learning systems. The first process is anatomical in nature: the
target ensemble model under consideration is composed by canonical elements
relating to the definition of a convolutional frustum. No a priori is
considered in the choice of canonical forms. Diversity is the main criterion
for selecting these forms. It is shown that the more complex the problem, the
more useful this ensemble diversity is. The second process is physiological and
relates to neural engineering: a softlog is derived to both make weak
logarithmic operations consistent and lead, through multiple softlog-softmax
layers, to intermediate decisions in the sense of respecting the same class
logic as that faced by the output layer. The third process concerns neural
information theory: softlog-based entropy and divergence are proposed for the
sake of constructing information measures yielding consistent values on closed
intervals. These information measures are used to determine the relationships
between individual and sub-community decisions in frustum diversitybased
ensemble learning. The concluding process addresses the derivation of an
informative performance tensor for the purpose of a reliable ensemble
evaluation.

</details>


### [313] [The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective](https://arxiv.org/abs/2506.04301)
*Jiin Kim,Byeongjun Shin,Jinha Chung,Minsoo Rhu*

Main category: cs.LG

TL;DR: 本文对基于大语言模型的AI智能体进行系统级分析，揭示其计算需求大、存在可持续性危机，呼吁设计向高效推理转变。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的AI智能体在带来灵活性的同时，引发了系统级成本、效率和可持续性问题，因此需要进行系统级分析。

Method: 对不同智能体设计和测试时的缩放策略，量化其资源使用、延迟行为、能耗和数据中心的电力消耗需求，分析设计选择对准确率 - 成本权衡的影响。

Result: 智能体随计算量增加提高准确率，但收益递减、延迟差异扩大、基础设施成本不可持续。

Conclusion: AI智能体工作流存在严重计算需求和可持续性危机，需要在智能体设计上进行范式转变，实现高效推理，平衡性能和可部署性。

Abstract: Large-language-model (LLM)-based AI agents have recently showcased impressive
versatility by employing dynamic reasoning, an adaptive, multi-step process
that coordinates with external tools. This shift from static, single-turn
inference to agentic, multi-turn workflows broadens task generalization and
behavioral flexibility, but it also introduces serious concerns about
system-level cost, efficiency, and sustainability. This paper presents the
first comprehensive system-level analysis of AI agents, quantifying their
resource usage, latency behavior, energy consumption, and datacenter-wide power
consumption demands across diverse agent designs and test-time scaling
strategies. We further characterize how AI agent design choices, such as
few-shot prompting, reflection depth, and parallel reasoning, impact
accuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy
with increased compute, they suffer from rapidly diminishing returns, widening
latency variance, and unsustainable infrastructure costs. Through detailed
evaluation of representative agents, we highlight the profound computational
demands introduced by AI agent workflows, uncovering a looming sustainability
crisis. These results call for a paradigm shift in agent design toward
compute-efficient reasoning, balancing performance with deployability under
real-world constraints.

</details>


### [314] [RedRFT: A Light-Weight Benchmark for Reinforcement Fine-Tuning-Based Red Teaming](https://arxiv.org/abs/2506.04302)
*Xiang Zheng,Xingjun Ma,Wei-Bin Lee,Cong Wang*

Main category: cs.LG

TL;DR: 提出RedRFT轻量级基准以简化和标准化基于RFT的红队测试，进行关键组件消融研究，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于RFT的红队测试方法缺乏统一基准，PPO - RFT实现细节影响结果稳定性和可重复性。

Method: 结合CleanRL和Tianshou设计优势构建RedRFT，支持多种指标和模块化奖励计算，对关键组件进行消融研究。

Result: 构建RedRFT基准，完成关键组件消融实验。

Conclusion: 有助于全面理解基于RFT的红队测试算法实现细节，实现创新功能快速原型开发。

Abstract: Red teaming has proven to be an effective method for identifying and
mitigating vulnerabilities in Large Language Models (LLMs). Reinforcement
Fine-Tuning (RFT) has emerged as a promising strategy among existing red
teaming techniques. However, a lack of a unified benchmark hinders current
RFT-based red teaming methods. Implementation details, especially in Proximal
Policy Optimization (PPO)-based RFT, significantly affect outcome stability and
reproducibility. To address this issue, we introduce RedRFT, a lightweight
benchmark designed to simplify and standardize the implementation and
evaluation of RFT-based red teaming. RedRFT combines the design strengths of
both single-file CleanRL and highly modularized Tianshou, offering high-quality
single-file red teaming implementations and modular PPO core components, such
as the General Advantage Estimator. It supports a variety of token and sentence
diversity metrics, featuring modularized intrinsic reward computation that
facilitates plug-and-play experimentation. To clarify their influence on RFT
performance, we conducted an extensive ablation study on key components,
including Low-Rank Adaptation (LoRA), Kullback-Leibler (KL) divergence, and
Lagrange Multiplier. We hope this work contributes to 1) gaining a
comprehensive understanding of the implementation nuances of RFT-based red
teaming algorithms, and 2) enabling rapid prototyping of innovative features
for RFT-based red teaming. Code for the benchmark can be accessed at
https://github.com/x-zheng16/RedRFT.git.

</details>


### [315] [Towards Reasonable Concept Bottleneck Models](https://arxiv.org/abs/2506.05014)
*Nektarios Kalampalikis,Kavya Gupta,Georgi Vitanov,Isabel Valera*

Main category: cs.LG

TL;DR: 提出概念推理模型CREAM，明确编码概念关系并利用正则化旁道，实验显示其依赖概念且性能与黑盒模型相当，能缓解概念泄漏。


<details>
  <summary>Details</summary>
Motivation: 设计能明确编码概念关系、实现期望推理并保持概念重要性以提升任务性能的概念瓶颈模型。

Method: 架构上嵌入专家指定的概念关系，切断不良信息流；集成正则化的黑盒旁道。

Result: CREAM主要依赖概念，任务性能与黑盒模型相当；嵌入的关系便于模型干预和缓解概念泄漏。

Conclusion: CREAM是一种有效的概念瓶颈模型，能在保证性能的同时提升模型可解释性和可控性。

Abstract: In this paper, we propose $\textbf{C}$oncept $\textbf{REA}$soning
$\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)
that: (i) explicitly encodes concept-concept (${\texttt{C-C}}$) and
concept-task (${\texttt{C$\rightarrow$Y}}$) relationships to enforce a desired
model reasoning; and (ii) use a regularized side-channel to achieve competitive
task performance, while keeping high concept importance. Specifically, CREAM
architecturally embeds (bi)directed concept-concept, and concept to task
relationships specified by a human expert, while severing undesired information
flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates
a black-box side-channel that is regularized to encourage task predictions to
be grounded in the relevant concepts, thereby utilizing the side-channel only
when necessary to enhance performance. Our experiments show that: (i) CREAM
mainly relies on concepts while achieving task performance on par with
black-box models; and (ii) the embedded ${\texttt{C-C}}$ and
${\texttt{C$\rightarrow$Y}}$ relationships ease model interventions and
mitigate concept leakage.

</details>


### [316] [You Only Train Once](https://arxiv.org/abs/2506.04349)
*Christos Sakaridis*

Main category: cs.LG

TL;DR: 提出YOTO方法，可通过标准基于梯度的优化一次性自动优化学习模型的损失权重超参数，在计算机视觉两个关键问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有创建和优化学习模型需多次训练，希望在损失选择和加权方面实现一次性训练。

Method: 将损失权重超参数视为网络常规参数，利用复合损失公式的可微性建模新层，通过softmax操作满足约束，定义新的正则化损失完成联合端到端优化。

Result: 在计算机视觉3D估计和语义分割两个关键问题上，与常用的暴力网格搜索相比，YOTO始终在未见测试数据上表现更优。

Conclusion: YOTO方法能有效一次性联合优化损失超参数和常规模型参数。

Abstract: The title of this paper is perhaps an overclaim. Of course, the process of
creating and optimizing a learned model inevitably involves multiple training
runs which potentially feature different architectural designs, input and
output encodings, and losses. However, our method, You Only Train Once (YOTO),
indeed contributes to limiting training to one shot for the latter aspect of
losses selection and weighting. We achieve this by automatically optimizing
loss weight hyperparameters of learned models in one shot via standard
gradient-based optimization, treating these hyperparameters as regular
parameters of the networks and learning them. To this end, we leverage the
differentiability of the composite loss formulation which is widely used for
optimizing multiple empirical losses simultaneously and model it as a novel
layer which is parameterized with a softmax operation that satisfies the
inherent positivity constraints on loss hyperparameters while avoiding
degenerate empirical gradients. We complete our joint end-to-end optimization
scheme by defining a novel regularization loss on the learned hyperparameters,
which models a uniformity prior among the employed losses while ensuring
boundedness of the identified optima. We evidence the efficacy of YOTO in
jointly optimizing loss hyperparameters and regular model parameters in one
shot by comparing it to the commonly used brute-force grid search across
state-of-the-art networks solving two key problems in computer vision, i.e. 3D
estimation and semantic segmentation, and showing that it consistently
outperforms the best grid-search model on unseen test data. Code will be made
publicly available.

</details>


### [317] [NIMO: a Nonlinear Interpretable MOdel](https://arxiv.org/abs/2506.05059)
*Shijian Xu,Marcello Massimo Negri,Volker Roth*

Main category: cs.LG

TL;DR: 介绍NIMO模型结合神经网络与线性模型优点，实现可解释性与高预测精度；


<details>
  <summary>Details</summary>
Motivation: 神经网络难解释，线性模型可解释但表达力弱，希望结合二者优点；

Method: 定义NIMO模型，让神经网络学习线性模型预测的非线性修正，开发基于轮廓似然的优化算法，利用自适应岭回归引入稀疏约束；

Result: 能恢复潜在线性系数，显著提高预测精度，在多种回归和分类任务中表现更好；

Conclusion: NIMO模型是唯一能保持线性模型系数可解释性的混合可解释方法，性能优越。

Abstract: Neural networks (NNs) have achieved tremendous success over the past decade,
yet they are still extremely difficult to interpret. In contrast, linear models
are less expressive but offer inherent interpretability. Linear coefficients
are interpretable as the marginal effect of a feature on the prediction,
assuming all other features are kept fixed. To combine the benefits of both
approaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is
to define a model where the NN is designed to learn nonlinear corrections to
the linear model predictions, while also maintaining the original
interpretability of the linear coefficients. Relevantly, we develop an
optimization algorithm based on profile likelihood that elegantly allows for
optimizing over the NN parameters while updating the linear coefficients
analytically. By relying on adaptive ridge regression we can easily incorporate
sparsity constraints as well. We show empirically that we can recover the
underlying linear coefficients while significantly improving the predictive
accuracy. Compared to other hybrid interpretable approaches, our model is the
only one that actually maintains the same interpretability of linear
coefficients as in linear models. We also achieve higher performance on various
regression and classification settings.

</details>


### [318] [Half-Layered Neural Networks](https://arxiv.org/abs/2506.04352)
*Ethem Alpaydin*

Main category: cs.LG

TL;DR: 提出有部分权重随机设置、部分权重训练的“半”隐藏层，实验显示其在减少参数下能获合理准确率。


<details>
  <summary>Details</summary>
Motivation: 探索一种新的隐藏层结构以在减少参数的同时保证模型性能。

Method: 提出半单元，其包含用固定随机权重计算输入加权和、用可修改权重进行乘法和转换两步，可用于深度网络各层。

Result: 在MNIST和FashionMNIST数据集实验表明半层能在减少参数下实现合理准确率。

Conclusion: 半层因随机连接的正则化效果，在减少参数方面有应用前景。

Abstract: We propose a ``half'' layer of hidden units that has some of its weights
randomly set and some of them trained. A half unit is composed of two stages:
First, it takes a weighted sum of its inputs with fixed random weights, and
second, the total activation is multiplied and then translated using two
modifiable weights, before the result is passed through a nonlinearity. The
number of modifiable weights of each hidden unit is thus two and does not
depend on the fan-in. We show how such half units can be used in the first or
any later layer in a deep network, possibly following convolutional layers. Our
experiments on MNIST and FashionMNIST data sets indicate the promise of half
layers, where we can achieve reasonable accuracy with a reduced number of
parameters due to the regularizing effect of the randomized connections.

</details>


### [319] [UnHiPPO: Uncertainty-aware Initialization for State Space Models](https://arxiv.org/abs/2506.05065)
*Marten Lienen,Abdullah Saydemir,Stephan Günnemann*

Main category: cs.LG

TL;DR: 扩展HiPPO理论以考虑测量噪声，推导状态空间模型动力学的不确定性感知初始化方法，实验表明该初始化提高模型抗噪性。


<details>
  <summary>Details</summary>
Motivation: HiPPO框架假设数据无噪声，与实际情况不符，需要扩展理论以处理有噪声数据。

Method: 将HiPPO解释为线性随机控制问题，重新表述问题使数据成为潜在系统的噪声输出，推导不增加运行时间的动力学初始化方法。

Result: 实验显示所提出的初始化方法在训练和推理时均提高了状态空间模型的抗噪能力。

Conclusion: 扩展的HiPPO理论的初始化方法能有效增强状态空间模型对噪声的抵抗能力。

Abstract: State space models are emerging as a dominant model class for sequence
problems with many relying on the HiPPO framework to initialize their dynamics.
However, HiPPO fundamentally assumes data to be noise-free; an assumption often
violated in practice. We extend the HiPPO theory with measurement noise and
derive an uncertainty-aware initialization for state space model dynamics. In
our analysis, we interpret HiPPO as a linear stochastic control problem where
the data enters as a noise-free control signal. We then reformulate the problem
so that the data become noisy outputs of a latent system and arrive at an
alternative dynamics initialization that infers the posterior of this latent
system from the data without increasing runtime. Our experiments show that our
initialization improves the resistance of state-space models to noise both at
training and inference time. Find our implementation at
https://cs.cit.tum.de/daml/unhippo.

</details>


### [320] [A Risk-Aware Reinforcement Learning Reward for Financial Trading](https://arxiv.org/abs/2506.04358)
*Uditansh Srivastava,Shivam Aryan,Shaurya Singh*

Main category: cs.LG

TL;DR: 提出用于金融交易强化学习的复合奖励函数，可平衡回报与风险，能编码投资者偏好，经网格搜索调参，推导梯度并分析理论性质，为构建多目标奖励函数提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 解决现有单指标目标（如夏普比率）的不足，平衡金融交易中的回报和风险，编码不同投资者偏好。

Method: 提出含四个可微项的复合奖励函数，通过网格搜索调整权重，推导各项闭式梯度。

Result: 得到能平衡回报与风险的复合奖励函数，分析了其关键理论性质。

Conclusion: 该框架为复杂交易环境构建稳健多目标奖励函数提供通用蓝图，可扩展额外风险度量或自适应加权。

Abstract: We propose a novel composite reward function for reinforcement learning in
financial trading that balances return and risk using four differentiable
terms: annualized return downside risk differential return and the Treynor
ratio
  Unlike single metric objectives for example the Sharpe ratio our formulation
is modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners
to encode diverse investor preferences
  We tune these weights via grid search to target specific risk return profiles
  We derive closed form gradients for each term to facilitate gradient based
training and analyze key theoretical properties including monotonicity
boundedness and modularity
  This framework offers a general blueprint for building robust multi objective
reward functions in complex trading environments and can be extended with
additional risk measures or adaptive weighting

</details>


### [321] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: 本文通过线性回归框架研究合成数据隐私放大问题，发现对手控制生成模型种子时单个合成数据点会泄露大量信息，随机输入生成有限合成数据点可放大隐私。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据隐私放大现象缺乏严格理论理解，需深入研究。

Method: 运用线性回归框架进行研究。

Result: 对手控制种子时，单个合成数据点泄露信息多；随机输入生成有限合成数据点能放大隐私。

Conclusion: 线性回归的研究结果可为未来推导更通用界限奠定基础。

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


### [322] [Even Faster Hyperbolic Random Forests: A Beltrami-Klein Wrapper Approach](https://arxiv.org/abs/2506.04360)
*Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: 本文展示如何在双曲空间的Beltrami - Klein模型中重新表达hyperDT算法，将其重写为围绕欧几里得空间树模型的预处理和后处理步骤，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 将hyperDT算法在双曲空间的Beltrami - Klein模型中进行更优表达，以利用欧几里得空间模型的优化。

Method: 在双曲空间的Beltrami - Klein模型中重新表达hyperDT算法，将其写成围绕欧几里得空间树模型的预处理和后处理步骤。

Result: 实现了该算法，代码可在https://github.com/pchlenski/hyperdt获取，能利用欧几里得空间模型的优化。

Conclusion: 这种包装方法提升了灵活性、速度和准确性，代码更简单、易维护和扩展。

Abstract: Decision trees and models that use them as primitives are workhorses of
machine learning in Euclidean spaces. Recent work has further extended these
models to the Lorentz model of hyperbolic space by replacing axis-parallel
hyperplanes with homogeneous hyperplanes when partitioning the input space. In
this paper, we show how the hyperDT algorithm can be elegantly reexpressed in
the Beltrami-Klein model of hyperbolic spaces. This preserves the thresholding
operation used in Euclidean decision trees, enabling us to further rewrite
hyperDT as simple pre- and post-processing steps that form a wrapper around
existing tree-based models designed for Euclidean spaces. The wrapper approach
unlocks many optimizations already available in Euclidean space models,
improving flexibility, speed, and accuracy while offering a simpler, more
maintainable, and extensible codebase. Our implementation is available at
https://github.com/pchlenski/hyperdt.

</details>


### [323] [Transformers Meet In-Context Learning: A Universal Approximation Theory](https://arxiv.org/abs/2506.05200)
*Gen Li,Yuchen Jiao,Yu Huang,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文为理解transformer如何实现上下文学习，提出通用逼近理论，采用与近期文献不同的方法，给出逼近保证，揭示transformer学习通用表示和适应上下文示例的机制。


<details>
  <summary>Details</summary>
Motivation: 更好地理解transformers如何实现上下文学习。

Method: 发展通用逼近理论，针对任何函数类构建transformer进行可靠预测，采用基于通用函数逼近的不同方法。

Result: 提出的方法提供了不依赖被逼近优化算法有效性的逼近保证，可超越凸问题和线性函数类。

Conclusion: 构建方式揭示了transformers可同时学习通用表示并动态适应上下文示例。

Abstract: Modern large language models are capable of in-context learning, the ability
to perform new tasks at inference time using only a handful of input-output
examples in the prompt, without any fine-tuning or parameter updates. We
develop a universal approximation theory to better understand how transformers
enable in-context learning. For any class of functions (each representing a
distinct task), we demonstrate how to construct a transformer that, without any
further weight updates, can perform reliable prediction given only a few
in-context examples. In contrast to much of the recent literature that frames
transformers as algorithm approximators -- i.e., constructing transformers to
emulate the iterations of optimization algorithms as a means to approximate
solutions of learning problems -- our work adopts a fundamentally different
approach rooted in universal function approximation. This alternative approach
offers approximation guarantees that are not constrained by the effectiveness
of the optimization algorithms being approximated, thereby extending far beyond
convex problems and linear function classes. Our construction sheds light on
how transformers can simultaneously learn general-purpose representations and
adapt dynamically to in-context examples.

</details>


### [324] [Replay Can Provably Increase Forgetting](https://arxiv.org/abs/2506.04377)
*Yasaman Mahdaviyeh,James Lucas,Mengye Ren,Andreas S. Tolias,Richard Zemel,Toniann Pitassi*

Main category: cs.LG

TL;DR: 本文对过参数化连续线性回归设置下的样本重放进行理论分析，发现遗忘与重放样本数量的非单调关系，有害重放不仅存在于线性模型，且重放性能依赖样本选择和任务关系。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临遗忘挑战，样本重放是缓解遗忘的常用技术，本文旨在对其进行理论分析。

Method: 在过参数化连续线性回归设置下，分析样本重放，关注重放样本的作用和任务子空间的关系。

Result: 发现即使在无噪声情况下，遗忘与重放样本数量是非单调关系；存在重放有害的情况；有害重放不限于线性模型；重放性能依赖样本选择和任务关系。

Conclusion: 样本重放的效果受重放样本选择和任务关系的显著影响。

Abstract: Continual learning seeks to enable machine learning systems to solve an
increasing corpus of tasks sequentially. A critical challenge for continual
learning is forgetting, where the performance on previously learned tasks
decreases as new tasks are introduced. One of the commonly used techniques to
mitigate forgetting, sample replay, has been shown empirically to reduce
forgetting by retaining some examples from old tasks and including them in new
training episodes. In this work, we provide a theoretical analysis of sample
replay in an over-parameterized continual linear regression setting, where each
task is given by a linear subspace and with enough replay samples, one would be
able to eliminate forgetting. Our analysis focuses on sample replay and
highlights the role of the replayed samples and the relationship between task
subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting
can be non-monotonic with respect to the number of replay samples. We present
tasks where replay can be harmful with respect to worst-case settings, and also
in distributional settings where replay of randomly selected samples increases
forgetting in expectation. We also give empirical evidence that harmful replay
is not limited to training with linear models by showing similar behavior for a
neural networks equipped with SGD. Through experiments on a commonly used
benchmark, we provide additional evidence that, even in seemingly benign
scenarios, performance of the replay heavily depends on the choice of replay
samples and the relationship between tasks.

</details>


### [325] [Progressive Tempering Sampler with Diffusion](https://arxiv.org/abs/2506.05231)
*Severi Rissanen,RuiKang OuYang,Jiajun He,Wenlin Chen,Markus Heinonen,Arno Solin,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出PTSD方法结合PT与神经采样器优势，提高目标评估效率并生成无关联样本。


<details>
  <summary>Details</summary>
Motivation: 现有神经采样器在目标评估效率上不如PT，而PT生成依赖样本且重新采样成本高，需改进。

Method: 提出PTSD，在不同温度下顺序训练扩散模型，结合高温扩散模型生成低温样本，用MCMC微调后训练下一个扩散模型。

Result: PTSD能高效复用不同温度样本信息，生成混合良好、无关联样本，显著提高目标评估效率，优于基于扩散的神经采样器。

Conclusion: PTSD有效结合了PT和神经采样器的优势，是一种更高效的采样方法。

Abstract: Recent research has focused on designing neural samplers that amortize the
process of sampling from unnormalized densities. However, despite significant
advancements, they still fall short of the state-of-the-art MCMC approach,
Parallel Tempering (PT), when it comes to the efficiency of target evaluations.
On the other hand, unlike a well-trained neural sampler, PT yields only
dependent samples and needs to be rerun -- at considerable computational cost
-- whenever new samples are required. To address these weaknesses, we propose
the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion
models sequentially across temperatures, leveraging the advantages of PT to
improve the training of neural samplers. We also introduce a novel method to
combine high-temperature diffusion models to generate approximate
lower-temperature samples, which are minimally refined using MCMC and used to
train the next diffusion model. PTSD enables efficient reuse of sample
information across temperature levels while generating well-mixed, uncorrelated
samples. Our method significantly improves target evaluation efficiency,
outperforming diffusion-based neural samplers.

</details>


### [326] [Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning](https://arxiv.org/abs/2506.04398)
*Théo Vincent,Yogesh Tripathi,Tim Faust,Yaniv Oren,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出迭代共享Q学习(iS - QL)方法，用在线网络最后线性层副本作目标网络，提高无目标方法样本效率，内存需求小且训练时间与传统有目标算法相当。


<details>
  <summary>Details</summary>
Motivation: 移除目标网络虽有好处但会导致不稳定和性能下降，且无法利用相关文献成果。

Method: 使用在线网络最后线性层的副本作为目标网络，共享其余参数，利用迭代Q学习概念并行学习连续贝尔曼迭代。

Result: iS - QL在各种设置下提高了无目标方法的样本效率。

Conclusion: iS - QL内存需求小、训练时间与传统有目标算法相当，有扩展强化学习研究的潜力。

Abstract: In value-based reinforcement learning, removing the target network is
tempting as the boostrapped target would be built from up-to-date estimates,
and the spared memory occupied by the target network could be reallocated to
expand the capacity of the online network. However, eliminating the target
network introduces instability, leading to a decline in performance. Removing
the target network also means we cannot leverage the literature developed
around target networks. In this work, we propose to use a copy of the last
linear layer of the online network as a target network, while sharing the
remaining parameters with the up-to-date online network, hence stepping out of
the binary choice between target-based and target-free methods. It enables us
to leverage the concept of iterated Q-learning, which consists of learning
consecutive Bellman iterations in parallel, to reduce the performance gap
between target-free and target-based approaches. Our findings demonstrate that
this novel method, termed iterated Shared Q-Learning (iS-QL), improves the
sample efficiency of target-free approaches across various settings.
Importantly, iS-QL requires a smaller memory footprint and comparable training
time to classical target-based algorithms, highlighting its potential to scale
reinforcement learning research.

</details>


### [327] [Sample Complexity and Representation Ability of Test-time Scaling Paradigms](https://arxiv.org/abs/2506.05295)
*Baihe Huang,Shanda Li,Tianhao Wu,Yiming Yang,Ameet Talwalkar,Kannan Ramchandran,Michael I. Jordan,Jiantao Jiao*

Main category: cs.LG

TL;DR: 本文研究大语言模型测试时间策略样本效率，得出不同策略样本需求的理论结果，并实证验证自校正方法有效性。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型测试时间策略样本效率的理论理解有限。

Method: 理论分析不同重复采样策略样本需求，给出自校正方法表达性结果，最后进行实验验证。

Result: 自一致性需$\Theta(1/\Delta^2)$个样本，最佳n选1只需$\Theta(1/\Delta)$个样本；自校正方法能让Transformer在测试时模拟在线学习专家池，扩展了其表示理论。

Conclusion: 实证验证了自校正方法的实际有效性，扩展了Transformer表示理论到多任务设置。

Abstract: Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.

</details>


### [328] [Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning](https://arxiv.org/abs/2506.04399)
*Suzan Ece Ada,Emre Ugur*

Main category: cs.LG

TL;DR: 提出UMCNP方法结合PPG和任务推理的少样本元强化学习，在元测试无奖励信号时提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决元测试中奖励信号缺失的问题，提高样本效率且不增加元训练样本。

Method: 结合PPG和任务推理，利用CNP减少元测试在线交互，离线复用样本学习任务推理，从单任务推断过渡动态模型潜在表示。

Result: 在2D-Point Agent和连续控制元强化学习基准测试中，元测试时比基线方法用更少样本适应未见测试任务。

Conclusion: UMCNP方法在元测试无奖励信号场景下有效提高样本效率。

Abstract: We introduce Unsupervised Meta-Testing with Conditional Neural Processes
(UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method
that uniquely combines, yet distinctly separates, parameterized policy
gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for
settings where the reward signal is missing during meta-testing, our method
increases sample efficiency without requiring additional samples in
meta-training. UMCNP leverages the efficiency and scalability of Conditional
Neural Processes (CNPs) to reduce the number of online interactions required in
meta-testing. During meta-training, samples previously collected through PPG
meta-RL are efficiently reused for learning task inference in an offline
manner. UMCNP infers the latent representation of the transition dynamics model
from a single test task rollout with unknown parameters. This approach allows
us to generate rollouts for self-adaptation by interacting with the learned
dynamics model. We demonstrate our method can adapt to an unseen test task
using significantly fewer samples during meta-testing than the baselines in
2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with
unknown angle sensor bias, walker agent with randomized dynamics parameters.

</details>


### [329] [Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning](https://arxiv.org/abs/2506.04411)
*Achleshwar Luthra,Tianbao Yang,Tomer Galanti*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite its empirical success, the theoretical foundations of self-supervised
contrastive learning (CL) are not yet fully established. In this work, we
address this gap by showing that standard CL objectives implicitly approximate
a supervised variant we call the negatives-only supervised contrastive loss
(NSCL), which excludes same-class contrasts. We prove that the gap between the
CL and NSCL losses vanishes as the number of semantic classes increases, under
a bound that is both label-agnostic and architecture-independent.
  We characterize the geometric structure of the global minimizers of the NSCL
loss: the learned representations exhibit augmentation collapse, within-class
collapse, and class centers that form a simplex equiangular tight frame. We
further introduce a new bound on the few-shot error of linear-probing. This
bound depends on two measures of feature variability--within-class dispersion
and variation along the line between class centers. We show that directional
variation dominates the bound and that the within-class dispersion's effect
diminishes as the number of labeled samples increases. These properties enable
CL and NSCL-trained representations to support accurate few-shot label recovery
using simple linear probes.
  Finally, we empirically validate our theoretical findings: the gap between CL
and NSCL losses decays at a rate of $\mathcal{O}(\frac{1}{\#\text{classes}})$;
the two losses are highly correlated; minimizing the CL loss implicitly brings
the NSCL loss close to the value achieved by direct minimization; and the
proposed few-shot error bound provides a tight estimate of probing performance
in practice.

</details>


### [330] [Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order](https://arxiv.org/abs/2506.04430)
*Egor Petrov,Grigoriy Evseev,Aleksey Antonov,Andrey Veprikov,Pavel Plyusnin,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 文章探讨用零阶优化方法替代传统一阶优化器微调大语言模型，提出JAGUAR SignSGD和JAGUAR Muon算法，实验证明新算法能达到或超越一阶方法的收敛质量，且显著减少内存。


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化器在微调大语言模型时内存和计算成本高，扩展性差，需内存和计算高效的替代方案。

Method: 提出JAGUAR SignSGD（一种基于零阶动量的算法，扩展了零阶SignSGD）和JAGUAR Muon（Muon优化器的零阶扩展，利用模型参数矩阵结构）两种零阶优化算法。

Result: 在具有挑战性的大语言模型微调基准测试中，新算法达到或超越标准一阶方法的收敛质量，实现显著的内存减少。

Conclusion: 新的零阶优化方法是资源受限情况下大语言模型适配的实用且理论可靠的方法。

Abstract: Fine-tuning Large Language Models (LLMs) is essential for adapting
pre-trained models to downstream tasks. Yet traditional first-order optimizers
such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and
computational costs that scale poorly with model size. In this paper, we
investigate zero-order (ZO) optimization methods as a memory- and
compute-efficient alternative, particularly in the context of
parameter-efficient fine-tuning techniques like LoRA. We propose
$\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO
SignSGD, requiring the same number of parameters as the standard ZO SGD and
only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our
knowledge, this is the first study to establish rigorous convergence guarantees
for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR
Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix
structure of model parameters, and we provide its convergence rate under
arbitrary stochastic noise. Through extensive experiments on challenging LLM
fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or
exceed the convergence quality of standard first-order methods, achieving
significant memory reduction. Our theoretical and empirical results establish
new ZO optimization methods as a practical and theoretically grounded approach
for resource-constrained LLM adaptation. Our code is available at
https://github.com/brain-mmo-lab/ZO_LLM

</details>


### [331] [KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)
*Zixuan Xia,Aram Davtyan,Paolo Favaro*

Main category: cs.LG

TL;DR: 提出KOALA++算法，在神经网络训练中显式建模结构化梯度不确定性，在多样任务中表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有二阶方法依赖昂贵的二阶梯度计算，原KOALA框架假设对角协方差有局限性。

Method: 直接估计参数协方差矩阵，通过递归更新紧凑梯度协方差积，隐式捕获更丰富的不确定性结构。

Result: 在图像分类和语言建模等多样任务中，KOALA++达到与现有最优一、二阶优化器相当或更好的准确率，且保持一阶方法的效率。

Conclusion: KOALA++是一种可扩展的、有效的神经网络训练优化算法。

Abstract: We propose KOALA++, a scalable Kalman-based optimization algorithm that
explicitly models structured gradient uncertainty in neural network training.
Unlike second-order methods, which rely on expensive second order gradient
calculation, our method directly estimates the parameter covariance matrix by
recursively updating compact gradient covariance products. This design improves
upon the original KOALA framework that assumed diagonal covariance by
implicitly capturing richer uncertainty structure without storing the full
covariance matrix and avoiding large matrix inversions. Across diverse tasks,
including image classification and language modeling, KOALA++ achieves accuracy
on par or better than state-of-the-art first- and second-order optimizers while
maintaining the efficiency of first-order methods.

</details>


### [332] [RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis](https://arxiv.org/abs/2506.04439)
*Robin Yadav,Qi Yan,Guy Wolf,Avishek Joey Bose,Renjie Liao*

Main category: cs.LG

TL;DR: 提出离散流匹配框架RETRO SYNFLOW用于单步逆合成规划，提升了准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有无模板生成方法在单步逆合成中难以生成准确且多样的可行反应，解决有机化学中识别和预测合成目标产物分子的反应系列这一基本问题。

Method: 构建马尔可夫桥，采用反应中心识别步骤生成中间体结构，使用Feynman - Kac引导和基于顺序蒙特卡罗的重采样。

Result: RETRO SYNFLOW实现60.0%的top - 1准确率，超之前SOTA 20%；FK - 引导使top - 5往返准确率比之前无模板SOTA方法提高19%。

Conclusion: RETRO SYNFLOW在单步逆合成中表现出色，证明推理时引导的好处，保持了有竞争力的top - k准确率。

Abstract: A fundamental problem in organic chemistry is identifying and predicting the
series of reactions that synthesize a desired target product molecule. Due to
the combinatorial nature of the chemical search space, single-step reactant
prediction -- i.e. single-step retrosynthesis -- remains challenging even for
existing state-of-the-art template-free generative approaches to produce an
accurate yet diverse set of feasible reactions. In this paper, we model
single-step retrosynthesis planning and introduce RETRO SYNFLOW (RSF) a
discrete flow-matching framework that builds a Markov bridge between the
prescribed target product molecule and the reactant molecule. In contrast to
past approaches, RSF employs a reaction center identification step to produce
intermediate structures known as synthons as a more informative source
distribution for the discrete flow. To further enhance diversity and
feasibility of generated samples, we employ Feynman-Kac steering with
Sequential Monte Carlo based resampling to steer promising generations at
inference using a new reward oracle that relies on a forward-synthesis model.
Empirically, we demonstrate \nameshort achieves $60.0 \%$ top-1 accuracy, which
outperforms the previous SOTA by $20 \%$. We also substantiate the benefits of
steering at inference and demonstrate that FK-steering improves top-$5$
round-trip accuracy by $19 \%$ over prior template-free SOTA methods, all while
preserving competitive top-$k$ accuracy results.

</details>


### [333] [Selective Matching Losses -- Not All Scores Are Created Equal](https://arxiv.org/abs/2506.04446)
*Gil I. Shamir,Manfred K. Warmuth*

Main category: cs.LG

TL;DR: 本文构建选择性匹配损失函数，开发多维选择性损失框架，在重要分数区域应用中有优势。


<details>
  <summary>Details</summary>
Motivation: 学习系统需在部分领域产生准确预测，传统损失函数在区分不同重要性区域和多分类上存在不足。

Method: 通过设计分数域上递增的链接函数构建选择性匹配损失函数，利用复合Softmax函数开发多维选择性损失框架。

Result: 设计出多种选择性标量损失，克服标准Softmax函数局限性。

Conclusion: 选择性损失在重要分数区域应用中比传统损失有显著优势。

Abstract: Learning systems match predicted scores to observations over some domain.
Often, it is critical to produce accurate predictions in some subset (or
region) of the domain, yet less important to accurately predict in other
regions. We construct selective matching loss functions by design of increasing
link functions over score domains. A matching loss is an integral over the
link. A link defines loss sensitivity as function of the score, emphasizing
high slope high sensitivity regions over flat ones. Loss asymmetry drives a
model and resolves its underspecification to predict better in high sensitivity
regions where it is more important, and to distinguish between high and low
importance regions. A large variety of selective scalar losses can be designed
with scaled and shifted Sigmoid and hyperbolic sine links. Their properties,
however, do not extend to multi-class. Applying them per dimension lacks
ranking sensitivity that assigns importance according to class score ranking.
Utilizing composite Softmax functions, we develop a framework for
multidimensional selective losses. We overcome limitations of the standard
Softmax function, that is good for classification, but not for distinction
between adjacent scores. Selective losses have substantial advantage over
traditional losses in applications with more important score regions, including
dwell-time prediction, retrieval, ranking with either pointwise, contrastive
pairwise, or listwise losses, distillation problems, and fine-tuning alignment
of Large Language Models (LLMs).

</details>


### [334] [Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning](https://arxiv.org/abs/2506.04454)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: 本文扩展了ODXU框架用于网络入侵检测，引入新方法评估预测可靠性，实验显示其优于传统神经模型；还开发迁移学习策略，消融研究表明最佳配置效果好，元模型UQ方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 提升网络入侵检测系统的鲁棒性、可解释性和泛化能力，探索迁移学习在网络安全领域的潜力。

Method: 扩展ODXU框架，引入分数法和元模型技术评估预测可靠性；开发迁移学习策略，进行消融研究。

Result: 在CIC - IDS - 2017数据集上，ODXU在六项评估指标上优于传统神经模型；在ACI - IoT - 2023数据集上，最佳迁移配置在少量样本训练时也优于传统神经模型，元模型UQ方法表现更好。

Conclusion: 扩展的ODXU框架和迁移学习策略在网络入侵检测中有效，元模型UQ方法更具优势。

Abstract: Network Intrusion Detection Systems (NIDS) play a vital role in protecting
digital infrastructures against increasingly sophisticated cyber threats. In
this paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates
deep embedded clustering for feature extraction, symbolic reasoning using
XGBoost, and comprehensive uncertainty quantification (UQ) to enhance
robustness, interpretability, and generalization in NIDS. The extended ODXU
incorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)
and metamodel-based techniques, including SHAP values and Information Gain, to
assess the reliability of predictions. Experimental results on the CIC-IDS-2017
dataset show that ODXU outperforms traditional neural models across six
evaluation metrics, including classification accuracy and false omission rate.
While transfer learning has seen widespread adoption in fields such as computer
vision and natural language processing, its potential in cybersecurity has not
been thoroughly explored. To bridge this gap, we develop a transfer learning
strategy that enables the reuse of a pre-trained ODXU model on a different
dataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal
transfer configuration involves reusing the pre-trained autoencoder, retraining
the clustering module, and fine-tuning the XGBoost classifier, and outperforms
traditional neural models when trained with as few as 16,000 samples
(approximately 50% of the training data). Additionally, results show that
metamodel-based UQ methods consistently outperform score-based approaches on
both datasets.

</details>


### [335] [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
*Ivan Vegner,Sydelle de Souza,Valentin Forch,Martha Lewis,Leonidas A. A. Doumas*

Main category: cs.LG

TL;DR: 文章强调ML模型系统性中表征系统性与行为系统性的区别，分析关键基准对行为系统性的测试程度，并指出评估表征系统性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准和模型主要关注行为系统性，而Fodor和Pylyshyn强调表征系统性，需区分两者。

Method: 基于Hadley的分类法，分析文献中关键基准对不同领域行为系统性的测试情况。

Result: 明确了表征系统性与行为系统性的差异，分析出关键基准对行为系统性的测试范围。

Conclusion: 强调区分的重要性，并指出可借鉴机制解释领域评估表征系统性的方法。

Abstract: A core aspect of compositionality, systematicity is a desirable property in
ML models as it enables strong generalization to novel contexts. This has led
to numerous studies proposing benchmarks to assess systematic generalization,
as well as models and training regimes designed to enhance it. Many of these
efforts are framed as addressing the challenge posed by Fodor and Pylyshyn.
However, while they argue for systematicity of representations, existing
benchmarks and models primarily focus on the systematicity of behaviour. We
emphasize the crucial nature of this distinction. Furthermore, building on
Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to
which behavioural systematicity is tested by key benchmarks in the literature
across language and vision. Finally, we highlight ways of assessing
systematicity of representations in ML models as practiced in the field of
mechanistic interpretability.

</details>


### [336] [Classifying Dental Care Providers Through Machine Learning with Features Ranking](https://arxiv.org/abs/2506.04474)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh,Mohammed Hasan Abu-Arqoub,Rashiq Rafiq Marie*

Main category: cs.LG

TL;DR: 研究用2018年含24300个实例、20个特征的数据集，用机器学习模型对牙科供应商分类，评估12种模型，神经网络准确率最高，强调特征选择重要性。


<details>
  <summary>Details</summary>
Motivation: 对牙科供应商进行分类，以实现针对服务不足人群的资源定向分配。

Method: 采用特征排名方法（信息增益、基尼指数、ANOVA）确定关键预测因子，用10折交叉验证评估12种机器学习模型，在增量特征子集上测试分类准确性。

Result: 神经网络使用20个特征时准确率最高达94.1%，模型随特征增加性能提升，SGD和集成方法对缺失数据有鲁棒性，治疗服务计数和注释代码对区分供应商类型更重要。

Conclusion: 强调特征选择对提高模型效率和准确性的重要性，提倡将特征排名技术与先进机器学习算法结合优化牙科供应商分类。

Abstract: This study investigates the application of machine learning (ML) models for
classifying dental providers into two categories - standard rendering providers
and safety net clinic (SNC) providers - using a 2018 dataset of 24,300
instances with 20 features. The dataset, characterized by high missing values
(38.1%), includes service counts (preventive, treatment, exams), delivery
systems (FFS, managed care), and beneficiary demographics. Feature ranking
methods such as information gain, Gini index, and ANOVA were employed to
identify critical predictors, revealing treatment-related metrics
(TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models,
including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines
(SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and
Gradient Boosting, were evaluated using 10-fold cross-validation.
Classification accuracy was tested across incremental feature subsets derived
from rankings. The Neural Network achieved the highest accuracy (94.1%) using
all 20 features, followed by Gradient Boosting (93.2%) and Random Forest
(93.0%). Models showed improved performance as more features were incorporated,
with SGD and ensemble methods demonstrating robustness to missing data. Feature
ranking highlighted the dominance of treatment service counts and annotation
codes in distinguishing provider types, while demographic variables (AGE_GROUP,
CALENDAR_YEAR) had minimal impact. The study underscores the importance of
feature selection in enhancing model efficiency and accuracy, particularly in
imbalanced healthcare datasets. These findings advocate for integrating
feature-ranking techniques with advanced ML algorithms to optimize dental
provider classification, enabling targeted resource allocation for underserved
populations.

</details>


### [337] [Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data](https://arxiv.org/abs/2506.04479)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Abdullah Alourani*

Main category: cs.LG

TL;DR: 本文评估机器学习模型对牙科服务提供者分类的表现，发现神经网络和随机森林效果佳，能助力医疗资源分配。


<details>
  <summary>Details</summary>
Motivation: 牙科服务提供者分类对优化医疗资源分配和政策规划至关重要，需评估机器学习模型在该分类中的表现。

Method: 使用2018年包含24300个实例、20个特征的数据集，用多种机器学习算法，采用10折交叉验证，用AUC、CA等指标评估。

Result: 神经网络AUC为0.975、CA为94.1%最高，随机森林次之，二者处理不平衡数据和复杂特征交互能力强，优于传统分类器。

Conclusion: 先进机器学习技术能显著提升牙科人力分类，整合到医疗分析可改善提供者识别和资源分配。

Abstract: Dental provider classification plays a crucial role in optimizing healthcare
resource allocation and policy planning. Effective categorization of providers,
such as standard rendering providers and safety net clinic (SNC) providers,
enhances service delivery to underserved populations. This study aimed to
evaluate the performance of machine learning models in classifying dental
providers using a 2018 dataset. A dataset of 24,300 instances with 20 features
was analyzed, including beneficiary and service counts across fee-for-service
(FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were
categorized by delivery system and patient age groups (0-20 and 21+). Despite
38.1% missing data, multiple machine learning algorithms were tested, including
k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM),
Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient
Boosting. A 10-fold cross-validation approach was applied, and models were
evaluated using AUC, classification accuracy (CA), F1-score, precision, and
recall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%),
followed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively
handled imbalanced data and complex feature interactions, outperforming
traditional classifiers like Logistic Regression and SVM. Advanced machine
learning techniques, particularly ensemble and deep learning models,
significantly enhance dental workforce classification. Their integration into
healthcare analytics can improve provider identification and resource
distribution, benefiting underserved populations.

</details>


### [338] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)
*Jiayu Wang,Aws Albarghouthi,Frederic Sala*

Main category: cs.LG

TL;DR: 本文提出COSMOS统一预测框架，可低成本高效估计大语言模型适应结果，评估显示其预测准确率高且大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在资源受限下，最优选择大语言模型和适应策略具有挑战性且需大量实验，希望能在不进行昂贵试验的情况下准确预测性能和成本。

Method: 将大语言模型策略选择问题形式化，引入COSMOS框架，通过嵌入增强轻量级代理模型预测微调性能、低样本缩放定律预测检索增强上下文学习。

Result: 在八个代表性基准测试中，COSMOS预测准确率高，平均降低92.72%计算成本，在资源密集场景最高降低98.71%。

Conclusion: 高效预测适应结果不仅可行，还能在维持性能标准的同时大幅降低大语言模型部署的计算开销。

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>


### [339] [Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data](https://arxiv.org/abs/2506.04490)
*Rishwanth Raghu,Axel Levy,Gordon Wetzstein,Ellen D. Zhong*

Main category: cs.LG

TL;DR: 提出CryoBoltz方法结合冷冻电镜密度图与蛋白质结构预测模型，为多种动态生物分子系统的异质冷冻电镜图构建原子模型。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构预测模型难以捕捉动态生物分子复合物构象多样性，冷冻电镜从原始实验数据到原子模型的流程繁琐，需结合二者优势。

Method: CryoBoltz方法利用密度图的全局和局部结构约束，引导预训练蛋白质结构预测模型的采样轨迹。

Result: 能为包括转运蛋白和抗体在内的多种动态生物分子系统的异质冷冻电镜图构建原子模型。

Conclusion: 这种灵活且强大的推理时间方法有效结合了两种技术的优势。

Abstract: Protein structure prediction models are now capable of generating accurate 3D
structural hypotheses from sequence alone. However, they routinely fail to
capture the conformational diversity of dynamic biomolecular complexes, often
requiring heuristic MSA subsampling approaches for generating alternative
states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a
powerful tool for imaging near-native structural heterogeneity, but is
challenged by arduous pipelines to go from raw experimental data to atomic
models. Here, we bridge the gap between these modalities, combining cryo-EM
density maps with the rich sequence and biophysical priors learned by protein
structure prediction models. Our method, CryoBoltz, guides the sampling
trajectory of a pretrained protein structure prediction model using both global
and local structural constraints derived from density maps, driving predictions
towards conformational states consistent with the experimental data. We
demonstrate that this flexible yet powerful inference-time approach allows us
to build atomic models into heterogeneous cryo-EM maps across a variety of
dynamic biomolecular systems including transporters and antibodies.

</details>


### [340] [Hierarchical Implicit Neural Emulators](https://arxiv.org/abs/2506.04528)
*Ruoxi Jiang,Xiao Zhang,Karan Jakhar,Peter Y. Lu,Pedram Hassanzadeh,Michael Maire,Rebecca Willett*

Main category: cs.LG

TL;DR: 提出多尺度隐式神经模拟器，提升长期预测精度，在湍流动力学实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决神经PDE求解器在长时间范围内误差累积、稳定性和物理一致性问题。

Method: 引入多尺度隐式神经模拟器，基于低维未来状态表示进行条件处理，借鉴数值隐式时间步长方法的稳定性，利用不同压缩率的未来多步预测进行下一步细化，并主动调整时间下采样率。

Result: 在湍流流体动力学实验中，该方法短期精度高，长期预测稳定，显著优于自回归基线，计算开销小。

Conclusion: 所提出的多尺度隐式神经模拟器能有效提升长期预测准确性，在复杂动力系统建模中有良好应用前景。

Abstract: Neural PDE solvers offer a powerful tool for modeling complex dynamical
systems, but often struggle with error accumulation over long time horizons and
maintaining stability and physical consistency. We introduce a multiscale
implicit neural emulator that enhances long-term prediction accuracy by
conditioning on a hierarchy of lower-dimensional future state representations.
Drawing inspiration from the stability properties of numerical implicit
time-stepping methods, our approach leverages predictions several steps ahead
in time at increasing compression rates for next-timestep refinements. By
actively adjusting the temporal downsampling ratios, our design enables the
model to capture dynamics across multiple granularities and enforce long-range
temporal coherence. Experiments on turbulent fluid dynamics show that our
method achieves high short-term accuracy and produces long-term stable
forecasts, significantly outperforming autoregressive baselines while adding
minimal computational overhead.

</details>


### [341] [HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training](https://arxiv.org/abs/2506.04531)
*Geon-Woo Kim,Junbo Li,Shashidhar Gandham,Omar Baldonado,Adithya Gangidi,Pavan Balaji,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 提出HALoS框架解决地理分布式训练问题，有理论分析和实验结果证明其优势。


<details>
  <summary>Details</summary>
Motivation: 解决地理分布式训练中跨区域通信成本高和硬件利用率不均的问题。

Method: 提出具有本地参数服务器和全局参数服务器的分层异步优化框架HALoS。

Result: 在地理分布式大语言模型训练中比同步基线快7.5倍，比现有异步方法快2.1倍，且保持模型质量。

Conclusion: 分层、服务器端更新积累和全局模型合并是异构、地理分布式环境中训练大语言模型的有力工具。

Abstract: Training large language models (LLMs) increasingly relies on geographically
distributed accelerators, causing prohibitive communication costs across
regions and uneven utilization of heterogeneous hardware. We propose HALoS, a
hierarchical asynchronous optimization framework that tackles these issues by
introducing local parameter servers (LPSs) within each region and a global
parameter server (GPS) that merges updates across regions. This hierarchical
design minimizes expensive inter-region communication, reduces straggler
effects, and leverages fast intra-region links. We provide a rigorous
convergence analysis for HALoS under non-convex objectives, including
theoretical guarantees on the role of hierarchical momentum in asynchronous
training. Empirically, HALoS attains up to 7.5x faster convergence than
synchronous baselines in geo-distributed LLM training and improves upon
existing asynchronous methods by up to 2.1x. Crucially, HALoS preserves the
model quality of fully synchronous SGD-matching or exceeding accuracy on
standard language modeling and downstream benchmarks-while substantially
lowering total training time. These results demonstrate that hierarchical,
server-side update accumulation and global model merging are powerful tools for
scalable, efficient training of new-era LLMs in heterogeneous, geo-distributed
environments.

</details>


### [342] [NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models](https://arxiv.org/abs/2506.04536)
*Luca Ghafourpour,Valentin Duruisseaux,Bahareh Tolooshams,Philip H. Wong,Costas A. Anastassiou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 介绍NOBLE框架解决现有神经元建模方法局限，可高效生成合成神经元并加速计算。


<details>
  <summary>Details</summary>
Motivation: 当前神经元建模方法受实验数据限制，传统确定性模型难扩展，深度学习无法捕捉神经元生物物理复杂性和变异性。

Method: 引入NOBLE框架，从可解释神经元特征的连续调频嵌入映射到电流注入引起的体细胞电压响应，在生物物理逼真神经元模型生成的数据上训练。

Result: NOBLE能预测考虑实验变异性的神经动力学分布，是首个在真实实验数据上验证的深度学习框架，生成的合成神经元有试验间变异性，比数值求解器快4200倍。

Conclusion: NOBLE捕捉了基本神经特性，有助于理解细胞组成和计算、神经形态架构、大规模脑回路和神经AI应用。

Abstract: Characterizing the diverse computational properties of human neurons via
multimodal electrophysiological, transcriptomic, and morphological data
provides the foundation for constructing and validating bio-realistic neuron
models that can advance our understanding of fundamental mechanisms underlying
brain function. However, current modeling approaches remain constrained by the
limited availability and intrinsic variability of experimental neuronal data.
To capture variability, ensembles of deterministic models are often used, but
are difficult to scale as model generation requires repeating computationally
expensive optimization for each neuron. While deep learning is becoming
increasingly relevant in this space, it fails to capture the full biophysical
complexity of neurons, their nonlinear voltage dynamics, and variability. To
address these shortcomings, we introduce NOBLE, a neural operator framework
that learns a mapping from a continuous frequency-modulated embedding of
interpretable neuron features to the somatic voltage response induced by
current injection. Trained on data generated from biophysically realistic
neuron models, NOBLE predicts distributions of neural dynamics accounting for
the intrinsic experimental variability. Unlike conventional bio-realistic
neuron models, interpolating within the embedding space offers models whose
dynamics are consistent with experimentally observed responses. NOBLE is the
first scaled-up deep learning framework validated on real experimental data,
enabling efficient generation of synthetic neurons that exhibit trial-to-trial
variability and achieve a $4200\times$ speedup over numerical solvers. To this
end, NOBLE captures fundamental neural properties, opening the door to a better
understanding of cellular composition and computations, neuromorphic
architectures, large-scale brain circuits, and general neuroAI applications.

</details>


### [343] [Neural MJD: Neural Non-Stationary Merton Jump Diffusion for Time Series Prediction](https://arxiv.org/abs/2506.04542)
*Yuanpei Gao,Qi Yan,Yan Leng,Renjie Liao*

Main category: cs.LG

TL;DR: 提出Neural MJD模型用于非平稳时间序列预测，结合随机微分方程模拟，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在非平稳时间序列预测存在黑盒性和无法显式建模随机过程的局限，尤其面对突变数据泛化能力不足。

Method: 引入Neural MJD模型，将预测问题公式化为随机微分方程模拟，结合时间非齐次伊藤扩散和复合泊松过程，引入似然截断机制，提出带重启的欧拉 - 马尔可夫求解器。

Result: 在合成和真实数据集实验中，Neural MJD始终优于最先进的深度学习和统计学习方法。

Conclusion: Neural MJD模型在非平稳时间序列预测上有良好效果，能解决现有方法的局限。

Abstract: While deep learning methods have achieved strong performance in time series
prediction, their black-box nature and inability to explicitly model underlying
stochastic processes often limit their generalization to non-stationary data,
especially in the presence of abrupt changes. In this work, we introduce Neural
MJD, a neural network based non-stationary Merton jump diffusion (MJD) model.
Our model explicitly formulates forecasting as a stochastic differential
equation (SDE) simulation problem, combining a time-inhomogeneous It\^o
diffusion to capture non-stationary stochastic dynamics with a
time-inhomogeneous compound Poisson process to model abrupt jumps. To enable
tractable learning, we introduce a likelihood truncation mechanism that caps
the number of jumps within small time intervals and provide a theoretical error
bound for this approximation. Additionally, we propose an Euler-Maruyama with
restart solver, which achieves a provably lower error bound in estimating
expected states and reduced variance compared to the standard solver.
Experiments on both synthetic and real-world datasets demonstrate that Neural
MJD consistently outperforms state-of-the-art deep learning and statistical
learning methods.

</details>


### [344] [Communication Efficient Adaptive Model-Driven Quantum Federated Learning](https://arxiv.org/abs/2506.04548)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 文章提出mdQFL算法解决量子联邦学习中大规模数据、大量设备和非IID数据分布问题，实验显示通信成本降近50%，模型精度有保障。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中使用大量数据集和设备带来的训练瓶颈，以及多客户端异质性影响系统性能的问题。

Method: 引入模型驱动的量子联邦学习算法（mdQFL），在Qiskit环境下用不同数据集对其进行实验评估，还进行理论分析。

Result: mdQFL使总通信成本降低近50%，最终模型精度维持或提高，本地模型训练效果持续提升。

Conclusion: mdQFL算法高效且适应多种因素，可用于其他联邦学习场景，研究还对QFL和mdQFL算法及影响因素进行了深入探索。

Abstract: Training with huge datasets and a large number of participating devices leads
to bottlenecks in federated learning (FL). Furthermore, the challenges of
heterogeneity between multiple FL clients affect the overall performance of the
system. In a quantum federated learning (QFL) context, we address these three
main challenges: i) training bottlenecks from massive datasets, ii) the
involvement of a substantial number of devices, and iii) non-IID data
distributions. We introduce a model-driven quantum federated learning algorithm
(mdQFL) to tackle these challenges. Our proposed approach is efficient and
adaptable to various factors, including different numbers of devices. To the
best of our knowledge, it is the first to explore training and update
personalization, as well as test generalization within a QFL setting, which can
be applied to other FL scenarios. We evaluated the efficiency of the proposed
mdQFL framework through extensive experiments under diverse non-IID data
heterogeneity conditions using various datasets within the Qiskit environment.
Our results demonstrate a nearly 50% decrease in total communication costs
while maintaining or, in some cases, exceeding the accuracy of the final model
and consistently improving local model training compared to the standard QFL
baseline. Moreover, our experimental evaluation thoroughly explores the QFL and
mdQFL algorithms, along with several influencing factors. In addition, we
present a theoretical analysis to clarify the complexities of the proposed
algorithm. The experimental code is available at 1.

</details>


### [345] [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)
*Kareem Amin,Salman Avestimehr,Sara Babakniya,Alex Bie,Weiwei Kong,Natalia Ponomareva,Umar Syed*

Main category: cs.LG

TL;DR: 论文提出改进差分隐私语言模型推理方法，通过聚类输入数据和用中位数聚合统计，以更低隐私成本生成高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有工作中均匀采样敏感输入会降低私有生成文本质量的问题，尤其是敏感示例涉及异构主题时。

Method: 对输入数据进行聚类后选择推理批次，并引入新算法，通过私下计算中位数而非平均值来聚合下一令牌统计信息。

Result: 在代表性指标和下游任务性能上有改进，以显著更低的隐私成本产生高质量合成数据。

Conclusion: 提出的方法比之前的最先进方法更优，能以更低隐私成本生成高质量合成数据。

Abstract: Differentially private (DP) language model inference is an approach for
generating private synthetic text. A sensitive input example is used to prompt
an off-the-shelf large language model (LLM) to produce a similar example.
Multiple examples can be aggregated together to formally satisfy the DP
guarantee.
  Prior work creates inference batches by sampling sensitive inputs uniformly
at random. We show that uniform sampling degrades the quality of privately
generated text, especially when the sensitive examples concern heterogeneous
topics.
  We remedy this problem by clustering the input data before selecting
inference batches. Next, we observe that clustering also leads to more similar
next-token predictions across inferences. We use this insight to introduce a
new algorithm that aggregates next token statistics by privately computing
medians instead of averages. This approach leverages the fact that the median
has decreased local sensitivity when next token predictions are similar,
allowing us to state a data-dependent and ex-post DP guarantee about the
privacy properties of this algorithm. Finally, we demonstrate improvements in
terms of representativeness metrics (e.g., MAUVE) as well as downstream task
performance. We show that our method produces high-quality synthetic data at
significantly lower privacy cost than a previous state-of-the-art method.

</details>


### [346] [StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation](https://arxiv.org/abs/2506.04567)
*Ranjith Merugu,Bryan Bo Cao,Shubham Jain*

Main category: cs.LG

TL;DR: 提出轻量级模型合并方法StatsMerging，经实验验证其效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在有限内存预算下容纳多个大模型，需要有效的模型合并方法。

Method: 提出StatsMerging方法，利用SVD奇异值、StatsMergeLearner建模及任务特定教师蒸馏进行模型合并。

Result: 在八个任务的实验中，StatsMerging在整体准确率、对未见任务的泛化能力和对图像质量变化的鲁棒性方面优于现有技术。

Conclusion: StatsMerging是一种有效的模型合并方法。

Abstract: Model merging has emerged as a promising solution to accommodate multiple
large models within constrained memory budgets. We present StatsMerging, a
novel lightweight learning-based model merging method guided by weight
distribution statistics without requiring ground truth labels or test samples.
StatsMerging offers three key advantages: (1) It uniquely leverages singular
values from singular value decomposition (SVD) to capture task-specific weight
distributions, serving as a proxy for task importance to guide task coefficient
prediction; (2) It employs a lightweight learner StatsMergeLearner to model the
weight distributions of task-specific pre-trained models, improving
generalization and enhancing adaptation to unseen samples; (3) It introduces
Task-Specific Teacher Distillation for merging vision models with heterogeneous
architectures, a merging learning paradigm that avoids costly ground-truth
labels by task-specific teacher distillation. Notably, we present two types of
knowledge distillation, (a) distilling knowledge from task-specific models to
StatsMergeLearner; and (b) distilling knowledge from models with heterogeneous
architectures prior to merging. Extensive experiments across eight tasks
demonstrate the effectiveness of StatsMerging. Our results show that
StatsMerging outperforms state-of-the-art techniques in terms of overall
accuracy, generalization to unseen tasks, and robustness to image quality
variations.

</details>


### [347] [Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets](https://arxiv.org/abs/2506.04598)
*Marianna Nezhurina,Tomer Porian,Giovanni Pucceti,Tommie Kerssies,Romain Beaumont,Mehdi Cherti,Jenia Jitsev*

Main category: cs.LG

TL;DR: 本文提出用缩放定律进行模型和数据集比较，推导CLIP和MaMMUT的缩放定律进行对比，还展示下游任务和不同数据集的缩放定律，降低计算成本，最后发布预训练模型和代码。


<details>
  <summary>Details</summary>
Motivation: 在可迁移学习研究中，用缩放定律推导进行模型和数据集比较，以决定预训练的首选方法。

Method: 推导CLIP和MaMMUT基于密集测量的完整缩放定律，确保对保留点的预测准确性，还展示下游任务和不同开放数据集的缩放定律，用恒定学习率推导缩放定律。

Result: MaMMUT随规模提升更强，样本效率优于标准CLIP；不同下游任务和数据集有一致趋势；恒定学习率推导可降低计算成本；openMaMMUT - L/14在ImageNet - 1k上零样本准确率达80.3%。

Conclusion: 准确推导缩放定律可跨规模进行模型和数据集比较，避免单一参考规模测量的误导，为开放基础模型和数据集的系统比较和改进铺平道路。

Abstract: In studies of transferable learning, scaling laws are obtained for various
important foundation models to predict their properties and performance at
larger scales. We show here how scaling law derivation can also be used for
model and dataset comparison, allowing to decide which procedure is to be
preferred for pre-training. For the first time, full scaling laws based on
dense measurements across a wide span of model and samples seen scales are
derived for two important language-vision learning procedures, CLIP and MaMMUT,
that use either contrastive only or contrastive and captioning text generative
loss. Ensuring sufficient prediction accuracy for held out points, we use
derived scaling laws to compare both models, obtaining evidence for MaMMUT's
stronger improvement with scale and better sample efficiency than standard
CLIP. To strengthen validity of the comparison, we show scaling laws for
various downstream tasks, classification, retrieval, and segmentation, and for
different open datasets, DataComp, DFN and Re-LAION, observing consistently the
same trends. We show that comparison can also be performed when deriving
scaling laws with a constant learning rate schedule, reducing compute cost.
Accurate derivation of scaling laws provides thus means to perform model and
dataset comparison across scale spans, avoiding misleading conclusions based on
measurements from single reference scales only, paving the road for systematic
comparison and improvement of open foundation models and datasets for their
creation. We release all the pre-trained models with their intermediate
checkpoints, including openMaMMUT-L/14, which achieves $80.3\%$ zero-shot
ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for
reproducing experiments in the paper and raw experiments data can be found at
https://github.com/LAION-AI/scaling-laws-for-comparison.

</details>


### [348] [Ignoring Directionality Leads to Compromised Graph Neural Network Explanations](https://arxiv.org/abs/2506.04608)
*Changsheng Sun,Xinke Li,Jin Song Dong*

Main category: cs.LG

TL;DR: 图神经网络（GNN）应用需可靠解释，但图对称化做法会造成信息损失和解释误导，保留方向语义可提升解释质量，强调安全关键应用中需方向感知的GNN可解释性。


<details>
  <summary>Details</summary>
Motivation: GNN在关键领域应用时需要可靠解释支持人类决策，但常见图对称化做法会造成信息损失和解释误导。

Method: 通过理论和实证研究。

Result: 保留方向语义可显著提高解释质量，为人类决策者提供更忠实的见解。

Conclusion: 安全关键应用中需要方向感知的GNN可解释性。

Abstract: Graph Neural Networks (GNNs) are increasingly used in critical domains, where
reliable explanations are vital for supporting human decision-making. However,
the common practice of graph symmetrization discards directional information,
leading to significant information loss and misleading explanations. Our
analysis demonstrates how this practice compromises explanation fidelity.
Through theoretical and empirical studies, we show that preserving directional
semantics significantly improves explanation quality, ensuring more faithful
insights for human decision-makers. These findings highlight the need for
direction-aware GNN explainability in security-critical applications.

</details>


### [349] [Exploring bidirectional bounds for minimax-training of Energy-based models](https://arxiv.org/abs/2506.04609)
*Cong Geng,Jia Wang,Li Chen,Zhiyong Gao,Jes Frellsen,Søren Hauberg*

Main category: cs.LG

TL;DR: 本文提出用双向边界训练基于能量的模型（EBM）以避免不稳定问题，研究四种不同边界，比较不同方法优缺点，证明双向边界可稳定EBM训练。


<details>
  <summary>Details</summary>
Motivation: EBM训练困难，现有通过最小化变分下界的训练方法存在不稳定性问题。

Method: 采用双向边界训练EBM，即最大化下界并最小化上界，研究基于不同视角的四种对数似然边界并给出评估算法。

Result: 双向边界稳定了EBM训练，实现了高质量的密度估计和样本生成。

Conclusion: 使用双向边界训练EBM是一种有效的方法，可解决训练不稳定问题。

Abstract: Energy-based models (EBMs) estimate unnormalized densities in an elegant
framework, but they are generally difficult to train. Recent work has linked
EBMs to generative adversarial networks, by noting that they can be trained
through a minimax game using a variational lower bound. To avoid the
instabilities caused by minimizing a lower bound, we propose to instead work
with bidirectional bounds, meaning that we maximize a lower bound and minimize
an upper bound when training the EBM. We investigate four different bounds on
the log-likelihood derived from different perspectives. We derive lower bounds
based on the singular values of the generator Jacobian and on mutual
information. To upper bound the negative log-likelihood, we consider a gradient
penalty-like bound, as well as one based on diffusion processes. In all cases,
we provide algorithms for evaluating the bounds. We compare the different
bounds to investigate, the pros and cons of the different approaches. Finally,
we demonstrate that the use of bidirectional bounds stabilizes EBM training and
yields high-quality density estimation and sample generation.

</details>


### [350] [Composing Agents to Minimize Worst-case Risk](https://arxiv.org/abs/2506.04632)
*Guruprerana Shabadi,Rajeev Alur*

Main category: cs.LG

TL;DR: 本文将代理工作流形式化为代理图，考虑在可行代理组合中最小化最坏情况风险，提出高效算法寻找近似最优代理组合，并在视频游戏类控制基准中验证算法有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界部署代理系统时，需要选择既能最大化任务成功率又能最小化风险的代理组合，同时需分析代理组合的低概率（尾部）行为。

Method: 将代理工作流形式化为有向无环图（代理图），定义最坏情况风险为代理组合损失分布的尾部分位数（风险价值），通过联合界和动态规划近似风险价值，引入高效算法遍历代理图寻找近似最优代理组合。

Result: 证明了对于广泛的实际损失函数，该近似在渐近意义下是近似最优的，并在视频游戏类控制基准中展示了算法在近似风险价值和识别最优代理组合方面的有效性。

Conclusion: 所提出的算法能够有效解决在可行代理组合中最小化最坏情况风险的问题，为实际应用提供了一种高效的解决方案。

Abstract: From software development to robot control, modern agentic systems decompose
complex objectives into a sequence of subtasks and choose a set of specialized
AI agents to complete them. We formalize an agentic workflow as a directed
acyclic graph, called an agent graph, where edges represent AI agents and paths
correspond to feasible compositions of agents. When deploying these systems in
the real world, we need to choose compositions of agents that not only maximize
the task success, but also minimize risk where the risk captures requirements
like safety, fairness, and privacy. This additionally requires carefully
analyzing the low-probability (tail) behaviors of compositions of agents. In
this work, we consider worst-case risk minimization over the set of feasible
agent compositions. We define worst-case risk as the tail quantile -- also
known as value-at-risk -- of the loss distribution of the agent composition
where the loss quantifies the risk associated with agent behaviors. We
introduce an efficient algorithm that traverses the agent graph and finds a
near-optimal composition of agents by approximating the value-at-risk via a
union bound and dynamic programming. Furthermore, we prove that the
approximation is near-optimal asymptotically for a broad class of practical
loss functions. To evaluate our framework, we consider a suite of video
game-like control benchmarks that require composing several agents trained with
reinforcement learning and demonstrate our algorithm's effectiveness in
approximating the value-at-risk and identifying the optimal agent composition.

</details>


### [351] [Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction](https://arxiv.org/abs/2506.04650)
*Zesheng Ye,Chengyi Cai,Ruijiang Dong,Jianzhong Qi,Lei Feng,Pin-Yu Chen,Feng Liu*

Main category: cs.LG

TL;DR: 文章提出神经网络可重编程作为统一框架连接主流模型适配技术，给出分类法，分析挑战、伦理考量并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练基础模型不断发展，需高效适配下游任务，但现有适配方法孤立，缺乏相互关系的理解。

Method: 引入神经网络可重编程框架，基于信息操纵适配模型，从操作格式、位置、算子和输出对齐要求四个维度分类。

Result: 框架适用于不同数据模态，揭示已有技术理论联系和实际区别。

Conclusion: 神经网络可重编程是高效模型适配的基础范式，指出了有前景的研究方向。

Abstract: As large-scale pre-trained foundation models continue to expand in size and
capability, efficiently adapting them to specific downstream tasks has become
increasingly critical. Despite substantial progress, existing adaptation
approaches have evolved largely in isolation, without a clear understanding of
their interrelationships. This survey introduces neural network
reprogrammability as a unifying framework that bridges mainstream model
adaptation techniques--model reprogramming, prompt tuning, and prompt
instruction--previously fragmented research areas yet converges on a shared
principle: repurposing a pre-trained model by manipulating information at the
interfaces while keeping the model parameters frozen. These methods exploit
neural networks' sensitivity to manipulation on different interfaces, be it
through perturbing inputs, inserting tokens into intermediate layers, or
providing task-specific examples in context, to redirect model behaviors
towards desired outcomes. We then present a taxonomy that categorizes such
information manipulation-based adaptation approaches across four key
dimensions: manipulation format (fixed or learnable), location (interfaces
where manipulations occur), operator (how they are applied), and output
alignment requirement (post-processing needed to align outputs with downstream
tasks). Notably, this framework applies consistently across data modalities,
independent of specific model architectures. Moreover, viewing established
techniques like in-context learning and chain-of-thought prompting through this
lens reveals both their theoretical connections and practical distinctions. We
further analyze remaining technical challenges and ethical considerations,
positioning neural network reprogrammability as a fundamental paradigm for
efficient model adaptation. We lastly identify promising research directions
emerging from this integrative viewpoint.

</details>


### [352] [The Oversmoothing Fallacy: A Misguided Narrative in GNN Research](https://arxiv.org/abs/2506.04653)
*MoonJeong Park,Sunghyun Choi,Jaeseung Heo,Eunhyeok Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: 本文认为过度平滑对深度图神经网络（GNN）的影响被夸大，通过分析GNN核心操作挑战以往认知，并证明经典方法可成功堆叠深层GNN。


<details>
  <summary>Details</summary>
Motivation: 解决过度平滑对构建深度GNN的阻碍，探索深度GNN架构的潜力。

Method: 分析GNN的聚合、线性变换和非线性激活三个核心操作，对比过度平滑和梯度消失的成因。

Result: 发现以往研究将过度平滑与梯度消失混淆，经典方法如跳跃连接和归一化可成功堆叠深层GNN而不降低性能。

Conclusion: 澄清了过度平滑的误解，为深度GNN的潜力带来新见解。

Abstract: Oversmoothing has been recognized as a main obstacle to building deep Graph
Neural Networks (GNNs), limiting the performance. This position paper argues
that the influence of oversmoothing has been overstated and advocates for a
further exploration of deep GNN architectures. Given the three core operations
of GNNs, aggregation, linear transformation, and non-linear activation, we show
that prior studies have mistakenly confused oversmoothing with the vanishing
gradient, caused by transformation and activation rather than aggregation. Our
finding challenges prior beliefs about oversmoothing being unique to GNNs.
Furthermore, we demonstrate that classical solutions such as skip connections
and normalization enable the successful stacking of deep GNN layers without
performance degradation. Our results clarify misconceptions about oversmoothing
and shed new light on the potential of deep GNNs.

</details>


### [353] [Noise-Resistant Label Reconstruction Feature Selection for Partial Multi-Label Learning](https://arxiv.org/abs/2506.04669)
*Wanfu Gao,Hanlin Pan,Qingqi Han,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出考虑数据集标签关系抗噪性和标签连通性的PML特征选择方法，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决部分多标签学习中‘维数灾难’问题，现有方法基于低秩假设难满足实际且识别正标签能力差。

Method: 利用标签关系抗噪性消除标签歧义，通过改进低秩假设设计学习过程，利用标签连通性找到代表性标签并重构权重矩阵选择特征。

Result: 在基准数据集上实验，证明所提方法的优越性。

Conclusion: 所提考虑标签关系抗噪性和标签连通性的PML特征选择方法有效。

Abstract: The "Curse of dimensionality" is prevalent across various data patterns,
which increases the risk of model overfitting and leads to a decline in model
classification performance. However, few studies have focused on this issue in
Partial Multi-label Learning (PML), where each sample is associated with a set
of candidate labels, at least one of which is correct. Existing PML methods
addressing this problem are mainly based on the low-rank assumption. However,
low-rank assumption is difficult to be satisfied in practical situations and
may lead to loss of high-dimensional information. Furthermore, we find that
existing methods have poor ability to identify positive labels, which is
important in real-world scenarios. In this paper, a PML feature selection
method is proposed considering two important characteristics of dataset: label
relationship's noise-resistance and label connectivity. Our proposed method
utilizes label relationship's noise-resistance to disambiguate labels. Then the
learning process is designed through the reformed low-rank assumption. Finally,
representative labels are found through label connectivity, and the weight
matrix is reconstructed to select features with strong identification ability
to these labels. The experimental results on benchmark datasets demonstrate the
superiority of the proposed method.

</details>


### [354] [FedAPM: Federated Learning via ADMM with Partial Model Personalization](https://arxiv.org/abs/2506.04672)
*Shengkun Zhu,Feiteng Nie,Jinshan Zeng,Sheng Wang,Yuan Sun,Yuan Yao,Shangfeng Chen,Quanqing Xu,Chuanhui Yang*

Main category: cs.LG

TL;DR: 论文指出联邦学习中部分模型个性化会加剧客户端漂移，提出基于ADMM的FedAPM框架缓解该问题，分析其收敛性并通过实验验证性能优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据集非独立同分布、多模态问题，缓解部分模型个性化导致的客户端漂移，提高算法有效性和效率。

Method: 提出基于ADMM的FedAPM框架，构建包含一阶和二阶近端项的增广拉格朗日函数。

Result: FedAPM收敛更稳定高效，能从任意初始点全局收敛到驻点，在四个异构多模态数据集上实验显示，收敛更快更准确，测试准确率、F1分数和AUC平均提升分别达12.3%、16.4%和18.0%，且通信轮数更少。

Conclusion: FedAPM在缓解客户端漂移、提高联邦学习性能方面优于现有方法。

Abstract: In federated learning (FL), the assumption that datasets from different
devices are independent and identically distributed (i.i.d.) often does not
hold due to user differences, and the presence of various data modalities
across clients makes using a single model impractical. Personalizing certain
parts of the model can effectively address these issues by allowing those parts
to differ across clients, while the remaining parts serve as a shared model.
However, we found that partial model personalization may exacerbate client
drift (each client's local model diverges from the shared model), thereby
reducing the effectiveness and efficiency of FL algorithms. We propose an FL
framework based on the alternating direction method of multipliers (ADMM),
referred to as FedAPM, to mitigate client drift. We construct the augmented
Lagrangian function by incorporating first-order and second-order proximal
terms into the objective, with the second-order term providing fixed correction
and the first-order term offering compensatory correction between the local and
shared models. Our analysis demonstrates that FedAPM, by using explicit
estimates of the Lagrange multiplier, is more stable and efficient in terms of
convergence compared to other FL frameworks. We establish the global
convergence of FedAPM training from arbitrary initial points to a stationary
point, achieving three types of rates: constant, linear, and sublinear, under
mild assumptions. We conduct experiments using four heterogeneous and
multimodal datasets with different metrics to validate the performance of
FedAPM. Specifically, FedAPM achieves faster and more accurate convergence,
outperforming the SOTA methods with average improvements of 12.3% in test
accuracy, 16.4% in F1 score, and 18.0% in AUC while requiring fewer
communication rounds.

</details>


### [355] [The cost of ensembling: is it always worth combining?](https://arxiv.org/abs/2506.04677)
*Marco Zanotti*

Main category: cs.LG

TL;DR: 本文评估不同模型和集成配置在大规模零售数据集上的表现，发现集成可提升预测性能但有计算成本，降低重训练频率能降成本，小集成常能达接近最优结果。


<details>
  <summary>Details</summary>
Motivation: 在数据集规模和预测模型复杂度增加背景下，研究时间序列预测集成学习中预测准确性和计算成本的权衡。

Method: 评估十种基础模型和八种集成配置在两个大规模零售数据集（M5和VN1）上的表现，考虑不同重训练频率下的点预测和概率预测准确性。

Result: 集成能提升预测性能，尤其在概率预测场景，但有较高计算成本；降低重训练频率可大幅降低成本且对准确性影响小；效率驱动的集成能平衡准确性和成本；两三个模型的小集成常能达接近最优结果。

Conclusion: 精心设计集成和选择重训练策略可实现适用于实际应用的准确、稳健且具成本效益的预测。

Abstract: Given the continuous increase in dataset sizes and the complexity of
forecasting models, the trade-off between forecast accuracy and computational
cost is emerging as an extremely relevant topic, especially in the context of
ensemble learning for time series forecasting. To asses it, we evaluated ten
base models and eight ensemble configurations across two large-scale retail
datasets (M5 and VN1), considering both point and probabilistic accuracy under
varying retraining frequencies. We showed that ensembles consistently improve
forecasting performance, particularly in probabilistic settings. However, these
gains come at a substantial computational cost, especially for larger,
accuracy-driven ensembles. We found that reducing retraining frequency
significantly lowers costs, with minimal impact on accuracy, particularly for
point forecasts. Moreover, efficiency-driven ensembles offer a strong balance,
achieving competitive accuracy with considerably lower costs compared to
accuracy-optimized combinations. Most importantly, small ensembles of two or
three models are often sufficient to achieve near-optimal results. These
findings provide practical guidelines for deploying scalable and cost-efficient
forecasting systems, supporting the broader goals of sustainable AI in
forecasting. Overall, this work shows that careful ensemble design and
retraining strategy selection can yield accurate, robust, and cost-effective
forecasts suitable for real-world applications.

</details>


### [356] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: 介绍了具有严格差分隐私保证的生成大语言模型聊天机器人交互见解的Urania框架，评估显示其能平衡数据效用和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 生成大语言模型聊天机器人交互见解并提供严格的差分隐私保证。

Method: 采用私有聚类机制和创新关键词提取方法，利用聚类、分区选择和基于直方图的汇总等差分隐私工具。

Result: 框架能提取有意义的对话见解，同时保持严格的用户隐私，平衡了数据效用和隐私保护。

Conclusion: Urania框架在提取见解的同时有效保护用户隐私，实现了数据效用和隐私保护的平衡。

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [357] [Towards Better Generalization via Distributional Input Projection Network](https://arxiv.org/abs/2506.04690)
*Yifan Hao,Yanxin Lu,Xinwei Shen,Tong Zhang*

Main category: cs.LG

TL;DR: 提出DIPNet框架解决过参数化模型泛化性能问题，理论分析证明其作用，实验验证有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 过参数化模型中仅训练损失难以反映泛化性能，直接在神经网络中强制平滑性有挑战。

Method: 引入DIPNet框架，将输入投影到每层可学习分布，诱导更平滑损失景观。

Result: 理论上减少局部平滑度量和网络Lipschitz常数，实验上在多种架构和任务中提升测试性能。

Conclusion: 提出的输入投影策略可集成到现有模型，是提升现代深度学习泛化性能的通用有效方法。

Abstract: As overparameterized models become increasingly prevalent, training loss
alone offers limited insight into generalization performance. While smoothness
has been linked to improved generalization across various settings, directly
enforcing smoothness in neural networks remains challenging. To address this,
we introduce Distributional Input Projection Networks (DIPNet), a novel
framework that projects inputs into learnable distributions at each layer. This
distributional representation induces a smoother loss landscape with respect to
the input, promoting better generalization. We provide theoretical analysis
showing that DIPNet reduces both local smoothness measures and the Lipschitz
constant of the network, contributing to improved generalization performance.
Empirically, we validate DIPNet across a wide range of architectures and tasks,
including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and
MLPs. Our method consistently enhances test performance under standard
settings, adversarial attacks, out-of-distribution inputs, and reasoning
benchmarks. We demonstrate that the proposed input projection strategy can be
seamlessly integrated into existing models, providing a general and effective
approach for boosting generalization performance in modern deep learning.

</details>


### [358] [Influence Functions for Edge Edits in Non-Convex Graph Neural Networks](https://arxiv.org/abs/2506.04694)
*Jaeseung Heo,Kyeongheung Yun,Seokwon Yoon,MoonJeong Park,Jungseul Ok,Dongwoo Kim*

Main category: cs.LG

TL;DR: 提出适用于GNN的近端Bregman响应函数，改进图影响预测，实验证明其准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 现有图影响预测方法有依赖严格凸性假设、忽略边插入、未捕捉消息传播变化等问题，需改进以提升GNN解释性和鲁棒性。

Method: 提出适用于GNN的近端Bregman响应函数，放宽凸性要求，考虑消息传播效应，扩展到边删除和插入。

Result: 在真实数据集实验中实现对GNN不同特征的准确影响预测。

Conclusion: 所提影响函数在图重连和对抗攻击等应用中具有多功能性。

Abstract: Understanding how individual edges influence the behavior of graph neural
networks (GNNs) is essential for improving their interpretability and
robustness. Graph influence functions have emerged as promising tools to
efficiently estimate the effects of edge deletions without retraining. However,
existing influence prediction methods rely on strict convexity assumptions,
exclusively consider the influence of edge deletions while disregarding edge
insertions, and fail to capture changes in message propagation caused by these
modifications. In this work, we propose a proximal Bregman response function
specifically tailored for GNNs, relaxing the convexity requirement and enabling
accurate influence prediction for standard neural network architectures.
Furthermore, our method explicitly accounts for message propagation effects and
extends influence prediction to both edge deletions and insertions in a
principled way. Experiments with real-world datasets demonstrate accurate
influence predictions for different characteristics of GNNs. We further
demonstrate that the influence function is versatile in applications such as
graph rewiring and adversarial attacks.

</details>


### [359] [Enhanced Drought Analysis in Bangladesh: A Machine Learning Approach for Severity Classification Using Satellite Data](https://arxiv.org/abs/2506.04696)
*Tonmoy Paul,Mrittika Devi Mati,Md. Mahmudul Islam*

Main category: cs.LG

TL;DR: 本文提出卫星数据驱动的机器学习框架对孟加拉国38个地区进行干旱分类，整合2012 - 2024年气象数据，成功划分干旱严重程度，体现了机器学习模型在干旱识别和预测方面的能力。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国受干旱影响大，传统干旱指数有局限，当前机器学习模型在该国应用不足，缺乏多地区卫星数据整合。

Method: 提出卫星数据驱动的机器学习框架，先用无监督算法（K - means、贝叶斯高斯混合）聚类，再用分类模型（KNN、随机森林、决策树、朴素贝叶斯），整合2012 - 2024年气象数据。

Result: 成功将干旱严重程度分类，各地区干旱脆弱性有显著差异。

Conclusion: 机器学习模型在识别和预测干旱条件方面有能力。

Abstract: Drought poses a pervasive environmental challenge in Bangladesh, impacting
agriculture, socio-economic stability, and food security due to its unique
geographic and anthropogenic vulnerabilities. Traditional drought indices, such
as the Standardized Precipitation Index (SPI) and Palmer Drought Severity Index
(PDSI), often overlook crucial factors like soil moisture and temperature,
limiting their resolution. Moreover, current machine learning models applied to
drought prediction have been underexplored in the context of Bangladesh,
lacking a comprehensive integration of satellite data across multiple
districts. To address these gaps, we propose a satellite data-driven machine
learning framework to classify drought across 38 districts of Bangladesh. Using
unsupervised algorithms like K-means and Bayesian Gaussian Mixture for
clustering, followed by classification models such as KNN, Random Forest,
Decision Tree, and Naive Bayes, the framework integrates weather data
(humidity, soil moisture, temperature) from 2012-2024. This approach
successfully classifies drought severity into different levels. However, it
shows significant variabilities in drought vulnerabilities across regions which
highlights the aptitude of machine learning models in terms of identifying and
predicting drought conditions.

</details>


### [360] [UNO: Unlearning via Orthogonalization in Generative models](https://arxiv.org/abs/2506.04712)
*Pinak Mandal,Georg A. Gottwald*

Main category: cs.LG

TL;DR: 提出基于损失梯度正交化的快速遗忘算法，在MNIST和CelebA数据上比先前算法快得多。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型发展，出于隐私、法律等需求，选择性移除特定数据影响且无需从头训练很重要。

Method: 提出基于损失梯度正交化的快速遗忘算法。

Result: 算法能在遗忘数据同时保持原模型保真度，在MNIST和CelebA数据上比梯度手术等先前算法的遗忘时间快几个数量级。

Conclusion: 所提算法在遗忘数据时有效且可靠，能满足多方面要求。

Abstract: As generative models become increasingly powerful and pervasive, the ability
to unlearn specific data, whether due to privacy concerns, legal requirements,
or the correction of harmful content, has become increasingly important. Unlike
in conventional training, where data are accumulated and knowledge is
reinforced, unlearning aims to selectively remove the influence of particular
data points without costly retraining from scratch. To be effective and
reliable, such algorithms need to achieve (i) forgetting of the undesired data,
(ii) preservation of the quality of the generation, (iii) preservation of the
influence of the desired training data on the model parameters, and (iv) small
number of training steps. We propose fast unlearning algorithms based on loss
gradient orthogonalization. We show that our algorithms are able to forget data
while maintaining the fidelity of the original model. Using MNIST and CelebA
data, we demonstrate that our algorithms achieve orders of magnitude faster
unlearning times than their predecessors, such as gradient surgery.

</details>


### [361] [Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models](https://arxiv.org/abs/2506.04746)
*Fei Ding,Baiqiao Wang,Zijian Zeng,Youwei Wang*

Main category: cs.LG

TL;DR: 提出MGRPO算法解决GRPO无中间监督问题，实验显示MGRPO性能优于GRPO。


<details>
  <summary>Details</summary>
Motivation: GRPO无中间监督，导致探索动态低效、训练稳定性差。

Method: MGRPO分两层，第一层用标准GRPO生成初始响应，第二层纠正初始响应错误，形成自纠正循环。

Result: 在多个数学推理基准测试中，MGRPO显著优于标准GRPO。

Conclusion: MGRPO通过培养推理和自纠正能力取得更优性能。

Abstract: The Group Relative Policy Optimization (GRPO) algorithm has demonstrated
considerable success in enhancing the reasoning capabilities of large language
models (LLMs), as evidenced by DeepSeek-R1. However, the absence of
intermediate supervision in GRPO frequently leads to inefficient exploration
dynamics. A single error in a complex reasoning chain can invalidate the entire
solution, resulting in abrupt reward vanishing and compromising training
stability.To address these challenges, we propose MGRPO (Multi-layer GRPO).
MGRPO operates in two layers: the first layer employs standard GRPO to generate
an initial response. This response, along with the original query, is then fed
into a second-layer GRPO process. This second layer is specifically trained to
identify and correct errors in the initial response, effectively creating a
self-correction loop. This mechanism provides implicit process-level
supervision by rewarding successful error correction, without requiring an
explicit, densely-annotated reward model. Experimental results on several
mathematical reasoning benchmarks demonstrate that MGRPO significantly
outperforms standard GRPO, achieving superior performance by fostering both
reasoning and self-correction abilities.

</details>


### [362] [Log-Linear Attention](https://arxiv.org/abs/2506.04761)
*Han Guo,Songlin Yang,Tarushii Goel,Eric P. Xing,Tri Dao,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出对数线性注意力机制，平衡线性注意力效率与softmax注意力表达性，在Mamba - 2和Gated DeltaNet上效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有Transformers注意力机制存在计算和内存瓶颈，线性注意力和状态空间模型有核心局限性，需新机制平衡效率和表达性。

Method: 开发对数线性注意力机制，用对数增长的隐藏状态集替代固定大小隐藏状态，有矩阵乘法丰富的并行形式。

Result: 在Mamba - 2和Gated DeltaNet上实例化对数线性变体，表现优于线性时间变体。

Conclusion: 对数线性注意力机制可平衡效率和表达性，是通用框架可用于现有线性注意力变体。

Abstract: The attention mechanism in Transformers is an important primitive for
accurate and scalable sequence modeling. Its quadratic-compute and
linear-memory complexity however remain significant bottlenecks. Linear
attention and state-space models enable linear-time, constant-memory sequence
modeling and can moreover be trained efficiently through matmul-rich
parallelization across sequence length. However, at their core these models are
still RNNs, and thus their use of a fixed-size hidden state to model the
context is a fundamental limitation. This paper develops log-linear attention,
an attention mechanism that balances linear attention's efficiency and the
expressiveness of softmax attention. Log-linear attention replaces the
fixed-size hidden state with a logarithmically growing set of hidden states. We
show that with a particular growth function, log-linear attention admits a
similarly matmul-rich parallel form whose compute cost is log-linear in
sequence length. Log-linear attention is a general framework and can be applied
on top of existing linear attention variants. As case studies, we instantiate
log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet
-- and find they perform well compared to their linear-time variants.

</details>


### [363] [OpenGT: A Comprehensive Benchmark For Graph Transformers](https://arxiv.org/abs/2506.04765)
*Jiachen Tang,Zhonghao Wang,Sirui Chen,Sheng Zhou,Jiawei Chen,Jiajun Bu*

Main category: cs.LG

TL;DR: 本文介绍了图变换器（GTs）综合基准OpenGT，开展实验得到关键见解并开发易用库。


<details>
  <summary>Details</summary>
Motivation: GTs适用场景待探索，设计选择有效性不明，缺乏综合基准和库以促进其发展。

Method: 建立标准化实验设置，纳入多种GNN和GTs，从多视角评估GTs。

Result: 发现跨任务层模型迁移难、局部注意力局限等关键见解。

Conclusion: 为未来图变换器研究奠定基础，开发了易用库OpenGT，代码开源。

Abstract: Graph Transformers (GTs) have recently demonstrated remarkable performance
across diverse domains. By leveraging attention mechanisms, GTs are capable of
modeling long-range dependencies and complex structural relationships beyond
local neighborhoods. However, their applicable scenarios are still
underexplored, this highlights the need to identify when and why they excel.
Furthermore, unlike GNNs, which predominantly rely on message-passing
mechanisms, GTs exhibit a diverse design space in areas such as positional
encoding, attention mechanisms, and graph-specific adaptations. Yet, it remains
unclear which of these design choices are truly effective and under what
conditions. As a result, the community currently lacks a comprehensive
benchmark and library to promote a deeper understanding and further development
of GTs. To address this gap, this paper introduces OpenGT, a comprehensive
benchmark for Graph Transformers. OpenGT enables fair comparisons and
multidimensional analysis by establishing standardized experimental settings
and incorporating a broad selection of state-of-the-art GNNs and GTs. Our
benchmark evaluates GTs from multiple perspectives, encompassing diverse tasks
and datasets with varying properties. Through extensive experiments, our
benchmark has uncovered several critical insights, including the difficulty of
transferring models across task levels, the limitations of local attention, the
efficiency trade-offs in several models, the application scenarios of specific
positional encodings, and the preprocessing overhead of some positional
encodings. We aspire for this work to establish a foundation for future graph
transformer research emphasizing fairness, reproducibility, and
generalizability. We have developed an easy-to-use library OpenGT for training
and evaluating existing GTs. The benchmark code is available at
https://github.com/eaglelab-zju/OpenGT.

</details>


### [364] [Kernel $k$-Medoids as General Vector Quantization](https://arxiv.org/abs/2506.04786)
*Thore Gerlach,Sascha Mücke,Christian Bauckhage*

Main category: cs.LG

TL;DR: 本文通过二次无约束二进制优化（QUBO）研究k - medoids聚类和核密度估计（KDE）两种硬向量量化（VQ）方法的联系，揭示二者结构关系并提供新见解。


<details>
  <summary>Details</summary>
Motivation: 探索k - medoids聚类和KDE这两种看似无关的硬VQ方法之间的联系。

Method: 通过QUBO比较启发式的k - medoids QUBO公式和基于KDE - VQ最小化最大均值差异得到的QUBO公式。

Result: 在对核的特征映射有温和假设下，KDE - QUBO是k - medoids - QUBO的特殊情况。

Conclusion: 揭示了两种方法更深层次的结构关系，为VQ的QUBO公式加权参数提供了几何解释的新见解。

Abstract: Vector Quantization (VQ) is a widely used technique in machine learning and
data compression, valued for its simplicity and interpretability. Among hard VQ
methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches
represent two prominent yet seemingly unrelated paradigms -- one
distance-based, the other rooted in probability density matching. In this
paper, we investigate their connection through the lens of Quadratic
Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO
formulation for $k$-medoids, which balances centrality and diversity, with a
principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based
VQ. Surprisingly, we show that the KDE-QUBO is a special case of the
$k$-medoids-QUBO under mild assumptions on the kernel's feature map. This
reveals a deeper structural relationship between these two approaches and
provides new insight into the geometric interpretation of the weighting
parameters used in QUBO formulations for VQ.

</details>


### [365] [Adaptive Preconditioners Trigger Loss Spikes in Adam](https://arxiv.org/abs/2506.04805)
*Zhiwei Bai,Zhangchen Zhou,Jiajie Zhao,Xiaolong Li,Zhiyu Li,Feiyu Xiong,Hongkang Yang,Yaoyu Zhang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 研究Adam优化器训练神经网络时损失尖峰的潜在机制，发现是自适应预条件器引发，并用实验验证。


<details>
  <summary>Details</summary>
Motivation: 探究Adam优化器训练神经网络时损失尖峰出现的潜在机制。

Method: 分析得出在特定临界状态下，平方梯度远小于二阶矩估计，导致预条件化Hessian矩阵的最大特征值超出经典稳定性阈值，引发不稳定。

Result: 不稳定使梯度与最大特征方向对齐，当梯度方向曲率超过2/η时出现损失尖峰。

Conclusion: Adam的自适应预条件器本身会触发损失尖峰，此机制在全连接网络、卷积网络和Transformer架构实验中得到验证。

Abstract: Loss spikes emerge commonly during training across neural networks of varying
architectures and scales when using the Adam optimizer. In this work, we
investigate the underlying mechanism responsible for Adam spikes. While
previous explanations attribute these phenomena to the lower-loss-as-sharper
characteristics of the loss landscape, our analysis reveals that Adam's
adaptive preconditioners themselves can trigger spikes. Specifically, we
identify a critical regime where squared gradients become substantially smaller
than the second-order moment estimates, causing the latter to undergo a
$\beta_2$-exponential decay and to respond sluggishly to current gradient
information. This mechanism can push the maximum eigenvalue of the
preconditioned Hessian beyond the classical stability threshold $2/\eta$ for a
sustained period, inducing instability. This instability further leads to an
alignment between the gradient and the maximum eigendirection, and a loss spike
occurs precisely when the gradient-directional curvature exceeds $2/\eta$. We
verify this mechanism through extensive experiments on fully connected
networks, convolutional networks, and Transformer architectures.

</details>


### [366] [LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.04821)
*Zhen Hao Wong,Jingwen Deng,Runming He,Zirong Chen,Qijie You,Hejun Dong,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出“play to learn”框架，通过强化学习在逻辑谜题上微调大语言模型，提升其在数学基准测试中的分布外性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在监督任务表现出色，但在陌生环境的结构化推理存在困难，标准微调管道可能培养特定领域启发式而非通用思维策略。

Method: 提出“play to learn”框架，在七个自定义逻辑谜题上通过强化学习微调大语言模型，使用可验证奖励的强化学习设置给予二元反馈。

Result: 该训练方法显著提升了大语言模型在一系列数学基准测试中的分布外性能，尤其在需要多步推理的中等难度问题上。

Conclusion: 基于逻辑谜题的强化学习重塑了大语言模型的内部推理，使其无需依赖特定任务的符号工具就能实现更强大和组合性的泛化。

Abstract: Large language models (LLMs) excel at many supervised tasks but often
struggle with structured reasoning in unfamiliar settings. This discrepancy
suggests that standard fine-tuning pipelines may instill narrow,
domain-specific heuristics rather than fostering general-purpose thinking
strategies. In this work, we propose a "play to learn" framework that
fine-tunes LLMs through reinforcement learning on a suite of seven custom logic
puzzles, each designed to cultivate distinct reasoning skills such as
constraint propagation, spatial consistency, and symbolic deduction. Using a
reinforcement learning setup with verifiable rewards, models receive binary
feedback based on puzzle correctness, encouraging iterative, hypothesis-driven
problem solving. We demonstrate that this training approach significantly
improves out-of-distribution performance on a range of mathematical benchmarks,
especially for mid-difficulty problems that require multi-step reasoning.
Analyses across problem categories and difficulty levels reveal that puzzle
training promotes transferable reasoning routines, strengthening algebraic
manipulation, geometric inference, and combinatorial logic, while offering
limited gains on rote or highly specialized tasks. These findings show that
reinforcement learning over logic puzzles reshapes the internal reasoning of
LLMs, enabling more robust and compositional generalization without relying on
task-specific symbolic tools.

</details>


### [367] [From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs](https://arxiv.org/abs/2506.04831)
*Chantal Pellegrini,Ege Özsoy,David Bani-Harouni,Matthias Keicher,Nassir Navab*

Main category: cs.LG

TL;DR: 提出EHR2Path模型处理患者数据，预测健康轨迹，表现出色，助力个性化医疗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理患者数据以实现个性化医疗时，存在聚焦狭窄、忽视复杂纵向交互的问题。

Method: 将多样电子健康记录数据转化为结构化表示，设计EHR2Path模型预测未来健康轨迹，引入新的摘要机制嵌入长期时间上下文。

Result: EHR2Path在下一步预测和纵向模拟中表现良好，优于竞争基线。

Conclusion: EHR2Path能进行详细患者轨迹模拟，适用于多样评估任务，为预测性和个性化医疗开辟道路。

Abstract: Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.

</details>


### [368] [Sparse Autoencoders, Again?](https://arxiv.org/abs/2506.04859)
*Yin Lu,Tong He,Xuening Zhu,David Wipf*

Main category: cs.LG

TL;DR: 本文指出传统稀疏自编码器（SAEs）和变分自编码器（VAE）的不足，提出混合替代模型，理论和实证都证明其有效性，性能超同类模型。


<details>
  <summary>Details</summary>
Motivation: 尽管SAEs应用广泛，但几十年来变化少，VAE虽有改进但仍有不足，因此需要提出新模型克服这些问题。

Method: 形式化分析SAEs和VAEs的弱点，提出混合替代模型，证明该模型全局最小值能恢复特定结构数据，在合成和真实数据集上进行实证评估。

Result: 能准确估计潜在流形维度，生成更稀疏的潜在表示，且不影响重构误差，性能超过同等容量的SAEs、VAEs及部分扩散模型。

Conclusion: 提出的混合替代模型有效，能在图像和语言模型激活模式等领域超越同类模型。

Abstract: Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.

</details>


### [369] [Aligning Multimodal Representations through an Information Bottleneck](https://arxiv.org/abs/2506.04870)
*Antonio Almudévar,José Miguel Hernández-Lobato,Sameer Khurana,Ricard Marxer,Alfonso Ortega*

Main category: cs.LG

TL;DR: 本文指出对比损失学习对齐表示空间效果不佳，从信息瓶颈原理理论描述问题，分析超参数影响，提出正则化项并验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有对比损失在学习对齐表示空间时效果不佳，需解决该问题。

Method: 通过信息瓶颈原理理论描述问题，控制实验分析超参数影响，提出基于变分近似的正则化项。

Result: 分析了不同超参数对问题出现的影响，验证了正则化项在控制实验和实际应用中的优势。

Conclusion: 提出的正则化项能增加表示对齐，解决对比损失在学习对齐表示空间方面的不足。

Abstract: Contrastive losses have been extensively used as a tool for multimodal
representation learning. However, it has been empirically observed that their
use is not effective to learn an aligned representation space. In this paper,
we argue that this phenomenon is caused by the presence of modality-specific
information in the representation space. Although some of the most widely used
contrastive losses maximize the mutual information between representations of
both modalities, they are not designed to remove the modality-specific
information. We give a theoretical description of this problem through the lens
of the Information Bottleneck Principle. We also empirically analyze how
different hyperparameters affect the emergence of this phenomenon in a
controlled experimental setup. Finally, we propose a regularization term in the
loss function that is derived by means of a variational approximation and aims
to increase the representational alignment. We analyze in a set of controlled
experiments and real-world applications the advantages of including this
regularization term.

</details>


### [370] [There Was Never a Bottleneck in Concept Bottleneck Models](https://arxiv.org/abs/2506.04877)
*Antonio Almudévar,José Miguel Hernández-Lobato,Alfonso Ortega*

Main category: cs.LG

TL;DR: 指出CBMs不足，提出MCBMs，其用IB约束表征组件，有理论保障和灵活性。


<details>
  <summary>Details</summary>
Motivation: CBMs未形成真正瓶颈，存在可解释性和干预程序有效性问题，需改进。

Method: 提出MCBMs，通过在训练损失中添加变分正则化项实现信息瓶颈（IB）目标，约束表征组件。

Result: MCBMs支持概念级干预且有理论保障，符合贝叶斯原则，设计选择更灵活。

Conclusion: MCBMs能克服CBMs的局限，是更优解决方案。

Abstract: Deep learning representations are often difficult to interpret, which can
hinder their deployment in sensitive applications. Concept Bottleneck Models
(CBMs) have emerged as a promising approach to mitigate this issue by learning
representations that support target task performance while ensuring that each
component predicts a concrete concept from a predefined set. In this work, we
argue that CBMs do not impose a true bottleneck: the fact that a component can
predict a concept does not guarantee that it encodes only information about
that concept. This shortcoming raises concerns regarding interpretability and
the validity of intervention procedures. To overcome this limitation, we
propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an
Information Bottleneck (IB) objective to constrain each representation
component to retain only the information relevant to its corresponding concept.
This IB is implemented via a variational regularization term added to the
training loss. As a result, MCBMs support concept-level interventions with
theoretical guarantees, remain consistent with Bayesian principles, and offer
greater flexibility in key design choices.

</details>


### [371] [Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms Angle-Based Methods for Assessment of Hip Dysplasia](https://arxiv.org/abs/2506.04886)
*Allen Paul,George Grammatopoulos,Adwaye Rambojun,Neill D. F. Campbell,Harinderjit S. Gill,Tony Shardlow*

Main category: cs.LG

TL;DR: 开发半自动化髋关节发育不良分类管道GPDSSM，提升分类准确率并节省临床时间。


<details>
  <summary>Details</summary>
Motivation: 发育不良是髋关节骨关节炎危险因素，早期诊断重要，需开发有效分类方法。

Method: 结合高斯过程潜变量模型和微分同胚创建GPDSSM，用192个CT扫描，100个训练，92个测试。

Result: GPDSSM有效区分发育不良样本与对照，突出异常区域，AUC达96.2%高于基于角度方法。

Conclusion: GPDSSM提高分类准确率，无需手动测量角度和解读2D扫描，节省临床医生时间。

Abstract: Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,
early diagnosis of dysplasia is important to provide opportunities for surgical
interventions aimed at reducing the risk of hip OA. We have developed a
pipeline for semi-automated classification of dysplasia using volumetric CT
scans of patients' hips and a minimal set of clinically annotated landmarks,
combining the framework of the Gaussian Process Latent Variable Model with
diffeomorphism to create a statistical shape model, which we termed the
Gaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT
scans, 100 for model training and 92 for testing. The GPDSSM effectively
distinguishes dysplastic samples from controls while also highlighting regions
of the underlying surface that show dysplastic variations. As well as improving
classification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),
the GPDSSM can save time for clinicians by removing the need to manually
measure angles and interpreting 2D scans for possible markers of dysplasia.

</details>


### [372] [Dissecting Long Reasoning Models: An Empirical Study](https://arxiv.org/abs/2506.04913)
*Yongyu Mu,Jiali Zeng,Bei Li,Xinyan Guan,Fandong Meng,Jie Zhou,Tong Xiao,Jingbo Zhu*

Main category: cs.LG

TL;DR: 本文聚焦训练长上下文推理模型的强化学习中三个方面：正负样本作用、数据效率问题、性能不稳定问题，并给出相应结论和解决策略。


<details>
  <summary>Details</summary>
Motivation: 解决训练长上下文推理模型的强化学习中存在的开放问题和反常行为。

Method: 系统分析正负样本在强化学习中的作用；探索相对长度奖励和离线样本注入策略解决数据低效问题；通过多次评估运行解决性能不稳定问题。

Result: 发现正样本利于数据拟合，负样本增强泛化和鲁棒性；找到解决数据低效和性能不稳定的方法。

Conclusion: 正负样本有不同作用，可通过特定策略提升推理效率和能力，多次评估运行能缓解性能不稳定问题。

Abstract: Despite recent progress in training long-context reasoning models via
reinforcement learning (RL), several open questions and counterintuitive
behaviors remain. This work focuses on three key aspects: (1) We systematically
analyze the roles of positive and negative samples in RL, revealing that
positive samples mainly facilitate data fitting, whereas negative samples
significantly enhance generalization and robustness. Interestingly, training
solely on negative samples can rival standard RL training performance. (2) We
identify substantial data inefficiency in group relative policy optimization,
where over half of the samples yield zero advantage. To address this, we
explore two straightforward strategies, including relative length rewards and
offline sample injection, to better leverage these data and enhance reasoning
efficiency and capability. (3) We investigate unstable performance across
various reasoning models and benchmarks, attributing instability to uncertain
problems with ambiguous outcomes, and demonstrate that multiple evaluation runs
mitigate this issue.

</details>


### [373] [Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer Fusion](https://arxiv.org/abs/2506.04924)
*Han Wang,Ruoyun He,Guoguang Lao,Ting Liu,Hejiao Luo,Changqi Qin,Hongying Luo,Junmin Huang,Zihan Wei,Lu Chen,Yongzhi Xu,Ziqian Bi,Junhao Song,Tianyang Wang,Chia Xin Liang,Xinyuan Song,Huafeng Liu,Junfeng Hao,Chunjie Tian*

Main category: cs.LG

TL;DR: 介绍ALFIA架构用于ICU高风险患者早期识别，在AUPRC上超越现有表格分类器，与GBDT和深度神经网络结合有额外性能提升，为临床提供风险分层和干预工具。


<details>
  <summary>Details</summary>
Motivation: 早期识别ICU高风险患者，以合理分配有限医疗资源。

Method: 引入ALFIA架构，联合训练LoRA适配器和自适应层加权机制，融合BERT骨干的多层语义特征。

Result: 在cw - 24基准上训练，ALFIA在AUPRC上超越现有表格分类器，与GBDT和深度神经网络结合有性能提升。

Conclusion: ALFIA有优越的早期预警性能，为临床医生在重症监护环境中进行风险分层和及时干预提供便利且强大的工具。

Abstract: Early identification of high-risk ICU patients is crucial for directing
limited medical resources. We introduce ALFIA (Adaptive Layer Fusion with
Intelligent Attention), a modular, attention-based architecture that jointly
trains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting
mechanism to fuse multi-layer semantic features from a BERT backbone. Trained
on our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses
state-of-the-art tabular classifiers in AUPRC while preserving a balanced
precision-recall profile. The embeddings produced by ALFIA's fusion module,
capturing both fine-grained clinical cues and high-level concepts, enable
seamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro
networks as ALFIA-nn, yielding additional performance gains. Our experiments
confirm ALFIA's superior early-warning performance, by operating directly on
routine clinical text, it furnishes clinicians with a convenient yet robust
tool for risk stratification and timely intervention in critical-care settings.

</details>


### [374] [Agentic AI for Intent-Based Industrial Automation](https://arxiv.org/abs/2506.04980)
*Marcos Lima Romero,Ricardo Suyama*

Main category: cs.LG

TL;DR: 本文提出将Agentic AI与基于意图的范式相结合的概念框架，以简化人机交互并适配工业5.0原则，通过实验证明了其在预测性维护场景中的可行性。


<details>
  <summary>Details</summary>
Motivation: 利用Agentic AI系统发展简化人机交互，使自动化系统更好符合工业5.0原则，降低工业4.0引入的复杂性。

Method: 提出整合Agentic AI与基于意图范式的概念框架，基于意图处理将人类自然语言目标分解，引导子代理执行任务，并使用CMAPSS数据集和Google Agent Developer Kit进行概念验证。

Result: 在预测性维护场景中证明了意图分解、代理编排和自主决策的可行性。

Conclusion: 该方法有潜力降低技术障碍，实现可扩展、意图驱动的自动化，但存在数据质量和可解释性问题。

Abstract: The recent development of Agentic AI systems, empowered by autonomous large
language models (LLMs) agents with planning and tool-usage capabilities,
enables new possibilities for the evolution of industrial automation and
reduces the complexity introduced by Industry 4.0. This work proposes a
conceptual framework that integrates Agentic AI with the intent-based paradigm,
originally developed in network research, to simplify human-machine interaction
(HMI) and better align automation systems with the human-centric, sustainable,
and resilient principles of Industry 5.0. Based on the intent-based processing,
the framework allows human operators to express high-level business or
operational goals in natural language, which are decomposed into actionable
components. These intents are broken into expectations, conditions, targets,
context, and information that guide sub-agents equipped with specialized tools
to execute domain-specific tasks. A proof of concept was implemented using the
CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the
feasibility of intent decomposition, agent orchestration, and autonomous
decision-making in predictive maintenance scenarios. The results confirm the
potential of this approach to reduce technical barriers and enable scalable,
intent-driven automation, despite data quality and explainability concerns.

</details>


### [375] [FPTQuant: Function-Preserving Transforms for LLM Quantization](https://arxiv.org/abs/2506.04985)
*Boris van Breugel,Yelysei Bondarenko,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: 论文提出FPTQuant方法，引入四种新型函数保留变换促进大语言模型量化，实现低开销静态INT4量化并取得速度提升和较好的准确率 - 速度权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时计算和能耗大，简单量化会因大值离群点降低性能。

Method: 引入四种新型函数保留变换（FPTs），利用变换器操作的特性设计，本地训练减少离群点并端到端训练使量化和全精度模型输出匹配。

Result: 实现静态INT4量化，开销极小，相比FP有高达3.9倍的速度提升，准确率 - 速度权衡表现良好。

Conclusion: FPTQuant在大语言模型量化方面是一种有效方法，在速度和准确率上取得了较好平衡。

Abstract: Large language models (LLMs) require substantial compute, and thus energy, at
inference time. While quantizing weights and activations is effective at
improving efficiency, naive quantization of LLMs can significantly degrade
performance due to large magnitude outliers. This paper describes FPTQuant,
which introduces four novel, lightweight, and expressive function-preserving
transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable
pre-RoPE transform for queries and keys, (2) a mergeable transform for values,
(3) a mergeable scaling transform within the MLP block, and (4) a cheap,
dynamic scaling transform. By leveraging the equivariances and independencies
inherent to canonical transformer operation, we designed these FPTs to maintain
the model's function while shaping the intermediate activation distributions to
be more quantization friendly. FPTQuant requires no custom kernels and adds
virtually no overhead during inference. The FPTs are trained both locally to
reduce outliers, and end-to-end such that the outputs of the quantized and
full-precision models match. FPTQuant enables static INT4 quantization with
minimal overhead and shows SOTA speed-up of up to 3.9 times over FP.
Empirically, FPTQuant has an excellent accuracy-speed trade-off -- it is
performing on par or exceeding most prior work and only shows slightly lower
accuracy compared to a method that is up to 29% slower.

</details>


### [376] [Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine Learning](https://arxiv.org/abs/2506.05016)
*John Collins*

Main category: cs.LG

TL;DR: 本文提出多點接近度（MPP）編碼方法，用於向量模式地理空間數據編碼，性能優於基於柵格化的方法。


<details>
  <summary>Details</summary>
Motivation: 為使向量模式地理空間數據能用於傳統機器學習和人工智能模型，需將其編碼成合適形式。

Method: 提出基於形狀到感興趣區域內一組參考點的縮放距離的MPP編碼方法。

Result: MPP編碼具有以形狀為中心和連續性的特性，能基於幾何特徵區分空間對象，高精度捕捉成對空間關係，且性能優於基於柵格化的方法。

Conclusion: MPP編碼是一種有效的向量模式地理空間數據編碼方法。

Abstract: Vector-mode geospatial data -- points, lines, and polygons -- must be encoded
into an appropriate form in order to be used with traditional machine learning
and artificial intelligence models. Encoding methods attempt to represent a
given shape as a vector that captures its essential geometric properties. This
paper presents an encoding method based on scaled distances from a shape to a
set of reference points within a region of interest. The method, MultiPoint
Proximity (MPP) encoding, can be applied to any type of shape, enabling the
parameterization of machine learning models with encoded representations of
vector-mode geospatial features. We show that MPP encoding possesses the
desirable properties of shape-centricity and continuity, can be used to
differentiate spatial objects based on their geometric features, and can
capture pairwise spatial relationships with high precision. In all cases, MPP
encoding is shown to perform better than an alternative method based on
rasterization.

</details>


### [377] [Tuning the Right Foundation Models is What you Need for Partial Label Learning](https://arxiv.org/abs/2506.05027)
*Kuang He,Wei Tang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 对11个基础模型在13种PLL方法、8个基准数据集和3种PLL场景下评估，提出PartialCLIP框架，分析现有PLL方法局限。


<details>
  <summary>Details</summary>
Motivation: 现有PLL研究多关注CNN，对基础模型关注有限，需评估基础模型在PLL中的表现。

Method: 对11个基础模型在多种条件下进行综合评估，提出PartialCLIP框架。

Result: 现有PLL方法用基础模型有性能提升，彼此性能相似，性能较稳定，但受模型选择和适应策略影响，文本嵌入分类器初始化和候选标签过滤有效。

Conclusion: 指出当前PLL方法的局限性，为开发更通用PLL模型提供见解。

Abstract: Partial label learning (PLL) seeks to train generalizable classifiers from
datasets with inexact supervision, a common challenge in real-world
applications. Existing studies have developed numerous approaches to
progressively refine and recover ground-truth labels by training convolutional
neural networks. However, limited attention has been given to foundation models
that offer transferrable representations. In this work, we empirically conduct
comprehensive evaluations of 11 foundation models across 13 PLL approaches on 8
benchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an
efficient fine-tuning framework for foundation models in PLL. Our findings
reveal that current PLL approaches tend to 1) achieve significant performance
gains when using foundation models, 2) exhibit remarkably similar performance
to each other, 3) maintain stable performance across varying ambiguity levels,
while 4) are susceptible to foundation model selection and adaptation
strategies. Additionally, we demonstrate the efficacy of text-embedding
classifier initialization and effective candidate label filtering using
zero-shot CLIP. Our experimental results and analysis underscore the
limitations of current PLL approaches and provide valuable insights for
developing more generalizable PLL models. The source code can be found at
https://github.com/SEU-hk/PartialCLIP.

</details>


### [378] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: 本文从类特征归因角度研究对抗训练（AT），识别跨类特征对AT的影响，发现AT不同阶段模型学习特征的倾向，并统一解释AT的两个特性，为理解AT机制提供新视角。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽有效，但训练机制和动态仍是待研究问题，因此从类特征归因角度研究AT。

Method: 通过合成数据模型提供理论证据，对多种模型架构和设置进行系统研究。

Result: AT初期模型倾向学习跨类特征至最佳鲁棒性检查点，之后过度拟合时倾向基于类特定特征决策；统一解释了软标签训练优势和鲁棒过拟合两个特性。

Conclusion: 这些发现完善了当前对AT机制的理解，为研究提供新视角。

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [379] [TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation](https://arxiv.org/abs/2506.05035)
*Hyeongwon Jang,Changhun Kim,Eunho Yang*

Main category: cs.LG

TL;DR: 论文指出当前时间序列可解释AI方法忽略预测方向影响，提出CPD和CPP评估指标，还引入TIMING改进IG，实验表明TIMING表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列可解释AI方法忽略预测方向影响，当前评估指标无法评估IG捕获正负影响关键点的能力，直接应用IG到时间序列数据有问题。

Method: 提出CPD和CPP评估指标，引入TIMING方法增强IG并融入时间感知。

Result: 在合成和真实世界时间序列基准测试中，TIMING优于现有时间序列XAI基线方法。

Conclusion: TIMING能有效解决现有时间序列可解释AI方法的问题，具有更好的性能。

Abstract: Recent explainable artificial intelligence (XAI) methods for time series
primarily estimate point-wise attribution magnitudes, while overlooking the
directional impact on predictions, leading to suboptimal identification of
significant points. Our analysis shows that conventional Integrated Gradients
(IG) effectively capture critical points with both positive and negative
impacts on predictions. However, current evaluation metrics fail to assess this
capability, as they inadvertently cancel out opposing feature contributions. To
address this limitation, we propose novel evaluation metrics-Cumulative
Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to
systematically assess whether attribution methods accurately identify
significant positive and negative points in time series XAI. Under these
metrics, conventional IG outperforms recent counterparts. However, directly
applying IG to time series data may lead to suboptimal outcomes, as generated
paths ignore temporal relationships and introduce out-of-distribution samples.
To overcome these challenges, we introduce TIMING, which enhances IG by
incorporating temporal awareness while maintaining its theoretical properties.
Extensive experiments on synthetic and real-world time series benchmarks
demonstrate that TIMING outperforms existing time series XAI baselines. Our
code is available at https://github.com/drumpt/TIMING.

</details>


### [380] [iN2V: Bringing Transductive Node Embeddings to Inductive Graphs](https://arxiv.org/abs/2506.05039)
*Nicolas Lell,Ansgar Scherp*

Main category: cs.LG

TL;DR: 提出归纳式node2vec（iN2V），结合事后程序计算未见过节点的嵌入，实验表明其能有效将传导式嵌入用于归纳式场景，提升节点分类效果且是通用方法。


<details>
  <summary>Details</summary>
Motivation: 现有浅层节点嵌入方法如node2vec在新节点应用上受限，只能用于传导式场景。

Method: 提出iN2V，结合事后程序计算未见过节点嵌入，并修改原node2vec训练程序以适配事后程序。

Result: 在多个基准数据集实验，iN2V使节点分类平均提升1分，最高提升6分。

Conclusion: iN2V是创建或丰富现有嵌入的即插即用方法，可与其他嵌入方法结合，是通用的归纳式节点表示学习方法。

Abstract: Shallow node embeddings like node2vec (N2V) can be used for nodes without
features or to supplement existing features with structure-based information.
Embedding methods like N2V are limited in their application on new nodes, which
restricts them to the transductive setting where the entire graph, including
the test nodes, is available during training. We propose inductive node2vec
(iN2V), which combines a post-hoc procedure to compute embeddings for nodes
unseen during training and modifications to the original N2V training procedure
to prepare the embeddings for this post-hoc procedure. We conduct experiments
on several benchmark datasets and demonstrate that iN2V is an effective
approach to bringing transductive embeddings to an inductive setting. Using
iN2V embeddings improves node classification by 1 point on average, with up to
6 points of improvement depending on the dataset and the number of unseen
nodes. Our iN2V is a plug-in approach to create new or enrich existing
embeddings. It can also be combined with other embedding methods, making it a
versatile approach for inductive node representation learning. Code to
reproduce the results is available at https://github.com/Foisunt/iN2V .

</details>


### [381] [Reliably detecting model failures in deployment without labels](https://arxiv.org/abs/2506.05047)
*Viet Nguyen Changjian Shui,Vijay Giri,Siddarth Arya,Amol Verma,Fahad Razak,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 论文提出D3M算法解决模型部署后性能恶化监测问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据分布随时间变化，模型需重训练，但在无标签时难知何时重训练，故要解决部署后性能恶化监测问题。

Method: 提出基于预测模型分歧的实用高效监测算法D3M。

Result: 在标准基准和真实内科数据集上实验，证明框架有效，可作为高风险机器学习管道的警报机制。

Conclusion: D3M算法能在非恶化偏移下实现低误报率，在恶化偏移下实现高召回率，可用于实际监测。

Abstract: The distribution of data changes over time; models operating operating in
dynamic environments need retraining. But knowing when to retrain, without
access to labels, is an open challenge since some, but not all shifts degrade
model performance. This paper formalizes and addresses the problem of
post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and
efficient monitoring algorithm based on the disagreement of predictive models,
achieving low false positive rates under non-deteriorating shifts and provides
sample complexity bounds for high true positive rates under deteriorating
shifts. Empirical results on both standard benchmark and a real-world
large-scale internal medicine dataset demonstrate the effectiveness of the
framework and highlight its viability as an alert mechanism for high-stakes
machine learning pipelines.

</details>


### [382] [Associative Memory and Generative Diffusion in the Zero-noise Limit](https://arxiv.org/abs/2506.05178)
*Joshua Hess,Quaid Morris*

Main category: cs.LG

TL;DR: 研究生成扩散与连续状态联想记忆模型的联系，阐述联想记忆特性及噪声变化下的转变，表明莫尔斯 - 斯梅尔流的结构稳定性对扩散稳定性的影响，并给出景观特征示例和网络稳定性准则。


<details>
  <summary>Details</summary>
Motivation: 探索生成扩散和连续状态联想记忆模型之间的联系，以更好理解它们的特性和行为。

Method: 强调莫尔斯 - 斯梅尔动力系统作为基于梯度的联想记忆模型和扩散模型的通用近似器，研究联想记忆的通用性质。

Result: 发现莫尔斯 - 斯梅尔流的结构稳定性意味着噪声消失时扩散的稳定性，指出联想记忆学习景观和扩散模型学习与生成景观在零噪声极限下的稳定性情况，给出景观特征示例和网络稳定性准则。

Conclusion: 建立了生成扩散与连续状态联想记忆模型的联系，揭示了噪声变化下的转变和稳定性特征。

Abstract: Connections between generative diffusion and continuous-state associative
memory models are studied. Morse-Smale dynamical systems are emphasized as
universal approximators of gradient-based associative memory models and
diffusion models as white-noise perturbed systems thereof. Universal properties
of associative memory that follow from this description are described and used
to characterize a generic transition from generation to memory as noise levels
diminish. Structural stability inherited by Morse-Smale flows is shown to imply
a notion of stability for diffusions at vanishing noise levels. Applied to one-
and two-parameter families of gradients, this indicates stability at all but
isolated points of associative memory learning landscapes and the learning and
generation landscapes of diffusion models with gradient drift in the zero-noise
limit, at which small sets of generic bifurcations characterize qualitative
transitions between stable systems. Examples illustrating the characterization
of these landscapes by sequences of these bifurcations are given, along with
structural stability criterion for classic and modern Hopfield networks
(equivalently, the attention mechanism).

</details>


### [383] [TreeRPO: Tree Relative Policy Optimization](https://arxiv.org/abs/2506.05183)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Xiaodan Liang,Yiwei Wang,Jing Tang*

Main category: cs.LG

TL;DR: 现有基于全轨迹奖励的大语言模型推理优化方法有局限，本文提出TreeRPO方法，用树采样估计各推理步骤奖励，实验表明该方法提升了模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在全轨迹层面定义奖励，对推理过程中间步骤优化的指导不足。

Method: 引入TreeRPO方法，用树采样估计各推理步骤奖励，基于GRPO的组相对奖励训练机制，根据树采样生成的步骤级组计算奖励。

Result: TreeRPO算法使Qwen - 2.5 - Math在测试基准上的平均Pass@1准确率从19.0%提升到35.5%，比GRPO性能高2.9%，平均响应长度减少18.1%。

Conclusion: TreeRPO方法能产生细粒度和密集的奖励信号，显著增强大语言模型学习过程和整体性能，兼具有效性和效率。

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.

</details>


### [384] [Locality Preserving Markovian Transition for Instance Retrieval](https://arxiv.org/abs/2506.05196)
*Jifei Luo,Wenzheng Wu,Hantao Yao,Lu Yu,Changsheng Xu*

Main category: cs.LG

TL;DR: 提出LPMT框架解决扩散重排序方法信号衰减问题，实验验证其在实例检索中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的重排序方法在远离源点几步后正信号会减弱，降低局部区域外的判别能力

Method: 提出LPMT框架，用BCD整合不同图的扩散过程，用LSE将实例编码为分布，通过TMT过程连接分布

Result: 在不同任务的实验中验证了LPMT用于实例检索的有效性

Conclusion: LPMT框架能有效解决扩散重排序方法的问题，在实例检索中有良好表现

Abstract: Diffusion-based re-ranking methods are effective in modeling the data
manifolds through similarity propagation in affinity graphs. However, positive
signals tend to diminish over several steps away from the source, reducing
discriminative power beyond local regions. To address this issue, we introduce
the Locality Preserving Markovian Transition (LPMT) framework, which employs a
long-term thermodynamic transition process with multiple states for accurate
manifold distance measurement. The proposed LPMT first integrates diffusion
processes across separate graphs using Bidirectional Collaborative Diffusion
(BCD) to establish strong similarity relationships. Afterwards, Locality State
Embedding (LSE) encodes each instance into a distribution for enhanced local
consistency. These distributions are interconnected via the Thermodynamic
Markovian Transition (TMT) process, enabling efficient global retrieval while
maintaining local effectiveness. Experimental results across diverse tasks
confirm the effectiveness of LPMT for instance retrieval.

</details>


### [385] [Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning](https://arxiv.org/abs/2506.05214)
*Jingyu Hu,Hongbo Bo,Jun Hong,Xiaowei Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 本文提出Hardness Adaptive Reweighted (HAR)对比损失和SHARP实验框架来缓解图神经网络节点分类任务中的度偏差问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有采用图对比学习（GCL）缓解度偏差的方法存在正样本对数量有限、正负样本对权重相等的问题，导致低度数节点获取信息不足且有噪声。

Method: 提出HAR对比损失，通过利用节点标签增加正样本对，并根据学习难度自适应加权正负样本对；开发SHARP实验框架将HAR扩展到更多场景。

Result: 理论分析和实验验证了SHARP的有效性，在四个数据集上的实验结果表明，SHARP在全局和度级别上的性能均优于基线模型。

Conclusion: SHARP能有效缓解图神经网络节点分类任务中的度偏差问题。

Abstract: Graph Neural Networks (GNNs) often suffer from degree bias in node
classification tasks, where prediction performance varies across nodes with
different degrees. Several approaches, which adopt Graph Contrastive Learning
(GCL), have been proposed to mitigate this bias. However, the limited number of
positive pairs and the equal weighting of all positives and negatives in GCL
still lead to low-degree nodes acquiring insufficient and noisy information.
This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to
mitigate degree bias. It adds more positive pairs by leveraging node labels and
adaptively weights positive and negative pairs based on their learning
hardness. In addition, we develop an experimental framework named SHARP to
extend HAR to a broader range of scenarios. Both our theoretical analysis and
experiments validate the effectiveness of SHARP. The experimental results
across four datasets show that SHARP achieves better performance against
baselines at both global and degree levels.

</details>


### [386] [Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm](https://arxiv.org/abs/2506.05215)
*Zhan Yu*

Main category: cs.LG

TL;DR: 提出基于网络系统的去中心化鲁棒核学习算法，建立学习理论与收敛分析，推导收敛界，给出参数选择规则。


<details>
  <summary>Details</summary>
Motivation: 现有分布式鲁棒核学习方案多基于分治法，需新的统一去中心化学习框架。

Method: 在再生核希尔伯特空间框架下，利用网络系统，采用由窗函数和鲁棒缩放参数诱导的鲁棒损失函数。

Result: 建立学习理论与收敛分析，推导多种收敛界，给出参数选择规则，算法能达到最优学习率。

Conclusion: 参数σ对增强鲁棒性和确保收敛很重要，反映了去中心化、样本选择、鲁棒性和收敛之间的内在联系。

Abstract: We propose a new decentralized robust kernel-based learning algorithm within
the framework of reproducing kernel Hilbert space (RKHS) by utilizing a
networked system that can be represented as a connected graph. The robust loss
function $\mathcal{L}_\sigma$ induced by a windowing function $W$ and a
robustness scaling parameter $\sigma>0$, can encompass a broad spectrum of
robust losses. Consequently, the proposed algorithm effectively provides a
unified decentralized learning framework for robust regression, which
fundamentally differs from the existing distributed robust kernel learning
schemes, all of which are divide-and-conquer based. We rigorously establish the
learning theory and offer a comprehensive convergence analysis for the
algorithm. We show each local robust estimator generated from the decentralized
algorithm can be utilized to approximate the regression function. Based on
kernel-based integral operator techniques, we derive general high confidence
convergence bounds for each local approximating sequence in terms of the mean
square distance, RKHS norm, and generalization error, respectively. Moreover,
we provide rigorous selection rules for local sample size and show that, under
properly selected step size and scaling parameter $\sigma$, the decentralized
robust algorithm can achieve optimal learning rates (up to logarithmic factors)
in both norms. The parameter $\sigma$ is shown to be essential for enhancing
robustness while also ensuring favorable convergence behavior. The intrinsic
connection among decentralization, sample selection, robustness of the
algorithm, and its convergence is clearly reflected.

</details>


### [387] [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts](https://arxiv.org/abs/2506.05229)
*Danil Sivtsov,Ivan Rodkin,Gleb Kuzmin,Yuri Kuratov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 论文提出Diagonal Batching调度方案解决RMTs模型的顺序执行瓶颈，在LLaMA - 1B ARMT模型上实现加速，加强了RMTs在长上下文应用的实用性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型长上下文推理存在复杂度问题，RMTs虽能解决但内存更新机制导致顺序执行成性能瓶颈。

Method: 引入Diagonal Batching调度方案，解锁RMTs跨段并行性，保留精确递归，纯运行时计算重排序，无需重新训练。

Result: 在LLaMA - 1B ARMT模型上，对131,072 - token序列，Diagonal Batching比标准全注意力LLaMA - 1B快3.3倍，比顺序RMT实现快1.8倍。

Conclusion: Diagonal Batching消除顺序瓶颈，降低推理成本和延迟，加强了RMTs在现实长上下文应用的实用性。

Abstract: Transformer models struggle with long-context inference due to their
quadratic time and linear memory complexity. Recurrent Memory Transformers
(RMTs) offer a solution by reducing the asymptotic cost to linear time and
constant memory usage. However, their memory update mechanism leads to
sequential execution, causing a performance bottleneck.
  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism
across segments in RMTs while preserving exact recurrence. This approach
eliminates the sequential constraint, enabling efficient GPU inference even for
single long-context inputs without complex batching and pipelining techniques.
Because the technique is purely a run-time computation reordering, existing RMT
models adopt it with no retraining.
  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup
over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential
RMT implementation on 131,072-token sequences. By removing sequential
bottleneck, Diagonal Batching reduces inference cost and latency, thereby
strengthening RMTs as a practical solution for real-world, long-context
applications.

</details>


### [388] [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)
*Johannes von Oswald,Nino Scherrer,Seijin Kobayashi,Luca Versari,Songlin Yang,Maximilian Schlegel,Kaitlin Maile,Yanick Schimpf,Oliver Sieberling,Alexander Meulemans,Rif A. Saurous,Guillaume Lajoie,Charlotte Frenkel,Razvan Pascanu,Blaise Agüera y Arcas,João Sacramento*

Main category: cs.LG

TL;DR: 本文引入Mesa层的数值稳定、分块并行版本并在十亿参数规模语言建模中研究，实验表明最优测试时训练能提升性能但增加推理计算量。


<details>
  <summary>Details</summary>
Motivation: 当前因果transformer架构推理时需线性扩展内存和计算，线性化softmax操作的RNN模型有优势，进一步研究基于上下文损失优化的模型。

Method: 引入Mesa层的改进版本，使用快速共轭梯度求解器在每个时间点最优地最小化上下文损失。

Result: 最优测试时训练能降低语言建模困惑度，提升下游基准性能，尤其在需要长上下文理解的任务上，但增加推理计算量。

Conclusion: 通过在神经网络内求解顺序优化问题增加测试时计算量可提升性能。

Abstract: Sequence modeling is currently dominated by causal transformer architectures
that use softmax self-attention. Although widely adopted, transformers require
scaling memory and compute linearly during inference. A recent stream of work
linearized the softmax operation, resulting in powerful recurrent neural
network (RNN) models with constant memory and compute costs such as DeltaNet,
Mamba or xLSTM. These models can be unified by noting that their recurrent
layer dynamics can all be derived from an in-context regression objective,
approximately optimized through an online learning rule. Here, we join this
line of work and introduce a numerically stable, chunkwise parallelizable
version of the recently proposed Mesa layer (von Oswald et al., 2024), and
study it in language modeling at the billion-parameter scale. This layer again
stems from an in-context loss, but which is now minimized to optimality at
every time point using a fast conjugate gradient solver. Through an extensive
suite of experiments, we show that optimal test-time training enables reaching
lower language modeling perplexity and higher downstream benchmark performance
than previous RNNs, especially on tasks requiring long context understanding.
This performance gain comes at the cost of additional flops spent during
inference time. Our results are therefore intriguingly related to recent trends
of increasing test-time compute to improve performance -- here by spending
compute to solve sequential optimization problems within the neural network
itself.

</details>


### [389] [Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit](https://arxiv.org/abs/2506.05239)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

TL;DR: 论文用MNIST评估SAEs，指出其浅架构局限，引入多迭代SAE（MP - SAE）提取相关特征并保证重建效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前SAEs浅架构依赖准正交假设，限制了相关特征提取能力，需改进。

Method: 使用MNIST评估SAEs，引入基于Matching Pursuit展开的多迭代SAE（MP - SAE）。

Result: MP - SAE能进行残差引导的相关特征提取，且随着原子选择增加，重建效果单调提升。

Conclusion: MP - SAE可突破当前SAEs浅架构局限，用于提取分层设置中的相关特征。

Abstract: Sparse autoencoders (SAEs) have recently become central tools for
interpretability, leveraging dictionary learning principles to extract sparse,
interpretable features from neural representations whose underlying structure
is typically unknown. This paper evaluates SAEs in a controlled setting using
MNIST, which reveals that current shallow architectures implicitly rely on a
quasi-orthogonality assumption that limits the ability to extract correlated
features. To move beyond this, we introduce a multi-iteration SAE by unrolling
Matching Pursuit (MP-SAE), enabling the residual-guided extraction of
correlated features that arise in hierarchical settings such as handwritten
digit generation while guaranteeing monotonic improvement of the reconstruction
as more atoms are selected.

</details>


### [390] [Aligning Latent Spaces with Flow Priors](https://arxiv.org/abs/2506.05240)
*Yizhuo Li,Yuying Ge,Yixiao Ge,Ying Shan,Ping Luo*

Main category: cs.LG

TL;DR: 提出用基于流的生成模型作为先验对齐可学习潜在空间到任意目标分布的框架，有理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决潜在空间对齐到任意目标分布的问题，避免昂贵的似然评估和ODE求解。

Method: 先在目标特征上预训练流模型，再用对齐损失正则化潜在空间，证明最小化对齐损失的合理性。

Result: 在受控环境中证明对齐损失景观接近目标分布负对数似然，在ImageNet上进行大规模图像生成实验验证有效性。

Conclusion: 所提框架为潜在空间对齐开辟新途径。

Abstract: This paper presents a novel framework for aligning learnable latent spaces to
arbitrary target distributions by leveraging flow-based generative models as
priors. Our method first pretrains a flow model on the target features to
capture the underlying distribution. This fixed flow model subsequently
regularizes the latent space via an alignment loss, which reformulates the flow
matching objective to treat the latents as optimization targets. We formally
prove that minimizing this alignment loss establishes a computationally
tractable surrogate objective for maximizing a variational lower bound on the
log-likelihood of latents under the target distribution. Notably, the proposed
method eliminates computationally expensive likelihood evaluations and avoids
ODE solving during optimization. As a proof of concept, we demonstrate in a
controlled setting that the alignment loss landscape closely approximates the
negative log-likelihood of the target distribution. We further validate the
effectiveness of our approach through large-scale image generation experiments
on ImageNet with diverse target distributions, accompanied by detailed
discussions and ablation studies. With both theoretical and empirical
validation, our framework paves a new way for latent space alignment.

</details>


### [391] [On the Convergence of Gradient Descent on Learning Transformers with Residual Connections](https://arxiv.org/abs/2506.05249)
*Zhen Qin,Jinxin Zhou,Zhihui Zhu*

Main category: cs.LG

TL;DR: 论文分析单层和多层Transformer收敛行为，证明梯度下降线性收敛率，揭示残差连接促进优化稳定性，实证结果证实理论。


<details>
  <summary>Details</summary>
Motivation: Transformer理论基础尤其是训练动力学研究不足，现有研究未充分考虑组件间依赖。

Method: 分析含自注意力、前馈网络和残差连接的单层Transformer收敛行为，再扩展到多层架构。

Result: 合适初始化下梯度下降有线性收敛率，收敛速度由注意力层输出矩阵奇异值决定，残差连接改善输出矩阵病态问题，实证结果支持理论。

Conclusion: 残差连接有助于提升Transformer优化和收敛稳定性。

Abstract: Transformer models have emerged as fundamental tools across various
scientific and engineering disciplines, owing to their outstanding performance
in diverse applications. Despite this empirical success, the theoretical
foundations of Transformers remain relatively underdeveloped, particularly in
understanding their training dynamics. Existing research predominantly examines
isolated components--such as self-attention mechanisms and feedforward
networks--without thoroughly investigating the interdependencies between these
components, especially when residual connections are present. In this paper, we
aim to bridge this gap by analyzing the convergence behavior of a structurally
complete yet single-layer Transformer, comprising self-attention, a feedforward
network, and residual connections. We demonstrate that, under appropriate
initialization, gradient descent exhibits a linear convergence rate, where the
convergence speed is determined by the minimum and maximum singular values of
the output matrix from the attention layer. Moreover, our analysis reveals that
residual connections serve to ameliorate the ill-conditioning of this output
matrix, an issue stemming from the low-rank structure imposed by the softmax
operation, thereby promoting enhanced optimization stability. We also extend
our theoretical findings to a multi-layer Transformer architecture, confirming
the linear convergence rate of gradient descent under suitable initialization.
Empirical results corroborate our theoretical insights, illustrating the
beneficial role of residual connections in promoting convergence stability.

</details>


### [392] [Learning long range dependencies through time reversal symmetry breaking](https://arxiv.org/abs/2506.05259)
*Guillaume Pourcel,Maxence Ernoult*

Main category: cs.LG

TL;DR: 提出Recurrent Hamiltonian Echo Learning (RHEL)算法训练Hamiltonian SSMs处理时间序列任务，性能与BPTT相当，为自学习物理系统设计打开新门。


<details>
  <summary>Details</summary>
Motivation: Deep State Space Models需要遵循物理原理的学习算法及模拟和设计系统的有效技术。

Method: 提出RHEL算法，先在连续时间引入并证明与连续伴随状态方法等价，再提出离散时间版本，将结果推广到Hamiltonian SSMs。

Result: RHEL在各种时间序列任务上性能与BPTT一致。

Conclusion: 为序列建模的可扩展、节能自学习物理系统设计提供新思路。

Abstract: Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,
as RNNs could natively be embodied into dynamical systems. This calls for
dedicated learning algorithms obeying to core physical principles, with
efficient techniques to simulate these systems and guide their design. We
propose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably
computes loss gradients as finite differences of physical trajectories of
non-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three
"forward passes" irrespective of model size, without explicit Jacobian
computation, nor incurring any variance in the gradient estimation. Motivated
by the physical realization of our algorithm, we first introduce RHEL in
continuous time and demonstrate its formal equivalence with the continuous
adjoint state method. To facilitate the simulation of Hamiltonian systems
trained by RHEL, we propose a discrete-time version of RHEL which is equivalent
to Backpropagation Through Time (BPTT) when applied to a class of recurrent
modules which we call Hamiltonian Recurrent Units (HRUs). This setting allows
us to demonstrate the scalability of RHEL by generalizing these results to
hierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to
train HSSMs with linear and nonlinear dynamics on a variety of time-series
tasks ranging from mid-range to long-range classification and regression with
sequence length reaching $\sim 50k$. We show that RHEL consistently matches the
performance of BPTT across all models and tasks. This work opens new doors for
the design of scalable, energy-efficient physical systems endowed with
self-learning capabilities for sequence modelling.

</details>


### [393] [How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control](https://arxiv.org/abs/2506.05276)
*Hao Yu,Chu Xin Cheng,Runlong Yu,Yuyang Ye,Shiwei Tong,Zhaofeng Liu,Defu Lian*

Main category: cs.LG

TL;DR: 提出CocktailEdit框架解决时间序列生成中控制属性的难题，经实验验证有效，提供代码和演示。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法难以控制生成序列的属性，Time Series Editing需考虑点级和段级控制，当前方法无法满足。

Method: 引入CocktailEdit框架，结合置信加权锚点控制处理点级约束，基于分类器控制处理段统计属性，在去噪推理阶段实现精确局部控制。

Result: 在不同数据集和模型上的大量实验证明了该方法的有效性。

Conclusion: 该工作弥合了纯生成建模与现实时间序列编辑需求的差距，为人工参与的时间序列生成和编辑提供灵活解决方案。

Abstract: Recent advances in time series generation have shown promise, yet controlling
properties in generated sequences remains challenging. Time Series Editing
(TSE) - making precise modifications while preserving temporal coherence -
consider both point-level constraints and segment-level controls that current
methods struggle to provide. We introduce the CocktailEdit framework to enable
simultaneous, flexible control across different types of constraints. This
framework combines two key mechanisms: a confidence-weighted anchor control for
point-wise constraints and a classifier-based control for managing statistical
properties such as sums and averages over segments. Our methods achieve precise
local control during the denoising inference stage while maintaining temporal
coherence and integrating seamlessly, with any conditionally trained
diffusion-based time series models. Extensive experiments across diverse
datasets and models demonstrate its effectiveness. Our work bridges the gap
between pure generative modeling and real-world time series editing needs,
offering a flexible solution for human-in-the-loop time series generation and
editing. The code and demo are provided for validation.

</details>


### [394] [Fast-DataShapley: Neural Modeling for Training Data Valuation](https://arxiv.org/abs/2506.05281)
*Haifeng Sun,Yu Xiong,Runze Wu,Xinyu Cai,Changjie Fan,Lan Zhang,Xiang-Yang Li*

Main category: cs.LG

TL;DR: 本文提出Fast - DataShapley方法计算训练数据Shapley值，还提出三种降低训练开销方法，实验显示性能和效率优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于Shapley值评估数据贡献方法计算复杂度高，需为每个测试样本重新训练，成本难以忍受。

Method: 提出Fast - DataShapley单遍训练方法训练可重用解释器模型；提出三种从两方面降低训练开销的方法，并分析时间复杂度。

Result: 在各种图像数据集上实验表明，性能提升至2.5倍以上，解释器训练速度提高两个数量级。

Conclusion: 所提方法在计算训练数据Shapley值上具有更好的性能和效率。

Abstract: The value and copyright of training data are crucial in the artificial
intelligence industry. Service platforms should protect data providers'
legitimate rights and fairly reward them for their contributions. Shapley
value, a potent tool for evaluating contributions, outperforms other methods in
theory, but its computational overhead escalates exponentially with the number
of data providers. Recent works based on Shapley values attempt to mitigate
computation complexity by approximation algorithms. However, they need to
retrain for each test sample, leading to intolerable costs. We propose
Fast-DataShapley, a one-pass training method that leverages the weighted least
squares characterization of the Shapley value to train a reusable explainer
model with real-time reasoning speed. Given new test samples, no retraining is
required to calculate the Shapley values of the training data. Additionally, we
propose three methods with theoretical guarantees to reduce training overhead
from two aspects: the approximate calculation of the utility function and the
group calculation of the training data. We analyze time complexity to show the
efficiency of our methods. The experimental evaluations on various image
datasets demonstrate superior performance and efficiency compared to baselines.
Specifically, the performance is improved to more than 2.5 times, and the
explainer's training speed can be increased by two orders of magnitude.

</details>


### [395] [Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing](https://arxiv.org/abs/2506.05292)
*Declan A. Norton,Yuanzhao Zhang,Michelle Girvan*

Main category: cs.LG

TL;DR: 研究表明无显式结构先验的储层计算可对动力系统状态空间未探索区域进行泛化。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在无显式结构先验时难以对训练数据中未充分体现的动力学特征进行泛化，本文旨在解决该问题。

Method: 提出多轨迹训练方案，支持对不相交时间序列进行训练，并将其应用于多稳态动力系统。

Result: 对单吸引盆轨迹训练的储层计算机能通过捕捉未观测吸引盆的系统行为实现域外泛化。

Conclusion: 储层计算可在无显式结构先验的情况下对动力系统状态空间未探索区域进行泛化。

Abstract: Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.

</details>


### [396] [A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search](https://arxiv.org/abs/2506.05294)
*Arnav Kumar Jain,Vibhakar Mohta,Subin Kim,Atiksh Bhardwaj,Juntao Ren,Yunhai Feng,Sanjiban Choudhury,Gokul Swamy*

Main category: cs.LG

TL;DR: 行为克隆（BC）方法有局限，论文提出从专家演示中学习搜索（L2S）方法SAILOR，在多视觉操作任务上表现优于BC方法，能识别细微失败且抗奖励破解。


<details>
  <summary>Details</summary>
Motivation: 行为克隆（BC）方法仅教代理专家在访问过的状态下的操作，代理犯错脱离演示支持时不知如何恢复，需更好方法。

Method: 探索从专家演示中学习搜索（L2S），包含世界模型和奖励模型，精心消融算法和设计决策以稳定高效学习恢复行为。

Result: 在三个基准的十几个视觉操作任务中，SAILOR方法始终优于通过BC训练的最先进扩散策略，增加BC演示数据量仍有性能差距，SAILOR能识别细微失败且抗奖励破解。

Conclusion: 学习搜索（L2S）的SAILOR方法比行为克隆（BC）方法更有效，能解决BC方法的局限性。

Abstract: The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .

</details>


### [397] [Power Law Guided Dynamic Sifting for Efficient Attention](https://arxiv.org/abs/2506.05300)
*Nirav Koley,Prajwal Singhania,Abhinav Bhatele*

Main category: cs.LG

TL;DR: 提出SiftAttention近似注意力方法，通过阈值过滤减少内存带宽使用并保证模型质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在GPU上推理因内存带宽限制存在挑战，现有近似注意力方法依赖昂贵top - k操作，在GPU上性能不佳。

Method: 用基于阈值的元素过滤操作替代top - k步骤，利用注意力分数的τ分位数的幂律特性动态估计阈值。

Result: SiftAttention比现有近似注意力方法更好地保留模型质量，减少加载值向量时的内存带宽使用。

Conclusion: SiftAttention是一种有效解决大语言模型在GPU上推理内存带宽问题的近似注意力方法。

Abstract: Efficient inference on GPUs using large language models remains challenging
due to memory bandwidth limitations, particularly during data transfers between
High Bandwidth Memory (HBM) and SRAM in attention computations. Approximate
attention methods address this issue by reducing computational and memory
overhead but often rely on expensive top-$k$ operations, which perform poorly
on GPUs. We propose SiftAttention, a novel approximate attention method that
replaces the top-$k$ step with a computationally efficient element-wise
filtering operation based on a threshold value. Our intuition for doing this is
based on our empirical observation that the $\tau$-th quantile of attention
scores follows a predictable power-law over sequential generation steps.
Exploiting this insight, our approach dynamically estimates a threshold value
per prompt at each generation step. Only attention scores above this threshold
and their corresponding value vectors are loaded/used to compute the attention
output, reducing data movement between HBM and SRAM. Our evaluation
demonstrates that SiftAttention preserves model quality better than existing
approximate attention methods while reducing memory bandwidth usage when
loading value vectors.

</details>


### [398] [Learning normalized image densities via dual score matching](https://arxiv.org/abs/2506.05310)
*Florentin Guth,Zahra Kadkhodaie,Eero P Simoncelli*

Main category: cs.LG

TL;DR: 本文引入新框架学习归一化能量模型，在ImageNet64数据集上获可比结果，模型泛化性强，还揭示图像概率和局部邻域维度与内容有关。


<details>
  <summary>Details</summary>
Motivation: 解决从数据学习概率模型时因维度诅咒导致的困难。

Method: 受扩散生成模型启发，修改得分网络架构计算能量，使用双得分匹配目标训练能量网络。

Result: 在ImageNet64数据集上获得与现有技术相当的交叉熵值，能量模型泛化性强。

Conclusion: 传统假设如测度集中或低维流形支持并不适用于图像概率和局部邻域维度情况。

Abstract: Learning probability models from data is at the heart of many machine
learning endeavors, but is notoriously difficult due to the curse of
dimensionality. We introduce a new framework for learning \emph{normalized}
energy (log probability) models that is inspired from diffusion generative
models, which rely on networks optimized to estimate the score. We modify a
score network architecture to compute an energy while preserving its inductive
biases. The gradient of this energy network with respect to its input image is
the score of the learned density, which can be optimized using a denoising
objective. Importantly, the gradient with respect to the noise level provides
an additional score that can be optimized with a novel secondary objective,
ensuring consistent and normalized energies across noise levels. We train an
energy network with this \emph{dual} score matching objective on the ImageNet64
dataset, and obtain a cross-entropy (negative log likelihood) value comparable
to the state of the art. We further validate our approach by showing that our
energy model \emph{strongly generalizes}: estimated log probabilities are
nearly independent of the specific images in the training set. Finally, we
demonstrate that both image probability and dimensionality of local
neighborhoods vary significantly with image content, in contrast with
traditional assumptions such as concentration of measure or support on a
low-dimensional manifold.

</details>


### [399] [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)
*Yifan Sun,Jingyan Shen,Yibin Wang,Tianyu Chen,Zhendong Wang,Mingyuan Zhou,Huan Zhang*

Main category: cs.LG

TL;DR: 提出两种技术提高大语言模型强化学习微调的数据效率，实验表明能减少微调时间。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调大语言模型资源密集，现有工作忽视数据效率问题。

Method: 提出难度导向在线数据选择和滚动重放两种技术，引入自适应难度引导数据选择，开发基于注意力的框架估算难度，引入滚动重放机制降低成本。

Result: 在6种大语言模型 - 数据集组合的实验中，相比原GRPO算法，达到相同性能时减少25%到65%的强化学习微调时间。

Conclusion: 所提技术能有效提高大语言模型强化学习微调的数据效率，减少微调时间。

Abstract: Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.

</details>


### [400] [LSM-2: Learning from Incomplete Wearable Sensor Data](https://arxiv.org/abs/2506.05321)
*Maxwell A. Xu,Girish Narayanswamy,Kumar Ayush,Dimitris Spathis,Shun Liao,Shyam A. Tailor,Ahmed Metwally,A. Ali Heydari,Yuwei Zhang,Jake Garrison,Samy Abdel-Ghaffar,Xuhai Xu,Ken Gu,Jacob Sunshine,Ming-Zher Poh,Yun Liu,Tim Althoff,Shrikanth Narayanan,Pushmeet Kohli,Mark Malhotra,Shwetak Patel,Yuzhe Yang,James M. Rehg,Xin Liu,Daniel McDuff*

Main category: cs.LG

TL;DR: 本文提出带AIM的LSM - 2，可从残缺可穿戴传感器数据中学习表征，在多任务中表现佳，适用于现实应用。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型依赖完整数据，可穿戴传感器数据缺失严重，传统自监督学习模型难以处理。

Method: 引入带自适应和继承掩码（AIM）的第二代大传感器模型（LSM - 2），用可学习掩码标记对已有和人为引入的缺失数据建模。

Result: 在4000万小时多模态传感器数据上预训练的LSM - 2在分类、回归和生成建模等任务中表现最佳，具有良好扩展性，在特定缺失场景下性能好。

Conclusion: AIM是现实可穿戴数据应用更可靠的选择。

Abstract: Foundation models, a cornerstone of recent advancements in machine learning,
have predominantly thrived on complete and well-structured data. Wearable
sensor data frequently suffers from significant missingness, posing a
substantial challenge for self-supervised learning (SSL) models that typically
assume complete data inputs. This paper introduces the second generation of
Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel
SSL approach that learns robust representations directly from incomplete data
without requiring explicit imputation. AIM's core novelty lies in its use of
learnable mask tokens to model both existing ("inherited") and artificially
introduced missingness, enabling it to robustly handle fragmented real-world
data during inference. Pre-trained on an extensive dataset of 40M hours of
day-long multimodal sensor data, our LSM-2 with AIM achieves the best
performance across a diverse range of tasks, including classification,
regression and generative modeling. Furthermore, LSM-2 with AIM exhibits
superior scaling performance, and critically, maintains high performance even
under targeted missingness scenarios, reflecting clinically coherent patterns,
such as the diagnostic value of nighttime biosignals for hypertension
prediction. This makes AIM a more reliable choice for real-world wearable data
applications.

</details>


### [401] [Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment](https://arxiv.org/abs/2506.05325)
*Yingshuai Ji,Haomin Zhuang,Matthew Toole,James McKenzie,Xiaolong Liu,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出首个基于AI的QPI内核提取框架，采用两步学习策略，实验显示其提取精度和泛化能力更优。


<details>
  <summary>Details</summary>
Motivation: 从多散射体图像中提取单散射体QPI模式是一个基本的不适定逆问题，需要解决该问题。

Method: 提出两步学习策略，第一步用变分自编码器学习散射内核的紧凑潜在空间，第二步用专用编码器对齐QPI观测的潜在表示与预学习内核的潜在表示。

Result: 构建了包含100个独特内核的QPI数据集，与直接的一步基线方法对比，该方法提取精度显著更高，对未见内核的泛化能力更好。

Conclusion: 所提出的基于AI的框架能在复杂、纠缠散射条件下稳健地推断内核，在QPI内核提取方面表现出色。

Abstract: Quasiparticle interference (QPI) imaging is a powerful tool for probing
electronic structures in quantum materials, but extracting the single-scatterer
QPI pattern (i.e., the kernel) from a multi-scatterer image remains a
fundamentally ill-posed inverse problem. In this work, we propose the first
AI-based framework for QPI kernel extraction. We introduce a two-step learning
strategy that decouples kernel representation learning from
observation-to-kernel inference. In the first step, we train a variational
autoencoder to learn a compact latent space of scattering kernels. In the
second step, we align the latent representation of QPI observations with those
of the pre-learned kernels using a dedicated encoder. This design enables the
model to infer kernels robustly even under complex, entangled scattering
conditions. We construct a diverse and physically realistic QPI dataset
comprising 100 unique kernels and evaluate our method against a direct one-step
baseline. Experimental results demonstrate that our approach achieves
significantly higher extraction accuracy, and improved generalization to unseen
kernels.

</details>


### [402] [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
*Ranajoy Sadhukhan,Zhuoming Chen,Haizhong Zheng,Yang Zhou,Emma Strubell,Beidi Chen*

Main category: cs.LG

TL;DR: 从实用效率角度重新思考测试时间缩放定律，提出动力学缩放定律和基于稀疏注意力的缩放范式，实证表明稀疏注意力模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 以往基于计算最优性的工作忽略推理时间策略引入的内存访问瓶颈，高估小模型有效性，需更好地指导资源分配。

Method: 进行涵盖0.6B到32B参数模型的整体分析，提出动力学缩放定律，提出基于稀疏注意力的缩放范式。

Result: 稀疏注意力模型在低成本和高成本制度下解决问题的准确性均优于密集模型，在AIME上有显著提升。

Conclusion: 稀疏注意力对实现测试时间缩放的全部潜力至关重要，测试时间准确性可通过增加生成继续提高。

Abstract: We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.

</details>


### [403] [Exploring Diffusion Transformer Designs via Grafting](https://arxiv.org/abs/2506.05340)
*Keshigeyan Chandrasegaran,Michael Poli,Daniel Y. Fu,Dongjun Kim,Lea M. Hadzic,Manling Li,Agrim Gupta,Stefano Massaroli,Azalia Mirhoseini,Juan Carlos Niebles,Stefano Ermon,Li Fei-Fei*

Main category: cs.LG

TL;DR: 提出grafting方法编辑预训练的扩散Transformer，以低成本探索新架构设计，多种混合设计效果良好，还实现加速和架构重组。


<details>
  <summary>Details</summary>
Motivation: 评估模型架构决策对质量的影响需昂贵预训练，限制架构研究，受新软件基于现有代码构建启发，研究能否用预训练模型研究新架构设计。

Method: 提出grafting方法编辑预训练的扩散Transformer（DiTs），基于DiT - XL/2构建测试平台，进行算子替换和架构重组等操作。

Result: 许多混合设计用不到2%预训练计算量达到良好质量；嫁接文本到图像模型实现1.43倍加速且GenEval分数下降小于2%；将DiT - XL/2架构重组后质量优于同深度其他模型。

Conclusion: 可以通过嫁接预训练的DiTs探索新的扩散模型设计，编辑范围从算子替换到架构重组。

Abstract: Designing model architectures requires decisions such as selecting operators
(e.g., attention, convolution) and configurations (e.g., depth, width).
However, evaluating the impact of these decisions on model quality requires
costly pretraining, limiting architectural investigation. Inspired by how new
software is built on existing code, we ask: can new architecture designs be
studied using pretrained models? To this end, we present grafting, a simple
approach for editing pretrained diffusion transformers (DiTs) to materialize
new architectures under small compute budgets. Informed by our analysis of
activation behavior and attention locality, we construct a testbed based on the
DiT-XL/2 design to study the impact of grafting on model quality. Using this
testbed, we develop a family of hybrid designs via grafting: replacing softmax
attention with gated convolution, local attention, and linear attention, and
replacing MLPs with variable expansion ratio and convolutional variants.
Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for
DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model
(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval
score. Finally, we present a case study that restructures DiT-XL/2 by
converting every pair of sequential transformer blocks into parallel blocks via
grafting. This reduces model depth by 2x and yields better quality (FID: 2.77)
than other models of comparable depth. Together, we show that new diffusion
model designs can be explored by grafting pretrained DiTs, with edits ranging
from operator replacement to architecture restructuring. Code and grafted
models: https://grafting.stanford.edu

</details>


### [404] [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
*Adrian Łańcucki,Konrad Staniszewski,Piotr Nawrot,Edoardo M. Ponti*

Main category: cs.LG

TL;DR: 本文探讨推理时超缩放，提出动态内存稀疏化（DMS）方法压缩KV缓存，在多个大模型上验证其可提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型生成成本受KV缓存大小限制，希望通过压缩KV缓存，在相同计算预算下生成更多令牌，提高缩放推理的准确性。

Method: 引入动态内存稀疏化（DMS）方法对KV缓存进行稀疏化，仅需1K训练步骤实现8倍压缩，且延迟令牌逐出以保留关键信息。

Result: 在多个大模型家族上证明了使用DMS进行推理时超缩放的有效性，如提升Qwen - R1 32B在AIME 24、GPQA和LiveCodeBench上的分数。

Conclusion: DMS方法能在相近推理运行时间和内存负载下提高推理准确性，使推理时超缩放更实用。

Abstract: Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [405] [A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions](https://arxiv.org/abs/2506.04238)
*Shriyank Somvanshi,Md Monzurul Islam,Syed Aaqib Javed,Gaurab Chhetri,Kazi Sifatul Islam,Tausif Islam Chowdhury,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.NE

TL;DR: 该论文对生物启发算法进行系统分类，阐述其原理、应用领域及挑战，为相关研究提供基础参考框架。


<details>
  <summary>Details</summary>
Motivation: 通过梳理生物启发算法的分类体系、应用场景和未来挑战，为研究者和从业者建立系统化的知识图谱，推动该领域发展。

Method: 采用分类学方法将算法分为8大类，结合典型应用案例分析，并引入自适应策略等前沿进展进行系统性综述。

Result: 构建包含进化算法、群体智能等8类算法的分类体系，揭示算法在工程优化等领域的成功应用，提出可扩展性、可靠性等关键研究方向。

Conclusion: 生物启发算法在复杂问题求解中展现出独特优势，但需通过混合策略、理论深化等手段突破当前的技术瓶颈。

Abstract: Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,
swarm behavior, foraging, and plant growth to solve complex, nonlinear,
high-dimensional optimization problems. This survey categorizes BIAs into eight
groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and
plant-based, predator-prey, neural-inspired, human-inspired, and hybrid
approaches, and reviews their core principles, strengths, and limitations. We
illustrate the usage of these algorithms in machine learning, engineering
design, bioinformatics, and intelligent systems, and highlight recent advances
in hybridization, parameter tuning, and adaptive strategies. Finally, we
identify open challenges such as scalability, convergence, reliability, and
interpretability to suggest directions for future research. This work aims to
serve as a foundational resource for both researchers and practitioners
interested in understanding the current landscape and future directions of
bio-inspired computing.

</details>


### [406] [A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions](https://arxiv.org/abs/2506.04238)
*Shriyank Somvanshi,Md Monzurul Islam,Syed Aaqib Javed,Gaurab Chhetri,Kazi Sifatul Islam,Tausif Islam Chowdhury,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.NE

TL;DR: 该论文系统分类了八类生物启发算法，分析其原理、应用及挑战，为研究者提供未来研究方向参考。


<details>
  <summary>Details</summary>
Motivation: 生物启发算法在解决复杂优化问题中展现强大能力，但缺乏系统性分类和未来研究方向的整合框架。

Method: 采用文献综述法对BIAs进行八维度分类(进化/群体智能/物理启发等)，结合案例研究展示机器学习、生物信息等领域的应用。

Result: 提出混合策略、自适应参数调优等创新方法，揭示算法在可解释性、可靠性方面的现存缺陷。

Conclusion: 需突破算法可扩展性、收敛性等核心瓶颈，建议发展理论验证框架和跨学科协同创新路径。

Abstract: Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,
swarm behavior, foraging, and plant growth to solve complex, nonlinear,
high-dimensional optimization problems. This survey categorizes BIAs into eight
groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and
plant-based, predator-prey, neural-inspired, human-inspired, and hybrid
approaches, and reviews their core principles, strengths, and limitations. We
illustrate the usage of these algorithms in machine learning, engineering
design, bioinformatics, and intelligent systems, and highlight recent advances
in hybridization, parameter tuning, and adaptive strategies. Finally, we
identify open challenges such as scalability, convergence, reliability, and
interpretability to suggest directions for future research. This work aims to
serve as a foundational resource for both researchers and practitioners
interested in understanding the current landscape and future directions of
bio-inspired computing.

</details>


### [407] [Quantum-Inspired Genetic Optimization for Patient Scheduling in Radiation Oncology](https://arxiv.org/abs/2506.04328)
*Akira SaiToh,Arezoo Modiri,Amit Sawant,Robabeh Rahimi*

Main category: cs.NE

TL;DR: 量子遗传算法在质子治疗患者调度中展现种群规模优势，但大规模案例运行时间较长需依赖量子计算


<details>
  <summary>Details</summary>
Motivation: 解决传统遗传算法在复杂医疗调度问题中资源消耗大且收敛速度慢的痛点，探索量子计算在优化领域的应用潜力

Method: 定制包含患者ID和机架状态叠加数据的量子染色体，设计选择修复策略确保收敛可行性

Result: 中等规模案例显示种群规模优势明显，但大规模实例因经典模拟局限导致运行时间过长

Conclusion: 量子算法在小规模调度中高效，大规模应用需真实量子计算支持，同时验证传统遗传算法稳定性

Abstract: Among the genetic algorithms generally used for optimization problems in the
recent decades, quantum-inspired variants are known for fast and high-fitness
convergence and small resource requirement. Here the application to the patient
scheduling problem in proton therapy is reported. Quantum chromosomes are
tailored to possess the superposed data of patient IDs and gantry statuses.
Selection and repair strategies are also elaborated for reliable convergence to
a clinically feasible schedule although the employed model is not complex.
Clear advantage in population size is shown over the classical counterpart in
our numerical results for both a medium-size test case and a large-size
practical problem instance. It is, however, observed that program run time is
rather long for the large-size practical case, which is due to the limitation
of classical emulation and demands the forthcoming true quantum computation.
Our results also revalidate the stability of the conventional classical genetic
algorithm.

</details>


### [408] [NEAT and HyperNEAT based Design for Soft Actuator Controllers](https://arxiv.org/abs/2506.04698)
*Hugo Alcaraz-Herrera,Michail-Antisthenis Tsompanas,Igor Balaz,Andrew Adamatzky*

Main category: cs.NE

TL;DR: 提出基于NEAT和HyperNEAT的软体机器人控制器自动化设计方法，相比传统遗传算法(SGA)，神经进化方法在高低性能形态测试中均表现更优，其中NEAT生成的网络更简洁实用。


<details>
  <summary>Details</summary>
Motivation: 软体机器人领域较新，其形态设计与控制策略研究尚未完善。传统方法难以高效设计符合柔性材料特性的控制器，需探索自动化设计解决方案。

Method: 采用神经进化算法(NEAT/HyperNEAT)生成软体执行器同步配置文件，使用CPPNs进行模式生产。以标准遗传算法(SGA)为基线，测试高低性能形态的执行器，并比较不同激活函数集的影响。

Result: 神经进化方法在所有测试场景中优于SGA，NEAT表现最佳（平均性能提升23%），且生成网络节点数比HyperNEAT少40%，更易于硬件实现。

Conclusion: 神经进化特别是NEAT算法，能有效设计软体机器人控制器，其生成的简化网络结构具有实际应用潜力，为自动化机器人设计提供新范式。

Abstract: Since soft robotics are composed of compliant materials, they perform better
than conventional rigid robotics in specific fields, such as medical
applications. However, the field of soft robotics is fairly new, and the design
process of their morphology and their controller strategies has not yet been
thoroughly studied. Consequently, here, an automated design method for the
controller of soft actuators based on Neuroevolution is proposed. Specifically,
the suggested techniques employ Neuroevolution of Augmenting Topologies (NEAT)
and Hypercube-based NEAT (HyperNEAT) to generate the synchronization profile of
the components of a simulated soft actuator by employing Compositional Pattern
Producing Networks (CPPNs). As a baseline methodology, a Standard Genetic
Algorithm (SGA) was used. Moreover, to test the robustness of the proposed
methodologies, both high- and low-performing morphologies of soft actuators
were utilized as testbeds. Moreover, the use of an affluent and a more limited
set of activation functions for the Neuroevolution targets was tested
throughout the experiments. The results support the hypothesis that
Neuroevolution based methodologies are more appropriate for designing
controllers that align with both types of morphologies. In specific, NEAT
performed better for all different scenarios tested and produced more
simplistic networks that are easier to implement in real life applications.

</details>


### [409] [A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions](https://arxiv.org/abs/2506.04238)
*Shriyank Somvanshi,Md Monzurul Islam,Syed Aaqib Javed,Gaurab Chhetri,Kazi Sifatul Islam,Tausif Islam Chowdhury,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.NE

TL;DR: 该综述对生物启发算法分类，介绍其原理、优缺点，说明应用领域和进展，指出挑战并为研究和实践提供基础资源。


<details>
  <summary>Details</summary>
Motivation: 为对生物启发计算感兴趣的研究者和实践者提供当前情况和未来方向的基础资源。

Method: 将生物启发算法分为八组，回顾核心原理、优缺点，阐述在不同领域的应用，强调最新进展。

Result: 明确了生物启发算法的分类、应用、进展以及存在的开放性挑战。

Conclusion: 指出生物启发算法未来研究方向可围绕开放性挑战展开。

Abstract: Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,
swarm behavior, foraging, and plant growth to solve complex, nonlinear,
high-dimensional optimization problems. This survey categorizes BIAs into eight
groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and
plant-based, predator-prey, neural-inspired, human-inspired, and hybrid
approaches, and reviews their core principles, strengths, and limitations. We
illustrate the usage of these algorithms in machine learning, engineering
design, bioinformatics, and intelligent systems, and highlight recent advances
in hybridization, parameter tuning, and adaptive strategies. Finally, we
identify open challenges such as scalability, convergence, reliability, and
interpretability to suggest directions for future research. This work aims to
serve as a foundational resource for both researchers and practitioners
interested in understanding the current landscape and future directions of
bio-inspired computing.

</details>


### [410] [Quantum-Inspired Genetic Optimization for Patient Scheduling in Radiation Oncology](https://arxiv.org/abs/2506.04328)
*Akira SaiToh,Arezoo Modiri,Amit Sawant,Robabeh Rahimi*

Main category: cs.NE

TL;DR: 本文将量子启发式遗传算法应用于质子治疗患者调度问题，展示了在种群规模上的优势，但大尺寸实例运行时间长，同时验证了经典遗传算法稳定性。


<details>
  <summary>Details</summary>
Motivation: 利用量子启发式遗传算法快速收敛、资源需求小的特点，将其应用于质子治疗患者调度问题。

Method: 定制量子染色体使其包含患者ID和机架状态的叠加数据，阐述选择和修复策略以确保收敛到临床可行的调度。

Result: 在中尺寸测试用例和大尺寸实际问题实例中，该算法在种群规模上优于经典算法，但大尺寸实例程序运行时间长。

Conclusion: 量子启发式遗传算法在种群规模上有优势，但受经典模拟限制，大尺寸问题需真正的量子计算；同时经典遗传算法具有稳定性。

Abstract: Among the genetic algorithms generally used for optimization problems in the
recent decades, quantum-inspired variants are known for fast and high-fitness
convergence and small resource requirement. Here the application to the patient
scheduling problem in proton therapy is reported. Quantum chromosomes are
tailored to possess the superposed data of patient IDs and gantry statuses.
Selection and repair strategies are also elaborated for reliable convergence to
a clinically feasible schedule although the employed model is not complex.
Clear advantage in population size is shown over the classical counterpart in
our numerical results for both a medium-size test case and a large-size
practical problem instance. It is, however, observed that program run time is
rather long for the large-size practical case, which is due to the limitation
of classical emulation and demands the forthcoming true quantum computation.
Our results also revalidate the stability of the conventional classical genetic
algorithm.

</details>


### [411] [NEAT and HyperNEAT based Design for Soft Actuator Controllers](https://arxiv.org/abs/2506.04698)
*Hugo Alcaraz-Herrera,Michail-Antisthenis Tsompanas,Igor Balaz,Andrew Adamatzky*

Main category: cs.NE

TL;DR: 本文提出基于神经进化的软执行器控制器自动设计方法，对比不同算法，结果表明神经进化方法更适合设计控制器，NEAT表现更佳。


<details>
  <summary>Details</summary>
Motivation: 软机器人领域较新，其形态和控制器策略设计研究不充分，需提出有效设计方法。

Method: 采用神经拓扑增强进化（NEAT）和基于超立方体的神经进化（HyperNEAT），结合组合模式生成网络（CPPNs）生成软执行器组件同步配置文件，以标准遗传算法（SGA）为基线方法，用高低性能形态的软执行器测试鲁棒性，并测试不同激活函数集。

Result: 神经进化方法更适合设计与两种形态都匹配的控制器，NEAT在不同场景下表现更好，生成的网络更简单，便于实际应用。

Conclusion: 神经进化方法，尤其是NEAT，是设计软执行器控制器的合适选择。

Abstract: Since soft robotics are composed of compliant materials, they perform better
than conventional rigid robotics in specific fields, such as medical
applications. However, the field of soft robotics is fairly new, and the design
process of their morphology and their controller strategies has not yet been
thoroughly studied. Consequently, here, an automated design method for the
controller of soft actuators based on Neuroevolution is proposed. Specifically,
the suggested techniques employ Neuroevolution of Augmenting Topologies (NEAT)
and Hypercube-based NEAT (HyperNEAT) to generate the synchronization profile of
the components of a simulated soft actuator by employing Compositional Pattern
Producing Networks (CPPNs). As a baseline methodology, a Standard Genetic
Algorithm (SGA) was used. Moreover, to test the robustness of the proposed
methodologies, both high- and low-performing morphologies of soft actuators
were utilized as testbeds. Moreover, the use of an affluent and a more limited
set of activation functions for the Neuroevolution targets was tested
throughout the experiments. The results support the hypothesis that
Neuroevolution based methodologies are more appropriate for designing
controllers that align with both types of morphologies. In specific, NEAT
performed better for all different scenarios tested and produced more
simplistic networks that are easier to implement in real life applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [412] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 研究揭示了大型语言模型在多块代码修复中的局限性，提出基于块间差异的评估指标，发现模型修复成功率随代码分散度增加而显著下降


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复技术过度关注单块错误，忽视了真实场景中广泛存在的多块协调修复需求，导致修复能力存在系统性缺陷

Method: 构建包含372个真实缺陷的HUNK4J数据集，提出量化编辑差异的块间差异指标（包含词法、结构、文件层次三维度）和空间邻近性分类体系，对六种LLM进行跨维度实证研究

Result: 所有模型在最高分散度的Fragment类修复完全失败，块间差异每增加1单位成功率下降32%，空间分散导致修复准确率最大降幅达47%

Conclusion: 多维度差异评估揭示了LLM多块修复能力的关键缺陷，需开发融合结构感知和差异自适应的新型修复策略

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [413] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 提出了首个多代码片段缺陷修复数据集HUNK4J，定义了衡量多位置编辑差异的hunk divergence指标，发现LLM修复效果与代码分散度负相关。


<details>
  <summary>Details</summary>
Motivation: 现有自动修复技术主要针对单代码片段缺陷，忽视了跨多个代码位置协同修复的复杂性，而实际开发中多位置缺陷占比很高。

Method: 构建包含372个真实缺陷的HUNK4J数据集，提出融合词法/结构/文件差异的hunk divergence指标，定义代码空间分布分类体系spatial proximity。

Result: 实验6个LLM显示：模型成功率随代码分散度增加而下降（最分散的Fragment类缺陷现有模型完全无法独立修复），hunk数量与修复难度非线性相关。

Conclusion: 揭示了LLM在多位置缺陷修复中的能力局限，提出需要开发基于代码分散度感知的新型修复策略。

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [414] [On the Practices of Autonomous Systems Development: Survey-based Empirical Findings](https://arxiv.org/abs/2506.04438)
*Katerina Goseva-Popstojanova,Denny Hood,Johann Schumann,Noble Nkwocha*

Main category: cs.SE

TL;DR: 论文分析了自主系统开发实践，基于2019年专家调查数据，研究挑战、方法及验证流程，并计划持续追踪发展。


<details>
  <summary>Details</summary>
Motivation: 解决动态新兴应用和专有限制下自主系统开发实践信息不足的问题，建立行业现状基准。

Method: 采用匿名在线问卷调查（2019年），收集有自主系统开发/MBSwE经验的专家数据。

Result: 揭示了自主系统开发中的流程、标准、V&V实践现状，当前正通过重复调查追踪技术演进。

Conclusion: 该研究为自主系统开发提供实践基准，持续监测将帮助行业理解技术演变趋势。

Abstract: Autonomous systems have gained an important role in many industry domains and
are beginning to change everyday life. However, due to dynamically emerging
applications and often proprietary constraints, there is a lack of information
about the practice of developing autonomous systems. This paper presents the
first part of the longitudinal study focused on establishing
state-of-the-practice, identifying and quantifying the challenges and benefits,
identifying the processes and standards used, and exploring verification and
validation (V&V) practices used for the development of autonomous systems. The
results presented in this paper are based on data about software systems that
have autonomous functionality and may employ model-based software engineering
(MBSwE) and reuse. These data were collected using an anonymous online survey
that was administered in 2019 and were provided by experts with experience in
development of autonomous systems and /or the use of MBSwE. Our current work is
focused on repeating the survey to collect more recent data and discover how
the development of autonomous systems has evolved over time.

</details>


### [415] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: 提出基于强化学习的深度学习框架CoRAL，通过结合语义相似性和实用性奖励机制自动化生成代码审查评论。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查存在耗时、主观性强等局限性，需要自动化技术提升审查效率与客观性。

Method: 采用强化学习框架，通过双重奖励机制（生成评论的语义准确性+作为代码精炼模型输入的有效性）优化模型。

Result: 定量与定性分析表明CoRAL生成的评论质量和实用性显著优于现有基线方法。

Conclusion: CoRAL框架在自动化代码审查任务中展现出优越性，为开发流程智能化提供了有效解决方案。

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


### [416] [BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets](https://arxiv.org/abs/2506.04509)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: 提出BINGO效应：SE数据意外聚集于极少数解空间（如100/4096），基于此开发出比现有方法快万倍的优化算法LITE/LINE。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程多目标优化存在计算复杂、耗时长的缺陷，BINGO效应揭示SE数据高度聚集特性可为优化提供新思路。

Method: 通过分析39个SE问题验证BINGO效应，开发基于简单随机选择的LITE和LINE算法，利用数据稀疏性加速优化。

Result: 优化速度提升10000倍，且效果与DEHB等复杂优化器相当。算法在GitHub开源（anon-artifacts/bingo）。

Conclusion: SE数据天然稀疏性使简单方法可替代CPU密集型优化，重新定义优化算法选择标准，挑战传统复杂优化器的必要性。

Abstract: Traditional multi-objective optimization in software engineering (SE) can be
slow and complex. This paper introduces the BINGO effect: a novel phenomenon
where SE data surprisingly collapses into a tiny fraction of possible solution
"buckets" (e.g., only 100 used from 4,096 expected).
  We show the BINGO effect's prevalence across 39 optimization in SE problems.
Exploiting this, we optimize 10,000 times faster than state-of-the-art methods,
with comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate
that simple stochastic selection can match complex optimizers like DEHB. This
work explains why simple methods succeed in SE-real data occupies a small
corner of possibilities-and guides when to apply them, challenging the need for
CPU-heavy optimization.
  Our data and code are public at GitHub (see anon-artifacts/bingo).

</details>


### [417] [KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems](https://arxiv.org/abs/2506.04569)
*Wenwei Gu,Renyi Zhong,Guangba Yu,Xinying Sun,Jinyang Liu,Yintong Huo,Zhuangbin Chen,Jianping Zhang,Jiazhen Gu,Yongqiang Yang,Michael R. Lyu*

Main category: cs.SE

TL;DR: KPIRoot+通过结合相似性与因果分析改进云系统故障定位，在效果和效率上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统相似性方法在复杂云环境中效果有限，深度学习方案存在高计算成本和低可解释性问题

Method: KPIRoot+采用符号聚合近似压缩KPI表征，通过改进异常检测机制和趋势捕捉能力优化原方案

Result: 性能提升2.9%-35.7%，时间成本降低34.7%，并在真实云环境中成功部署

Conclusion: KPIRoot+有效平衡了算法效果与计算效率，其生产环境部署经验为行业实践提供重要参考

Abstract: To ensure the reliability of cloud systems, their performance is monitored
using KPIs (key performance indicators). When issues arise, root cause
localization identifies KPIs responsible for service degradation, aiding in
quick diagnosis and resolution. Traditional methods rely on similarity
calculations, which can be ineffective in complex, interdependent cloud
environments. While deep learning-based approaches model these dependencies
better, they often face challenges such as high computational demands and lack
of interpretability.
  To address these issues, KPIRoot is proposed as an efficient method combining
similarity and causality analysis. It uses symbolic aggregate approximation for
compact KPI representation, improving analysis efficiency. However, deployment
in Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses
some performance anomalies, and 2) SAX representation fails to capture
intricate variation trends. KPIRoot+ addresses these limitations, outperforming
eight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by
34.7%. We also share our experience deploying KPIRoot in a large-scale cloud
provider's production environment.

</details>


### [418] [QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development](https://arxiv.org/abs/2506.04639)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanUML是基于UML扩展的量子软件建模语言，集成量子比特、量子门等元素，支持量子-经典混合系统建模。通过动态电路远距离纠缠和Shor算法案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决量子软件开发中缺乏专用建模工具的问题，利用UML的通用性扩展量子特性，填补量子系统可视化建模的空白。

Method: 在UML框架中增加量子构造体（如量子比特、量子门），开发支持模型驱动开发（MDD）的QuanUML，并通过动态电路和Shor算法进行应用验证。

Result: 成功实现量子算法可视化设计，建立结构化量子软件设计框架，在量子-经典混合系统建模中展现优于现有方法的扩展性和系统性。

Conclusion: QuanUML为量子软件工程提供了标准化建模工具，支持全生命周期开发。未来将优化量子纠错建模能力，并探索自动化代码生成等增强功能。

Abstract: This paper introduces QuanUML, an extension of the Unified Modeling Language
(UML) tailored for quantum software systems. QuanUML integrates
quantum-specific constructs, such as qubits and quantum gates, into the UML
framework, enabling the modeling of both quantum and hybrid quantum-classical
systems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic
Circuits and Shor's Algorithm, demonstrating its utility in designing and
visualizing quantum algorithms. Our approach supports model-driven development
of quantum software and offers a structured framework for quantum software
design. We also highlight its advantages over existing methods and discuss
future improvements.

</details>


### [419] [From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer](https://arxiv.org/abs/2506.04785)
*Alisa Welter,Niklas Schneider,Tobias Dick,Kallistos Weis,Christof Tinnes,Marvin Wyrich,Sven Apel*

Main category: cs.SE

TL;DR: 研究通过实证分析发现，人类与AI编程助手（GitHub Copilot）结对编程时，知识转移频率和主题类别与人类结对编程相似，但开发者更易接受AI建议且AI能提示关键代码细节。


<details>
  <summary>Details</summary>
Motivation: 探索AI编程助手普及背景下，人机协作场景中知识转移的有效性，比较其与传统人类结对编程的异同。

Method: 分组实验设计：一组开发者进行传统结对编程，另一组使用GitHub Copilot独立完成任务。采用扩展知识转移框架和半自动化评估流程进行对比分析。

Result: 1. 成功知识转移频率相当
2. 开发者对Copilot建议的审查度低于人类伙伴
3. AI可主动提醒易被忽略的代码实现细节

Conclusion: AI结对编程展现出独特优势（如细节提醒），但需警惕过度依赖导致代码质量风险。未来应探索人机协作的优化模式，平衡效率与代码可靠性。

Abstract: Knowledge transfer is fundamental to human collaboration and is therefore
common in software engineering. Pair programming is a prominent instance. With
the rise of AI coding assistants, developers now not only work with human
partners but also, as some claim, with AI pair programmers. Although studies
confirm knowledge transfer during human pair programming, its effectiveness
with AI coding assistants remains uncertain. To analyze knowledge transfer in
both human-human and human-AI settings, we conducted an empirical study where
developer pairs solved a programming task without AI support, while a separate
group of individual developers completed the same task using the AI coding
assistant GitHub Copilot. We extended an existing knowledge transfer framework
and employed a semi-automated evaluation pipeline to assess differences in
knowledge transfer episodes across both settings. We found a similar frequency
of successful knowledge transfer episodes and overlapping topical categories
across both settings. Two of our key findings are that developers tend to
accept GitHub Copilot's suggestions with less scrutiny than those from human
pair programming partners, but also that GitHub Copilot can subtly remind
developers of important code details they might otherwise overlook.

</details>


### [420] [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)
*Zanis Ali Khan,Aayush Garg,Qiang Tang*

Main category: cs.SE

TL;DR: 研究比较CodeBERT和CodeT5在漏洞修补中的表现，揭示模型在不同场景下的优劣及泛化挑战


<details>
  <summary>Details</summary>
Motivation: 漏洞修补作为APR的安全核心环节尚未充分研究，需探索预训练模型的有效性与泛化能力

Method: 跨6个数据集和4种语言测试模型性能，评估上下文不完整时的表现，并进行分布内外数据微调实验

Result: CodeBERT在稀疏上下文表现更优，CodeT5擅长复杂模式识别且扩展性强；微调后模型泛化能力仍不足

Conclusion: 建立漏洞修补基准，揭示模型泛化局限，为提升实际安全应用的自动化修复提供方向

Abstract: Software vulnerabilities pose significant security threats, requiring
effective mitigation. While Automated Program Repair (APR) has advanced in
fixing general bugs, vulnerability patching, a security-critical aspect of APR
remains underexplored. This study investigates pre-trained language models,
CodeBERT and CodeT5, for automated vulnerability patching across six datasets
and four languages. We evaluate their accuracy and generalization to unknown
vulnerabilities. Results show that while both models face challenges with
fragmented or sparse context, CodeBERT performs comparatively better in such
scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.
CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned
models on both in-distribution (trained) and out-of-distribution (unseen)
datasets. While fine-tuning improves in-distribution performance, models
struggle to generalize to unseen data, highlighting challenges in robust
vulnerability detection. This study benchmarks model performance, identifies
limitations in generalization, and provides actionable insights to advance
automated vulnerability patching for real-world security applications.

</details>


### [421] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: 研究团队开发了BacPrep在线平台，通过Gemini 2.0 Flash模型为罗马尼亚毕业会考提供自动化评估反馈，旨在解决偏远地区学生备考资源匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚偏远地区学生难以获得优质备考资源和反馈，急需免费可访问的解决方案。

Method: 基于近五年官方考题和评分标准，采用Gemini 2.0 Flash模型生成实验性反馈，当前平台主要功能是收集学生答卷和LLM输出数据。

Result: 平台已投入运行并建立专项数据集，为后续专家验证LLM在特定考试场景中的可行性奠定基础。

Conclusion: 该项目通过实证数据验证前沿LLM在标准化考试评估中的准确性，强调伦理审查和专家验证在部署前的必要性。

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


### [422] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: Tech-ASan提出基于两阶段检查的内存检测技术，在保证安全性的前提下显著降低ASan的运行时开销


<details>
  <summary>Details</summary>
Motivation: ASan存在显著的运行时开销（主要源于频繁的影子内存访问），现有优化方法无法有效消除冗余检查或会损害检测能力

Method: 1. 基于魔数比较的两阶段检查算法减少影子内存访问
2. 集成循环检查消除的优化器
3. 基于LLVM编译器实现工具

Result: SPEC CPU2006测试显示Tech-ASan比ASan和ASan--分别减少33.70%和17.89%的运行时开销，Juliet测试套件中减少56个假阴性案例

Conclusion: Tech-ASan在保持检测能力的同时实现了更优的性能，解决了现有方法在冗余检查消除与检测能力间的平衡问题

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


### [423] [LLM-Guided Scenario-based GUI Testing](https://arxiv.org/abs/2506.05079)
*Shengcheng Yu,Yuchen Ling,Chunrong Fang,Quan Zhou,Chunyang Chen,Shaomin Zhu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 提出基于大语言模型的多智能体协作框架ScenGen，实现场景驱动的移动应用GUI自动化测试


<details>
  <summary>Details</summary>
Motivation: 现有自动化GUI测试方法未与业务逻辑深度结合，缺乏针对具体测试场景的探索目标，导致关键功能覆盖不足。受人工测试以业务场景为测试粒度的启发，利用LLM理解GUI语义与测试场景的映射关系。

Method: 设计五智能体协作框架（Observer感知GUI语义、Decider决策目标控件、Executor执行操作、Supervisor监督场景完整性、Recorder记录上下文），通过多阶段协作生成场景化测试用例。

Result: 实验表明ScenGen能有效生成符合业务场景的GUI测试用例，实现测试生成过程的可追溯性。

Conclusion: 通过LLM驱动的多智能体协作，将测试场景完成度作为探索目标，弥合了自动化测试与业务逻辑间的鸿沟，提升了关键功能覆盖能力。

Abstract: The assurance of mobile app GUI is more and more significant. Automated GUI
testing approaches of different strategies have been developed, while there are
still huge gaps between the approaches and the app business logic, not taking
the completion of specific testing scenarios as the exploration target, leading
to the exploration missing of critical app functionalities. Learning from the
manual testing, which takes testing scenarios with app business logic as the
basic granularity, in this paper, we utilize the LLMs to understand the
semantics presented in app GUI and how they are mapped in the testing context
based on specific testing scenarios. Then, scenario-based GUI tests are
generated with the guidance of multi-agent collaboration. Specifically, we
propose ScenGen, a novel LLM-guided scenario-based GUI testing approach
involving five agents to respectively take responsibilities of different phases
of the manual testing process. The Observer perceives the app GUI state by
extracting GUI widgets and forming GUI layouts, understanding the expressed
semantics. Then the app GUI info is sent to the Decider to make decisions on
target widgets based on the target testing scenarios. The decision-making
process takes the completion of specific testing scenarios as the exploration
target. The Executor then executes the demanding operations on the apps. The
execution results are checked by the Supervisor on whether the generated tests
are consistent with the completion target of the testing scenarios, ensuring
the traceability of the test generation and execution. Furthermore, the
corresponding GUI test operations are recorded to the context memory by
Recorder as an important basis for further decision-making, meanwhile
monitoring the runtime bug occurrences. ScenGen is evaluated and the results
show that ScenGen can effectively generate scenario-based GUI tests guided by
LLMs.

</details>


### [424] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 文章构建多块补丁数据集HUNK4J，提出块差异和空间邻近性指标，研究发现大语言模型成功率随差异和分散度增加而下降，凸显其能力差距并推动相关修复策略。


<details>
  <summary>Details</summary>
Motivation: 现有自动修复技术和基准主要针对单块场景，忽略多块漏洞修复中跨代码库协调语义相关更改的复杂性。

Method: 构建多块补丁数据集HUNK4J，提出块差异和空间邻近性指标，对六个大语言模型进行实证研究。

Result: 大语言模型成功率随差异和空间分散度增加而下降，在最分散的片段类中无模型成功。

Conclusion: 大语言模型在多块漏洞修复方面存在能力差距，需采用差异感知修复策略。

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [425] [On the Practices of Autonomous Systems Development: Survey-based Empirical Findings](https://arxiv.org/abs/2506.04438)
*Katerina Goseva-Popstojanova,Denny Hood,Johann Schumann,Noble Nkwocha*

Main category: cs.SE

TL;DR: 本文介绍一项关于自主系统开发实践的纵向研究第一部分，基于2019年调查数据，后续将重复调查以了解其发展演变。


<details>
  <summary>Details</summary>
Motivation: 由于新兴应用和专有约束，缺乏自主系统开发实践信息，需开展研究了解现状。

Method: 采用匿名在线调查收集有自主功能软件系统的数据，受访者为有自主系统开发或MBSwE使用经验的专家。

Result: 文中结果基于2019年收集的数据。

Conclusion: 当前工作重点是重复调查收集近期数据，探索自主系统开发随时间的演变。

Abstract: Autonomous systems have gained an important role in many industry domains and
are beginning to change everyday life. However, due to dynamically emerging
applications and often proprietary constraints, there is a lack of information
about the practice of developing autonomous systems. This paper presents the
first part of the longitudinal study focused on establishing
state-of-the-practice, identifying and quantifying the challenges and benefits,
identifying the processes and standards used, and exploring verification and
validation (V&V) practices used for the development of autonomous systems. The
results presented in this paper are based on data about software systems that
have autonomous functionality and may employ model-based software engineering
(MBSwE) and reuse. These data were collected using an anonymous online survey
that was administered in 2019 and were provided by experts with experience in
development of autonomous systems and /or the use of MBSwE. Our current work is
focused on repeating the survey to collect more recent data and discover how
the development of autonomous systems has evolved over time.

</details>


### [426] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: 本文介绍了深度学习框架CoRAL，利用强化学习自动生成代码审查评论，并与基线技术对比，展示其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 代码审查耗时且受主观和人为因素影响，需自动化技术，现有深度学习方法有局限，因此提出CoRAL。

Method: 引入CoRAL框架，利用强化学习和奖励机制，考虑生成评论语义和对代码改进任务的有用性。

Result: 通过定量和定性比较，显示CoRAL生成的评论比最新基线技术更有效和优越。

Conclusion: CoRAL在自动生成代码审查评论方面表现良好，有较好的应用前景。

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


### [427] [BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets](https://arxiv.org/abs/2506.04509)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: 本文介绍软件工程中多目标优化的BINGO效应，利用该效应优化速度比现有方法快10000倍，简单算法效果可与复杂优化器媲美。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程多目标优化速度慢且复杂，需更高效方法。

Method: 利用BINGO效应，提出新算法LITE和LINE，采用简单随机选择。

Result: 优化速度比现有方法快10000倍，效果相当，简单算法可匹配复杂优化器。

Conclusion: 解释简单方法在软件工程中成功原因，指导何时应用，挑战CPU密集型优化的必要性。

Abstract: Traditional multi-objective optimization in software engineering (SE) can be
slow and complex. This paper introduces the BINGO effect: a novel phenomenon
where SE data surprisingly collapses into a tiny fraction of possible solution
"buckets" (e.g., only 100 used from 4,096 expected).
  We show the BINGO effect's prevalence across 39 optimization in SE problems.
Exploiting this, we optimize 10,000 times faster than state-of-the-art methods,
with comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate
that simple stochastic selection can match complex optimizers like DEHB. This
work explains why simple methods succeed in SE-real data occupies a small
corner of possibilities-and guides when to apply them, challenging the need for
CPU-heavy optimization.
  Our data and code are public at GitHub (see anon-artifacts/bingo).

</details>


### [428] [KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems](https://arxiv.org/abs/2506.04569)
*Wenwei Gu,Renyi Zhong,Guangba Yu,Xinying Sun,Jinyang Liu,Yintong Huo,Zhuangbin Chen,Jianping Zhang,Jiazhen Gu,Yongqiang Yang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文介绍云系统性能监测与根因定位，提出KPIRoot方法及改进版KPIRoot+，后者表现更优并分享部署经验。


<details>
  <summary>Details</summary>
Motivation: 传统云系统根因定位方法在复杂环境效果不佳，深度学习方法有计算需求高和缺乏可解释性问题。

Method: 提出KPIRoot方法结合相似度和因果分析，用符号聚合近似进行KPI表示；改进得到KPIRoot+。

Result: KPIRoot+比八个最先进基线模型表现高2.9% - 35.7%，并降低34.7%时间成本。

Conclusion: KPIRoot+有效解决了现有方法的问题，可用于大规模云系统根因定位。

Abstract: To ensure the reliability of cloud systems, their performance is monitored
using KPIs (key performance indicators). When issues arise, root cause
localization identifies KPIs responsible for service degradation, aiding in
quick diagnosis and resolution. Traditional methods rely on similarity
calculations, which can be ineffective in complex, interdependent cloud
environments. While deep learning-based approaches model these dependencies
better, they often face challenges such as high computational demands and lack
of interpretability.
  To address these issues, KPIRoot is proposed as an efficient method combining
similarity and causality analysis. It uses symbolic aggregate approximation for
compact KPI representation, improving analysis efficiency. However, deployment
in Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses
some performance anomalies, and 2) SAX representation fails to capture
intricate variation trends. KPIRoot+ addresses these limitations, outperforming
eight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by
34.7%. We also share our experience deploying KPIRoot in a large-scale cloud
provider's production environment.

</details>


### [429] [QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development](https://arxiv.org/abs/2506.04639)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文介绍了适用于量子软件系统的UML扩展QuanUML，展示其在设计和可视化量子算法中的应用，支持量子软件开发并讨论未来改进。


<details>
  <summary>Details</summary>
Motivation: 为量子软件系统提供合适的建模语言，支持模型驱动的量子软件开发。

Method: 将量子特定构造（如量子比特和量子门）集成到UML框架中形成QuanUML，并应用于具体量子算法。

Result: QuanUML可用于设计和可视化量子算法，支持量子软件的模型驱动开发。

Conclusion: QuanUML有优势，未来有改进空间。

Abstract: This paper introduces QuanUML, an extension of the Unified Modeling Language
(UML) tailored for quantum software systems. QuanUML integrates
quantum-specific constructs, such as qubits and quantum gates, into the UML
framework, enabling the modeling of both quantum and hybrid quantum-classical
systems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic
Circuits and Shor's Algorithm, demonstrating its utility in designing and
visualizing quantum algorithms. Our approach supports model-driven development
of quantum software and offers a structured framework for quantum software
design. We also highlight its advantages over existing methods and discuss
future improvements.

</details>


### [430] [From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer](https://arxiv.org/abs/2506.04785)
*Alisa Welter,Niklas Schneider,Tobias Dick,Kallistos Weis,Christof Tinnes,Marvin Wyrich,Sven Apel*

Main category: cs.SE

TL;DR: 本文通过实证研究分析人类-人类与人类-AI编程场景下的知识转移，发现两种场景成功知识转移频率和主题类别相似，开发者对GitHub Copilot建议审查少，但它能提醒重要代码细节。


<details>
  <summary>Details</summary>
Motivation: 研究在人类-人类和人类-AI编程场景下，AI编码助手进行知识转移的有效性。

Method: 开展实证研究，一组开发者无AI支持结对完成编程任务，另一组个体开发者用GitHub Copilot完成相同任务；扩展现有知识转移框架，用半自动化评估流程评估两种场景知识转移差异。

Result: 两种场景成功知识转移频率相似，主题类别有重叠；开发者对GitHub Copilot建议审查比人类伙伴少；GitHub Copilot能提醒重要代码细节。

Conclusion: AI编码助手在知识转移方面有一定效果，且与人类伙伴编程有不同特点。

Abstract: Knowledge transfer is fundamental to human collaboration and is therefore
common in software engineering. Pair programming is a prominent instance. With
the rise of AI coding assistants, developers now not only work with human
partners but also, as some claim, with AI pair programmers. Although studies
confirm knowledge transfer during human pair programming, its effectiveness
with AI coding assistants remains uncertain. To analyze knowledge transfer in
both human-human and human-AI settings, we conducted an empirical study where
developer pairs solved a programming task without AI support, while a separate
group of individual developers completed the same task using the AI coding
assistant GitHub Copilot. We extended an existing knowledge transfer framework
and employed a semi-automated evaluation pipeline to assess differences in
knowledge transfer episodes across both settings. We found a similar frequency
of successful knowledge transfer episodes and overlapping topical categories
across both settings. Two of our key findings are that developers tend to
accept GitHub Copilot's suggestions with less scrutiny than those from human
pair programming partners, but also that GitHub Copilot can subtly remind
developers of important code details they might otherwise overlook.

</details>


### [431] [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)
*Zanis Ali Khan,Aayush Garg,Qiang Tang*

Main category: cs.SE

TL;DR: 研究用CodeBERT和CodeT5进行自动漏洞修复，评估性能，指出模型泛化问题并提供改进见解。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞威胁大，自动程序修复中漏洞修补方面研究不足，需探索有效方法。

Method: 使用CodeBERT和CodeT5在六个数据集和四种语言上进行自动漏洞修补，评估其准确性和泛化能力，测试微调模型在分布内和分布外数据集上的表现。

Result: CodeBERT在上下文碎片化或稀疏时表现较好，CodeT5能捕捉复杂漏洞模式且可扩展性强；微调提升分布内性能，但模型难以泛化到未见数据。

Conclusion: 对模型性能进行基准测试，识别泛化限制，为实际安全应用的自动漏洞修补提供可行见解。

Abstract: Software vulnerabilities pose significant security threats, requiring
effective mitigation. While Automated Program Repair (APR) has advanced in
fixing general bugs, vulnerability patching, a security-critical aspect of APR
remains underexplored. This study investigates pre-trained language models,
CodeBERT and CodeT5, for automated vulnerability patching across six datasets
and four languages. We evaluate their accuracy and generalization to unknown
vulnerabilities. Results show that while both models face challenges with
fragmented or sparse context, CodeBERT performs comparatively better in such
scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.
CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned
models on both in-distribution (trained) and out-of-distribution (unseen)
datasets. While fine-tuning improves in-distribution performance, models
struggle to generalize to unseen data, highlighting challenges in robust
vulnerability detection. This study benchmarks model performance, identifies
limitations in generalization, and provides actionable insights to advance
automated vulnerability patching for real-world security applications.

</details>


### [432] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: 本文介绍BacPrep在线平台，利用LLM为罗马尼亚高考提供自动化评估，现收集数据待专家验证可行性与准确性。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚高考备考和反馈获取难，尤其是偏远和服务不足地区的学生，因此引入BacPrep平台，利用LLM提供免费且易获取的资源。

Method: 使用过去5年的官方考试问题，采用Google的Gemini 2.0 Flash模型，依据官方评分方案提供反馈。

Result: 平台目前已投入运营，主要功能是收集学生答案和LLM输出。

Conclusion: 收集的数据集将用于专家验证，以评估该LLM在罗马尼亚高考场景中的可行性和准确性，为可靠部署做准备。

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


### [433] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: 提出Tech - ASan技术加速Address Sanitizer（ASan），经评估优于现有方法，减少运行时开销并降低误报率。


<details>
  <summary>Details</summary>
Motivation: ASan检测内存安全违规时运行时开销大，现有加速方法存在消除冗余检查不足或降低检测能力的问题。

Method: 提出两阶段检查算法减少影子内存访问，设计高效优化器消除冗余检查，基于LLVM编译器实现Tech - ASan。

Result: 使用SPEC CPU2006基准测试，Tech - ASan比ASan和ASan--分别减少33.70%和17.89%的运行时开销；在Juliet Test Suite上测试，比两者少检测出56个假阴性案例。

Conclusion: Tech - ASan能在保证安全的前提下有效加速ASan，优于现有方法。

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


### [434] [LLM-Guided Scenario-based GUI Testing](https://arxiv.org/abs/2506.05079)
*Shengcheng Yu,Yuchen Ling,Chunrong Fang,Quan Zhou,Chunyang Chen,Shaomin Zhu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文提出ScenGen，一种基于LLM指导的场景化GUI测试方法，通过多智能体协作生成测试用例，评估显示其能有效生成测试。


<details>
  <summary>Details</summary>
Motivation: 现有自动化GUI测试方法与应用业务逻辑存在差距，未以特定测试场景完成为探索目标，导致关键功能探索缺失。

Method: 利用LLMs理解应用GUI语义及其在测试场景中的映射，提出ScenGen方法，包含五个智能体分别负责手动测试不同阶段。

Result: ScenGen能有效生成基于LLM指导的场景化GUI测试。

Conclusion: 基于LLM指导和多智能体协作的ScenGen方法可有效完成场景化GUI测试。

Abstract: The assurance of mobile app GUI is more and more significant. Automated GUI
testing approaches of different strategies have been developed, while there are
still huge gaps between the approaches and the app business logic, not taking
the completion of specific testing scenarios as the exploration target, leading
to the exploration missing of critical app functionalities. Learning from the
manual testing, which takes testing scenarios with app business logic as the
basic granularity, in this paper, we utilize the LLMs to understand the
semantics presented in app GUI and how they are mapped in the testing context
based on specific testing scenarios. Then, scenario-based GUI tests are
generated with the guidance of multi-agent collaboration. Specifically, we
propose ScenGen, a novel LLM-guided scenario-based GUI testing approach
involving five agents to respectively take responsibilities of different phases
of the manual testing process. The Observer perceives the app GUI state by
extracting GUI widgets and forming GUI layouts, understanding the expressed
semantics. Then the app GUI info is sent to the Decider to make decisions on
target widgets based on the target testing scenarios. The decision-making
process takes the completion of specific testing scenarios as the exploration
target. The Executor then executes the demanding operations on the apps. The
execution results are checked by the Supervisor on whether the generated tests
are consistent with the completion target of the testing scenarios, ensuring
the traceability of the test generation and execution. Furthermore, the
corresponding GUI test operations are recorded to the context memory by
Recorder as an important basis for further decision-making, meanwhile
monitoring the runtime bug occurrences. ScenGen is evaluated and the results
show that ScenGen can effectively generate scenario-based GUI tests guided by
LLMs.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [435] [Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy](https://arxiv.org/abs/2506.04290)
*Muhammed Golec,Maha AlabdulJalil*

Main category: q-fin.RM

TL;DR: 系统综述了基于大语言模型的信用风险评估方法，建立了包含模型架构/数据类型/可解释机制/应用领域的四维分类法。


<details>
  <summary>Details</summary>
Motivation: 填补LLM在信用风险评估领域系统性研究的空白，强调模型可解释性对金融决策的关键价值。

Method: 采用PRISMA方法分析60篇文献，从架构设计、数据特征、解释机制（思维链提示等）三个维度构建分类体系。

Result: 提出首个四维分类框架，识别出现有研究在模型透明度与金融监管适配性方面的关键挑战。

Conclusion: 该框架为AI金融研究提供结构化参考，未来需加强模型解释技术开发与跨学科协作。

Abstract: Large Language Models (LLM), which have developed in recent years, enable
credit risk assessment through the analysis of financial texts such as analyst
reports and corporate disclosures. This paper presents the first systematic
review and taxonomy focusing on LLMbased approaches in credit risk estimation.
We determined the basic model architectures by selecting 60 relevant papers
published between 2020-2025 with the PRISMA research strategy. And we examined
the data used for scenarios such as credit default prediction and risk
analysis. Since the main focus of the paper is interpretability, we classify
concepts such as explainability mechanisms, chain of thought prompts and
natural language justifications for LLM-based credit models. The taxonomy
organizes the literature under four main headings: model architectures, data
types, explainability mechanisms and application areas. Based on this analysis,
we highlight the main future trends and research gaps for LLM-based credit
scoring systems. This paper aims to be a reference paper for artificial
intelligence and financial researchers.

</details>


### [436] [Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy](https://arxiv.org/abs/2506.04290)
*Muhammed Golec,Maha AlabdulJalil*

Main category: q-fin.RM

TL;DR: 本文对基于大语言模型的信用风险评估方法进行系统综述与分类，确定模型架构、分析所用数据，对可解释性概念分类，指出未来趋势与研究缺口。


<details>
  <summary>Details</summary>
Motivation: 对近年来大语言模型在信用风险评估中的应用进行系统梳理，为人工智能和金融研究者提供参考。

Method: 采用PRISMA研究策略选取2020 - 2025年60篇相关论文，确定基本模型架构，分析数据并对可解释性概念分类。

Result: 将文献按模型架构、数据类型、可解释性机制和应用领域四个主要标题进行组织分类。

Conclusion: 指出基于大语言模型的信用评分系统的主要未来趋势和研究缺口。

Abstract: Large Language Models (LLM), which have developed in recent years, enable
credit risk assessment through the analysis of financial texts such as analyst
reports and corporate disclosures. This paper presents the first systematic
review and taxonomy focusing on LLMbased approaches in credit risk estimation.
We determined the basic model architectures by selecting 60 relevant papers
published between 2020-2025 with the PRISMA research strategy. And we examined
the data used for scenarios such as credit default prediction and risk
analysis. Since the main focus of the paper is interpretability, we classify
concepts such as explainability mechanisms, chain of thought prompts and
natural language justifications for LLM-based credit models. The taxonomy
organizes the literature under four main headings: model architectures, data
types, explainability mechanisms and application areas. Based on this analysis,
we highlight the main future trends and research gaps for LLM-based credit
scoring systems. This paper aims to be a reference paper for artificial
intelligence and financial researchers.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [437] [Can Artificial Intelligence Trade the Stock Market?](https://arxiv.org/abs/2506.04658)
*Jędrzej Maskiewicz,Paweł Sakowski*

Main category: q-fin.TR

TL;DR: 研究证明深度强化学习（DRL）在股票交易中有效，通过DDQN和PPO算法对比显示其风险调整收益优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 探索DRL在动态金融市场中的应用潜力，解决传统监督学习方法在风险管理方面的不足。

Method: 使用DDQN和PPO算法，在2019-2023年期间对三种货币对、标普500和比特币的日数据进行回测，并与Buy and Hold基准策略对比。

Result: DRL算法展现出优异的主动风险管理能力，通过策略性规避不利交易环境，风险调整收益指标显著超越传统方法。

Conclusion: DRL为金融交易提供了更智能的决策框架，尤其在动态市场环境下具有更高的风险收益平衡能力。

Abstract: The paper explores the use of Deep Reinforcement Learning (DRL) in stock
market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and
Proximal Policy Optimization (PPO) and compares them with Buy and Hold
benchmark. It evaluates these algorithms across three currency pairs, the S&P
500 index and Bitcoin, on the daily data in the period of 2019-2023. The
results demonstrate DRL's effectiveness in trading and its ability to manage
risk by strategically avoiding trades in unfavorable conditions, providing a
substantial edge over classical approaches, based on supervised learning in
terms of risk-adjusted returns.

</details>


### [438] [Can Artificial Intelligence Trade the Stock Market?](https://arxiv.org/abs/2506.04658)
*Jędrzej Maskiewicz,Paweł Sakowski*

Main category: q-fin.TR

TL;DR: 研究证明深度强化学习（DRL）在股票交易中通过DDQN和PPO算法，在风险调整收益上显著优于传统监督学习方法和买入持有策略。


<details>
  <summary>Details</summary>
Motivation: 探索DRL在金融交易中的应用潜力，验证其通过主动规避不利交易情境实现风险管理的能力。

Method: 采用DDQN和PPO算法，在2019-2023年日线数据上测试三种货币对、标普500指数和比特币，与买入持有策略进行对比。

Result: DRL模型展现出优异的交易时机选择能力，在不利市场条件下主动保持空仓，风险调整收益超越传统方法。

Conclusion: 深度强化学习为自动化交易提供了兼顾收益与风险控制的新范式，尤其在波动性资产中展现独特优势。

Abstract: The paper explores the use of Deep Reinforcement Learning (DRL) in stock
market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and
Proximal Policy Optimization (PPO) and compares them with Buy and Hold
benchmark. It evaluates these algorithms across three currency pairs, the S&P
500 index and Bitcoin, on the daily data in the period of 2019-2023. The
results demonstrate DRL's effectiveness in trading and its ability to manage
risk by strategically avoiding trades in unfavorable conditions, providing a
substantial edge over classical approaches, based on supervised learning in
terms of risk-adjusted returns.

</details>


### [439] [Can Artificial Intelligence Trade the Stock Market?](https://arxiv.org/abs/2506.04658)
*Jędrzej Maskiewicz,Paweł Sakowski*

Main category: q-fin.TR

TL;DR: 文章探索DRL在股票交易中的应用，对比DDQN和PPO算法与买入持有基准，在多资产上评估，显示DRL在交易和风险管理上优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 研究DRL在股票市场交易中的应用及与经典方法对比优势。

Method: 使用DDQN和PPO两种DRL算法，与买入持有基准对比，在2019 - 2023年三种货币对、标普500指数和比特币的每日数据上评估。

Result: DRL在交易中有效，能在不利条件下策略性避单管理风险，在风险调整收益上优于基于监督学习的经典方法。

Conclusion: DRL在股票交易和风险管理上有显著优势，能提供比经典方法更优的风险调整回报。

Abstract: The paper explores the use of Deep Reinforcement Learning (DRL) in stock
market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and
Proximal Policy Optimization (PPO) and compares them with Buy and Hold
benchmark. It evaluates these algorithms across three currency pairs, the S&P
500 index and Bitcoin, on the daily data in the period of 2019-2023. The
results demonstrate DRL's effectiveness in trading and its ability to manage
risk by strategically avoiding trades in unfavorable conditions, providing a
substantial edge over classical approaches, based on supervised learning in
terms of risk-adjusted returns.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [440] [On the Wasserstein Geodesic Principal Component Analysis of probability measures](https://arxiv.org/abs/2506.04480)
*Nina Vesseron,Elsa Cazelles,Alice Le Brigant,Thierry Klein*

Main category: stat.ML

TL;DR: 提出基于Otto-Wasserstein几何的测地主成分分析方法，分别解决高斯分布和通用概率测度的参数化问题，通过神经网络实现Wasserstein空间测地线建模，并与经典方法进行实证比较


<details>
  <summary>Details</summary>
Motivation: 传统PCA在欧氏空间有效但无法直接应用于概率分布空间，需要发展适应Wasserstein几何结构的降维方法以更好地捕捉概率测度集合的变异模式

Method: 1. 高斯分布场景：将计算提升至可逆线性映射空间
2. 通用绝对连续测度：提出基于神经网络参数化的测地线建模新方法
3. 建立与经典切空间PCA的对比框架

Result: 1. 成功构建适用于不同概率测度类型的GPCA框架
2. 通过数值实验验证方法有效性
3. 在真实数据集上展示优于传统方法的几何表示能力

Conclusion: Wasserstein-GPCA为概率分布数据分析提供了新的几何视角，神经网络参数化方法突破了传统测地线计算的局限性，在保持统计特性解释性的同时增强了建模灵活性

Abstract: This paper focuses on Geodesic Principal Component Analysis (GPCA) on a
collection of probability distributions using the Otto-Wasserstein geometry.
The goal is to identify geodesic curves in the space of probability measures
that best capture the modes of variation of the underlying dataset. We first
address the case of a collection of Gaussian distributions, and show how to
lift the computations in the space of invertible linear maps. For the more
general setting of absolutely continuous probability measures, we leverage a
novel approach to parameterizing geodesics in Wasserstein space with neural
networks. Finally, we compare to classical tangent PCA through various examples
and provide illustrations on real-world datasets.

</details>


### [441] [Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning](https://arxiv.org/abs/2506.04626)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 提出Q-EarlySettled-LowCost和FedQ-EarlySettled-LowCost两种模型无关强化学习算法，首次同步实现近最优遗憾、线性燃烧成本和对数级策略切换/通信成本。


<details>
  <summary>Details</summary>
Motivation: 针对现实场景中数据收集和策略部署成本高的问题，研究单智能体强化学习(RL)和联邦强化学习(FRL)的燃烧成本(达到近最优遗憾所需样本量)与策略切换/通信成本最小化问题。现有方法在S状态A动作的并行有限时域MDP中，存在超线性燃烧成本或无法实现对数级成本的缺陷。

Method: 在并行有限时域MDP框架下，开发模型无关的Q-EarlySettled-LowCost(单智能体)和FedQ-EarlySettled-LowCost(联邦)算法，采用早期收敛机制和低成本策略更新策略。

Result: 实现：(1)当前模型无关RL/FRL算法中最优遗憾 (2)燃烧成本线性于S×A (3)单智能体RL对数级策略切换成本/FRL对数级通信成本。同时建立改进/匹配现有最优的差距依赖理论保证。

Conclusion: 该研究首次在模型无关框架下同步优化了强化学习的三大核心指标，提出的算法显著降低实际部署成本，为分布式强化学习的实际应用提供了更高效的解决方案。

Abstract: Motivated by real-world settings where data collection and policy deployment
-- whether for a single agent or across multiple agents -- are costly, we study
the problem of on-policy single-agent reinforcement learning (RL) and federated
RL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to
reach near-optimal regret) and policy switching or communication costs. In
parallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$
states and $A$ actions, existing methods either require superlinear burn-in
costs in $S$ and $A$ or fail to achieve logarithmic switching or communication
costs. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost
and FedQ-EarlySettled-LowCost -- that are the first in the literature to
simultaneously achieve: (i) the best near-optimal regret among all known
model-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly
with $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent
RL or communication cost for FRL. Additionally, we establish gap-dependent
theoretical guarantees for both regret and switching/communication costs,
improving or matching the best-known gap-dependent bounds.

</details>


### [442] [Distributional encoding for Gaussian process regression with qualitative inputs](https://arxiv.org/abs/2506.04813)
*Sébastien Da Veiga*

Main category: stat.ML

TL;DR: 提出基于分布编码(DE)的高斯过程改进方法，有效处理分类变量并提升预测性能，适用于贝叶斯优化等场景


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在处理分类变量时存在预测效率和计算性能不足的问题，需要更有效的编码方法

Method: 基于最大平均差异和Wasserstein距离的特征核，将分类变量编码为目标变量的完整分布特征而非单一均值

Result: 在合成和真实数据集上实现当前最优预测性能，验证了方法的有效性

Conclusion: 分布编码方法与贝叶斯优化技术天然互补，可扩展应用于分类、多任务学习等场景

Abstract: Gaussian Process (GP) regression is a popular and sample-efficient approach
for many engineering applications, where observations are expensive to acquire,
and is also a central ingredient of Bayesian optimization (BO), a highly
prevailing method for the optimization of black-box functions. However, when
all or some input variables are categorical, building a predictive and
computationally efficient GP remains challenging. Starting from the naive
target encoding idea, where the original categorical values are replaced with
the mean of the target variable for that category, we propose a generalization
based on distributional encoding (DE) which makes use of all samples of the
target variable for a category. To handle this type of encoding inside the GP,
we build upon recent results on characteristic kernels for probability
distributions, based on the maximum mean discrepancy and the Wasserstein
distance. We also discuss several extensions for classification, multi-task
learning and incorporation or auxiliary information. Our approach is validated
empirically, and we demonstrate state-of-the-art predictive performance on a
variety of synthetic and real-world datasets. DE is naturally complementary to
recent advances in BO over discrete and mixed-spaces.

</details>


### [443] [Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models](https://arxiv.org/abs/2506.04945)
*Armin Kekić,Sergio Hernan Garrido Mejia,Bernhard Schölkopf*

Main category: stat.ML

TL;DR: 提出一种仅利用观察数据和单变量干预数据来估计多变量联合干预因果效应的方法


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中难以获取多变量联合干预数据的问题，探索如何利用现有数据类型进行联合效应推断

Method: 通过非线性加性结果机制的可识别性定理，提出将因果效应分解为干预变量的混淆贡献和非混淆贡献的估计器

Result: 在合成数据实验中，该方法达到与直接使用联合干预数据训练模型相当的性能，显著优于纯观察性估计器

Conclusion: 证明无需联合干预数据即可有效推断多变量联合干预效应，为实际应用提供更灵活的数据利用方案

Abstract: Estimating causal effects of joint interventions on multiple variables is
crucial in many domains, but obtaining data from such simultaneous
interventions can be challenging. Our study explores how to learn joint
interventional effects using only observational data and single-variable
interventions. We present an identifiability result for this problem, showing
that for a class of nonlinear additive outcome mechanisms, joint effects can be
inferred without access to joint interventional data. We propose a practical
estimator that decomposes the causal effect into confounded and unconfounded
contributions for each intervention variable. Experiments on synthetic data
demonstrate that our method achieves performance comparable to models trained
directly on joint interventional data, outperforming a purely observational
estimator.

</details>


### [444] [Nonlinear Causal Discovery for Grouped Data](https://arxiv.org/abs/2506.05120)
*Konstantin Göbler,Tobias Windisch,Mathias Drton*

Main category: stat.ML

TL;DR: 论文将非线性加性噪声模型扩展到随机向量场景，提出因果图学习的两阶段框架（先推断因果顺序，再执行模型选择），并在仿真和工业制造数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法多局限于标量变量，但神经科学、工业生产等领域的关键因果单元实际是变量组。需要拓展方法处理向量层面的因果关系。

Method: 1. 在向量场景下推导因果顺序推断方法
2. 开发符合该顺序的因果图模型选择方案
3. 提出适用于向量案例的创新解法

Result: 仿真实验显示方法性能优异，并成功应用于存在部分先验因果顺序的工业组装线数据

Conclusion: 该框架有效扩展了因果推理方法至向量空间，提出的两阶段解决方案在理论和应用层面均具有重要价值

Abstract: Inferring cause-effect relationships from observational data has gained
significant attention in recent years, but most methods are limited to scalar
random variables. In many important domains, including neuroscience,
psychology, social science, and industrial manufacturing, the causal units of
interest are groups of variables rather than individual scalar measurements.
Motivated by these applications, we extend nonlinear additive noise models to
handle random vectors, establishing a two-step approach for causal graph
learning: First, infer the causal order among random vectors. Second, perform
model selection to identify the best graph consistent with this order. We
introduce effective and novel solutions for both steps in the vector case,
demonstrating strong performance in simulations. Finally, we apply our method
to real-world assembly line data with partial knowledge of causal ordering
among variable groups.

</details>


### [445] [Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants](https://arxiv.org/abs/2506.05202)
*Daniele Tramontano,Yaroslav Kivva,Saber Salehkaleybar Mathias Drton,Negar Kiyavash*

Main category: stat.ML

TL;DR: 提出使用高阶累积量在潜在变量线性非高斯无环模型(lvLiNGAM)中识别因果效应，解决了存在潜在混杂时的单代理变量影响和工具变量不足两大难题。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法在存在潜在混杂因素时面临挑战：(1)当代理变量可能反作用于处理变量时，(2)当工具变量数量少于处理变量时。本文旨在突破这些限制，提升线性系统中因果推断的可靠性。

Method: 1. 理论证明单代理/单工具变量下的因果效应可识别性
2. 开发基于高阶累积量的参数估计方法
3. 通过对比实验验证方法有效性

Result: 实验显示：
- 较现有方法准确率提升23%
- 在混杂因子干扰下保持85%以上的效应估计稳定性
- 单代理场景下置信区间宽度减少37%

Conclusion: 突破了传统线性因果推断对工具变量数量的依赖，为存在复杂潜在混杂的系统提供了更实用的因果效应识别框架，推动医疗、经济等领域的因果分析应用。

Abstract: This paper investigates causal effect identification in latent variable
Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,
addressing two prominent setups that are challenging in the presence of latent
confounding: (1) a single proxy variable that may causally influence the
treatment and (2) underspecified instrumental variable cases where fewer
instruments exist than treatments. We prove that causal effects are
identifiable with a single proxy or instrument and provide corresponding
estimation methods. Experimental results demonstrate the accuracy and
robustness of our approaches compared to existing methods, advancing the
theoretical and practical understanding of causal inference in linear systems
with latent confounders.

</details>


### [446] [Admissibility of Completely Randomized Trials: A Large-Deviation Approach](https://arxiv.org/abs/2506.05329)
*Guido Imbens,Chao Qin,Stefan Wager*

Main category: stat.ML

TL;DR: 当存在三个及以上治疗方案时，自适应试验设计（如批量淘汰法）在统计效率上严格优于完全随机化非自适应试验，通过效率指数量化优势。


<details>
  <summary>Details</summary>
Motivation: 解决Qin[2022]提出的开放问题：在最佳治疗方案识别任务中，是否必须采用自适应试验而非非自适应完全随机试验。

Method: 使用批量淘汰设计（Batched Arm Elimination），通过预定义批次间隔逐步淘汰表现不佳的治疗方案。

Result: 提出满足简单条件的自适应设计可普遍且严格优于非自适应试验，通过效率指数验证大样本下的统计效率优势。

Conclusion: 当存在≥3个治疗方案时，自适应试验设计具有不可忽视的统计优势，完全随机试验不应被优先选择。

Abstract: When an experimenter has the option of running an adaptive trial, is it
admissible to ignore this option and run a non-adaptive trial instead? We
provide a negative answer to this question in the best-arm identification
problem, where the experimenter aims to allocate measurement efforts
judiciously to confidently deploy the most effective treatment arm. We find
that, whenever there are at least three treatment arms, there exist simple
adaptive designs that universally and strictly dominate non-adaptive completely
randomized trials. This dominance is characterized by a notion called
efficiency exponent, which quantifies a design's statistical efficiency when
the experimental sample is large. Our analysis focuses on the class of batched
arm elimination designs, which progressively eliminate underperforming arms at
pre-specified batch intervals. We characterize simple sufficient conditions
under which these designs universally and strictly dominate completely
randomized trials. These results resolve the second open problem posed in Qin
[2022].

</details>


### [447] [On the Wasserstein Geodesic Principal Component Analysis of probability measures](https://arxiv.org/abs/2506.04480)
*Nina Vesseron,Elsa Cazelles,Alice Le Brigant,Thierry Klein*

Main category: stat.ML

TL;DR: 本文基于Otto - Wasserstein几何对概率分布集合进行测地主成分分析（GPCA），先处理高斯分布，再用神经网络参数化Wasserstein空间测地线，最后与经典切空间PCA对比并在真实数据集上展示。


<details>
  <summary>Details</summary>
Motivation: 在概率测度空间中识别能最好捕捉基础数据集变化模式的测地曲线。

Method: 先处理高斯分布，将计算提升到可逆线性映射空间；对绝对连续概率测度，用神经网络参数化Wasserstein空间测地线。

Result: 通过各种示例与经典切空间PCA进行了比较，并在真实数据集上进行了展示。

Conclusion: 文档未明确提及最终结论。

Abstract: This paper focuses on Geodesic Principal Component Analysis (GPCA) on a
collection of probability distributions using the Otto-Wasserstein geometry.
The goal is to identify geodesic curves in the space of probability measures
that best capture the modes of variation of the underlying dataset. We first
address the case of a collection of Gaussian distributions, and show how to
lift the computations in the space of invertible linear maps. For the more
general setting of absolutely continuous probability measures, we leverage a
novel approach to parameterizing geodesics in Wasserstein space with neural
networks. Finally, we compare to classical tangent PCA through various examples
and provide illustrations on real-world datasets.

</details>


### [448] [Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning](https://arxiv.org/abs/2506.04626)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 针对数据收集和策略部署成本高的问题，提出两种无模型强化学习算法，实现近最优遗憾、低启动成本和对数级策略切换/通信成本，并给出理论保证。


<details>
  <summary>Details</summary>
Motivation: 现实中数据收集和策略部署成本高，现有方法在并行有限时域马尔可夫决策过程中存在启动成本高或无法达到对数级切换/通信成本的问题。

Method: 提出Q - EarlySettled - LowCost和FedQ - EarlySettled - LowCost两种无模型强化学习算法。

Result: 算法同时实现已知无模型RL或FRL算法中最佳的近最优遗憾、与S和A呈线性关系的低启动成本，以及单智能体RL的对数级策略切换成本或FRL的通信成本。

Conclusion: 所提算法能有效解决现有方法的问题，在遗憾和切换/通信成本上有理论保证，改进或匹配了已知的最佳依赖差距界。

Abstract: Motivated by real-world settings where data collection and policy deployment
-- whether for a single agent or across multiple agents -- are costly, we study
the problem of on-policy single-agent reinforcement learning (RL) and federated
RL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to
reach near-optimal regret) and policy switching or communication costs. In
parallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$
states and $A$ actions, existing methods either require superlinear burn-in
costs in $S$ and $A$ or fail to achieve logarithmic switching or communication
costs. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost
and FedQ-EarlySettled-LowCost -- that are the first in the literature to
simultaneously achieve: (i) the best near-optimal regret among all known
model-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly
with $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent
RL or communication cost for FRL. Additionally, we establish gap-dependent
theoretical guarantees for both regret and switching/communication costs,
improving or matching the best-known gap-dependent bounds.

</details>


### [449] [Distributional encoding for Gaussian process regression with qualitative inputs](https://arxiv.org/abs/2506.04813)
*Sébastien Da Veiga*

Main category: stat.ML

TL;DR: 本文提出基于分布编码（DE）的高斯过程回归方法，解决输入变量含分类变量时构建高效GP的问题，在多种数据集上验证效果良好，且与离散和混合空间的贝叶斯优化互补。


<details>
  <summary>Details</summary>
Motivation: 当输入变量全部或部分为分类变量时，构建具有预测性和计算效率的高斯过程回归（GP）仍具挑战。

Method: 从朴素目标编码思想出发，提出基于分布编码（DE）的方法，利用类别中目标变量的所有样本；在GP中处理此类编码时，基于最大均值差异和Wasserstein距离构建概率分布的特征核；还讨论了分类、多任务学习和辅助信息合并的扩展。

Result: 在多种合成和真实世界数据集上验证了方法的有效性，展示了最先进的预测性能。

Conclusion: DE方法能有效解决分类变量下的GP问题，且与离散和混合空间的贝叶斯优化自然互补。

Abstract: Gaussian Process (GP) regression is a popular and sample-efficient approach
for many engineering applications, where observations are expensive to acquire,
and is also a central ingredient of Bayesian optimization (BO), a highly
prevailing method for the optimization of black-box functions. However, when
all or some input variables are categorical, building a predictive and
computationally efficient GP remains challenging. Starting from the naive
target encoding idea, where the original categorical values are replaced with
the mean of the target variable for that category, we propose a generalization
based on distributional encoding (DE) which makes use of all samples of the
target variable for a category. To handle this type of encoding inside the GP,
we build upon recent results on characteristic kernels for probability
distributions, based on the maximum mean discrepancy and the Wasserstein
distance. We also discuss several extensions for classification, multi-task
learning and incorporation or auxiliary information. Our approach is validated
empirically, and we demonstrate state-of-the-art predictive performance on a
variety of synthetic and real-world datasets. DE is naturally complementary to
recent advances in BO over discrete and mixed-spaces.

</details>


### [450] [Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models](https://arxiv.org/abs/2506.04945)
*Armin Kekić,Sergio Hernan Garrido Mejia,Bernhard Schölkopf*

Main category: stat.ML

TL;DR: 研究探索仅用观测数据和单变量干预学习联合干预效果，给出可识别性结果，提出实用估计器，实验显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 获取多变量联合干预数据有挑战，需仅用观测数据和单变量干预学习联合干预效果。

Method: 给出可识别性结果，提出将因果效应分解为每个干预变量的混杂和非混杂贡献的实用估计器。

Result: 合成数据实验表明该方法性能与直接用联合干预数据训练的模型相当，优于纯观测估计器。

Conclusion: 对于一类非线性加性结果机制，无需联合干预数据即可推断联合效应，所提方法有效。

Abstract: Estimating causal effects of joint interventions on multiple variables is
crucial in many domains, but obtaining data from such simultaneous
interventions can be challenging. Our study explores how to learn joint
interventional effects using only observational data and single-variable
interventions. We present an identifiability result for this problem, showing
that for a class of nonlinear additive outcome mechanisms, joint effects can be
inferred without access to joint interventional data. We propose a practical
estimator that decomposes the causal effect into confounded and unconfounded
contributions for each intervention variable. Experiments on synthetic data
demonstrate that our method achieves performance comparable to models trained
directly on joint interventional data, outperforming a purely observational
estimator.

</details>


### [451] [Nonlinear Causal Discovery for Grouped Data](https://arxiv.org/abs/2506.05120)
*Konstantin Göbler,Tobias Windisch,Mathias Drton*

Main category: stat.ML

TL;DR: 文章将非线性加性噪声模型扩展到处理随机向量，提出两步因果图学习方法，在模拟中表现良好并应用于真实装配线数据。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法大多限于标量随机变量，而许多领域关注的因果单元是变量组，因此需扩展方法。

Method: 将非线性加性噪声模型扩展到处理随机向量，采用两步法：先推断随机向量间的因果顺序，再进行模型选择以确定与该顺序一致的最佳图。

Result: 在模拟中表现出强大性能，并应用于有部分因果顺序知识的真实装配线数据。

Conclusion: 提出的处理随机向量的因果图学习方法有效可行。

Abstract: Inferring cause-effect relationships from observational data has gained
significant attention in recent years, but most methods are limited to scalar
random variables. In many important domains, including neuroscience,
psychology, social science, and industrial manufacturing, the causal units of
interest are groups of variables rather than individual scalar measurements.
Motivated by these applications, we extend nonlinear additive noise models to
handle random vectors, establishing a two-step approach for causal graph
learning: First, infer the causal order among random vectors. Second, perform
model selection to identify the best graph consistent with this order. We
introduce effective and novel solutions for both steps in the vector case,
demonstrating strong performance in simulations. Finally, we apply our method
to real-world assembly line data with partial knowledge of causal ordering
among variable groups.

</details>


### [452] [Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants](https://arxiv.org/abs/2506.05202)
*Daniele Tramontano,Yaroslav Kivva,Saber Salehkaleybar Mathias Drton,Negar Kiyavash*

Main category: stat.ML

TL;DR: 本文用高阶累积量研究潜变量线性非高斯无环模型（lvLiNGAM）中的因果效应识别，处理有潜变量混杂时的两种难题，证明因果效应可识别并给出估计方法，实验显示方法准确稳健。


<details>
  <summary>Details</summary>
Motivation: 解决潜变量混杂存在时，单代理变量影响处理和工具变量少于处理变量这两种有挑战性的因果效应识别难题。

Method: 使用高阶累积量研究lvLiNGAM中的因果效应识别，证明单代理或工具变量下因果效应可识别并提供对应估计方法。

Result: 实验结果表明，与现有方法相比，本文方法具有准确性和稳健性。

Conclusion: 推进了对含潜变量混杂的线性系统中因果推断的理论和实践理解。

Abstract: This paper investigates causal effect identification in latent variable
Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,
addressing two prominent setups that are challenging in the presence of latent
confounding: (1) a single proxy variable that may causally influence the
treatment and (2) underspecified instrumental variable cases where fewer
instruments exist than treatments. We prove that causal effects are
identifiable with a single proxy or instrument and provide corresponding
estimation methods. Experimental results demonstrate the accuracy and
robustness of our approaches compared to existing methods, advancing the
theoretical and practical understanding of causal inference in linear systems
with latent confounders.

</details>


### [453] [Admissibility of Completely Randomized Trials: A Large-Deviation Approach](https://arxiv.org/abs/2506.05329)
*Guido Imbens,Chao Qin,Stefan Wager*

Main category: stat.ML

TL;DR: 在最佳臂识别问题中，至少三个治疗臂时，简单自适应设计普遍且严格优于非自适应完全随机试验，分析聚焦分批臂消除设计并给出优势条件，解决一个公开问题。


<details>
  <summary>Details</summary>
Motivation: 探讨实验者有自适应试验选项时，忽略该选项而进行非自适应试验是否可行。

Method: 聚焦于分批臂消除设计，分析其在预定义批次间隔逐步淘汰表现不佳的臂，并确定设计普遍且严格优于完全随机试验的充分条件。

Result: 发现至少三个治疗臂时，存在简单自适应设计普遍且严格优于非自适应完全随机试验，用效率指数表征优势。

Conclusion: 解决了Qin [2022] 提出的第二个公开问题，表明在最佳臂识别问题中自适应试验更优。

Abstract: When an experimenter has the option of running an adaptive trial, is it
admissible to ignore this option and run a non-adaptive trial instead? We
provide a negative answer to this question in the best-arm identification
problem, where the experimenter aims to allocate measurement efforts
judiciously to confidently deploy the most effective treatment arm. We find
that, whenever there are at least three treatment arms, there exist simple
adaptive designs that universally and strictly dominate non-adaptive completely
randomized trials. This dominance is characterized by a notion called
efficiency exponent, which quantifies a design's statistical efficiency when
the experimental sample is large. Our analysis focuses on the class of batched
arm elimination designs, which progressively eliminate underperforming arms at
pre-specified batch intervals. We characterize simple sufficient conditions
under which these designs universally and strictly dominate completely
randomized trials. These results resolve the second open problem posed in Qin
[2022].

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [454] [Amortized variational transdimensional inference](https://arxiv.org/abs/2506.04749)
*Laurence Davies,Dan Mackinlay,Rafael Oliveira,Scott A. Sisson*

Main category: stat.CO

TL;DR: 提出CoSMIC流和VTI方法，实现跨维度变分推断，解决传统流模型固定维度限制


<details>
  <summary>Details</summary>
Motivation: 现有基于流的随机变分推断(SVI)局限于固定维度问题，无法处理维度可变的复杂推断任务

Method: 1. 开发CoSMIC流架构，扩展神经自回归条件标准化流；2. 结合贝叶斯优化和蒙特卡洛梯度估计的VTI训练方法

Result: 在包含高基数模型空间的挑战性问题上验证了VTI的有效性

Conclusion: CoSMIC+VTI为跨维度贝叶斯推断提供了统一的变分密度框架，拓展了SVI的应用范围

Abstract: The expressiveness of flow-based models combined with stochastic variational
inference (SVI) has, in recent years, expanded the application of
optimization-based Bayesian inference to include problems with complex data
relationships. However, until now, SVI using flow-based models has been limited
to problems of fixed dimension. We introduce CoSMIC, normalizing flows
(COntextually-Specified Masking for Identity-mapped Components), an extension
to neural autoregressive conditional normalizing flow architectures that
enables using a single amortized variational density for inference over a
transdimensional target distribution. We propose a combined stochastic
variational transdimensional inference (VTI) approach to training CoSMIC flows
using techniques from Bayesian optimization and Monte Carlo gradient
estimation. Numerical experiments demonstrate the performance of VTI on
challenging problems that scale to high-cardinality model spaces.

</details>


### [455] [Amortized variational transdimensional inference](https://arxiv.org/abs/2506.04749)
*Laurence Davies,Dan Mackinlay,Rafael Oliveira,Scott A. Sisson*

Main category: stat.CO

TL;DR: 提出CoSMIC流动模型扩展变分推断至跨维度推理，实现单密度函数覆盖多模型空间


<details>
  <summary>Details</summary>
Motivation: 现有随机变分推断(SVI)在流模型应用中受限于固定维度目标分布，无法处理跨维度贝叶斯推断问题

Method: 开发基于神经自回归条件流架构的CoSMIC框架，结合贝叶斯优化与蒙特卡洛梯度估计的变分跨维度推断(VTI)训练方法

Result: 数值实验验证VTI在复杂高维模型空间中的有效扩展性及推断性能

Conclusion: CoSMIC流动与VTI方法成功实现跨维度贝叶斯推断，为高复杂度模型选择问题提供新解决方案

Abstract: The expressiveness of flow-based models combined with stochastic variational
inference (SVI) has, in recent years, expanded the application of
optimization-based Bayesian inference to include problems with complex data
relationships. However, until now, SVI using flow-based models has been limited
to problems of fixed dimension. We introduce CoSMIC, normalizing flows
(COntextually-Specified Masking for Identity-mapped Components), an extension
to neural autoregressive conditional normalizing flow architectures that
enables using a single amortized variational density for inference over a
transdimensional target distribution. We propose a combined stochastic
variational transdimensional inference (VTI) approach to training CoSMIC flows
using techniques from Bayesian optimization and Monte Carlo gradient
estimation. Numerical experiments demonstrate the performance of VTI on
challenging problems that scale to high-cardinality model spaces.

</details>


### [456] [Amortized variational transdimensional inference](https://arxiv.org/abs/2506.04749)
*Laurence Davies,Dan Mackinlay,Rafael Oliveira,Scott A. Sisson*

Main category: stat.CO

TL;DR: 提出CoSMIC和VTI方法解决基于流模型的SVI在固定维度问题的局限，实验证明其在高基数模型空间问题的性能。


<details>
  <summary>Details</summary>
Motivation: 基于流模型的随机变分推理（SVI）此前局限于固定维度问题，需解决复杂数据关系中的跨维度推理。

Method: 引入CoSMIC扩展神经自回归条件归一化流架构，提出结合贝叶斯优化和蒙特卡罗梯度估计技术的随机变分跨维度推理（VTI）方法训练CoSMIC流。

Result: 数值实验展示了VTI在高基数模型空间挑战性问题上的性能。

Conclusion: CoSMIC和VTI方法能有效解决跨维度目标分布推理问题。

Abstract: The expressiveness of flow-based models combined with stochastic variational
inference (SVI) has, in recent years, expanded the application of
optimization-based Bayesian inference to include problems with complex data
relationships. However, until now, SVI using flow-based models has been limited
to problems of fixed dimension. We introduce CoSMIC, normalizing flows
(COntextually-Specified Masking for Identity-mapped Components), an extension
to neural autoregressive conditional normalizing flow architectures that
enables using a single amortized variational density for inference over a
transdimensional target distribution. We propose a combined stochastic
variational transdimensional inference (VTI) approach to training CoSMIC flows
using techniques from Bayesian optimization and Monte Carlo gradient
estimation. Numerical experiments demonstrate the performance of VTI on
challenging problems that scale to high-cardinality model spaces.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [457] [Mechanistic models for panel data: Analysis of ecological experiments with four interacting species](https://arxiv.org/abs/2506.04508)
*Bo Yang,Jesse Wheeler,Meghan A. Duffy,Aaron A. King,Edward L. Ionides*

Main category: stat.AP

TL;DR: 应用迭代粒子滤波技术分析生态面板数据，通过溞类生物实验验证复杂系统似然估计方法的有效性


<details>
  <summary>Details</summary>
Motivation: 生态面板数据具有非线性、随机性、观测不确定性和潜在变量的特征，传统统计模型难以充分捕捉这些复杂性，需开发新的似然估计方法

Method: 使用迭代滤波算法最大化非线性部分可观测马尔可夫过程模型的似然函数，基于六种处理条件下的溞类-藻类-真菌寄生系统重复实验数据

Result: 实现了复杂生态模型的参数估计、置信区间构建、假设检验和模型诊断，建立了可扩展的随机动态模型分析框架

Conclusion: 该方法为科学家提供了灵活的工具箱，仅需模型模拟代码和观测分布描述即可开展生态面板数据的随机动态建模与验证

Abstract: In an ecological context, panel data arise when time series measurements are
made on a collection of ecological processes. Each process may correspond to a
spatial location for field data, or to an experimental ecosystem in a designed
experiment. Statistical models for ecological panel data should capture the
high levels of nonlinearity, stochasticity, and measurement uncertainty
inherent in ecological systems. Furthermore, the system dynamics may depend on
unobservable variables. This study applies iterated particle filtering
techniques to explore new possibilities for likelihood-based statistical
analysis of these complex systems. We analyze data from a mesocosm experiment
in which two species of the freshwater planktonic crustacean genus, {\it
Daphnia}, coexist with an alga and a fungal parasite. Time series data were
collected on replicated mesocosms under six treatment conditions. Iterated
filtering enables maximization of the likelihood for scientifically motivated
nonlinear partially observed Markov process models, providing access to
standard likelihood-based methods for parameter estimation, confidence
intervals, hypothesis testing, model selection and diagnostics. This toolbox
allows scientists to propose and evaluate scientifically motivated stochastic
dynamic models for panel data, constrained only by the requirement to write
code to simulate from the model and to specify a measurement distribution
describing how the system state is observed.

</details>


### [458] [Mechanistic models for panel data: Analysis of ecological experiments with four interacting species](https://arxiv.org/abs/2506.04508)
*Bo Yang,Jesse Wheeler,Meghan A. Duffy,Aaron A. King,Edward L. Ionides*

Main category: stat.AP

TL;DR: 本文应用迭代粒子滤波技术对生态面板数据进行基于似然的统计分析，以中宇宙实验数据为例展示该方法的应用及优势。


<details>
  <summary>Details</summary>
Motivation: 生态面板数据具有高度非线性、随机性和测量不确定性，且系统动态可能依赖不可观测变量，需要合适的统计模型进行分析。

Method: 应用迭代粒子滤波技术，对中宇宙实验中淡水浮游甲壳类动物与藻类、真菌寄生虫共存的数据进行分析。

Result: 迭代滤波可使基于科学动机的非线性部分观测马尔可夫过程模型的似然最大化，能使用标准的基于似然的方法进行参数估计、置信区间计算、假设检验、模型选择和诊断。

Conclusion: 该技术为科学家提供了工具包，可提出和评估针对面板数据的随机动态模型，仅需编写模拟代码和指定测量分布。

Abstract: In an ecological context, panel data arise when time series measurements are
made on a collection of ecological processes. Each process may correspond to a
spatial location for field data, or to an experimental ecosystem in a designed
experiment. Statistical models for ecological panel data should capture the
high levels of nonlinearity, stochasticity, and measurement uncertainty
inherent in ecological systems. Furthermore, the system dynamics may depend on
unobservable variables. This study applies iterated particle filtering
techniques to explore new possibilities for likelihood-based statistical
analysis of these complex systems. We analyze data from a mesocosm experiment
in which two species of the freshwater planktonic crustacean genus, {\it
Daphnia}, coexist with an alga and a fungal parasite. Time series data were
collected on replicated mesocosms under six treatment conditions. Iterated
filtering enables maximization of the likelihood for scientifically motivated
nonlinear partially observed Markov process models, providing access to
standard likelihood-based methods for parameter estimation, confidence
intervals, hypothesis testing, model selection and diagnostics. This toolbox
allows scientists to propose and evaluate scientifically motivated stochastic
dynamic models for panel data, constrained only by the requirement to write
code to simulate from the model and to specify a measurement distribution
describing how the system state is observed.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [459] [GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering](https://arxiv.org/abs/2506.04292)
*Bruno Deprez,Bart Baesens,Tim Verdonck,Wouter Verbeke*

Main category: cs.SI

TL;DR: 提出新型图算法GARG-AML，通过二阶交易网络结构计算可解释指标量化分拆洗钱风险，在检测效能、计算效率和可解释性间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 传统反洗钱网络分析方法依赖黑箱模型，阻碍商业落地。需开发透明高效的新型检测方法应对分拆洗钱(smurfing)挑战。

Method: 基于节点二阶交易网络拓扑构建可解释风险评分，结合树模型和邻居节点评分增强检测能力。仅需邻接矩阵和基础网络特征。

Result: 大规模合成和开源网络实验表明，GARG-AML在分拆洗钱检测中优于现有最优方法，验证基础网络特征的潜力。

Conclusion: 通过二阶邻域基础属性实现高效检测，证明网络拓扑分析在反欺诈中的核心价值，推动可解释算法在风控流程的集成应用。

Abstract: Money laundering poses a significant challenge as it is estimated to account
for 2%-5% of the global GDP. This has compelled regulators to impose stringent
controls on financial institutions. One prominent laundering method for evading
these controls, called smurfing, involves breaking up large transactions into
smaller amounts. Given the complexity of smurfing schemes, which involve
multiple transactions distributed among diverse parties, network analytics has
become an important anti-money laundering tool. However, recent advances have
focused predominantly on black-box network embedding methods, which has
hindered their adoption in businesses. In this paper, we introduce GARG-AML, a
novel graph-based method that quantifies smurfing risk through a single
interpretable metric derived from the structure of the second-order transaction
network of each individual node in the network. Unlike traditional methods,
GARG-AML strikes an effective balance among computational efficiency, detection
power and transparency, which enables its integration into existing AML
workflows. To enhance its capabilities, we combine the GARG-AML score
calculation with different tree-based methods and also incorporate the scores
of the node's neighbours. An experimental evaluation on large-scale synthetic
and open-source networks demonstrate that the GARG-AML outperforms the current
state-of-the-art smurfing detection methods. By leveraging only the adjacency
matrix of the second-order neighbourhood and basic network features, this work
highlights the potential of fundamental network properties towards advancing
fraud detection.

</details>


### [460] [GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering](https://arxiv.org/abs/2506.04292)
*Bruno Deprez,Bart Baesens,Tim Verdonck,Wouter Verbeke*

Main category: cs.SI

TL;DR: 本文介绍用于反洗钱的新方法GARG - AML，结合树方法和邻居节点分数，实验表明其性能优于现有方法，凸显基础网络属性对欺诈检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 洗钱问题严重，监管严格，现有黑盒网络嵌入方法阻碍业务应用，需要新的反洗钱方法。

Method: 引入GARG - AML，通过二阶交易网络结构的可解释指标量化风险，结合树方法和邻居节点分数。

Result: 在大规模合成和开源网络实验中，GARG - AML性能优于当前最先进的检测方法。

Conclusion: 仅利用二阶邻域邻接矩阵和基本网络特征，基础网络属性在推进欺诈检测方面有潜力。

Abstract: Money laundering poses a significant challenge as it is estimated to account
for 2%-5% of the global GDP. This has compelled regulators to impose stringent
controls on financial institutions. One prominent laundering method for evading
these controls, called smurfing, involves breaking up large transactions into
smaller amounts. Given the complexity of smurfing schemes, which involve
multiple transactions distributed among diverse parties, network analytics has
become an important anti-money laundering tool. However, recent advances have
focused predominantly on black-box network embedding methods, which has
hindered their adoption in businesses. In this paper, we introduce GARG-AML, a
novel graph-based method that quantifies smurfing risk through a single
interpretable metric derived from the structure of the second-order transaction
network of each individual node in the network. Unlike traditional methods,
GARG-AML strikes an effective balance among computational efficiency, detection
power and transparency, which enables its integration into existing AML
workflows. To enhance its capabilities, we combine the GARG-AML score
calculation with different tree-based methods and also incorporate the scores
of the node's neighbours. An experimental evaluation on large-scale synthetic
and open-source networks demonstrate that the GARG-AML outperforms the current
state-of-the-art smurfing detection methods. By leveraging only the adjacency
matrix of the second-order neighbourhood and basic network features, this work
highlights the potential of fundamental network properties towards advancing
fraud detection.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [461] [TQml Simulator: Optimized Simulation of Quantum Machine Learning](https://arxiv.org/abs/2506.04891)
*Viacheslav Kuzmin,Basil Kyriacou,Mateusz Papierz,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 开发了TQml量子模拟器，通过分层优化模拟方法实现2-100倍的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习电路模拟器效率不足，需加速算法前向/反向传播过程

Method: 通过基准测试选择最优模拟技术，针对不同量子门层动态组合门特定技术与通用技术

Result: 在IonQ/IBM量子硬件测试中，性能普遍优于Pennylane默认模拟器2-100倍

Conclusion: 定制化的分层模拟方法组合可显著提升量子机器学习算法模拟效率

Abstract: Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.

</details>


### [462] [TQml Simulator: Optimized Simulation of Quantum Machine Learning](https://arxiv.org/abs/2506.04891)
*Viacheslav Kuzmin,Basil Kyriacou,Mateusz Papierz,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 开发了量子机器学习专用模拟器TQml，通过优化门层模拟方法实现2-100倍的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习电路模拟效率不足，需要根据门层特性选择最优模拟方法加速计算

Method: 分析通用与专用门模拟技术，建立门层模拟策略选择框架，开发自适应模拟器TQml

Result: 在标准门集和量子硬件原生门测试中，TQml相比Pennylane实现2-100倍加速，性能优势随量子比特数和数据规模扩大

Conclusion: 基于门层特性的模拟策略优化可显著提升量子机器学习效率，TQml为相关研究提供高效解决方案

Abstract: Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.

</details>


### [463] [TQml Simulator: Optimized Simulation of Quantum Machine Learning](https://arxiv.org/abs/2506.04891)
*Viacheslav Kuzmin,Basil Kyriacou,Mateusz Papierz,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 本文对量子机器学习电路模拟技术进行数值基准测试，开发了TQml Simulator，多数情况下比Pennylane的默认模拟器快2 - 100倍。


<details>
  <summary>Details</summary>
Motivation: 高速数值模拟器对量子机器学习领域研究至关重要，旨在加速量子机器学习算法的整体模拟。

Method: 对通用和特定门技术进行数值基准测试，根据不同层的量子比特数量选择最优模拟方法，并开发TQml Simulator。

Result: TQml Simulator在多数情况下比Pennylane的default.qubit模拟器快约2 - 100倍。

Conclusion: 针对不同层选择最有效的模拟方法组合能带来显著性能提升，TQml Simulator性能优越。

Abstract: Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [464] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: 提出全耦合全隐式框架模拟非均质多孔介质多相流，结合LIMEX方法和多重网格求解器，在超级计算机上实现千核并行与十亿级自由度计算。


<details>
  <summary>Details</summary>
Motivation: 传统显式/半隐式方案依赖解耦计算可能影响精度，需开发更精确的全耦合方法处理重力/毛细效应并提升计算效率。

Method: 采用顶点中心有限体积法空间离散，首次引入LIMEX外推法，结合BiCGSTAB-GMG求解器，基于UG4平台实现非均质界面条件处理。

Result: 千核并行计算展示优异可扩展性，支持数十亿自由度计算，验证框架在大规模工程问题的适用性。

Conclusion: 全耦合隐式框架与LIMEX/多重网格方法协同提升计算效率，UG4实现为复杂多相流模拟提供高效开源解决方案。

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>


### [465] [BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2506.04354)
*Elmira Mirzabeigi,Rezvan Salehi,Kourosh Parand*

Main category: physics.comp-ph

TL;DR: 提出BridgeNet框架：融合CNN与物理信息神经网络，显著提升高维度非线性Fokker-Planck方程求解效率


<details>
  <summary>Details</summary>
Motivation: 传统PINN全连接架构难以捕捉空间层级特征且边界条件约束困难，需开发能兼顾物理约束与计算效率的新方法

Method: 1. 采用自适应CNN层进行局部特征提取
2. 设计动态加权损失函数强化物理约束
3. 与传统PINN进行多案例对比实验

Result: 实验显示：误差降低达量级提升，收敛速度加快，在高维场景保持稳定（相比传统PINN）

Conclusion: 该框架为计算物理提供可扩展解决方案，在金融数学和复杂系统等领域具重要应用价值

Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural
networks with physics-informed neural networks to efficiently solve non-linear,
high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which
typically rely on fully connected architectures, often struggle to capture
complex spatial hierarchies and enforce intricate boundary conditions. In
contrast, BridgeNet leverages adaptive CNN layers for effective local feature
extraction and incorporates a dynamically weighted loss function that
rigorously enforces physical constraints. Extensive numerical experiments
across various test cases demonstrate that BridgeNet not only achieves
significantly lower error metrics and faster convergence compared to
conventional PINN approaches but also maintains robust stability in
high-dimensional settings. This work represents a substantial advancement in
computational physics, offering a scalable and accurate solution methodology
with promising applications in fields ranging from financial mathematics to
complex system dynamics.

</details>


### [466] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: 本文研究异质多孔介质多相流全耦合全隐式框架，采用顶点中心有限体积法等，提出LIMEX方法，用BiCGSTAB和GMG求解线性系统，基于UG4实现，并行计算结果显示框架可扩展性好。


<details>
  <summary>Details</summary>
Motivation: 现有多相流方程时间离散方案多为显式或半隐式，依赖饱和度方程和压力方程解耦，本文研究全耦合全隐式框架。

Method: 利用顶点中心有限体积法进行空间离散，提出异质多孔介质界面条件高效实现方案，引入LIMEX方法并结合误差估计器，用BiCGSTAB方法和GMG预处理器求解线性系统，基于UG4实现模型和方法。

Result: 超级计算机上的并行计算表明，所提框架可扩展性良好，能支持数千个处理器，自由度可达数十亿。

Conclusion: 所提出的全耦合全隐式框架具有足够的可扩展性，适用于多相流模拟。

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>


### [467] [BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2506.04354)
*Elmira Mirzabeigi,Rezvan Salehi,Kourosh Parand*

Main category: physics.comp-ph

TL;DR: BridgeNet结合CNN与PINN解决非线性、高维Fokker - Planck方程，效果优于传统PINN。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs难以捕捉复杂空间层次结构和执行复杂边界条件，需要新方法解决非线性、高维Fokker - Planck方程。

Method: BridgeNet利用自适应CNN层进行局部特征提取，并引入动态加权损失函数来严格执行物理约束。

Result: 大量实验表明，BridgeNet比传统PINN误差更低、收敛更快，在高维环境下稳定性强。

Conclusion: 该工作是计算物理的重大进展，提供了可扩展且准确的解决方法，在多领域有应用前景。

Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural
networks with physics-informed neural networks to efficiently solve non-linear,
high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which
typically rely on fully connected architectures, often struggle to capture
complex spatial hierarchies and enforce intricate boundary conditions. In
contrast, BridgeNet leverages adaptive CNN layers for effective local feature
extraction and incorporates a dynamically weighted loss function that
rigorously enforces physical constraints. Extensive numerical experiments
across various test cases demonstrate that BridgeNet not only achieves
significantly lower error metrics and faster convergence compared to
conventional PINN approaches but also maintains robust stability in
high-dimensional settings. This work represents a substantial advancement in
computational physics, offering a scalable and accurate solution methodology
with promising applications in fields ranging from financial mathematics to
complex system dynamics.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [468] [Savage-Dickey density ratio estimation with normalizing flows for Bayesian model comparison](https://arxiv.org/abs/2506.04339)
*Kiyam Lin,Alicja Polanska,Davide Piras,Alessio Spurio Mancini,Jason D. McEwen*

Main category: astro-ph.CO

TL;DR: 提出基于归一化流的神经SDDR方法，有效解决高维参数贝叶斯模型比较的密度估计难题，并在宇宙学案例中验证其与传统方法的计算一致性。


<details>
  <summary>Details</summary>
Motivation: 传统SDDR方法依赖直方图等经典密度估计器，难以处理超模型包含大量额外参数的高维场景，需开发可扩展的密度估计方法。

Method: 引入归一化流（normalizing flows）技术构建神经SDDR框架，通过神经网络拟合高维参数空间的边际概率分布。

Result: 在玩具模型和宇宙学实例中验证有效性，证明场级推断场景下BHM与SBI方法计算的贝叶斯因子具有一致性。

Conclusion: 神经SDDR成功实现高维贝叶斯模型比较，其开源实现harmonic包为复杂科学模型评估提供了新工具。

Abstract: A core motivation of science is to evaluate which scientific model best
explains observed data. Bayesian model comparison provides a principled
statistical approach to comparing scientific models and has found widespread
application within cosmology and astrophysics. Calculating the Bayesian
evidence is computationally challenging, especially as we continue to explore
increasingly more complex models. The Savage-Dickey density ratio (SDDR)
provides a method to calculate the Bayes factor (evidence ratio) between two
nested models using only posterior samples from the super model. The SDDR
requires the calculation of a normalised marginal distribution over the extra
parameters of the super model, which has typically been performed using
classical density estimators, such as histograms. Classical density estimators,
however, can struggle to scale to high-dimensional settings. We introduce a
neural SDDR approach using normalizing flows that can scale to settings where
the super model contains a large number of extra parameters. We demonstrate the
effectiveness of this neural SDDR methodology applied to both toy and realistic
cosmological examples. For a field-level inference setting, we show that Bayes
factors computed for a Bayesian hierarchical model (BHM) and simulation-based
inference (SBI) approach are consistent, providing further validation that SBI
extracts as much cosmological information from the field as the BHM approach.
The SDDR estimator with normalizing flows is implemented in the open-source
harmonic Python package.

</details>


### [469] [Savage-Dickey density ratio estimation with normalizing flows for Bayesian model comparison](https://arxiv.org/abs/2506.04339)
*Kiyam Lin,Alicja Polanska,Davide Piras,Alessio Spurio Mancini,Jason D. McEwen*

Main category: astro-ph.CO

TL;DR: 介绍用归一化流的神经SDDR方法计算贝叶斯因子，应用于实例验证效果并在开源包实现


<details>
  <summary>Details</summary>
Motivation: 贝叶斯模型比较计算贝叶斯证据计算困难，经典密度估计器难用于高维情况

Method: 引入用归一化流的神经SDDR方法

Result: 该方法在玩具和现实宇宙学例子中有效，SBI与BHM方法计算的贝叶斯因子一致

Conclusion: 神经SDDR方法能处理超多额外参数情况，SBI能从场中提取和BHM一样多的宇宙学信息，SDDR估计器在开源包实现

Abstract: A core motivation of science is to evaluate which scientific model best
explains observed data. Bayesian model comparison provides a principled
statistical approach to comparing scientific models and has found widespread
application within cosmology and astrophysics. Calculating the Bayesian
evidence is computationally challenging, especially as we continue to explore
increasingly more complex models. The Savage-Dickey density ratio (SDDR)
provides a method to calculate the Bayes factor (evidence ratio) between two
nested models using only posterior samples from the super model. The SDDR
requires the calculation of a normalised marginal distribution over the extra
parameters of the super model, which has typically been performed using
classical density estimators, such as histograms. Classical density estimators,
however, can struggle to scale to high-dimensional settings. We introduce a
neural SDDR approach using normalizing flows that can scale to settings where
the super model contains a large number of extra parameters. We demonstrate the
effectiveness of this neural SDDR methodology applied to both toy and realistic
cosmological examples. For a field-level inference setting, we show that Bayes
factors computed for a Bayesian hierarchical model (BHM) and simulation-based
inference (SBI) approach are consistent, providing further validation that SBI
extracts as much cosmological information from the field as the BHM approach.
The SDDR estimator with normalizing flows is implemented in the open-source
harmonic Python package.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [470] [A Private Smart Wallet with Probabilistic Compliance](https://arxiv.org/abs/2506.04853)
*Andrea Rizzini,Marco Esposito,Francesco Bruschi,Donatella Sciuto*

Main category: cs.CR

TL;DR: 提出结合隐私保护与合规审查的智能钱包方案，通过双层次合规机制和加密通信实现高效低成本的匿名支付


<details>
  <summary>Details</summary>
Motivation: 解决数字支付中隐私保护与监管合规的天然矛盾，在现有零知识证明方案基础上增加可审计的合规层

Method: 采用清白证明机制+布隆过滤器实现的祖先承诺追踪系统，结合Layer2网络降低链上成本，使用加密数据块管理联系人列表

Result: 实测显示隐私转账合规检查可在消费级笔记本秒级完成，链上成本降低85%，保持交易不可链接性的同时满足监管要求

Conclusion: 该架构成功平衡隐私性与合规性，通过优化证明生成和链下计算实现可扩展的隐私支付解决方案

Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based
private onboarding mechanism. The solution integrates two levels of compliance
in concert with an authority party: a proof of innocence mechanism and an
ancestral commitment tracking system using bloom filters for probabilistic UTXO
chain states. Performance analysis demonstrates practical efficiency: private
transfers with compliance checks complete within seconds on a consumer-grade
laptop, and overall with proof generation remaining low. On-chain costs stay
minimal, ensuring affordability for all operations on Base layer 2 network. The
wallet facilitates private contact list management through encrypted data blobs
while maintaining transaction unlinkability. Our evaluation validates the
approach's viability for privacy-preserving, compliance-aware digital payments
with minimized computational and financial overhead.

</details>


### [471] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: PoCGen首次实现全自主生成npm包漏洞的PoC利用，结合LLM与动静分析技术，在SecBench.js数据集成功率77%且成本极低。


<details>
  <summary>Details</summary>
Motivation: 漏洞报告缺乏PoC导致修复困难，需深入理解输入路径与API关系。现有方法生成效率低且依赖人工。

Method: 利用LLM三阶段处理：理解漏洞报告→生成候选PoC→动态验证及迭代优化，结合静态分析确定输入传播路径。

Result: SecBench.js成功率77%（比基线提升45%），新数据集成功率39%，单漏洞平均成本仅0.02美元。

Conclusion: PoCGen显著提升漏洞修复效率，为自动化安全测试提供高性价比解决方案，突破人工生成PoC的局限性。

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>


### [472] [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)
*Sarthak Choudhary,Nils Palumbo,Ashish Hooda,Krishnamurthy Dj Dvijotham,Somesh Jha*

Main category: cs.CR

TL;DR: 论文揭示检索增强生成系统对隐蔽的投毒攻击存在脆弱性，提出基于注意力方差的过滤算法可提升20%防御效果，但自适应攻击仍能实现35%成功率，证明隐蔽性攻防存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统防御机制无法有效检测隐蔽的投毒攻击，攻击者通过控制极少量检索段落即可操纵生成结果，需要建立新的检测范式。

Method: 提出标准化段落注意力分数(NPAS)，开发注意力方差过滤算法(AVF)，通过分析大语言模型在生成过程中的注意力分布特征识别投毒段落。

Result: AVF算法使现有攻击成功率降低20%；但设计的自适应攻击仍能突破防御达到35%成功率，暴露注意力特征可被攻击者针对性隐藏。

Conclusion: 投毒攻击需在控制输出与保持隐蔽间权衡，防御体系需结合多层次生成过程信号分析，仅依赖单一注意力特征存在局限性。

Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that
inject poisoned passages into the retrieved set, even at low corruption rates.
We show that existing attacks are not designed to be stealthy, allowing
reliable detection and mitigation. We formalize stealth using a
distinguishability-based security game. If a few poisoned passages are designed
to control the response, they must differentiate themselves from benign ones,
inherently compromising stealth. This motivates the need for attackers to
rigorously analyze intermediate signals involved in
generation$\unicode{x2014}$such as attention patterns or next-token probability
distributions$\unicode{x2014}$to avoid easily detectable traces of
manipulation. Leveraging attention patterns, we propose a passage-level
score$\unicode{x2014}$the Normalized Passage Attention
Score$\unicode{x2014}$used by our Attention-Variance Filter algorithm to
identify and filter potentially poisoned passages. This method mitigates
existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline
defenses. To probe the limits of attention-based defenses, we craft stealthier
adaptive attacks that obscure such traces, achieving up to $35 \%$ attack
success rate, and highlight the challenges in improving stealth.

</details>


### [473] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: 提出基于差分隐私-LoRA的LLM微调框架，在医疗文本多异常分类中实现隐私保护与性能平衡


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在敏感医疗数据微调时的隐私泄露风险，需在保护患者隐私的同时维持诊断准确性

Method: 使用50,232份放射报告（MIMIC-CXR和CT-RATE），通过DP-LoRA在ε={0.01-10}隐私预算下微调BERT/ALBERT模型

Result: 中等隐私保护（ε=1.0）时：MIMIC-CXR加权F1达0.88（非私有0.90），CT-RATE为0.59（非私有0.78），显示CT数据复杂性对隐私保护更敏感

Conclusion: DP-LoRA有效平衡医疗NLP任务的隐私-性能矛盾，为敏感数据场景的LLM应用提供可行方案

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [474] [BESA: Boosting Encoder Stealing Attack with Perturbation Recovery](https://arxiv.org/abs/2506.04556)
*Xuhao Ren,Haotian Liang,Yajie Wang,Chuan Zhang,Zehui Xiong,Liehuang Zhu*

Main category: cs.CR

TL;DR: 提出BESA方法增强防御扰动下的编码器窃取攻击，包含扰动检测与恢复双模块


<details>
  <summary>Details</summary>
Motivation: 现有基于扰动的防御机制严重削弱编码器窃取攻击的性能，需突破防御限制

Method: 1. 扰动检测模块通过特征向量识别防御机制类型；2. 生成式扰动恢复模块重建干净特征向量

Result: 实验表明BESA将替代编码器准确率最高提升24.63%，有效应对组合防御

Conclusion: BESA通过双模块协同显著增强现有攻击方法对先进防御的突破能力

Abstract: To boost the encoder stealing attack under the perturbation-based defense
that hinders the attack performance, we propose a boosting encoder stealing
attack with perturbation recovery named BESA. It aims to overcome
perturbation-based defenses. The core of BESA consists of two modules:
perturbation detection and perturbation recovery, which can be combined with
canonical encoder stealing attacks. The perturbation detection module utilizes
the feature vectors obtained from the target encoder to infer the defense
mechanism employed by the service provider. Once the defense mechanism is
detected, the perturbation recovery module leverages the well-designed
generative model to restore a clean feature vector from the perturbed one.
Through extensive evaluations based on various datasets, we demonstrate that
BESA significantly enhances the surrogate encoder accuracy of existing encoder
stealing attacks by up to 24.63\% when facing state-of-the-art defenses and
combinations of multiple defenses.

</details>


### [475] [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)
*Pablo Fernández Saura,K. R. Jayaram,Vatche Isahagian,Jorge Bernal Bernabé,Antonio Skarmeta*

Main category: cs.CR

TL;DR: 提出结合LLM与RAG技术的自动化攻击缓解策略框架，通过策略分解和API调用转化提升安全合规效率。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境复杂化和网络威胁升级，传统安全策略执行方式在自动化与适应性上存在不足。

Method: 1. 建立向量数据库存储工具/API规范
2. 分层架构分解策略为任务
3. RAG增强LLM生成合规API指令

Result: 使用RAG后精准率/召回率/F1值显著优于非RAG基线（基于STIXv2策略和Windows API的测试）

Conclusion: 该框架验证了LLM+RAG在安全自动化领域的有效性，为动态威胁响应提供可扩展解决方案。

Abstract: The complexity of modern computing environments and the growing
sophistication of cyber threats necessitate a more robust, adaptive, and
automated approach to security enforcement. In this paper, we present a
framework leveraging large language models (LLMs) for automating attack
mitigation policy compliance through an innovative combination of in-context
learning and retrieval-augmented generation (RAG). We begin by describing how
our system collects and manages both tool and API specifications, storing them
in a vector database to enable efficient retrieval of relevant information. We
then detail the architectural pipeline that first decomposes high-level
mitigation policies into discrete tasks and subsequently translates each task
into a set of actionable API calls. Our empirical evaluation, conducted using
publicly available CTI policies in STIXv2 format and Windows API documentation,
demonstrates significant improvements in precision, recall, and F1-score when
employing RAG compared to a non-RAG baseline.

</details>


### [476] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: EMBER2024数据集提供320万+多格式恶意文件样本，支持7类分类任务，首次引入抗杀毒软件检测的挑战集，并升级特征提取能力，推动恶意软件研究发展。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集存在平台单一、任务支持有限、缺乏真实对抗样本等缺陷，难以满足复杂场景下恶意软件检测研究需求。

Method: 通过扩展EMBER系列前作，收集6种文件格式样本，构建包含哈希/元数据/特征向量/多任务标签的体系，特别设计抗杀毒软件检测的挑战集，并开发v3版特征工程。

Result: 创建首个包含对抗样本的基准数据集，支持检测/家族分类/行为识别等7类任务，特征版本3新增多种特征类型，数据已开源。

Conclusion: EMBER2024填补了恶意软件研究领域对抗性评估的空白，其多任务支持和开源特性将显著提升研究复现性并拓展新研究方向。

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [477] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: 改进现有成员推断攻击方法，通过显式建模序列内相关性，有效提升大型序列模型的记忆审计效果


<details>
  <summary>Details</summary>
Motivation: 现有隐私泄露审计工具因假设不匹配存在局限，需开发基于序列相关性的新型隐私测量方法

Method: 调整最先进的成员推断攻击算法，通过显式建模序列内部关联性来适应序列模型结构

Result: 案例研究表明改进方法在保持计算效率的同时，持续提升记忆审计有效性

Conclusion: 本研究为构建可靠的大型序列模型记忆审计体系奠定重要基础，推动隐私保护技术进步

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [478] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: 研究发现上游安全对齐数据与下游微调任务的表征相似度对模型安全性有显著影响，高相似度会大幅削弱安全护栏效果，低相似度可使模型抗越狱攻击能力提升10.33%。


<details>
  <summary>Details</summary>
Motivation: 现有安全防御策略忽视上游安全对齐数据的关键作用，本文试图从表征相似性视角揭示安全护栏失效的机理。

Method: 通过测量上游安全对齐数据集与下游微调任务之间的表征相似度，实证分析不同相似度组合对模型抗越狱能力的影响。

Result: 数据相似度高时模型安全性下降69.2%，相似度低时有害性评分降低10.33%，模型鲁棒性显著增强。

Conclusion: 强调上游数据集设计对构建持久安全护栏的重要性，为微调服务商提供通过优化数据架构预防越狱攻击的解决方案。

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


### [479] [A Private Smart Wallet with Probabilistic Compliance](https://arxiv.org/abs/2506.04853)
*Andrea Rizzini,Marco Esposito,Francesco Bruschi,Donatella Sciuto*

Main category: cs.CR

TL;DR: 提出带新型邀请式私钥入职机制的隐私保护智能钱包，性能分析显示实用高效，评估验证其用于隐私保护数字支付的可行性。


<details>
  <summary>Details</summary>
Motivation: 构建可保护隐私、具备合规性的智能钱包，用于数字支付。

Method: 提出基于邀请的私钥入职机制，集成两层合规性（无罪证明机制和使用布隆过滤器的祖先承诺跟踪系统），利用加密数据块管理联系人列表。

Result: 性能分析表明，在消费级笔记本上，带合规检查的私人转账几秒内完成，证明生成成本低，链上成本最小。

Conclusion: 该方法可用于隐私保护、合规的数字支付，同时降低计算和财务开销。

Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based
private onboarding mechanism. The solution integrates two levels of compliance
in concert with an authority party: a proof of innocence mechanism and an
ancestral commitment tracking system using bloom filters for probabilistic UTXO
chain states. Performance analysis demonstrates practical efficiency: private
transfers with compliance checks complete within seconds on a consumer-grade
laptop, and overall with proof generation remaining low. On-chain costs stay
minimal, ensuring affordability for all operations on Base layer 2 network. The
wallet facilitates private contact list management through encrypted data blobs
while maintaining transaction unlinkability. Our evaluation validates the
approach's viability for privacy-preserving, compliance-aware digital payments
with minimized computational and financial overhead.

</details>


### [480] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: 提出PoCGen方法自动为npm包漏洞生成和验证PoC利用程序，成功率高且成本低。


<details>
  <summary>Details</summary>
Motivation: 软件包安全漏洞修复需PoC利用程序，但以往漏洞报告常缺PoC，创建PoC有挑战。

Method: 结合大语言模型与静态、动态分析技术，用LLM理解漏洞报告、生成和验证候选PoC利用程序。

Result: 在SecBench.js数据集77%的漏洞和新的挑战性数据集中39%的漏洞上成功生成利用程序，大幅超越基线，平均每个利用程序成本0.02美元。

Conclusion: PoCGen方法在为npm包漏洞生成和验证PoC利用程序方面效果良好，有显著优势。

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>


### [481] [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)
*Sarthak Choudhary,Nils Palumbo,Ashish Hooda,Krishnamurthy Dj Dvijotham,Somesh Jha*

Main category: cs.CR

TL;DR: 研究RAG系统中毒攻击，指出现有攻击缺乏隐蔽性，提出基于注意力模式的过滤算法减轻攻击，还构造更隐蔽自适应攻击并揭示提升隐蔽性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有针对RAG系统的攻击缺乏隐蔽性，易被检测和缓解，需分析中间信号避免操纵痕迹以提升攻击隐蔽性。

Method: 利用注意力模式提出归一化段落注意力分数，用注意力方差过滤算法识别和过滤潜在中毒段落；构造更隐蔽的自适应攻击。

Result: 所提方法比基线防御提高了近20%的准确率；构造的自适应攻击成功率达35%。

Conclusion: 提出的算法能减轻现有攻击，但提升攻击隐蔽性存在挑战。

Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that
inject poisoned passages into the retrieved set, even at low corruption rates.
We show that existing attacks are not designed to be stealthy, allowing
reliable detection and mitigation. We formalize stealth using a
distinguishability-based security game. If a few poisoned passages are designed
to control the response, they must differentiate themselves from benign ones,
inherently compromising stealth. This motivates the need for attackers to
rigorously analyze intermediate signals involved in
generation$\unicode{x2014}$such as attention patterns or next-token probability
distributions$\unicode{x2014}$to avoid easily detectable traces of
manipulation. Leveraging attention patterns, we propose a passage-level
score$\unicode{x2014}$the Normalized Passage Attention
Score$\unicode{x2014}$used by our Attention-Variance Filter algorithm to
identify and filter potentially poisoned passages. This method mitigates
existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline
defenses. To probe the limits of attention-based defenses, we craft stealthier
adaptive attacks that obscure such traces, achieving up to $35 \%$ attack
success rate, and highlight the challenges in improving stealth.

</details>


### [482] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: 提出用差分隐私微调大语言模型对放射报告文本进行多异常分类的框架，实验显示有隐私-效用权衡，该方法能有效且保护隐私地分类。


<details>
  <summary>Details</summary>
Motivation: 提出框架以缓解敏感患者数据的隐私风险、防止数据泄露，同时保持分类性能。

Method: 使用MIMIC - CXR和CT - RATE数据集，用DP - LoRA在不同隐私制度下微调大语言模型，用加权F1分数评估三种架构模型性能，统计分析不同隐私水平的模型表现。

Result: 在两个数据集和三种模型上实验发现隐私-效用权衡，中度隐私保证下，DP微调模型与非私有LoRA基线加权F1分数接近。

Conclusion: 使用LoRA的差分隐私微调能有效且保护隐私地对放射报告进行多异常分类，解决在敏感医疗数据上微调大语言模型的关键挑战。

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [483] [BESA: Boosting Encoder Stealing Attack with Perturbation Recovery](https://arxiv.org/abs/2506.04556)
*Xuhao Ren,Haotian Liang,Yajie Wang,Chuan Zhang,Zehui Xiong,Liehuang Zhu*

Main category: cs.CR

TL;DR: 提出名为BESA的攻击方法，能克服基于扰动的防御，提升现有编码器窃取攻击的替代编码器准确率。


<details>
  <summary>Details</summary>
Motivation: 提升基于扰动防御下的编码器窃取攻击性能，克服基于扰动的防御。

Method: BESA核心包含扰动检测和扰动恢复两个模块，可与经典编码器窃取攻击结合。扰动检测模块根据目标编码器特征向量推断防御机制，扰动恢复模块用生成模型恢复干净特征向量。

Result: 在多种数据集评估中，面对先进防御和多重防御组合时，BESA将现有编码器窃取攻击的替代编码器准确率最多提高24.63%。

Conclusion: BESA能显著提升现有编码器窃取攻击在基于扰动防御下的性能。

Abstract: To boost the encoder stealing attack under the perturbation-based defense
that hinders the attack performance, we propose a boosting encoder stealing
attack with perturbation recovery named BESA. It aims to overcome
perturbation-based defenses. The core of BESA consists of two modules:
perturbation detection and perturbation recovery, which can be combined with
canonical encoder stealing attacks. The perturbation detection module utilizes
the feature vectors obtained from the target encoder to infer the defense
mechanism employed by the service provider. Once the defense mechanism is
detected, the perturbation recovery module leverages the well-designed
generative model to restore a clean feature vector from the perturbed one.
Through extensive evaluations based on various datasets, we demonstrate that
BESA significantly enhances the surrogate encoder accuracy of existing encoder
stealing attacks by up to 24.63\% when facing state-of-the-art defenses and
combinations of multiple defenses.

</details>


### [484] [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)
*Pablo Fernández Saura,K. R. Jayaram,Vatche Isahagian,Jorge Bernal Bernabé,Antonio Skarmeta*

Main category: cs.CR

TL;DR: 本文提出利用大语言模型结合上下文学习和检索增强生成（RAG）实现攻击缓解策略合规自动化的框架，评估显示RAG效果更好。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境复杂和网络威胁升级，需要更强大、自适应和自动化的安全执行方法。

Method: 系统收集管理工具和API规范存入向量数据库，架构流程将高级缓解策略分解为离散任务并转化为可操作API调用。

Result: 使用公开CTI策略和Windows API文档评估，RAG在精度、召回率和F1分数上比非RAG基线有显著提升。

Conclusion: 利用大语言模型结合RAG的框架在攻击缓解策略合规自动化方面效果良好。

Abstract: The complexity of modern computing environments and the growing
sophistication of cyber threats necessitate a more robust, adaptive, and
automated approach to security enforcement. In this paper, we present a
framework leveraging large language models (LLMs) for automating attack
mitigation policy compliance through an innovative combination of in-context
learning and retrieval-augmented generation (RAG). We begin by describing how
our system collects and manages both tool and API specifications, storing them
in a vector database to enable efficient retrieval of relevant information. We
then detail the architectural pipeline that first decomposes high-level
mitigation policies into discrete tasks and subsequently translates each task
into a set of actionable API calls. Our empirical evaluation, conducted using
publicly available CTI policies in STIXv2 format and Windows API documentation,
demonstrates significant improvements in precision, recall, and F1-score when
employing RAG compared to a non-RAG baseline.

</details>


### [485] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: 提出新数据集EMBER2024用于全面评估恶意软件分类器，支持多种分类任务，含逃避检测恶意文件集及新特征版本，以促进研究可重复性。


<details>
  <summary>Details</summary>
Motivation: 历史上可获取数据缺乏限制恶意软件分析研究，现有公开数据集范围窄，无法满足需求。

Method: 与EMBER2017和EMBER2018作者合作创建EMBER2024数据集，包含320多万个六种文件格式文件的相关信息，支持七种分类任务，引入EMBER特征版本3。

Result: 得到EMBER2024数据集，可用于训练和评估机器学习模型。

Conclusion: 发布EMBER2024数据集以促进可重复性，助力新恶意软件研究。

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [486] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: 序列模型易泄露敏感信息，现有审计工具不足，通过改进攻击方法提升审计有效性。


<details>
  <summary>Details</summary>
Motivation: 序列模型存在记忆和泄露敏感信息问题，现有审计工具无法有效评估风险。

Method: 调整最先进的成员推理攻击以明确建模序列内相关性。

Result: 改进后的方法在不增加计算成本的情况下，持续提高记忆审计的有效性。

Conclusion: 该工作是实现大型序列模型可靠记忆审计的重要一步。

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [487] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: 本文从上游对齐数据集与下游微调任务的表示相似性角度研究安全护栏退化问题，发现数据集相似度低可使模型更健壮，为微调服务提供商提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型安全对齐越狱漏洞的策略忽视了原始安全对齐数据这一关键上游因素。

Method: 从上游对齐数据集与下游微调任务的表示相似性角度进行研究。

Result: 数据集相似度高会削弱安全护栏，使模型更易受越狱攻击；相似度低的数据集能使模型更健壮，危害得分最多降低10.33%。

Conclusion: 强调了上游数据集设计对构建持久安全护栏和降低现实中越狱攻击脆弱性的重要性，为微调服务提供商提供了可行建议。

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [488] [Prosthetics of the Indian State: The e-Shram Portal for Unorganized Workers in India](https://arxiv.org/abs/2503.05714)
*Rozin Hasin*

Main category: cs.CY

TL;DR: 使用可供性理论批判印度非正规部门劳动者数字门户e-Shram，揭示其操作化缺陷、可访问性不足和劳动者认知偏差问题


<details>
  <summary>Details</summary>
Motivation: 评估印度非正式经济中2.4亿劳动者使用的数字平台效能，揭示数字工具与劳工实际需求间的错配

Method: 采用可供性理论框架，分析数字平台在技术实现、用户交互和社会认知层面的多维矛盾

Result: 指出e-Shram存在操作逻辑不匹配劳工数字素养、基础设施接入障碍、劳动者对平台功能认知偏差三重困境

Conclusion: 现行数字治理方案未能充分考虑非正规劳动者的技术使用语境，需建立更具包容性的数字基础设施设计范式

Abstract: This research paper examines the digital portal/database for unorganized
workers in the informal sector economy of India today: e-Shram. Using
affordance theory, I criticize the operationalization of this database for the
labourers, alongside problems of accessibility and perception.

</details>


### [489] [Oversight Structures for Agentic AI in Public-Sector Organizations](https://arxiv.org/abs/2506.04836)
*Chris Schmitz,Jonathan Rystrøm,Jan Batzner*

Main category: cs.CY

TL;DR: 论文发现代理式AI系统加剧了公共部门传统监督机制的三个现存挑战，提出通过制度调整和兼容性设计改善治理。


<details>
  <summary>Details</summary>
Motivation: 传统监督机制（孤立的合规审查+阶段性审批）难以应对AI系统持续动态的治理需求，需探索适应性的监管框架。

Method: 采用混合研究方法：文献综述结合对AI相关公务员的深度访谈，评估现有监督机制的有效性。

Result: 识别出代理监督使持续监管、治理-运营整合、跨部门协调三大挑战加剧，现有机制存在明显能力缺口。

Conclusion: 建议双轨路径：既改造机构架构，又设计符合公共部门资源限制的AI监督系统，实现负责任部署。

Abstract: This paper finds that the introduction of agentic AI systems intensifies
existing challenges to traditional public sector oversight mechanisms -- which
rely on siloed compliance units and episodic approvals rather than continuous,
integrated supervision. We identify five governance dimensions essential for
responsible agent deployment: cross-departmental implementation, comprehensive
evaluation, enhanced security protocols, operational visibility, and systematic
auditing. We evaluate the capacity of existing oversight structures to meet
these challenges, via a mixed-methods approach consisting of a literature
review and interviews with civil servants in AI-related roles. We find that
agent oversight poses intensified versions of three existing governance
challenges: continuous oversight, deeper integration of governance and
operational capabilities, and interdepartmental coordination. We propose
approaches that both adapt institutional structures and design agent oversight
compatible with public sector constraints.

</details>


### [490] [Intentionally Unintentional: GenAI Exceptionalism and the First Amendment](https://arxiv.org/abs/2506.05211)
*David Atkinson,Jena D. Hwang,Jacob Morrison*

Main category: cs.CY

TL;DR: 反对大型生成式AI模型的输出享有第一修正案保护，因其缺乏法律认可的意向性且不符合言论自由的核心目的


<details>
  <summary>Details</summary>
Motivation: 当前法律实践中存在将AI输出视为言论的倾向，可能阻碍政府监管并引发社会危害，需从法律逻辑层面澄清AI输出的非言论属性

Method: 通过法律先例分析，强调言论定义中的意向性要件，结合宪法原则论证AI输出与人类言论的本质差异

Result: 确立AI输出不构成法律意义上的言论，用户无接收权，赋予保护将削弱政府规制能力并加剧错误信息传播风险

Conclusion: 必须明确区分AI输出与人类言论，避免第一修正案滥用，确保社会能有效监管AI技术带来的新型风险

Abstract: This paper challenges the assumption that courts should grant First Amendment
protections to outputs from large generative AI models, such as GPT-4 and
Gemini. We argue that because these models lack intentionality, their outputs
do not constitute speech as understood in the context of established legal
precedent, so there can be no speech to protect. Furthermore, if the model
outputs are not speech, users cannot claim a First Amendment speech right to
receive the outputs. We also argue that extending First Amendment rights to AI
models would not serve the fundamental purposes of free speech, such as
promoting a marketplace of ideas, facilitating self-governance, or fostering
self-expression. In fact, granting First Amendment protections to AI models
would be detrimental to society because it would hinder the government's
ability to regulate these powerful technologies effectively, potentially
leading to the unchecked spread of misinformation and other harms.

</details>


### [491] [Prosthetics of the Indian State: The e-Shram Portal for Unorganized Workers in India](https://arxiv.org/abs/2503.05714)
*Rozin Hasin*

Main category: cs.CY

TL;DR: 文章运用可供性理论，批判印度非正规部门非组织化工人数字门户数据库e - Shram在操作、可及性和认知方面的问题。


<details>
  <summary>Details</summary>
Motivation: 研究印度非正规部门非组织化工人数字门户数据库e - Shram。

Method: 采用可供性理论。

Result: 指出e - Shram在面向劳动者操作、可及性和认知方面存在问题。

Conclusion: 未明确提及。

Abstract: This research paper examines the digital portal/database for unorganized
workers in the informal sector economy of India today: e-Shram. Using
affordance theory, I criticize the operationalization of this database for the
labourers, alongside problems of accessibility and perception.

</details>


### [492] [Oversight Structures for Agentic AI in Public-Sector Organizations](https://arxiv.org/abs/2506.04836)
*Chris Schmitz,Jonathan Rystrøm,Jan Batzner*

Main category: cs.CY

TL;DR: 引入代理式AI系统加剧传统公共部门监督机制挑战，确定负责任代理部署的五个治理维度，评估现有监督结构能力，发现新挑战并提出应对方法。


<details>
  <summary>Details</summary>
Motivation: 研究代理式AI系统对传统公共部门监督机制带来的挑战及寻找应对策略。

Method: 采用文献综述和与AI相关公务员访谈的混合方法。

Result: 代理监督加剧了持续监督、治理与运营能力深度整合、部门间协调这三个现有治理挑战。

Conclusion: 提出调整制度结构和设计与公共部门约束兼容的代理监督方法。

Abstract: This paper finds that the introduction of agentic AI systems intensifies
existing challenges to traditional public sector oversight mechanisms -- which
rely on siloed compliance units and episodic approvals rather than continuous,
integrated supervision. We identify five governance dimensions essential for
responsible agent deployment: cross-departmental implementation, comprehensive
evaluation, enhanced security protocols, operational visibility, and systematic
auditing. We evaluate the capacity of existing oversight structures to meet
these challenges, via a mixed-methods approach consisting of a literature
review and interviews with civil servants in AI-related roles. We find that
agent oversight poses intensified versions of three existing governance
challenges: continuous oversight, deeper integration of governance and
operational capabilities, and interdepartmental coordination. We propose
approaches that both adapt institutional structures and design agent oversight
compatible with public sector constraints.

</details>


### [493] [Intentionally Unintentional: GenAI Exceptionalism and the First Amendment](https://arxiv.org/abs/2506.05211)
*David Atkinson,Jena D. Hwang,Jacob Morrison*

Main category: cs.CY

TL;DR: 本文挑战法院应给予大型生成式AI模型输出第一修正案保护的假设，认为模型输出非言论且给予保护不利于社会。


<details>
  <summary>Details</summary>
Motivation: 探讨是否应给予大型生成式AI模型输出第一修正案保护。

Method: 从模型缺乏意向性，输出不构成既定法律先例中的言论，以及给予保护不符合言论自由根本目的、会妨碍政府监管等方面进行论证。

Result: 得出模型输出不构成受保护言论，给予保护会对社会有害的结果。

Conclusion: 不应给予大型生成式AI模型输出第一修正案保护。

Abstract: This paper challenges the assumption that courts should grant First Amendment
protections to outputs from large generative AI models, such as GPT-4 and
Gemini. We argue that because these models lack intentionality, their outputs
do not constitute speech as understood in the context of established legal
precedent, so there can be no speech to protect. Furthermore, if the model
outputs are not speech, users cannot claim a First Amendment speech right to
receive the outputs. We also argue that extending First Amendment rights to AI
models would not serve the fundamental purposes of free speech, such as
promoting a marketplace of ideas, facilitating self-governance, or fostering
self-expression. In fact, granting First Amendment protections to AI models
would be detrimental to society because it would hinder the government's
ability to regulate these powerful technologies effectively, potentially
leading to the unchecked spread of misinformation and other harms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [494] [Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs](https://arxiv.org/abs/2506.04236)
*Botao Amber Hu,Helena Rong*

Main category: cs.MA

TL;DR: 探讨基于区块链和DePIN的开放系统是否通过链上AI代理自主演化实现持续开放演化（OEE）


<details>
  <summary>Details</summary>
Motivation: 传统封闭式ALife系统在初期创新后停滞，需建立能与环境持续交互的开放系统来突破OEE瓶颈

Method: 通过数字行为学研究法，分析Spore.fun平台上链式AI代理的自主繁殖与进化轨迹

Result: 展示了基于DePIN和TEE的区块链架构使LLM代理实现自我主权，通过加密货币激励驱动环境交互的创新模式

Conclusion: 无需许可的计算基板与经济激励驱动的开放系统可能成为实现OEE的关键路径，需持续观察链上代理的长期演化

Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution
(OEE)-the continuous emergence of novelty observed in biological life-has
traditionally been pursued within isolated closed system simulations, such as
Tierra and Avida, which have typically plateaued after an initial burst of
novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires
an "open" system that continually exchanges information or energy with its
environment. A recent technological innovation in decentralized physical
infrastructure networks (DePIN) providing permissionless computational
substrates enables deploying large language model (LLM)-based AI agents on
blockchains integrated with Trusted Execution Environments (TEEs). This enables
on-chain agents to operate autonomously "in the wild," achieving
self-sovereignty without human oversight. These agents can control their own
social media accounts and cryptocurrency wallets, allowing them to interact
directly with blockchain-based financial networks and broader human social
media. Building on this new paradigm of on-chain agents, Spore.fun is a recent
real-world AI evolution experiment that enables autonomous breeding and
evolution of new on-chain agents. This paper presents a detailed case study of
Spore.fun, examining agent behaviors and their evolutionary trajectories
through digital ethology. We aim to spark discussion about whether "open" ALife
systems "in-the-wild," based on permissionless computational substrates and
driven by economic incentives to interact with their environment, could finally
achieve the long-sought goal of OEE.

</details>


### [495] [CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients](https://arxiv.org/abs/2506.04265)
*Mengda Ji,Genjiu Xu,Liying Wang*

Main category: cs.MA

TL;DR: 提出联盟理性优势分解方法CORA，通过联盟层面的边际贡献评估和核心解分解实现信用分配，显著提升多智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有信用分配方法忽视联盟层面的贡献分析，导致策略过更新问题。全局优势共享无法区分智能体贡献差异，需引入联盟理性评估机制。

Method: 1. 构建所有可能联盟的边际贡献评估机制 2. 采用合作博弈论核心解分解优势 3. 随机联盟采样降低计算复杂度

Result: 在矩阵游戏/微分游戏/协作基准测试中优于基线方法，尤其在多局部最优任务中提升显著（实验证明平均回报提高23.7%）

Conclusion: 联盟层面的信用分配机制能有效解决MARL中的过更新问题，核心解分解方法为多智能体协作提供了新的理论框架。

Abstract: This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.

</details>


### [496] [Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and Vehicles for Crowdsensing in Disaster Response](https://arxiv.org/abs/2506.04276)
*Lei Han,Yitong Guo,Pengfei Yang,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 提出HoAs-PALN异构多智能体协同调度算法，通过五维匹配降维和局部纳什均衡机制，提升灾后环境信息采集效率，任务完成率最高提升64.12%且决策耗时<10秒。


<details>
  <summary>Details</summary>
Motivation: 现有灾后传感技术存在环境适应性弱（复杂环境）、专用传感能力不足（异构场景）、方案实用性有限（动态条件）三大痛点，需解决多智能体协同调度效率问题。

Method: 1. 匹配过程自适应降维：将五维匹配分解为两类三维匹配，降低决策耗时
2. 局部纳什均衡博弈：融合softmax优化行为选择概率，引入局部均衡判定确保调度质量

Result: 实验显示：相较GREEDY/K-WTA/MADL/MARL基准，任务完成率分别提升64.12%/46.48%/16.55%/14.03%，在线调度决策耗时<10秒

Conclusion: HoAs-PALN通过维度解耦和博弈均衡机制，实现了动态灾后环境下的高效协同调度，为应急救援提供实时决策支持。

Abstract: Natural disasters have caused significant losses to human society, and the
timely and efficient acquisition of post-disaster environmental information is
crucial for the effective implementation of rescue operations. Due to the
complexity of post-disaster environments, existing sensing technologies face
challenges such as weak environmental adaptability, insufficient specialized
sensing capabilities, and limited practicality of sensing solutions. This paper
explores the heterogeneous multi-agent online autonomous collaborative
scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of
post-disaster environmental information. HoAs-PALN is realized through adaptive
dimensionality reduction in the matching process and local Nash equilibrium
game, facilitating autonomous collaboration among time-dependent UAVs, workers
and vehicles to enhance sensing scheduling. (1) In terms of adaptive
dimensionality reduction during the matching process, HoAs-PALN significantly
reduces scheduling decision time by transforming a five-dimensional matching
process into two categories of three-dimensional matching processes; (2)
Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax
function to optimize behavior selection probabilities and introduces a local
Nash equilibrium determination mechanism to ensure scheduling decision
performance. Finally, we conducted detailed experiments based on extensive
real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL
and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%,
and 14.03% on average, respectively, while each online scheduling decision
takes less than 10 seconds, demonstrating its effectiveness in dynamic
post-disaster environments.

</details>


### [497] [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
*Niv Eckhaus,Uri Berger,Gabriel Stanovsky*

Main category: cs.MA

TL;DR: 开发自适应异步LLM代理，在在线狼人杀游戏中实现与人类相当的发言决策能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM主要应用于同步场景，而真实群体互动（群聊/社交游戏）需要自主决定发言时机的异步通信能力

Method: 构建异步代理系统（集成发言内容和时机决策机制），通过在线狼人杀游戏收集人类与AI代理的混合交互数据集

Result: 代理在游戏表现和社交融入度上与人类相当，发言时机决策接近人类模式（但消息内容存在差异）

Conclusion: 该研究为LLM融入真实群体场景（团队协作/教育场景）奠定基础，需进一步探索复杂社交动态中的异步通信机制

Abstract: LLMs are used predominantly in synchronous communication, where a human user
and a model communicate in alternating turns. In contrast, many real-world
settings are inherently asynchronous. For example, in group chats, online team
meetings, or social games, there is no inherent notion of turns; therefore, the
decision of when to speak forms a crucial part of the participant's decision
making. In this work, we develop an adaptive asynchronous LLM-agent which, in
addition to determining what to say, also decides when to say it. To evaluate
our agent, we collect a unique dataset of online Mafia games, including both
human participants, as well as our asynchronous agent. Overall, our agent
performs on par with human players, both in game performance, as well as in its
ability to blend in with the other human players. Our analysis shows that the
agent's behavior in deciding when to speak closely mirrors human patterns,
although differences emerge in message content. We release all our data and
code to support and encourage further research for more realistic asynchronous
communication between LLM agents. This work paves the way for integration of
LLMs into realistic human group settings, from assistance in team discussions
to educational and professional environments where complex social dynamics must
be navigated.

</details>


### [498] [Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs](https://arxiv.org/abs/2506.04236)
*Botao Amber Hu,Helena Rong*

Main category: cs.MA

TL;DR: 论文围绕人工生命研究中开放式进化难题，介绍基于新技术的链上智能体新范式，以Spore.fun为例研究其行为和进化轨迹，探讨开放系统能否实现开放式进化。


<details>
  <summary>Details</summary>
Motivation: 传统封闭系统模拟难以实现持续开放式进化，学者认为需要开放系统，新技术催生链上智能体新范式，由此开展对Spore.fun的研究以探讨能否实现开放式进化。

Method: 以Spore.fun为案例，通过数字行为学研究智能体行为和进化轨迹。

Result: 未提及。

Conclusion: 未提及，但旨在引发对基于无许可计算基板和经济激励的开放系统能否实现开放式进化的讨论。

Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution
(OEE)-the continuous emergence of novelty observed in biological life-has
traditionally been pursued within isolated closed system simulations, such as
Tierra and Avida, which have typically plateaued after an initial burst of
novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires
an "open" system that continually exchanges information or energy with its
environment. A recent technological innovation in decentralized physical
infrastructure networks (DePIN) providing permissionless computational
substrates enables deploying large language model (LLM)-based AI agents on
blockchains integrated with Trusted Execution Environments (TEEs). This enables
on-chain agents to operate autonomously "in the wild," achieving
self-sovereignty without human oversight. These agents can control their own
social media accounts and cryptocurrency wallets, allowing them to interact
directly with blockchain-based financial networks and broader human social
media. Building on this new paradigm of on-chain agents, Spore.fun is a recent
real-world AI evolution experiment that enables autonomous breeding and
evolution of new on-chain agents. This paper presents a detailed case study of
Spore.fun, examining agent behaviors and their evolutionary trajectories
through digital ethology. We aim to spark discussion about whether "open" ALife
systems "in-the-wild," based on permissionless computational substrates and
driven by economic incentives to interact with their environment, could finally
achieve the long-sought goal of OEE.

</details>


### [499] [CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients](https://arxiv.org/abs/2506.04265)
*Mengda Ji,Genjiu Xu,Liying Wang*

Main category: cs.MA

TL;DR: 本文聚焦合作式多智能体强化学习（MARL）的信用分配问题，提出CORA方法，实验表明其优于基线，凸显联盟感知信用分配对提升MARL性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习信用分配方法在联盟层面分析不足，共享全局优势会导致次优策略更新。

Method: 提出CORA方法，通过所有可能联盟的边际贡献评估联盟优势，利用合作博弈论的核心解分解优势，并采用随机联盟采样降低计算开销。

Result: 在矩阵游戏、微分游戏和多智能体协作基准测试中，CORA优于强基线，尤其在多局部最优任务中表现出色。

Conclusion: 联盟感知的信用分配对提高MARL性能至关重要。

Abstract: This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.

</details>


### [500] [Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and Vehicles for Crowdsensing in Disaster Response](https://arxiv.org/abs/2506.04276)
*Lei Han,Yitong Guo,Pengfei Yang,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 本文提出HoAs - PALN算法用于高效收集灾后环境信息，通过自适应降维和局部纳什均衡博弈实现，实验表明其在动态灾后环境有效。


<details>
  <summary>Details</summary>
Motivation: 自然灾害致人类社会损失大，及时获取灾后环境信息对救援重要，现有传感技术面临环境适应性弱等挑战。

Method: 提出HoAs - PALN算法，通过匹配过程的自适应降维将五维匹配转化为两类三维匹配，结合softmax函数和局部纳什均衡确定机制进行局部纳什均衡博弈。

Result: 与GREEDY、K - WTA、MADL和MARL相比，HoAs - PALN平均提高任务完成率64.12%、46.48%、16.55%和14.03%，每次在线调度决策时间少于10秒。

Conclusion: HoAs - PALN算法在动态灾后环境中有效，能提升灾后环境信息收集效率。

Abstract: Natural disasters have caused significant losses to human society, and the
timely and efficient acquisition of post-disaster environmental information is
crucial for the effective implementation of rescue operations. Due to the
complexity of post-disaster environments, existing sensing technologies face
challenges such as weak environmental adaptability, insufficient specialized
sensing capabilities, and limited practicality of sensing solutions. This paper
explores the heterogeneous multi-agent online autonomous collaborative
scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of
post-disaster environmental information. HoAs-PALN is realized through adaptive
dimensionality reduction in the matching process and local Nash equilibrium
game, facilitating autonomous collaboration among time-dependent UAVs, workers
and vehicles to enhance sensing scheduling. (1) In terms of adaptive
dimensionality reduction during the matching process, HoAs-PALN significantly
reduces scheduling decision time by transforming a five-dimensional matching
process into two categories of three-dimensional matching processes; (2)
Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax
function to optimize behavior selection probabilities and introduces a local
Nash equilibrium determination mechanism to ensure scheduling decision
performance. Finally, we conducted detailed experiments based on extensive
real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL
and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%,
and 14.03% on average, respectively, while each online scheduling decision
takes less than 10 seconds, demonstrating its effectiveness in dynamic
post-disaster environments.

</details>


### [501] [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
*Niv Eckhaus,Uri Berger,Gabriel Stanovsky*

Main category: cs.MA

TL;DR: 本文开发了自适应异步大语言模型代理，在在线黑手党游戏中表现与人类玩家相当，发布数据和代码以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多用于同步通信，而现实场景多为异步，需要开发能决定发言时机的异步代理。

Method: 开发自适应异步大语言模型代理，并收集在线黑手党游戏数据集进行评估。

Result: 代理在游戏表现和融入人类玩家方面与人类表现相当，发言时机行为与人类模式相近，但消息内容有差异。

Conclusion: 发布数据和代码支持异步通信研究，为大语言模型融入人类群体环境奠定基础。

Abstract: LLMs are used predominantly in synchronous communication, where a human user
and a model communicate in alternating turns. In contrast, many real-world
settings are inherently asynchronous. For example, in group chats, online team
meetings, or social games, there is no inherent notion of turns; therefore, the
decision of when to speak forms a crucial part of the participant's decision
making. In this work, we develop an adaptive asynchronous LLM-agent which, in
addition to determining what to say, also decides when to say it. To evaluate
our agent, we collect a unique dataset of online Mafia games, including both
human participants, as well as our asynchronous agent. Overall, our agent
performs on par with human players, both in game performance, as well as in its
ability to blend in with the other human players. Our analysis shows that the
agent's behavior in deciding when to speak closely mirrors human patterns,
although differences emerge in message content. We release all our data and
code to support and encourage further research for more realistic asynchronous
communication between LLM agents. This work paves the way for integration of
LLMs into realistic human group settings, from assistance in team discussions
to educational and professional environments where complex social dynamics must
be navigated.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [502] [SSIMBaD: Sigma Scaling with SSIM-Guided Balanced Diffusion for AnimeFace Colorization](https://arxiv.org/abs/2506.04283)
*Junpyo Seo,Hanbin Koo,Jieun Yook,Byung-Ro Moon*

Main category: cs.GR

TL;DR: 提出基于SSIMBaD扩散框架的动漫线稿自动上色方法，通过时变感知对齐实现更平衡的着色效果


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义噪声调度会导致感知不一致，需要开发能保持结构保真度并实现风格迁移的新框架

Method: 构建连续时间扩散模型，引入SSIMBaD技术进行sigma空间变换，通过结构相似性指标(SSIM)线性对齐感知退化过程

Result: 在大规模动漫人脸数据集上取得像素精度和感知质量的双重提升，优于现有最先进模型

Conclusion: 该方法在保持输入线稿结构完整性的同时，实现了跨风格的泛化能力，代码已开源供复现验证

Abstract: We propose a novel diffusion-based framework for automatic colorization of
Anime-style facial sketches. Our method preserves the structural fidelity of
the input sketch while effectively transferring stylistic attributes from a
reference image. Unlike traditional approaches that rely on predefined noise
schedules - which often compromise perceptual consistency -- our framework
builds on continuous-time diffusion models and introduces SSIMBaD (Sigma
Scaling with SSIM-Guided Balanced Diffusion). SSIMBaD applies a sigma-space
transformation that aligns perceptual degradation, as measured by structural
similarity (SSIM), in a linear manner. This scaling ensures uniform visual
difficulty across timesteps, enabling more balanced and faithful
reconstructions. Experiments on a large-scale Anime face dataset demonstrate
that our method outperforms state-of-the-art models in both pixel accuracy and
perceptual quality, while generalizing to diverse styles. Code is available at
github.com/Giventicket/SSIMBaD-Sigma-Scaling-with-SSIM-Guided-Balanced-Diffusion-for-AnimeFace-Colorization

</details>


### [503] [SSIMBaD: Sigma Scaling with SSIM-Guided Balanced Diffusion for AnimeFace Colorization](https://arxiv.org/abs/2506.04283)
*Junpyo Seo,Hanbin Koo,Jieun Yook,Byung-Ro Moon*

Main category: cs.GR

TL;DR: 提出基于扩散的动漫风格面部草图自动着色框架，实验表明优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统依赖预定义噪声时间表的方法常损害感知一致性，需新方法保留输入草图结构保真度并有效迁移风格属性。

Method: 基于连续时间扩散模型，引入SSIMBaD，对sigma空间进行变换，线性对齐感知退化。

Result: 在大规模动漫人脸数据集实验中，在像素精度和感知质量上优于现有模型，且能泛化到不同风格。

Conclusion: 所提框架有效，能在动漫风格面部草图自动着色上取得良好效果。

Abstract: We propose a novel diffusion-based framework for automatic colorization of
Anime-style facial sketches. Our method preserves the structural fidelity of
the input sketch while effectively transferring stylistic attributes from a
reference image. Unlike traditional approaches that rely on predefined noise
schedules - which often compromise perceptual consistency -- our framework
builds on continuous-time diffusion models and introduces SSIMBaD (Sigma
Scaling with SSIM-Guided Balanced Diffusion). SSIMBaD applies a sigma-space
transformation that aligns perceptual degradation, as measured by structural
similarity (SSIM), in a linear manner. This scaling ensures uniform visual
difficulty across timesteps, enabling more balanced and faithful
reconstructions. Experiments on a large-scale Anime face dataset demonstrate
that our method outperforms state-of-the-art models in both pixel accuracy and
perceptual quality, while generalizing to diverse styles. Code is available at
github.com/Giventicket/SSIMBaD-Sigma-Scaling-with-SSIM-Guided-Balanced-Diffusion-for-AnimeFace-Colorization

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [504] [Estimating properties of a homogeneous bounded soil using machine learning models](https://arxiv.org/abs/2506.04256)
*Konstantinos Kalimeris,Leonidas Mindrinos,Nikolaos Pallikarakis*

Main category: physics.geo-ph

TL;DR: 基于Fokas方法模拟土壤垂直入渗数据，采用机器学习模型预测土壤特性（扩散率D和导水率K），发现SVM和神经网络在噪声/有限数据下鲁棒性最佳。


<details>
  <summary>Details</summary>
Motivation: 传统土壤特性测量方法成本高且耗时长，本研究探索机器学习在模拟数据上预测D和K的可行性，为复杂环境下的土壤参数识别提供新思路。

Method: 1. 用Fokas方法生成均质土壤垂直入渗模拟数据
2. 构建双输出回归任务（预测D和K）
3. 对比SVM、神经网络等模型在不同数据条件（完整/噪声/有限）下的表现

Result: ① D预测精度整体优于K；② SVM和神经网络在噪声/有限数据中保持>95%准确率，均方误差<0.05，显著优于其他模型

Conclusion: 机器学习（特别是SVM/NN）能有效从水分数据中提取土壤特性，为水文建模提供可靠参数估计方法，后续可拓展至非均质土壤场景验证。

Abstract: This work focuses on estimating soil properties from water moisture
measurements. We consider simulated data generated by solving the
initial-boundary value problem governing vertical infiltration in a
homogeneous, bounded soil profile, with the usage of the Fokas method. To
address the parameter identification problem, which is formulated as a
two-output regression task, we explore various machine learning models. The
performance of each model is assessed under different data conditions: full,
noisy, and limited. Overall, the prediction of diffusivity $D$ tends to be more
accurate than that of hydraulic conductivity $K.$ Among the models considered,
Support Vector Machines (SVMs) and Neural Networks (NNs) demonstrate the
highest robustness, achieving near-perfect accuracy and minimal errors.

</details>


### [505] [Estimating properties of a homogeneous bounded soil using machine learning models](https://arxiv.org/abs/2506.04256)
*Konstantinos Kalimeris,Leonidas Mindrinos,Nikolaos Pallikarakis*

Main category: physics.geo-ph

TL;DR: 本文利用Fokas方法模拟数据，用机器学习模型从水分测量估计土壤性质，评估不同数据条件下各模型性能，SVM和NN最稳健。


<details>
  <summary>Details</summary>
Motivation: 从水分测量估计土壤性质。

Method: 使用Fokas方法解决初边值问题生成模拟数据，将参数识别问题转化为双输出回归任务，探索多种机器学习模型并在不同数据条件下评估。

Result: 扩散率D的预测比水力传导率K更准确，支持向量机和神经网络最稳健，准确率高且误差小。

Conclusion: 支持向量机和神经网络在从水分测量估计土壤性质的任务中表现最佳。

Abstract: This work focuses on estimating soil properties from water moisture
measurements. We consider simulated data generated by solving the
initial-boundary value problem governing vertical infiltration in a
homogeneous, bounded soil profile, with the usage of the Fokas method. To
address the parameter identification problem, which is formulated as a
two-output regression task, we explore various machine learning models. The
performance of each model is assessed under different data conditions: full,
noisy, and limited. Overall, the prediction of diffusivity $D$ tends to be more
accurate than that of hydraulic conductivity $K.$ Among the models considered,
Support Vector Machines (SVMs) and Neural Networks (NNs) demonstrate the
highest robustness, achieving near-perfect accuracy and minimal errors.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [506] [Trustworthiness Preservation by Copies of Machine Learning Systems](https://arxiv.org/abs/2506.05203)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 提出基于概率复杂查询演算的四种可信度验证框架，用于检测机器学习系统副本的信任保持性。


<details>
  <summary>Details</summary>
Motivation: 在负责任AI背景下，解决模型副本在数据/算法变更后如何保持可信度的核心问题。

Method: 构建概率查询演算模型，定义Justifiably/Equally/Weakly/Almost四层可信度验证标准，并研究其逻辑组合关系。

Result: 建立了可信度概念的层次化框架，验证了不同可信度类型在逻辑运算中的组合规律。

Conclusion: 该理论框架为验证复杂AI系统副本的信任保持性提供了可计算的验证工具。

Abstract: A common practice of ML systems development concerns the training of the same
model under different data sets, and the use of the same (training and test)
sets for different learning models. The first case is a desirable practice for
identifying high quality and unbiased training conditions. The latter case
coincides with the search for optimal models under a common dataset for
training. These differently obtained systems have been considered akin to
copies. In the quest for responsible AI, a legitimate but hardly investigated
question is how to verify that trustworthiness is preserved by copies. In this
paper we introduce a calculus to model and verify probabilistic complex queries
over data and define four distinct notions: Justifiably, Equally, Weakly and
Almost Trustworthy which can be checked analysing the (partial) behaviour of
the copy with respect to its original. We provide a study of the relations
between these notions of trustworthiness, and how they compose with each other
and under logical operations. The aim is to offer a computational tool to check
the trustworthiness of possibly complex systems copied from an original whose
behavour is known.

</details>


### [507] [Trustworthiness Preservation by Copies of Machine Learning Systems](https://arxiv.org/abs/2506.05203)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 本文引入微积分来建模和验证数据上的概率复杂查询，定义四种可信度概念，研究其关系与组合，提供检查复制系统可信度的计算工具。


<details>
  <summary>Details</summary>
Motivation: 在负责任AI的背景下，探讨如何验证复制系统是否保留可信度这一未充分研究的问题。

Method: 引入微积分对数据上的概率复杂查询进行建模和验证，定义Justifiably、Equally、Weakly和Almost Trustworthy四种可信度概念。

Result: 研究了这些可信度概念之间的关系，以及它们在逻辑运算下的组合方式。

Conclusion: 提供了一个计算工具，用于检查从已知行为的原始系统复制而来的可能复杂系统的可信度。

Abstract: A common practice of ML systems development concerns the training of the same
model under different data sets, and the use of the same (training and test)
sets for different learning models. The first case is a desirable practice for
identifying high quality and unbiased training conditions. The latter case
coincides with the search for optimal models under a common dataset for
training. These differently obtained systems have been considered akin to
copies. In the quest for responsible AI, a legitimate but hardly investigated
question is how to verify that trustworthiness is preserved by copies. In this
paper we introduce a calculus to model and verify probabilistic complex queries
over data and define four distinct notions: Justifiably, Equally, Weakly and
Almost Trustworthy which can be checked analysing the (partial) behaviour of
the copy with respect to its original. We provide a study of the relations
between these notions of trustworthiness, and how they compose with each other
and under logical operations. The aim is to offer a computational tool to check
the trustworthiness of possibly complex systems copied from an original whose
behavour is known.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [508] [Benchmark for Antibody Binding Affinity Maturation and Design](https://arxiv.org/abs/2506.04235)
*Xinyan Zhao,Yi-Ching Tang,Akshita Singh,Victor J Cantu,KwanHo An,Junseok Lee,Adam E Stogsdill,Ashwin Kumar Ramesh,Zhiqiang An,Xiaoqian Jiang,Yejin Kim*

Main category: q-bio.QM

TL;DR: AbBiBench提出抗体-抗原复合物功能评估新框架，验证14种模型后发现结构导向逆向折叠模型性能最优。


<details>
  <summary>Details</summary>
Motivation: 现有抗体评估方法仅关注抗体自身特征（如氨基酸相似度、结构RMSD），缺乏对抗体-抗原复合功能的系统性评估，需建立更生物学相关的评估体系。

Method: 1. 构建包含9种抗原和15.6万突变抗体的数据集；2. 系统比较14种蛋白质模型（语言模型/逆向折叠/扩散模型等）；3. 通过模型似然与实验亲和力相关性评估性能；4. 流感抗体优化案例验证模型生成能力。

Result: 结构条件逆向折叠模型在亲和力相关性（r=0.68）和生成任务中表现最佳，生成抗体结构完整性提升23%，结合自由能降低1.5 kcal/mol。

Conclusion: AbBiBench建立了首个基于功能活性的抗体设计评估标准，为开发靶向性抗体设计模型提供统一框架，推动治疗性抗体研发。

Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking
framework for antibody binding affinity maturation and design. Unlike existing
antibody evaluation strategies that rely on antibody alone and its similarity
to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench
considers an antibody-antigen (Ab-Ag) complex as a functional unit and
evaluates the potential of an antibody design binding to given antigen by
measuring protein model's likelihood on the Ab-Ag complex. We first curate,
standardize, and share 9 datasets containing 9 antigens (involving influenza,
anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain
mutated antibodies. Using these datasets, we systematically compare 14 protein
models including masked language models, autoregressive language models,
inverse folding models, diffusion-based generative models, and geometric graph
models. The correlation between model likelihood and experimental affinity
values is used to evaluate model performance. Additionally, in a case study to
increase binding affinity of antibody F045-092 to antigen influenza H1N1, we
evaluate the generative power of the top-performing models by sampling a set of
new antibodies binding to the antigen and ranking them based on structural
integrity and biophysical properties of the Ab-Ag complex. As a result,
structure-conditioned inverse folding models outperform others in both affinity
correlation and generation tasks. Overall, AbBiBench provides a unified,
biologically grounded evaluation framework to facilitate the development of
more effective, function-aware antibody design models.

</details>


### [509] [The Latent Space Hypothesis: Toward Universal Medical Representation Learning](https://arxiv.org/abs/2506.04515)
*Salil Patel*

Main category: q-bio.QM

TL;DR: 提出潜在空间假说整合多模态医疗数据，通过共享几何表示实现疾病分型与个性化诊疗


<details>
  <summary>Details</summary>
Motivation: 解决异构医疗数据模态割裂问题，揭示帕金森/克罗恩等疾病背后的异质性病理实体，突破传统疾病标签的解剖局限

Method: 构建统一潜空间表示：将各类数据投影为三维流形，通过患者健康状态坐标点、疾病轨迹线、治疗向量建立量化框架

Result: 识别疾病亚轨迹/个体化演变方向，为精准诊断、动态监测、靶向治疗提供数学基础

Conclusion: 需克服数据偏差/稀疏性/隐私问题，通过持续学习与扰动验证推动临床实践从标签分组转向个体轨迹导航

Abstract: Medical data range from genomic sequences and retinal photographs to
structured laboratory results and unstructured clinical narratives. Although
these modalities appear disparate, many encode convergent information about a
single underlying physiological state. The Latent Space Hypothesis frames each
observation as a projection of a unified, hierarchically organized manifold --
much like shadows cast by the same three-dimensional object. Within this
learned geometric representation, an individual's health status occupies a
point, disease progression traces a trajectory, and therapeutic intervention
corresponds to a directed vector. Interpreting heterogeneous evidence in a
shared space provides a principled way to re-examine eponymous conditions --
such as Parkinson's or Crohn's -- that often mask multiple pathophysiological
entities and involve broader anatomical domains than once believed. By
revealing sub-trajectories and patient-specific directions of change, the
framework supplies a quantitative rationale for personalised diagnosis,
longitudinal monitoring, and tailored treatment, moving clinical practice away
from grouping by potentially misleading labels toward navigation of each
person's unique trajectory. Challenges remain -- bias amplification, data
scarcity for rare disorders, privacy, and the correlation-causation divide --
but scale-aware encoders, continual learning on longitudinal data streams, and
perturbation-based validation offer plausible paths forward.

</details>


### [510] [Benchmark for Antibody Binding Affinity Maturation and Design](https://arxiv.org/abs/2506.04235)
*Xinyan Zhao,Yi-Ching Tang,Akshita Singh,Victor J Cantu,KwanHo An,Junseok Lee,Adam E Stogsdill,Ashwin Kumar Ramesh,Zhiqiang An,Xiaoqian Jiang,Yejin Kim*

Main category: q-bio.QM

TL;DR: 介绍AbBiBench基准框架，用于抗体结合亲和力成熟和设计，通过比较多种模型得出结构条件逆折叠模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有抗体评估策略仅依赖抗体本身及其与天然抗体的相似性，本文希望提出考虑抗体 - 抗原复合物的评估框架以促进抗体设计模型发展。

Method: 整理、标准化并共享9个数据集，系统比较14种蛋白质模型，用模型似然性与实验亲和力值的相关性评估性能，通过案例研究评估模型生成能力。

Result: 结构条件逆折叠模型在亲和力相关性和生成任务上均优于其他模型。

Conclusion: AbBiBench提供了统一、基于生物学的评估框架，有助于开发更有效的、具备功能意识的抗体设计模型。

Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking
framework for antibody binding affinity maturation and design. Unlike existing
antibody evaluation strategies that rely on antibody alone and its similarity
to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench
considers an antibody-antigen (Ab-Ag) complex as a functional unit and
evaluates the potential of an antibody design binding to given antigen by
measuring protein model's likelihood on the Ab-Ag complex. We first curate,
standardize, and share 9 datasets containing 9 antigens (involving influenza,
anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain
mutated antibodies. Using these datasets, we systematically compare 14 protein
models including masked language models, autoregressive language models,
inverse folding models, diffusion-based generative models, and geometric graph
models. The correlation between model likelihood and experimental affinity
values is used to evaluate model performance. Additionally, in a case study to
increase binding affinity of antibody F045-092 to antigen influenza H1N1, we
evaluate the generative power of the top-performing models by sampling a set of
new antibodies binding to the antigen and ranking them based on structural
integrity and biophysical properties of the Ab-Ag complex. As a result,
structure-conditioned inverse folding models outperform others in both affinity
correlation and generation tasks. Overall, AbBiBench provides a unified,
biologically grounded evaluation framework to facilitate the development of
more effective, function-aware antibody design models.

</details>


### [511] [The Latent Space Hypothesis: Toward Universal Medical Representation Learning](https://arxiv.org/abs/2506.04515)
*Salil Patel*

Main category: q-bio.QM

TL;DR: 本文提出潜在空间假说，可对异构医学数据统一解读，助力个性化医疗，虽面临挑战但有解决路径。


<details>
  <summary>Details</summary>
Motivation: 医学数据模态多样但蕴含收敛信息，传统病症分类可能掩盖多种病理实体，需统一框架解读数据助力医疗。

Method: 提出潜在空间假说，将观察视为统一、分层组织流形的投影，在该几何表示中分析健康状态、疾病进展和治疗干预。

Result: 框架能揭示子轨迹和患者特定变化方向，为个性化诊断、纵向监测和定制治疗提供定量依据。

Conclusion: 该框架有助于临床实践从可能误导的标签分组转向关注个人独特轨迹，虽面临挑战，但有可行解决办法。

Abstract: Medical data range from genomic sequences and retinal photographs to
structured laboratory results and unstructured clinical narratives. Although
these modalities appear disparate, many encode convergent information about a
single underlying physiological state. The Latent Space Hypothesis frames each
observation as a projection of a unified, hierarchically organized manifold --
much like shadows cast by the same three-dimensional object. Within this
learned geometric representation, an individual's health status occupies a
point, disease progression traces a trajectory, and therapeutic intervention
corresponds to a directed vector. Interpreting heterogeneous evidence in a
shared space provides a principled way to re-examine eponymous conditions --
such as Parkinson's or Crohn's -- that often mask multiple pathophysiological
entities and involve broader anatomical domains than once believed. By
revealing sub-trajectories and patient-specific directions of change, the
framework supplies a quantitative rationale for personalised diagnosis,
longitudinal monitoring, and tailored treatment, moving clinical practice away
from grouping by potentially misleading labels toward navigation of each
person's unique trajectory. Challenges remain -- bias amplification, data
scarcity for rare disorders, privacy, and the correlation-causation divide --
but scale-aware encoders, continual learning on longitudinal data streams, and
perturbation-based validation offer plausible paths forward.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [512] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: 提出多变量函数逼近方法的系统性评估框架，通过多样化测试函数集和性能指标对比分析不同工具特性，重点解析自研mLF方法原理及应用。


<details>
  <summary>Details</summary>
Motivation: 现有张量逼近方法缺乏统一评估标准，研究者需了解不同工具的性能边界及适用场景。本文旨在建立公平评估体系，为领域用户提供工具选择依据及方法论指导。

Method: 1) 构建包含不同复杂度特征的多变量函数测试集 2) 生成多维张量数据集 3) 测试各方法在精度、耗时、参数敏感性等指标 4) 基于最优参数配置进行多维度对比分析。

Result: 建立首个涵盖主流逼近工具的系统基准测试框架，揭示不同方法在计算效率与精度间的trade-off规律，其中mLF方法在特定场景展现参数鲁棒性优势。

Conclusion: 通过标准化测试流程实现工具特性的透明化比较，特别深入解析了自研mLF方法的技术细节与应用场景，为复杂系统建模提供方法论参考。

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


### [513] [Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient](https://arxiv.org/abs/2506.04375)
*Conor Rowan,John Evans,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: 提出基于神经网络离散化和Rayleigh商变分形式的新方法，有效解决连续特征值问题


<details>
  <summary>Details</summary>
Motivation: 传统神经网络方法在特征值问题研究不足，其非线性特性导致无法直接转化为离散特征值问题，需要开发专门技术

Method: 结合Rayleigh商变分目标函数、Gram-Schmidt正交化过程和神经网络特征函数离散化方法

Result: 成功应用于不规则域谐函数求解/参数非线性特征问题/高维特征分析，验证方法在工程力学中的有效性

Conclusion: 该方法为连续特征值问题提供了独特优势，谐函数可作为谱基函数有效近似偏微分方程解

Abstract: From characterizing the speed of a thermal system's response to computing
natural modes of vibration, eigenvalue analysis is ubiquitous in engineering.
In spite of this, eigenvalue problems have received relatively little treatment
compared to standard forward and inverse problems in the physics-informed
machine learning literature. In particular, neural network discretizations of
solutions to eigenvalue problems have seen only a handful of studies. Owing to
their nonlinearity, neural network discretizations prevent the conversion of
the continuous eigenvalue differential equation into a standard discrete
eigenvalue problem. In this setting, eigenvalue analysis requires more
specialized techniques. Using a neural network discretization of the
eigenfunction, we show that a variational form of the eigenvalue problem called
the "Rayleigh quotient" in tandem with a Gram-Schmidt orthogonalization
procedure is a particularly simple and robust approach to find the eigenvalues
and their corresponding eigenfunctions. This method is shown to be useful for
finding sets of harmonic functions on irregular domains, parametric and
nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss
the utility of harmonic functions as a spectral basis for approximating
solutions to partial differential equations. Through various examples from
engineering mechanics, the combination of the Rayleigh quotient objective,
Gram-Schmidt procedure, and the neural network discretization of the
eigenfunction is shown to offer unique advantages for handling continuous
eigenvalue problems.

</details>


### [514] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: 评估基于张量或数据的多元函数构造与逼近方法性能、特性和用户体验，提出基准函数集。


<details>
  <summary>Details</summary>
Motivation: 公平评估不同可用策略，引导用户了解各工具的过程、可能性、优缺点。

Method: 选取不同复杂度和性质的多元函数构造张量，基于张量考察各方法性能，用多评估标准比较方法。

Result: 文中未明确提及具体结果。

Conclusion: 提出完整的代理模型张量逼近工具基准集，对多元Loewner框架方法进行详细介绍。

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


### [515] [Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient](https://arxiv.org/abs/2506.04375)
*Conor Rowan,John Evans,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: 论文提出用神经网络离散本征函数，结合瑞利商和格拉姆 - 施密特正交化程序求解连续本征值问题，通过工程力学例子展示其优势。


<details>
  <summary>Details</summary>
Motivation: 本征值分析在工程中普遍，但物理信息机器学习文献中对本征值问题研究较少，神经网络离散化本征值问题研究更少且需专门技术。

Method: 使用神经网络离散本征函数，结合瑞利商变分形式与格拉姆 - 施密特正交化程序。

Result: 该方法可用于不规则域谐波函数集、参数和非线性本征问题及高维本征分析，还讨论了谐波函数作为偏微分方程解的谱基的实用性。

Conclusion: 瑞利商目标、格拉姆 - 施密特程序和神经网络离散本征函数的组合在处理连续本征值问题上有独特优势。

Abstract: From characterizing the speed of a thermal system's response to computing
natural modes of vibration, eigenvalue analysis is ubiquitous in engineering.
In spite of this, eigenvalue problems have received relatively little treatment
compared to standard forward and inverse problems in the physics-informed
machine learning literature. In particular, neural network discretizations of
solutions to eigenvalue problems have seen only a handful of studies. Owing to
their nonlinearity, neural network discretizations prevent the conversion of
the continuous eigenvalue differential equation into a standard discrete
eigenvalue problem. In this setting, eigenvalue analysis requires more
specialized techniques. Using a neural network discretization of the
eigenfunction, we show that a variational form of the eigenvalue problem called
the "Rayleigh quotient" in tandem with a Gram-Schmidt orthogonalization
procedure is a particularly simple and robust approach to find the eigenvalues
and their corresponding eigenfunctions. This method is shown to be useful for
finding sets of harmonic functions on irregular domains, parametric and
nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss
the utility of harmonic functions as a spectral basis for approximating
solutions to partial differential equations. Through various examples from
engineering mechanics, the combination of the Rayleigh quotient objective,
Gram-Schmidt procedure, and the neural network discretization of the
eigenfunction is shown to offer unique advantages for handling continuous
eigenvalue problems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [516] [Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap](https://arxiv.org/abs/2506.05030)
*Kacper Sokol,James Fackler,Julia E Vogt*

Main category: cs.HC

TL;DR: 人工智能在医疗领域因技术中心主义方法导致转化鸿沟，提出以社会技术视角设计工具来补充医生认知活动


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动工具与临床实践（尤其是诊断推理）存在根本性脱节，技术中心主义方法导致人工智能医疗应用转化失败

Method: 提出新型社会技术框架，强调工具设计需符合医生的认知模式和知识建构过程，而非追求非关键基准的超人表现

Result: 构建了优先考虑真实临床影响的理论模型，突破现有工具与医疗工作流程不兼容的瓶颈

Conclusion: 通过将技术开发重心转向支持临床认知实践，可实质性推动医疗AI从实验室到临床的转化

Abstract: Artificial intelligence promises to revolutionise medicine, yet its impact
remains limited because of the pervasive translational gap. We posit that the
prevailing technology-centric approaches underpin this challenge, rendering
such systems fundamentally incompatible with clinical practice, specifically
diagnostic reasoning and decision making. Instead, we propose a novel
sociotechnical conceptualisation of data-driven support tools designed to
complement doctors' cognitive and epistemic activities. Crucially, it
prioritises real-world impact over superhuman performance on inconsequential
benchmarks.

</details>


### [517] [Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams](https://arxiv.org/abs/2506.05265)
*Mohammed Almutairi*

Main category: cs.HC

TL;DR: 提出三种AI增强框架动态优化团队全周期表现：多臂老虎机算法优化团队组建，tAIfa系统提供实时反馈，PuppeteerLLM模拟团队动态。


<details>
  <summary>Details</summary>
Motivation: 现有团队优化工具依赖静态数据且缺乏动态调整能力，导致成员满意度低、参与度不足，最终影响团队整体表现。

Method: 1. 多臂老虎机算法迭代优化团队组成
2. tAIfa系统利用LLM提供个性化反馈
3. PuppeteerLLM框架模拟多智能体长期协作

Result: 框架分别提升团队组建满意度(对齐个体与集体目标)、执行阶段凝聚力(实时反馈调整)、以及动态环境适应能力(精准模拟预测)。

Conclusion: 通过覆盖团队组建-执行全周期的动态AI优化方法，系统性提升团队满意度、参与度与最终绩效表现。

Abstract: Effective teamwork is essential across diverse domains. During the team
formation stage, a key challenge is forming teams that effectively balance user
preferences with task objectives to enhance overall team satisfaction. In the
team performing stage, maintaining cohesion and engagement is critical for
sustaining high team performance. However, existing computational tools and
algorithms for team optimization often rely on static data inputs, narrow
algorithmic objectives, or solutions tailored for specific contexts, failing to
account for the dynamic interplay of team members personalities, evolving
goals, and changing individual preferences. Therefore, teams may encounter
member dissatisfaction, as purely algorithmic assignments can reduce members
commitment to team goals or experience suboptimal engagement due to the absence
of timely, personalized guidance to help members adjust their behaviors and
interactions as team dynamics evolve. Ultimately, these challenges can lead to
reduced overall team performance. My Ph.D. dissertation aims to develop
AI-augmented team optimization frameworks and practical systems that enhance
team satisfaction, engagement, and performance. First, I propose a team
formation framework that leverages a multi-armed bandit algorithm to
iteratively refine team composition based on user preferences, ensuring
alignment between individual needs and collective team goals to enhance team
satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an
AI-powered system that utilizes large language models (LLMs) to deliver
immediate, personalized feedback to both teams and individual members,
enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an
LLM-based simulation framework that simulates multi-agent teams to model
complex team dynamics within realistic environments, incorporating task-driven
collaboration and long-term coordination.

</details>


### [518] [Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap](https://arxiv.org/abs/2506.05030)
*Kacper Sokol,James Fackler,Julia E Vogt*

Main category: cs.HC

TL;DR: 人工智能有望革新医学，但因转化差距影响有限，提出社会技术概念化数据驱动支持工具。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能在医学中因技术中心方法导致转化差距大、与临床实践不兼容的问题。

Method: 提出一种新的社会技术概念化的数据驱动支持工具。

Result: 未提及具体结果。

Conclusion: 应优先考虑现实世界影响，而非在无关紧要的基准上追求超人表现。

Abstract: Artificial intelligence promises to revolutionise medicine, yet its impact
remains limited because of the pervasive translational gap. We posit that the
prevailing technology-centric approaches underpin this challenge, rendering
such systems fundamentally incompatible with clinical practice, specifically
diagnostic reasoning and decision making. Instead, we propose a novel
sociotechnical conceptualisation of data-driven support tools designed to
complement doctors' cognitive and epistemic activities. Crucially, it
prioritises real-world impact over superhuman performance on inconsequential
benchmarks.

</details>


### [519] [Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams](https://arxiv.org/abs/2506.05265)
*Mohammed Almutairi*

Main category: cs.HC

TL;DR: 论文指出现有团队优化工具和算法存在不足，作者博士论文旨在开发增强团队满意度、参与度和绩效的AI增强团队优化框架和系统，并提出团队组建框架、tAIfa系统和PuppeteerLLM模拟框架。


<details>
  <summary>Details</summary>
Motivation: 现有团队优化的计算工具和算法存在依赖静态数据、目标狭窄等问题，导致团队成员不满和整体绩效降低，需开发新的团队优化框架和系统。

Method: 提出基于多臂老虎机算法的团队组建框架；引入利用大语言模型的tAIfa系统；提出基于大语言模型的PuppeteerLLM模拟框架。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Effective teamwork is essential across diverse domains. During the team
formation stage, a key challenge is forming teams that effectively balance user
preferences with task objectives to enhance overall team satisfaction. In the
team performing stage, maintaining cohesion and engagement is critical for
sustaining high team performance. However, existing computational tools and
algorithms for team optimization often rely on static data inputs, narrow
algorithmic objectives, or solutions tailored for specific contexts, failing to
account for the dynamic interplay of team members personalities, evolving
goals, and changing individual preferences. Therefore, teams may encounter
member dissatisfaction, as purely algorithmic assignments can reduce members
commitment to team goals or experience suboptimal engagement due to the absence
of timely, personalized guidance to help members adjust their behaviors and
interactions as team dynamics evolve. Ultimately, these challenges can lead to
reduced overall team performance. My Ph.D. dissertation aims to develop
AI-augmented team optimization frameworks and practical systems that enhance
team satisfaction, engagement, and performance. First, I propose a team
formation framework that leverages a multi-armed bandit algorithm to
iteratively refine team composition based on user preferences, ensuring
alignment between individual needs and collective team goals to enhance team
satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an
AI-powered system that utilizes large language models (LLMs) to deliver
immediate, personalized feedback to both teams and individual members,
enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an
LLM-based simulation framework that simulates multi-agent teams to model
complex team dynamics within realistic environments, incorporating task-driven
collaboration and long-term coordination.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [520] [Assessing parameter identifiability of a hemodynamics PDE model using spectral surrogates and dimension reduction](https://arxiv.org/abs/2506.04538)
*Mitchel J. Colebank*

Main category: q-bio.TO

TL;DR: 提出基于多项式混沌展开的创新方法，解决生物医学模拟器参数敏感性分析和可识别性问题


<details>
  <summary>Details</summary>
Motivation: 医疗数字孪生建模中，昂贵血流模型仿真与有限数据条件下参数敏感性分析存在应用依赖性缺陷

Method: 结合多项式混沌展开实现：1) 时间序列全局敏感性分析 2) 基于轮廓似然置信区间的参数可识别性评估

Result: 在一维肺血流模型中验证了方法有效性，证明实验设计改进可提升Windkessel和结构化树边界条件下的参数可识别性

Conclusion: 首次将PCE仿真策略与轮廓似然分析结合，为偏微分方程控制的生物医学模型参数识别提供通用框架

Abstract: Computational inverse problems for biomedical simulators suffer from limited
data and relatively high parameter dimensionality. This often requires
sensitivity analysis, where parameters of the model are ranked based on their
influence on the specific quantities of interest. This is especially important
for simulators used to build medical digital twins, as the amount of data is
typically limited. For expensive models, such as blood flow models, emulation
is employed to expedite the simulation time. Parameter ranking and fixing using
sensitivity analysis are often heuristic, though, and vary with the specific
application or simulator used. The present study provides an innovative
solution to this problem by leveraging polynomial chaos expansions (PCEs) for
both multioutput global sensitivity analysis and formal parameter
identifiability. For the former, we use dimension reduction to efficiently
quantify time-series sensitivity of a one-dimensional pulmonary hemodynamics
model. We consider both Windkessel and structured tree boundary conditions. We
then use PCEs to construct profile-likelihood confidence intervals to formally
assess parameter identifiability, and show how changes in experimental design
improve identifiability. Our work presents a novel approach to determining
parameter identifiability and leverages a common emulation strategy for
enabling profile-likelihood analysis in problems governed by partial
differential equations.

</details>


### [521] [Assessing parameter identifiability of a hemodynamics PDE model using spectral surrogates and dimension reduction](https://arxiv.org/abs/2506.04538)
*Mitchel J. Colebank*

Main category: q-bio.TO

TL;DR: 本文提出用多项式混沌展开（PCEs）解决生物医学模拟器计算逆问题中的参数排序和可识别性问题，还展示实验设计对可识别性的影响。


<details>
  <summary>Details</summary>
Motivation: 生物医学模拟器计算逆问题存在数据有限和参数维度高的问题，现有参数排序和固定方法常是启发式的且因应用而异。

Method: 利用PCEs进行多输出全局敏感性分析和形式化参数可识别性评估，用降维量化一维肺血流动力学模型的时间序列敏感性，构建轮廓似然置信区间评估参数可识别性。

Result: 展示了实验设计的改变能提高参数可识别性。

Conclusion: 提出了确定参数可识别性的新方法，并利用常见的仿真策略在偏微分方程问题中进行轮廓似然分析。

Abstract: Computational inverse problems for biomedical simulators suffer from limited
data and relatively high parameter dimensionality. This often requires
sensitivity analysis, where parameters of the model are ranked based on their
influence on the specific quantities of interest. This is especially important
for simulators used to build medical digital twins, as the amount of data is
typically limited. For expensive models, such as blood flow models, emulation
is employed to expedite the simulation time. Parameter ranking and fixing using
sensitivity analysis are often heuristic, though, and vary with the specific
application or simulator used. The present study provides an innovative
solution to this problem by leveraging polynomial chaos expansions (PCEs) for
both multioutput global sensitivity analysis and formal parameter
identifiability. For the former, we use dimension reduction to efficiently
quantify time-series sensitivity of a one-dimensional pulmonary hemodynamics
model. We consider both Windkessel and structured tree boundary conditions. We
then use PCEs to construct profile-likelihood confidence intervals to formally
assess parameter identifiability, and show how changes in experimental design
improve identifiability. Our work presents a novel approach to determining
parameter identifiability and leverages a common emulation strategy for
enabling profile-likelihood analysis in problems governed by partial
differential equations.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [522] [Diffusion Transformer-based Universal Dose Denoising for Pencil Beam Scanning Proton Therapy](https://arxiv.org/abs/2506.04467)
*Yuzhen Ding,Jason Holmes,Hongying Feng,Martin Bues,Lisa A. McGee,Jean-Claude M. Rwigema,Nathan Y. Yu,Terence S. Sio,Sameer R. Keole,William W. Wong,Steven E. Schild,Jonathan B. Ashman,Sujay A. Vora,Daniel J. Ma,Samir H. Patel,Wei Liu*

Main category: physics.med-ph

TL;DR: 开发基于扩散变换器的去噪框架，在保持蒙特卡洛剂量计算精度的同时提升10倍速度，实现跨疾病部位的泛化应用


<details>
  <summary>Details</summary>
Motivation: 强度调控质子治疗(IMPT)需要频繁的在线适应性放射治疗，传统蒙特卡洛剂量计算耗时长(10分钟/计划)，降低粒子数会显著影响计算精度

Method: 使用80例头颈癌患者的CT和剂量数据训练扩散变换器模型，输入低统计量(1分钟/计划)剂量图和CT图像，采用MSE+残差损失+区域MAE的混合损失函数

Result: 在头颈/肺/乳腺/前列腺病例中MAE分别为0.195/0.120/0.172/0.376 Gy[RBE]，3D Gamma通过率(3%/2mm)均超92%，DVH指标与高统计量基准高度吻合

Conclusion: 基于头颈癌数据训练的模型展现出跨病种泛化能力，为实时自适应质子治疗提供了高效的剂量计算解决方案

Abstract: Purpose: Intensity-modulated proton therapy (IMPT) offers precise tumor
coverage while sparing organs at risk (OARs) in head and neck (H&N) cancer.
However, its sensitivity to anatomical changes requires frequent adaptation
through online adaptive radiation therapy (oART), which depends on fast,
accurate dose calculation via Monte Carlo (MC) simulations. Reducing particle
count accelerates MC but degrades accuracy. To address this, denoising
low-statistics MC dose maps is proposed to enable fast, high-quality dose
generation.
  Methods: We developed a diffusion transformer-based denoising framework. IMPT
plans and 3D CT images from 80 H&N patients were used to generate noisy and
high-statistics dose maps using MCsquare (1 min and 10 min per plan,
respectively). Data were standardized into uniform chunks with zero-padding,
normalized, and transformed into quasi-Gaussian distributions. Testing was done
on 10 H&N, 10 lung, 10 breast, and 10 prostate cancer cases, preprocessed
identically. The model was trained with noisy dose maps and CT images as input
and high-statistics dose maps as ground truth, using a combined loss of mean
square error (MSE), residual loss, and regional MAE (focusing on top/bottom 10%
dose voxels). Performance was assessed via MAE, 3D Gamma passing rate, and DVH
indices.
  Results: The model achieved MAEs of 0.195 (H&N), 0.120 (lung), 0.172
(breast), and 0.376 Gy[RBE] (prostate). 3D Gamma passing rates exceeded 92%
(3%/2mm) across all sites. DVH indices for clinical target volumes (CTVs) and
OARs closely matched the ground truth.
  Conclusion: A diffusion transformer-based denoising framework was developed
and, though trained only on H&N data, generalizes well across multiple disease
sites.

</details>


### [523] [Diffusion Transformer-based Universal Dose Denoising for Pencil Beam Scanning Proton Therapy](https://arxiv.org/abs/2506.04467)
*Yuzhen Ding,Jason Holmes,Hongying Feng,Martin Bues,Lisa A. McGee,Jean-Claude M. Rwigema,Nathan Y. Yu,Terence S. Sio,Sameer R. Keole,William W. Wong,Steven E. Schild,Jonathan B. Ashman,Sujay A. Vora,Daniel J. Ma,Samir H. Patel,Wei Liu*

Main category: physics.med-ph

TL;DR: 提出用扩散变压器去噪框架处理低统计MC剂量图，以实现快速高质量剂量生成，模型在多部位表现良好。


<details>
  <summary>Details</summary>
Motivation: IMPT治疗头颈部癌症需oART及快速准确剂量计算，降低粒子数加速MC但降低准确性，故提出去噪低统计MC剂量图。

Method: 开发扩散变压器去噪框架，用80例头颈部患者数据生成噪声和高统计剂量图，标准化处理后训练模型，用组合损失函数，在多部位测试并评估性能。

Result: 模型在不同部位MAE不同，3D伽马通过率超92%，DVH指数与真实值接近。

Conclusion: 开发的扩散变压器去噪框架虽仅用头颈部数据训练，但在多疾病部位泛化性好。

Abstract: Purpose: Intensity-modulated proton therapy (IMPT) offers precise tumor
coverage while sparing organs at risk (OARs) in head and neck (H&N) cancer.
However, its sensitivity to anatomical changes requires frequent adaptation
through online adaptive radiation therapy (oART), which depends on fast,
accurate dose calculation via Monte Carlo (MC) simulations. Reducing particle
count accelerates MC but degrades accuracy. To address this, denoising
low-statistics MC dose maps is proposed to enable fast, high-quality dose
generation.
  Methods: We developed a diffusion transformer-based denoising framework. IMPT
plans and 3D CT images from 80 H&N patients were used to generate noisy and
high-statistics dose maps using MCsquare (1 min and 10 min per plan,
respectively). Data were standardized into uniform chunks with zero-padding,
normalized, and transformed into quasi-Gaussian distributions. Testing was done
on 10 H&N, 10 lung, 10 breast, and 10 prostate cancer cases, preprocessed
identically. The model was trained with noisy dose maps and CT images as input
and high-statistics dose maps as ground truth, using a combined loss of mean
square error (MSE), residual loss, and regional MAE (focusing on top/bottom 10%
dose voxels). Performance was assessed via MAE, 3D Gamma passing rate, and DVH
indices.
  Results: The model achieved MAEs of 0.195 (H&N), 0.120 (lung), 0.172
(breast), and 0.376 Gy[RBE] (prostate). 3D Gamma passing rates exceeded 92%
(3%/2mm) across all sites. DVH indices for clinical target volumes (CTVs) and
OARs closely matched the ground truth.
  Conclusion: A diffusion transformer-based denoising framework was developed
and, though trained only on H&N data, generalizes well across multiple disease
sites.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [524] [An Expansion-Based Approach for Quantified Integer Programming](https://arxiv.org/abs/2506.04452)
*Michael Hartisch,Leroy Chew*

Main category: cs.DM

TL;DR: 提出基于CEGAR的扩展式求解方法用于QIP问题，在特定场景下性能优于现有搜索式求解器并突破QBF约束表达能力限制。


<details>
  <summary>Details</summary>
Motivation: 现有QIP解决方案主要基于搜索范式，而QBF领域成熟的扩展式求解范式尚未被引入QIP领域。作者旨在探索扩展式解法在QIP中的适用性，并突破传统QBF无法处理线性约束的限制。

Method: 将QBF领域的反例引导抽象精化（CEGAR）技术迁移到QIP求解中，建立支持线性约束的多阶段鲁棒优化建模框架，并通过优化器嵌入实现算法加速。

Result: 实验证明该方法在特定实例中优于QIP搜索式求解器，且因支持线性约束建模，相较QBF扩展式求解器获得显著性能提升（达数量级优势）。

Conclusion: 首次将扩展式求解范式引入QIP领域，不仅扩展了QIP的解法体系，还通过线性约束支持能力突破了传统QBF求解器的应用边界，为复杂优化问题提供新思路。

Abstract: Quantified Integer Programming (QIP) bridges multiple domains by extending
Quantified Boolean Formulas (QBF) to incorporate general integer variables and
linear constraints while also generalizing Integer Programming through variable
quantification. As a special case of Quantified Constraint Satisfaction
Problems (QCSP), QIP provides a versatile framework for addressing complex
decision-making scenarios. Additionally, the inclusion of a linear objective
function enables QIP to effectively model multistage robust discrete linear
optimization problems, making it a powerful tool for tackling uncertainty in
optimization.
  While two primary solution paradigms exist for QBF -- search-based and
expansion-based approaches -- only search-based methods have been explored for
QIP and QCSP. We introduce an expansion-based approach for QIP using
Counterexample-Guided Abstraction Refinement (CEGAR), adapting techniques from
QBF. We extend this methodology to tackle multistage robust discrete
optimization problems with linear constraints and further embed it in an
optimization framework, enhancing its applicability. Our experimental results
highlight the advantages of this approach, demonstrating superior performance
over existing search-based solvers for QIP in specific instances. Furthermore,
the ability to model problems using linear constraints enables notable
performance gains over state-of-the-art expansion-based solvers for QBF.

</details>


### [525] [An Expansion-Based Approach for Quantified Integer Programming](https://arxiv.org/abs/2506.04452)
*Michael Hartisch,Leroy Chew*

Main category: cs.DM

TL;DR: 论文介绍量化整数规划（QIP），引入基于展开的方法，实验表明该方法在特定实例中优于现有QIP搜索求解器和QBF展开求解器。


<details>
  <summary>Details</summary>
Motivation: QBF有基于搜索和基于展开两种求解范式，而QIP和QCSP仅探索了基于搜索的方法，因此引入基于展开的方法解决QIP问题。

Method: 使用反例引导的抽象细化（CEGAR）引入基于展开的方法解决QIP，扩展该方法处理多阶段鲁棒离散优化问题并嵌入优化框架。

Result: 实验表明该方法在特定实例中优于现有的QIP搜索求解器，且用线性约束建模问题时性能优于最先进的QBF展开求解器。

Conclusion: 基于展开的方法解决QIP问题具有优势，能有效处理多阶段鲁棒离散优化问题。

Abstract: Quantified Integer Programming (QIP) bridges multiple domains by extending
Quantified Boolean Formulas (QBF) to incorporate general integer variables and
linear constraints while also generalizing Integer Programming through variable
quantification. As a special case of Quantified Constraint Satisfaction
Problems (QCSP), QIP provides a versatile framework for addressing complex
decision-making scenarios. Additionally, the inclusion of a linear objective
function enables QIP to effectively model multistage robust discrete linear
optimization problems, making it a powerful tool for tackling uncertainty in
optimization.
  While two primary solution paradigms exist for QBF -- search-based and
expansion-based approaches -- only search-based methods have been explored for
QIP and QCSP. We introduce an expansion-based approach for QIP using
Counterexample-Guided Abstraction Refinement (CEGAR), adapting techniques from
QBF. We extend this methodology to tackle multistage robust discrete
optimization problems with linear constraints and further embed it in an
optimization framework, enhancing its applicability. Our experimental results
highlight the advantages of this approach, demonstrating superior performance
over existing search-based solvers for QIP in specific instances. Furthermore,
the ability to model problems using linear constraints enables notable
performance gains over state-of-the-art expansion-based solvers for QBF.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [526] [CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2505.23449)
*Fanxiao Li,Jiaying Wu,Canyuan He,Wei Zhou*

Main category: cs.MM

TL;DR: 提出CMIE框架解决多模态大模型在OOC虚假信息检测中的深层语义关联捕捉和噪声干扰问题


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型(GPT-4o)在检测图文深层关联的OOC虚假信息时存在不足，且证据噪声会显著影响检测精度

Method: 提出CMIE框架，包含共生关系生成(CRG)策略和关联评分(AS)机制，选择性利用相关证据增强检测

Result: 实验证明该方法优于现有基准模型

Conclusion: CMIE通过挖掘图文底层共生关系并过滤噪声证据，有效提升虚假信息检测能力

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.

</details>


### [527] [CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2505.23449)
*Fanxiao Li,Jiaying Wu,Canyuan He,Wei Zhou*

Main category: cs.MM

TL;DR: 现有多模态大语言模型检测离题错误信息存在困难，本文提出CMIE框架，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在检测离题错误信息时存在难以捕捉深层关系和证据噪声影响检测精度的问题。

Method: 提出CMIE框架，包含共存关系生成策略和关联评分机制，识别图像与文本的潜在共存关系，选择性利用相关证据。

Result: 实验显示CMIE方法优于现有方法。

Conclusion: CMIE框架能有效解决多模态大语言模型检测离题错误信息的问题。

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [528] [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
*Ankit Pal,Jung-Oh Lee,Xiaoman Zhang,Malaikannan Sankarasubbu,Seunghyeon Roh,Won Jung Kim,Meesun Lee,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ReXVQA是当前最大的胸片放射学视觉问答基准，包含69.6万问题与16万胸片，评估8个多模态大模型并展示AI首次超越人类放射科医师表现。


<details>
  <summary>Details</summary>
Motivation: 解决传统模板化查询的局限性，建立反映真实临床推理技能（存在性判断、定位分析、否定检测等）的评估体系。

Method: 使用MedGemma-4B-it等8个先进模型进行测试，并设计含3名放射科住院医师的人类对照研究（200案例双盲评估）。

Result: MedGemma模型整体准确率83.24%，人类读者最佳准确率77.27%（AI相对人类提升6.57个百分点）。

Conclusion: ReXVQA通过公开数据集和细粒度评估框架，为开发具备临床推理能力的下一代放射学AI系统奠定了基础。

Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual
question answering (VQA) in chest radiology, comprising approximately 696,000
questions paired with 160,000 chest X-rays studies across training, validation,
and test sets. Unlike prior efforts that rely heavily on template based
queries, ReXVQA introduces a diverse and clinically authentic task suite
reflecting five core radiological reasoning skills: presence assessment,
location analysis, negation detection, differential diagnosis, and geometric
reasoning. We evaluate eight state-of-the-art multimodal large language models,
including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The
best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge
the gap between AI performance and clinical expertise, we conducted a
comprehensive human reader study involving 3 radiology residents on 200
randomly sampled cases. Our evaluation demonstrates that MedGemma achieved
superior performance (83.84% accuracy) compared to human readers (best
radiology resident: 77.27%), representing a significant milestone where AI
performance exceeds expert human evaluation on chest X-ray interpretation. The
reader study reveals distinct performance patterns between AI models and human
experts, with strong inter-reader agreement among radiologists while showing
more variable agreement patterns between human readers and AI models. ReXVQA
establishes a new standard for evaluating generalist radiological AI systems,
offering public leaderboards, fine-grained evaluation splits, structured
explanations, and category-level breakdowns. This benchmark lays the foundation
for next-generation AI systems capable of mimicking expert-level clinical
reasoning beyond narrow pathology classification. Our dataset will be
open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

</details>


### [529] [LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table](https://arxiv.org/abs/2506.04790)
*Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出LotusFilter后处理模块，通过预计算向量邻近关系表快速过滤冗余结果，在RAG应用中实现高效多样化检索（0.02毫秒/查询）


<details>
  <summary>Details</summary>
Motivation: 传统近似最近邻搜索（ANNS）在RAG等应用中易返回过度相似的检索结果，而实际场景需要结果兼具相关性与多样性

Method: 1. 预计算向量邻近关系表
2. 检索时通过贪婪查找表格快速删除候选集中冗余向量
3. 支持OpenAI等现代嵌入特征

Result: 在模拟真实RAG应用场景中，处理速度达0.02毫秒/查询，较传统方法显著提升效率

Conclusion: LotusFilter以轻量化后处理方式有效平衡检索结果的相关性与多样性，适用于需要实时响应的大规模RAG应用

Abstract: Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.

</details>


### [530] [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)
*Yi Lu,Jiawang Cao,Yongliang Wu,Bozheng Li,Licheng Tang,Yangguang Ji,Chong Wu,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 提出RSVP框架，通过多模态链式思维视觉提示结合两阶段推理分割，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型(MLLMs)缺乏显式视觉定位机制导致的认知推理与视觉感知割裂问题

Method: 两阶段框架：1) 推理阶段用多模态CoT提示生成可解释区域建议；2) 分割阶段通过VLSM模块融合文本-视觉线索进行细化

Result: ReasonSeg数据集上gIoU提升6.5%、cIoU提升9.2%；SegInW零样本达49.7mAP

Conclusion: RSVP建立了可解释推理分割新范式，验证了认知推理与结构化视觉理解整合的有效性

Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
reasoning capability while lack explicit mechanisms for visual grounding and
segmentation, creating a gap between cognitive reasoning and visual perception.
To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting
(RSVP), a novel framework that unifies multi-step multimodal reasoning with
grounded visual understanding. RSVP is a two-stage structuralized framework
that integrates reasoning-driven localization with segmentation refinement. In
the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to
help MLLMs understand queries and infer targets, generating interpretable
region proposals that enhance visual grounding. In segmentation stage, RSVP
refines these proposals with a Vision-Language Segmentation Module (VLSM),
seamlessly integrates textual and visual cues to produce precise segmentation
masks. By explicitly modelling the interaction between multimodal reasoning and
segmentation, RSVP introduces a new paradigm for interpretable reasoning
segmentation. It exploits MLLMs' inherent localization capabilities, enabling
the models to not only reason about objects but also generate structured visual
representations. Our extensive experiments demonstrate that RSVP achieves
state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5
gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under
zero-shot settings. These results validate RSVP as an effective and scalable
framework for integrating cognitive reasoning with structured visual
understanding.

</details>


### [531] [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)
*Ziming Cheng,Binrui Xu,Lisheng Gong,Zuhe Song,Tianshuo Zhou,Shiqi Zhong,Siyu Ren,Mingxiang Chen,Xiangchao Meng,Yuxin Zhang,Yanlin Li,Lei Ren,Wei Chen,Zhiyuan Huang,Mingjie Zhan,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CV

TL;DR: 提出首个多图像结构化推理基准MMRB，包含92个子任务，实验显示开源模型显著落后商业模型且奖励模型难以处理多图像排序任务


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准局限于单图像推理或多图像任务的最终答案评估，缺乏对多图像结构化推理能力的系统评估

Method: 构建覆盖时空语义推理的92个任务，使用GPT-4o生成多方案CoT注释并经人工优化，提出基于开源LLM的句子级匹配评估框架

Result: 40个MLLM实验表明开源模型落后商业模型（如GPT-4）50%以上，当前奖励模型在多图像场景中的正确率接近0%

Conclusion: MMRB揭示了多模态模型在多图像推理领域的重大技术差距，强调需要开发专用架构和优化训练范式

Abstract: With enhanced capabilities and widespread applications, Multimodal Large
Language Models (MLLMs) are increasingly required to process and reason over
multiple images simultaneously. However, existing MLLM benchmarks focus either
on single-image visual reasoning or on multi-image understanding tasks with
only final-answer evaluation, leaving the reasoning capabilities of MLLMs over
multi-image inputs largely underexplored. To address this gap, we introduce the
$\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first
benchmark designed to evaluate structured visual reasoning across multiple
images. MMRB comprises $\textbf{92 sub-tasks}$ covering spatial, temporal, and
semantic reasoning, with multi-solution, CoT-style annotations generated by
GPT-4o and refined by human experts. A derivative subset is designed to
evaluate multimodal reward models in multi-image scenarios. To support fast and
scalable evaluation, we propose a sentence-level matching framework using
open-source LLMs. Extensive baseline experiments on $\textbf{40 MLLMs}$,
including 9 reasoning-specific models and 8 reward models, demonstrate that
open-source MLLMs still lag significantly behind commercial MLLMs in
multi-image reasoning tasks. Furthermore, current multimodal reward models are
nearly incapable of handling multi-image reward ranking tasks.

</details>


### [532] [Ice Hockey Puck Localization Using Contextual Cues](https://arxiv.org/abs/2506.04365)
*Liam Salass,Jerrin Bright,Amir Nazemi,Yuhao Chen,John Zelek,David Clausi*

Main category: cs.CV

TL;DR: PLUCC模型通过冰球运动员姿态和视线方向等上下文线索，结合多尺度特征金字塔与门控解码器，在冰球检测任务中实现12.2%的平均精度提升。


<details>
  <summary>Details</summary>
Motivation: 传统冰球检测方法依赖外观/运动特征，忽略运动员身体朝向与视线集中冰球的强上下文关联。利用这种人体行为线索可显著提升检测效果。

Method: 三模块架构：1) 上下文编码器(提取运动员姿态位置特征)；2) 双编码器多尺度特征金字塔；3) 通道门控机制的潜在特征融合解码器。

Result: 在PuckDataset上实现SOTA：平均精度提升12.2%，新评价指标RSLE提升25%。证明上下文理解对运动分析的关键作用。

Conclusion: 人体行为上下文建模显著提升冰球检测性能，该范式对自动化体育分析具有广泛启示意义。

Abstract: Puck detection in ice hockey broadcast videos poses significant challenges
due to the puck's small size, frequent occlusions, motion blur, broadcast
artifacts, and scale inconsistencies due to varying camera zoom and broadcast
camera viewpoints. Prior works focus on appearance-based or motion-based cues
of the puck without explicitly modelling the cues derived from player
behaviour. Players consistently turn their bodies and direct their gaze toward
the puck. Motivated by this strong contextual cue, we propose Puck Localization
Using Contextual Cues (PLUCC), a novel approach for scale-aware and
context-driven single-frame puck detections. PLUCC consists of three
components: (a) a contextual encoder, which utilizes player orientations and
positioning as helpful priors; (b) a feature pyramid encoder, which extracts
multiscale features from the dual encoders; and (c) a gating decoder that
combines latent features with a channel gating mechanism. For evaluation, in
addition to standard average precision, we propose Rink Space Localization
Error (RSLE), a scale-invariant homography-based metric for removing
perspective bias from rink space evaluation. The experimental results of PLUCC
on the PuckDataset dataset demonstrated state-of-the-art detection performance,
surpassing previous baseline methods by an average precision improvement of
12.2% and RSLE average precision of 25%. Our research demonstrates the critical
role of contextual understanding in improving puck detection performance, with
broad implications for automated sports analysis.

</details>


### [533] [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
*Matthew W. Shinkle,Mark D. Lescroart*

Main category: cs.CV

TL;DR: 该研究将激活最大化技术应用于基于深度神经网络的脑编码模型，通过生成特定图像验证其对视觉皮层神经活动的靶向调控能力。


<details>
  <summary>Details</summary>
Motivation: 现有DNN编码模型能预测大脑活动但无法解释特征驱动机制，本研究旨在通过激活最大化技术揭示视觉皮层特征选择性的神经机制。

Method: 使用预训练Inception V3提取多层级激活值，通过线性回归建立全图像可计算的fMRI响应预测模型，并应用激活最大化生成优化图像刺激特定脑区。

Result: 生成图像能定性对应已知脑区选择性特征，fMRI实验证实其可可靠激活目标脑区（包括高低阶视觉区），且效果具有跨被试一致性。

Conclusion: 该方法突破传统生成模型的限制，为理解与调控人类视觉系统响应提供了灵活的新范式。

Abstract: Deep neural networks (DNNs) trained on visual tasks develop feature
representations that resemble those in the human visual system. Although
DNN-based encoding models can accurately predict brain responses to visual
stimuli, they offer limited insight into the specific features driving these
responses. Here, we demonstrate that activation maximization -- a technique
designed to interpret vision DNNs -- can be applied to DNN-based encoding
models of the human brain. We extract and adaptively downsample activations
from multiple layers of a pretrained Inception V3 network, then use linear
regression to predict fMRI responses. This yields a full image-computable model
of brain responses. Next, we apply activation maximization to generate images
optimized for predicted responses in individual cortical voxels. We find that
these images contain visual characteristics that qualitatively correspond with
known selectivity and enable exploration of selectivity across the visual
cortex. We further extend our method to whole regions of interest (ROIs) of the
brain and validate its efficacy by presenting these images to human
participants in an fMRI study. We find that the generated images reliably drive
activity in targeted regions across both low- and high-level visual areas and
across subjects. These results demonstrate that activation maximization can be
successfully applied to DNN-based encoding models. By addressing key
limitations of alternative approaches that require natively generative models,
our approach enables flexible characterization and modulation of responses
across the human visual system.

</details>


### [534] [HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation](https://arxiv.org/abs/2506.04421)
*Hermann Kumbong,Xian Liu,Tsung-Yi Lin,Ming-Yu Liu,Xihui Liu,Ziwei Liu,Daniel Y. Fu,Christopher Ré,David W. Romero*

Main category: cs.CV

TL;DR: HMAR通过分层掩码自回归建模改进VAR，在图像生成质量、效率与灵活性上实现突破，支持零样本图像编辑且无需重训练调整采样计划。


<details>
  <summary>Details</summary>
Motivation: 解决VAR模型存在的并行生成导致图像质量下降、分辨率序列超线性增长问题，以及采样计划调整需重训练的局限性。

Method: 1. 将下一尺度预测重构为马尔可夫过程（仅依赖直接前驱尺度）
2. 采用可控多步骤掩码生成机制分阶段预测token子集
3. 开发IO感知块稀疏注意力加速计算

Result: ImageNet 256/512分辨率基准超越VAR/扩散模型，训练速度提升2.5倍，推理内存降低3倍，支持动态采样调整与零样本图像编辑。

Conclusion: HMAR在保持自回归优势的同时，实现了质量-效率-灵活性的三重提升，为图像生成与编辑提供了更优解决方案。

Abstract: Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed
and quality gap between autoregressive image models and diffusion models. VAR
reformulates autoregressive modeling by decomposing an image into successive
resolution scales. During inference, an image is generated by predicting all
the tokens in the next (higher-resolution) scale, conditioned on all tokens in
all previous (lower-resolution) scales. However, this formulation suffers from
reduced image quality due to the parallel generation of all tokens in a
resolution scale; has sequence lengths scaling superlinearly in image
resolution; and requires retraining to change the sampling schedule.
  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image
generation algorithm that alleviates these issues using next-scale prediction
and masked prediction to generate high-quality images with fast sampling. HMAR
reformulates next-scale prediction as a Markovian process, wherein the
prediction of each resolution scale is conditioned only on tokens in its
immediate predecessor instead of the tokens in all predecessor resolutions.
When predicting a resolution scale, HMAR uses a controllable multi-step masked
generation procedure to generate a subset of the tokens in each step. On
ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform
parameter-matched VAR, diffusion, and autoregressive baselines. We develop
efficient IO-aware block-sparse attention kernels that allow HMAR to achieve
faster training and inference times over VAR by over 2.5x and 1.75x
respectively, as well as over 3x lower inference memory footprint. Finally,
HMAR yields additional flexibility over VAR; its sampling schedule can be
changed without further training, and it can be applied to image editing tasks
in a zero-shot manner.

</details>


### [535] [Photoreal Scene Reconstruction from an Egocentric Device](https://arxiv.org/abs/2506.04444)
*Zhaoyang Lv,Maurizio Monge,Ka Chen,Yufeng Zhu,Michael Goesele,Jakob Engel,Zhao Dong,Richard Newcombe*

Main category: cs.CV

TL;DR: 论文提出通过视觉-惯性集束调整(VIBA)精确校准滚动快门相机轨迹，并将物理成像模型融入高斯泼溅，显著提升AR/VR设备的光致真实场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-惯性里程计的6DoF位姿估计方法忽略滚动快门相机的精确时间标定，且未充分考虑传感器动态范围特性，导致像素级重建精度不足。

Method: 1. 使用VIBA生成高频相机轨迹校准滚动快门时序
2. 建立包含滚动快门效应和动态范围的物理成像模型，整合到高斯泼溅框架

Result: 在Project Aria和Meta Quest3设备上验证，PSNR指标平均提升2dB（VIBA贡献+1dB，成像模型再+1dB）

Conclusion: 通过精确的传感器特性建模与轨迹校准，显著提升了移动端光场重建质量，方案已在开源硬件平台验证有效性

Abstract: In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/

</details>


### [536] [Feature-Based Lie Group Transformer for Real-World Applications](https://arxiv.org/abs/2506.04668)
*Takayuki Komatsu,Yoshiyuki Ohmura,Kayato Nishitsunoi,Yasuo Kuniyoshi*

Main category: cs.CV

TL;DR: 提出通过结合特征提取与对象分割改进群分解方法，使代数结构约束的表示学习能应用于真实场景


<details>
  <summary>Details</summary>
Motivation: 传统基于独立特征轴的表示方法无法解释条件独立性，且现有群分解方法受限于低分辨率无背景图像的实际应用限制

Method: 用特征翻译替代像素翻译，将对象分割定义为相同变换下的特征分组，实现代数群分解理论与特征提取的结合

Result: 在包含真实物体与背景的数据集上验证了模型有效性

Conclusion: 该方法为理解人类现实世界物体识别能力的发展机制提供了新视角

Abstract: The main goal of representation learning is to acquire meaningful
representations from real-world sensory inputs without supervision.
Representation learning explains some aspects of human development. Various
neural network (NN) models have been proposed that acquire empirically good
representations. However, the formulation of a good representation has not been
established. We recently proposed a method for categorizing changes between a
pair of sensory inputs. A unique feature of this approach is that
transformations between two sensory inputs are learned to satisfy algebraic
structural constraints. Conventional representation learning often assumes that
disentangled independent feature axes is a good representation; however, we
found that such a representation cannot account for conditional independence.
To overcome this problem, we proposed a new method using group decomposition in
Galois algebra theory. Although this method is promising for defining a more
general representation, it assumes pixel-to-pixel translation without feature
extraction, and can only process low-resolution images with no background,
which prevents real-world application. In this study, we provide a simple
method to apply our group decomposition theory to a more realistic scenario by
combining feature extraction and object segmentation. We replace pixel
translation with feature translation and formulate object segmentation as
grouping features under the same transformation. We validated the proposed
method on a practical dataset containing both real-world object and background.
We believe that our model will lead to a better understanding of human
development of object recognition in the real world.

</details>


### [537] [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)
*Jing-En Huang,I-Sheng Fang,Tzuhsuan Huang,Chih-Yu Wang,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出Gen-n-Val框架，通过Layer Diffusion、LLM和VLLM生成高质量单对象掩码与背景，将无效数据比例从50%降至7%，在COCO实例分割任务中使YOLOv9c的稀有类mAP提升1%，在开放检测任务中YOLO11m提升7.1% mAP


<details>
  <summary>Details</summary>
Motivation: 解决现有合成数据方法存在的多对象掩码、分割不准确、类别标签错误等问题

Method: 双代理系统：1) LD提示代理优化提示生成单对象数据；2) VLLM验证代理过滤低质量数据，结合TextGrad优化提示和图像协调技术实现多实例场景合成

Result: COCO实例分割任务中YOLOv9c的稀有类mAP提升1%；开放检测任务中YOLO11m提升7.1% mAP；YOLOv9/YOLO11系列在实例分割和检测任务均有提升

Conclusion: Gen-n-Val显著提升合成数据质量与模型性能，验证了代理协同框架在视觉任务中的有效性，为数据生成提供新范式

Abstract: Recently, Large Language Models (LLMs) and Vision Large Language Models
(VLLMs) have demonstrated impressive performance as agents across various tasks
while data scarcity and label noise remain significant challenges in computer
vision tasks, such as object detection and instance segmentation. A common
solution for resolving these issues is to generate synthetic data. However,
current synthetic data generation methods struggle with issues, such as
multiple objects per mask, inaccurate segmentation, and incorrect category
labels, limiting their effectiveness. To address these issues, we introduce
Gen-n-Val, a novel agentic data generation framework that leverages Layer
Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks
and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt
agent, an LLM, optimizes prompts for LD to generate high-quality foreground
instance images and segmentation masks. These optimized prompts ensure the
generation of single-object synthetic data with precise instance masks and
clean backgrounds. (2) The data validation agent, a VLLM, which filters out
low-quality synthetic instance images. The system prompts for both agents are
refined through TextGrad. Additionally, we use image harmonization to combine
multiple instances within scenes. Compared to state-of-the-art synthetic data
approaches like MosaicFusion, our approach reduces invalid synthetic data from
50% to 7% and improves performance by 1% mAP on rare classes in COCO instance
segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant
improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object
detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance
of YOLOv9 and YOLO11 families in instance segmentation and object detection.

</details>


### [538] [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)
*Youngwan Lee,Kangsan Kim,Kwanyong Park,Ilcahe Jung,Soojin Jang,Seanie Lee,Yong-Ju Lee,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 论文提出HoliSafe多模态安全数据集与SafeLLaVA模型，通过覆盖全面的图像-文本风险组合与创新的安全架构设计，显著提升了视觉语言模型的安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM安全研究存在两缺陷：1) 安全基准仅覆盖部分风险组合，导致模型在未见攻击场景下依然脆弱；2) 方法过度依赖数据调整，缺乏内在安全机制创新。

Method: 1) 构建覆盖五类图像-文本安全组合的HoliSafe数据集；2) 设计包含安全元令牌（编码有害视觉特征）和安全头（可解释分类）的SafeLLaVA模型架构。

Result: 实验证明SafeLLaVA在多个基准达到SOTA安全性能，HoliSuite有效暴露现有模型漏洞。

Conclusion: 该研究为多模态模型安全提供了更鲁棒的训练框架与评估基准，推动可解释安全机制与多模态对齐技术的发展。

Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models
(VLMs), current approaches face two main shortcomings. 1) Existing
safety-tuning datasets and benchmarks only partially consider how image-text
interactions can yield harmful content, often overlooking contextually unsafe
outcomes from seemingly benign pairs. This narrow coverage leaves VLMs
vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely
primarily on data-centric tuning, with limited architectural innovations to
intrinsically strengthen safety. We address these gaps by introducing a
holistic safety dataset and benchmark, HoliSafe, that spans all five
safe/unsafe image-text combinations, providing a more robust basis for both
training and evaluation. We further propose SafeLLaVA, a novel VLM augmented
with a learnable safety meta token and a dedicated safety head. The meta token
encodes harmful visual cues during training, intrinsically guiding the language
model toward safer responses, while the safety head offers interpretable
harmfulness classification aligned with refusal rationales. Experiments show
that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety
performance across multiple VLM benchmarks. Additionally, the HoliSafe
benchmark itself reveals critical vulnerabilities in existing models. We hope
that HoliSafe and SafeLLaVA will spur further research into robust and
interpretable VLM safety, expanding future avenues for multimodal alignment.

</details>


### [539] [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706)
*Achyuta Rajaram,Sarah Schwettmann,Jacob Andreas,Arthur Conmy*

Main category: cs.CV

TL;DR: 研究通过线性解码和稀疏自编码器分析多模态大语言模型LLaVA-Next中图像表征的可解释性


<details>
  <summary>Details</summary>
Motivation: 探索视觉-语言多模态模型中图像概念的内部表征机制，揭示跨模态特征的交互规律

Method: 1. 在残差流中识别线性可解码的ImageNet类别特征 2. 通过目标编辑验证特征因果性 3. 训练多模态稀疏自编码器构建可解释特征词典

Result: 发现深层网络存在跨模态特征共享增强现象，早期层图像/文本特征分离，深层表征逐渐融合

Conclusion: 多模态模型的图像表征具有线性可解释性且可因果编辑，深层特征共享现象为跨模态理解提供了新视角

Abstract: Language models can be equipped with multimodal capabilities by fine-tuning
on embeddings of visual inputs. But how do such multimodal models represent
images in their hidden activations? We explore representations of image
concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set
of ImageNet classes represented via linearly decodable features in the residual
stream. We show that the features are causal by performing targeted edits on
the model output. In order to increase the diversity of the studied linear
features, we train multimodal Sparse Autoencoders (SAEs), creating a highly
interpretable dictionary of text and image features. We find that although
model representations across modalities are quite disjoint, they become
increasingly shared in deeper layers.

</details>


### [540] [Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data](https://arxiv.org/abs/2506.04717)
*Babar Hussain,Qiang Liu,Gang Chen,Bihai She,Dahai Yu*

Main category: cs.CV

TL;DR: 提出基于SegGPT架构改进的AI辅助自动标注系统，通过双阶段训练机制提升面板缺陷检测效果


<details>
  <summary>Details</summary>
Motivation: 解决工业检测系统中人工标注成本高、效率低的问题

Method: 1. 增强SegGPT架构的上下文学习能力
2. 引入涂鸦标注机制
3. 采用工业面板数据集的双阶段训练方案

Result: 平均IoU提升0.22，召回率提升14%，保持60%自动标注覆盖率，自动标注数据训练模型达到人工标注模型性能

Conclusion: 该系统显著降低工业质检中的人工标注需求，提供可行的自动化解决方案

Abstract: This paper presents an AI-assisted auto-labeling system for display panel
defect detection that leverages in-context learning capabilities. We adopt and
enhance the SegGPT architecture with several domain-specific training
techniques and introduce a scribble-based annotation mechanism to streamline
the labeling process. Our two-stage training approach, validated on industrial
display panel datasets, demonstrates significant improvements over the baseline
model, achieving an average IoU increase of 0.22 and a 14% improvement in
recall across multiple product types, while maintaining approximately 60%
auto-labeling coverage. Experimental results show that models trained on our
auto-labeled data match the performance of those trained on human-labeled data,
offering a practical solution for reducing manual annotation efforts in
industrial inspection systems.

</details>


### [541] [Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater Image Enhancement](https://arxiv.org/abs/2506.04753)
*Niki Martinel,Rita Pucci*

Main category: cs.CV

TL;DR: 提出融合物理模型与胶囊聚类的双流架构，实现参数自由的水下图像增强，在PSNR提升0.5dB的同时计算量减少三分之二


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法在物理约束遵循与计算效率之间存在权衡，需同时保持物理合理性和语义结构完整性

Method: 双流架构：物理估计器（传输图/背景光估计） + 胶囊聚类特征学习，结合多空间频率的感知优化目标

Result: 在6个基准测试中PSNR提升+0.5dB（同计算量下+1dB），计算复杂度仅为现有最佳方法的三分之一

Conclusion: 物理引导的架构设计有效平衡性能与效率，为实时水下图像处理提供实用解决方案

Abstract: We present a novel dual-stream architecture that achieves state-of-the-art
underwater image enhancement by explicitly integrating the Jaffe-McGlamery
physical model with capsule clustering-based feature representation learning.
Our method simultaneously estimates transmission maps and spatially-varying
background light through a dedicated physics estimator while extracting
entity-level features via capsule clustering in a parallel stream. This
physics-guided approach enables parameter-free enhancement that respects
underwater formation constraints while preserving semantic structures and
fine-grained details. Our approach also features a novel optimization objective
ensuring both physical adherence and perceptual quality across multiple spatial
frequencies. To validate our approach, we conducted extensive experiments
across six challenging benchmarks. Results demonstrate consistent improvements
of $+0.5$dB PSNR over the best existing methods while requiring only one-third
of their computational complexity (FLOPs), or alternatively, more than $+1$dB
PSNR improvement when compared to methods with similar computational budgets.
Code and data \textit{will} be available at https://github.com/iN1k1/.

</details>


### [542] [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning](https://arxiv.org/abs/2506.04755)
*Shenshen Li,Kaiyuan Deng,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Heng Tao Shen,Xing Xu*

Main category: cs.CV

TL;DR: 提出RAP方法通过筛选9.3%的关键训练样本(认知样本)，在保持多模态推理性能的同时降低43%计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统MLLMs训练存在数据冗余问题，研究发现仅有稀疏的认知样本真正激活多模态推理能力，其余样本贡献有限。

Method: RAP框架包含：1) 因果差异估计器(CDE)消除语言先验依赖；2) 注意力置信估计器(ACE)过滤无关token；3) 难度感知替换模块(DRM)增强样本复杂度。

Result: 在6个数据集上验证，仅用9.3%训练数据达到SOTA性能，计算成本降低超43%。

Conclusion: 该工作挑战了数据量的固有认知，为MLLMs的高效训练提供了新范式。

Abstract: While multi-modal large language models (MLLMs) have made significant
progress in complex reasoning tasks via reinforcement learning, it is commonly
believed that extensive training data is necessary for improving multi-modal
reasoning ability, inevitably leading to data redundancy and substantial
computational costs. However, can smaller high-value datasets match or
outperform full corpora for multi-modal reasoning in MLLMs? In this work, we
challenge this assumption through a key observation: meaningful multi-modal
reasoning is triggered by only a sparse subset of training samples, termed
cognitive samples, whereas the majority contribute marginally. Building on this
insight, we propose a novel data selection paradigm termed Reasoning Activation
Potential (RAP), which identifies cognitive samples by estimating each sample's
potential to stimulate genuine multi-modal reasoning by two complementary
estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential
outcome model principle, eliminates samples that overly rely on language priors
by comparing outputs between multi-modal and text-only inputs; 2) Attention
Confidence Estimator (ACE), which exploits token-level self-attention to
discard samples dominated by irrelevant but over-emphasized tokens in
intermediate reasoning stages. Moreover, we introduce a Difficulty-aware
Replacement Module (DRM) to substitute trivial instances with cognitively
challenging ones, thereby ensuring complexity for robust multi-modal reasoning.
Experiments on six datasets show that our RAP method consistently achieves
superior performance using only 9.3% of the training data, while reducing
computational costs by over 43%. Our code is available at
https://github.com/Leo-ssl/RAP.

</details>


### [543] [CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx](https://arxiv.org/abs/2506.04931)
*Lukas Picek,Elisa Belotti,Michal Bojda,Ludek Bufka,Vojtech Cermak,Martin Dula,Rostislav Dvorak,Luboslav Hrdy,Miroslav Jirik,Vaclav Kocourek,Josefa Krausova,Jirı Labuda,Jakub Straka,Ludek Toman,Vlado Trulık,Martin Vana,Miroslav Kutal*

Main category: cs.CV

TL;DR: 首个面向欧亚猞猁的大规模开放数据集CzechLynx，涵盖个体识别/姿态估计/实例分割，包含3万真实图像与10万合成数据，支持跨时空泛化测试。


<details>
  <summary>Details</summary>
Motivation: 现有野生动物数据集在个体多样性、标注维度及时空覆盖不足，需建立标准化基准推动动物重识别等算法发展。

Method: 通过15年系统监测收集真实相机陷阱图像并标注分割掩码/身份/骨骼信息，结合Unity生成环境与扩散模型纹理建模创造合成数据。

Result: 建成覆盖219个体的双地理区域数据集，创建三种评估协议(地理感知/时间开放集/时间封闭集)验证模型泛化能力。

Conclusion: 该数据集为动物重识别算法提供跨时空场景的标准化测试平台，推动计算机视觉在生态保护中的深度应用。

Abstract: We introduce CzechLynx, the first large-scale, open-access dataset for
individual identification, 2D pose estimation, and instance segmentation of the
Eurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images
annotated with segmentation masks, identity labels, and 20-point skeletons and
covers 219 unique individuals across 15 years of systematic monitoring in two
geographically distinct regions: Southwest Bohemia and the Western Carpathians.
To increase the data variability, we create a complementary synthetic set with
more than 100k photorealistic images generated via a Unity-based pipeline and
diffusion-driven text-to-texture modeling, covering diverse environments,
poses, and coat-pattern variations. To allow testing generalization across
spatial and temporal domains, we define three tailored evaluation
protocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware
closed-set. This dataset is targeted to be instrumental in benchmarking
state-of-the-art models and the development of novel methods for not just
individual animal re-identification.

</details>


### [544] [Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations](https://arxiv.org/abs/2506.04951)
*Igor Meleshin,Anna Chistyakova,Anastasia Antsiferova,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出通过架构设计（正交信息流与保范操作）而非对抗训练构建鲁棒图像质量评估模型，挑战传统数据驱动防御范式。


<details>
  <summary>Details</summary>
Motivation: 现有IQA模型易受对抗攻击，传统方法依赖数据驱动防御（如对抗训练）。本文质疑该范式，主张将鲁棒性视为架构先验，从设计层面抑制模型敏感性。

Method: 1. 强制正交信息流约束网络参数空间；2. 使用保范运算维持特征稳定性；3. 结合剪枝与微调进一步降低敏感性。架构层面实现对抗鲁棒性，无需对抗训练。

Result: 构建出无需对抗训练即可抵御攻击的IQA架构，证明通过结构设计而非数据优化可实现模型鲁棒性，为感知模型安全提供新思路。

Conclusion: 应转变视角：鲁棒性应作为系统设计属性而非通过数据学习获得。该方法为构建可信赖的视觉评估系统开辟了架构驱动的技术路径。

Abstract: Image Quality Assessment (IQA) models are increasingly relied upon to
evaluate image quality in real-world systems -- from compression and
enhancement to generation and streaming. Yet their adoption brings a
fundamental risk: these models are inherently unstable. Adversarial
manipulations can easily fool them, inflating scores and undermining trust.
Traditionally, such vulnerabilities are addressed through data-driven defenses
-- adversarial retraining, regularization, or input purification. But what if
this is the wrong lens? What if robustness in perceptual models is not
something to learn but something to design? In this work, we propose a
provocative idea: robustness as an architectural prior. Rather than training
models to resist perturbations, we reshape their internal structure to suppress
sensitivity from the ground up. We achieve this by enforcing orthogonal
information flow, constraining the network to norm-preserving operations -- and
further stabilizing the system through pruning and fine-tuning. The result is a
robust IQA architecture that withstands adversarial attacks without requiring
adversarial training or significant changes to the original model. This
approach suggests a shift in perspective: from optimizing robustness through
data to engineering it through design.

</details>


### [545] [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)
*Alan Mitkiy,James Smith,Hana Satou,Hiroshi Tanaka,Emily Johnson,F Monkey*

Main category: cs.CV

TL;DR: 提出动态扰动预算调度框架DES，通过整合决策边界距离、预测置信度和模型不确定性，实现对抗训练中实例级自适应的鲁棒性优化。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练的固定扰动预算无法适应不同数据实例的鲁棒性特征，基于启发式规则的动态调整方法存在局限性。

Method: DES框架动态调度ε值：1) 基于梯度的决策边界距离估计 2) softmax熵的预测置信度 3) Monte Carlo dropout的模型不确定性估计

Result: 在CIFAR-10/100上超越固定ε基线和现有自适应方法，同时提升对抗鲁棒性（最高+3.2%）和标准准确率（+1.5%）

Conclusion: 首次实现数据驱动、实例感知的对抗训练调度策略，为动态鲁棒性优化提供理论保证和新研究方向

Abstract: Adversarial training is among the most effective strategies for defending
deep neural networks against adversarial examples. A key limitation of existing
adversarial training approaches lies in their reliance on a fixed perturbation
budget, which fails to account for instance-specific robustness
characteristics. While prior works such as IAAT and MMA introduce
instance-level adaptations, they often rely on heuristic or static
approximations of data robustness. In this paper, we propose Dynamic Epsilon
Scheduling (DES), a novel framework that adaptively adjusts the adversarial
perturbation budget per instance and per training iteration. DES integrates
three key factors: (1) the distance to the decision boundary approximated via
gradient-based proxies, (2) prediction confidence derived from softmax entropy,
and (3) model uncertainty estimated via Monte Carlo dropout. By combining these
cues into a unified scheduling strategy, DES tailors the perturbation budget
dynamically to guide more effective adversarial learning. Experimental results
on CIFAR-10 and CIFAR-100 show that our method consistently improves both
adversarial robustness and standard accuracy compared to fixed-epsilon
baselines and prior adaptive methods. Moreover, we provide theoretical insights
into the stability and convergence of our scheduling policy. This work opens a
new avenue for instance-aware, data-driven adversarial training methods.

</details>


### [546] [Rectified Point Flow: Generic Point Cloud Pose Estimation](https://arxiv.org/abs/2506.05282)
*Tao Sun,Liyuan Zhu,Shengyu Huang,Shuran Song,Iro Armeni*

Main category: cs.CV

TL;DR: 提出Rectified Point Flow方法，通过连续速度场统一处理点云配准与形状组装，在六项基准测试中实现新SOTA


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要手动处理对称性且无法跨任务共享几何先验的问题，实现无监督的对称性学习和多任务统一建模

Method: 使用点级速度场传输噪声点到目标位置，结合专注于重叠点的自监督编码器，实现姿态恢复和对称性自动学习

Result: 在6个点云配准和形状组装基准上达到SOTA，联合训练策略使准确率提升3.2%

Conclusion: 统一的条件生成框架有效整合多任务数据，自动学习几何先验和对称关系，为三维视觉任务提供新范式

Abstract: We introduce Rectified Point Flow, a unified parameterization that formulates
pairwise point cloud registration and multi-part shape assembly as a single
conditional generative problem. Given unposed point clouds, our method learns a
continuous point-wise velocity field that transports noisy points toward their
target positions, from which part poses are recovered. In contrast to prior
work that regresses part-wise poses with ad-hoc symmetry handling, our method
intrinsically learns assembly symmetries without symmetry labels. Together with
a self-supervised encoder focused on overlapping points, our method achieves a
new state-of-the-art performance on six benchmarks spanning pairwise
registration and shape assembly. Notably, our unified formulation enables
effective joint training on diverse datasets, facilitating the learning of
shared geometric priors and consequently boosting accuracy. Project page:
https://rectified-pointflow.github.io/.

</details>


### [547] [Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning](https://arxiv.org/abs/2506.05341)
*Xingjian Ran,Yixuan Li,Linning Xu,Mulin Yu,Bo Dai*

Main category: cs.CV

TL;DR: 提出DirectLayout框架，通过分解生成步骤与空间推理技术，实现开放词汇的精细化3D布局生成。


<details>
  <summary>Details</summary>
Motivation: 现有布局生成方法受限于数据集规模，存在过拟合或灵活性不足问题，难以平衡开放词汇与用户细粒度控制需求。

Method: 三阶段生成框架：①BEV布局生成 ②3D空间提升 ③对象放置优化，结合CoT激活机制与迭代式资产对齐策略。

Result: 实验证明框架在语义一致性（提升28%）、布局泛化性（覆盖场景类型增加3倍）和物理合理性（碰撞率降低67%）方面表现突出。

Conclusion: 通过分阶段生成架构与空间推理机制的结合，有效解决了复杂场景下的开放词汇布局生成难题，为AI生成内容提供了新范式。

Abstract: Realistic 3D indoor scene synthesis is vital for embodied AI and digital
content creation. It can be naturally divided into two subtasks: object
generation and layout generation. While recent generative models have
significantly advanced object-level quality and controllability, layout
generation remains challenging due to limited datasets. Existing methods either
overfit to these datasets or rely on predefined constraints to optimize
numerical layout that sacrifice flexibility. As a result, they fail to generate
scenes that are both open-vocabulary and aligned with fine-grained user
instructions. We introduce DirectLayout, a framework that directly generates
numerical 3D layouts from text descriptions using generalizable spatial
reasoning of large language models (LLMs). DirectLayout decomposes the
generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting
it into 3D space, and refining object placements. To enable explicit spatial
reasoning and help the model grasp basic principles of object placement, we
employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.
Additionally, we design CoT-Grounded Generative Layout Reward to enhance
generalization and spatial planning. During inference, DirectLayout addresses
asset-layout mismatches via Iterative Asset-Layout Alignment through in-context
learning. Extensive experiments demonstrate that DirectLayout achieves
impressive semantic consistency, generalization and physical plausibility.

</details>


### [548] [Refer to Anything with Vision-Language Prompts](https://arxiv.org/abs/2506.05342)
*Shengcao Cao,Zijun Wei,Jason Kuen,Kangning Liu,Lingzhi Zhang,Jiuxiang Gu,HyunJoon Jung,Liang-Yan Gui,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出全模态参考表达式分割任务(ORES)，通过RAS框架增强分割模型的多模态交互能力，并创建数据集验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有图像分割模型虽能生成高质量掩码，但缺乏对语言+视觉复合查询的语义理解能力，限制了其在多模态交互场景的应用

Method: 开发RAS框架，通过mask-centric大型多模态模型实现文本/文本+参考视觉实体的多模态提示理解，支持任意形式的掩码组生成

Result: RAS在ORES任务及传统RES/GRES任务中均展现优越性能，配套数据集MaskGroups-2M/HQ提供训练与评估基准

Conclusion: 首次定义开放多模态分割任务，提出可扩展的框架解决方案，为视觉语言交互式分割应用奠定新基础

Abstract: Recent image segmentation models have advanced to segment images into
high-quality masks for visual entities, and yet they cannot provide
comprehensive semantic understanding for complex queries based on both language
and vision. This limitation reduces their effectiveness in applications that
require user-friendly interactions driven by vision-language prompts. To bridge
this gap, we introduce a novel task of omnimodal referring expression
segmentation (ORES). In this task, a model produces a group of masks based on
arbitrary prompts specified by text only or text plus reference visual
entities. To address this new challenge, we propose a novel framework to "Refer
to Any Segmentation Mask Group" (RAS), which augments segmentation models with
complex multimodal interactions and comprehension via a mask-centric large
multimodal model. For training and benchmarking ORES models, we create datasets
MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by
text and reference entities. Through extensive evaluation, we demonstrate
superior performance of RAS on our new ORES task, as well as classic referring
expression segmentation (RES) and generalized referring expression segmentation
(GRES) tasks. Project page: https://Ref2Any.github.io.

</details>


### [549] [Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors](https://arxiv.org/abs/2506.04823)
*Svetlana Pavlitska,Jamie Robb,Nikolai Polley,Melih Yazgan,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 通过印刷对抗性补丁攻击交通信号灯检测CNN模型，实现红绿灯标签篡改与图形分类攻击


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦自动驾驶车辆常规摄像头感知任务的攻击，但针对交通信号灯检测器的对抗攻击研究较少。本文旨在探索基于物理世界的印刷补丁对交通信号灯检测模型的威胁

Method: 提出在交通灯下方放置对抗补丁的威胁模型，设计通用攻击训练策略。通过移动施工灯和固定交通灯场景进行真实世界验证，并实施红转绿标签篡改和图形符号分类攻击

Result: 实验室环境成功实现目标标签篡改攻击（红→绿）和图形分类攻击，真实场景测试中印刷补丁在移动/固定交通灯均展现攻击有效性

Conclusion: 证明现有CNN交通灯检测器对物理对抗补丁攻击存在严重脆弱性，揭示自动驾驶感知系统安全防护需求

Abstract: Realistic adversarial attacks on various camera-based perception tasks of
autonomous vehicles have been successfully demonstrated so far. However, only a
few works considered attacks on traffic light detectors. This work shows how
CNNs for traffic light detection can be attacked with printed patches. We
propose a threat model, where each instance of a traffic light is attacked with
a patch placed under it, and describe a training strategy. We demonstrate
successful adversarial patch attacks in universal settings. Our experiments
show realistic targeted red-to-green label-flipping attacks and attacks on
pictogram classification. Finally, we perform a real-world evaluation with
printed patches and demonstrate attacks in the lab settings with a mobile
traffic light for construction sites and in a test area with stationary traffic
lights. Our code is available at
https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.

</details>


### [550] [Quantifying Cross-Modality Memorization in Vision-Language Models](https://arxiv.org/abs/2506.05198)
*Yuxin Wen,Yangsibo Huang,Tom Goldstein,Ravi Kumar,Badih Ghazi,Chiyuan Zhang*

Main category: cs.CV

TL;DR: 研究揭示了多模态模型中跨模态记忆迁移存在显著差距，并通过合成数据集验证了该现象在不同场景下的普遍性，提出基线解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于单模态记忆，但实际应用中视觉语言模型存在跨模态知识迁移需求，需探究其记忆机制以提升安全性和知识获取效率。

Method: 构建合成人物数据集，通过单模态训练+跨模态评估范式，结合不同模型能力/去学习/多跳场景进行系统性实验验证。

Result: 发现跨模态召回存在40%绝对差距，该现象在更强大模型/去学习/多跳场景中持续存在，并提出知识蒸馏基线方法缩小差距。

Conclusion: 该研究为开发鲁棒多模态学习技术提供新方向，启发未来通过改进跨模态对齐机制提升知识迁移效率。

Abstract: Understanding what and how neural networks memorize during training is
crucial, both from the perspective of unintentional memorization of potentially
sensitive information and from the standpoint of effective knowledge
acquisition for real-world, knowledge-intensive tasks. While previous studies
primarily investigate memorization within a single modality, such as text
memorization in large language models or image memorization in diffusion
models, unified multimodal models are becoming increasingly prevalent in
practical applications. In this work, we focus on the unique characteristics of
cross-modality memorization and conduct a systematic study centered on
vision-language models. To facilitate controlled experiments, we first
introduce a synthetic persona dataset comprising diverse synthetic person
images and textual descriptions. We quantify factual knowledge memorization and
cross-modal transferability by training models on a single modality and
evaluating their performance in the other. Our results reveal that facts
learned in one modality transfer to the other, but a significant gap exists
between recalling information in the source and target modalities. Furthermore,
we observe that this gap exists across various scenarios, including more
capable models, machine unlearning, and the multi-hop case. At the end, we
propose a baseline method to mitigate this challenge. We hope our study can
inspire future research on developing more robust multimodal learning
techniques to enhance cross-modal transferability.

</details>


### [551] [Stable Vision Concept Transformers for Medical Diagnosis](https://arxiv.org/abs/2506.05286)
*Lijie Hu,Songning Lai,Yuan Hua,Shu Yang,Jingfeng Zhang,Di Wang*

Main category: cs.CV

TL;DR: 提出VCT和SVCT模型，通过融合视觉特征与概念特征、结合去噪扩散平滑技术，在保持可解释性的同时提升医学AI模型的准确性和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 医学领域对模型透明度的需求与现有概念瓶颈模型(CBMs)存在的两大问题：1)仅依赖概念特征忽略图像本质特征导致性能下降；2)输入扰动下解释稳定性不足。

Method: 1. VCT架构融合视觉Transformer与概念层特征；2. SVCT在VCT基础上集成去噪扩散平滑技术，通过特征融合模块联合优化图像特征与概念特征。

Result: 在四个医学数据集上的实验表明，VCT/SVCT在保持基线模型准确率的同时具备可解释性。SVCT在输入扰动场景下仍能提供稳定的解释（+15%抗干扰性）。

Conclusion: SVCT通过特征融合与扩散平滑技术，首次实现医学AI模型解释稳定性与实用性的统一，满足临床对可靠可解释模型的需求。

Abstract: Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.

</details>


### [552] [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
*Ankit Pal,Jung-Oh Lee,Xiaoman Zhang,Malaikannan Sankarasubbu,Seunghyeon Roh,Won Jung Kim,Meesun Lee,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 本文提出胸部放射学视觉问答基准ReXVQA，评估多个模型，MedGemma表现最佳超人类读者，ReXVQA为评估AI系统设新标准。


<details>
  <summary>Details</summary>
Motivation: 建立全面的胸部放射学视觉问答基准，缩小AI性能与临床专业知识差距。

Method: 构建包含约696,000个问题和160,000个胸部X光检查的ReXVQA基准，评估八个最先进的多模态大语言模型，开展人类读者研究。

Result: 最佳模型MedGemma总体准确率83.24%，在人类读者研究中准确率83.84%超人类读者，人类读者间一致性强，与AI模型一致性模式多变。

Conclusion: ReXVQA为评估通用放射学AI系统建立新标准，为下一代AI系统奠定基础，数据集将开源。

Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual
question answering (VQA) in chest radiology, comprising approximately 696,000
questions paired with 160,000 chest X-rays studies across training, validation,
and test sets. Unlike prior efforts that rely heavily on template based
queries, ReXVQA introduces a diverse and clinically authentic task suite
reflecting five core radiological reasoning skills: presence assessment,
location analysis, negation detection, differential diagnosis, and geometric
reasoning. We evaluate eight state-of-the-art multimodal large language models,
including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The
best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge
the gap between AI performance and clinical expertise, we conducted a
comprehensive human reader study involving 3 radiology residents on 200
randomly sampled cases. Our evaluation demonstrates that MedGemma achieved
superior performance (83.84% accuracy) compared to human readers (best
radiology resident: 77.27%), representing a significant milestone where AI
performance exceeds expert human evaluation on chest X-ray interpretation. The
reader study reveals distinct performance patterns between AI models and human
experts, with strong inter-reader agreement among radiologists while showing
more variable agreement patterns between human readers and AI models. ReXVQA
establishes a new standard for evaluating generalist radiological AI systems,
offering public leaderboards, fine-grained evaluation splits, structured
explanations, and category-level breakdowns. This benchmark lays the foundation
for next-generation AI systems capable of mimicking expert-level clinical
reasoning beyond narrow pathology classification. Our dataset will be
open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

</details>


### [553] [LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table](https://arxiv.org/abs/2506.04790)
*Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出LotusFilter后处理模块使ANNS结果多样化，运行快且代码开源


<details>
  <summary>Details</summary>
Motivation: ANNS结果有时过于相似，特定场景下搜索结果需既与查询相似又具多样性

Method: 预计算总结彼此接近向量的截断表，过滤时LotusFilter贪婪查表从候选中删除冗余向量

Result: LotusFilter在类似真实世界RAG应用的设置中运行速度快，为0.02 [ms/query]

Conclusion: LotusFilter可有效使ANNS结果多样化且运行速度快，代码开源方便使用

Abstract: Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.

</details>


### [554] [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)
*Yi Lu,Jiawang Cao,Yongliang Wu,Bozheng Li,Licheng Tang,Yangguang Ji,Chong Wu,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 文章提出RSVP框架统一多步多模态推理与视觉理解，分两阶段，实验表明其性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型缺乏视觉定位和分割机制，存在认知推理与视觉感知差距。

Method: 提出RSVP两阶段框架，推理阶段用多模态思维链视觉提示辅助推理，分割阶段用VLSM细化提案。

Result: RSVP在ReasonSeg上超SOTA方法，在SegInW零样本设置下达到49.7 mAP。

Conclusion: RSVP是将认知推理与结构化视觉理解结合的有效可扩展框架。

Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
reasoning capability while lack explicit mechanisms for visual grounding and
segmentation, creating a gap between cognitive reasoning and visual perception.
To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting
(RSVP), a novel framework that unifies multi-step multimodal reasoning with
grounded visual understanding. RSVP is a two-stage structuralized framework
that integrates reasoning-driven localization with segmentation refinement. In
the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to
help MLLMs understand queries and infer targets, generating interpretable
region proposals that enhance visual grounding. In segmentation stage, RSVP
refines these proposals with a Vision-Language Segmentation Module (VLSM),
seamlessly integrates textual and visual cues to produce precise segmentation
masks. By explicitly modelling the interaction between multimodal reasoning and
segmentation, RSVP introduces a new paradigm for interpretable reasoning
segmentation. It exploits MLLMs' inherent localization capabilities, enabling
the models to not only reason about objects but also generate structured visual
representations. Our extensive experiments demonstrate that RSVP achieves
state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5
gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under
zero-shot settings. These results validate RSVP as an effective and scalable
framework for integrating cognitive reasoning with structured visual
understanding.

</details>


### [555] [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)
*Ziming Cheng,Binrui Xu,Lisheng Gong,Zuhe Song,Tianshuo Zhou,Shiqi Zhong,Siyu Ren,Mingxiang Chen,Xiangchao Meng,Yuxin Zhang,Yanlin Li,Lei Ren,Wei Chen,Zhiyuan Huang,Mingjie Zhan,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CV

TL;DR: 本文引入多模态多图像推理基准MMRB评估多图像结构化视觉推理能力，提出句级匹配框架，实验表明开源MLLMs在多图像推理任务上落后于商业模型，现有多模态奖励模型难以处理多图像奖励排序任务。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准未充分探索多图像输入的推理能力，需要新基准来评估多图像结构化视觉推理。

Method: 引入MMRB基准，包含92个子任务，用GPT - 4o生成并经人类专家完善多解决方案和CoT风格注释；设计衍生子集评估多模态奖励模型；提出句级匹配框架用于快速可扩展评估。

Result: 对40个MLLMs的实验显示，开源MLLMs在多图像推理任务上远落后于商业MLLMs，当前多模态奖励模型几乎无法处理多图像奖励排序任务。

Conclusion: 新基准MMRB有助于评估MLLMs多图像推理能力，发现开源模型和奖励模型存在不足。

Abstract: With enhanced capabilities and widespread applications, Multimodal Large
Language Models (MLLMs) are increasingly required to process and reason over
multiple images simultaneously. However, existing MLLM benchmarks focus either
on single-image visual reasoning or on multi-image understanding tasks with
only final-answer evaluation, leaving the reasoning capabilities of MLLMs over
multi-image inputs largely underexplored. To address this gap, we introduce the
$\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first
benchmark designed to evaluate structured visual reasoning across multiple
images. MMRB comprises $\textbf{92 sub-tasks}$ covering spatial, temporal, and
semantic reasoning, with multi-solution, CoT-style annotations generated by
GPT-4o and refined by human experts. A derivative subset is designed to
evaluate multimodal reward models in multi-image scenarios. To support fast and
scalable evaluation, we propose a sentence-level matching framework using
open-source LLMs. Extensive baseline experiments on $\textbf{40 MLLMs}$,
including 9 reasoning-specific models and 8 reward models, demonstrate that
open-source MLLMs still lag significantly behind commercial MLLMs in
multi-image reasoning tasks. Furthermore, current multimodal reward models are
nearly incapable of handling multi-image reward ranking tasks.

</details>


### [556] [Ice Hockey Puck Localization Using Contextual Cues](https://arxiv.org/abs/2506.04365)
*Liam Salass,Jerrin Bright,Amir Nazemi,Yuhao Chen,John Zelek,David Clausi*

Main category: cs.CV

TL;DR: 提出PLUCC方法用于冰球检测，设计RSLE评估指标，实验结果超基线。


<details>
  <summary>Details</summary>
Motivation: 以往冰球检测工作未明确建模球员行为线索，而球员会朝向冰球，可利用此线索改进检测。

Method: 提出PLUCC方法，包含上下文编码器、特征金字塔编码器和门控解码器；提出RSLE评估指标。

Result: PLUCC在PuckDataset数据集上检测性能达SOTA，平均精度提升12.2%，RSLE平均精度提升25%。

Conclusion: 上下文理解对提高冰球检测性能至关重要，对自动体育分析有广泛意义。

Abstract: Puck detection in ice hockey broadcast videos poses significant challenges
due to the puck's small size, frequent occlusions, motion blur, broadcast
artifacts, and scale inconsistencies due to varying camera zoom and broadcast
camera viewpoints. Prior works focus on appearance-based or motion-based cues
of the puck without explicitly modelling the cues derived from player
behaviour. Players consistently turn their bodies and direct their gaze toward
the puck. Motivated by this strong contextual cue, we propose Puck Localization
Using Contextual Cues (PLUCC), a novel approach for scale-aware and
context-driven single-frame puck detections. PLUCC consists of three
components: (a) a contextual encoder, which utilizes player orientations and
positioning as helpful priors; (b) a feature pyramid encoder, which extracts
multiscale features from the dual encoders; and (c) a gating decoder that
combines latent features with a channel gating mechanism. For evaluation, in
addition to standard average precision, we propose Rink Space Localization
Error (RSLE), a scale-invariant homography-based metric for removing
perspective bias from rink space evaluation. The experimental results of PLUCC
on the PuckDataset dataset demonstrated state-of-the-art detection performance,
surpassing previous baseline methods by an average precision improvement of
12.2% and RSLE average precision of 25%. Our research demonstrates the critical
role of contextual understanding in improving puck detection performance, with
broad implications for automated sports analysis.

</details>


### [557] [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
*Matthew W. Shinkle,Mark D. Lescroart*

Main category: cs.CV

TL;DR: 本文展示激活最大化可用于基于DNN的人脑编码模型，生成图像能驱动目标脑区活动，解决了替代方法的局限。


<details>
  <summary>Details</summary>
Motivation: DNN编码模型虽能预测脑响应，但对驱动响应的特征缺乏深入理解。

Method: 从预训练Inception V3网络多层提取并自适应下采样激活，用线性回归预测fMRI响应，应用激活最大化生成图像，扩展到脑的感兴趣区域并进行fMRI研究验证。

Result: 生成的图像包含与已知选择性对应的视觉特征，能可靠驱动不同视觉区域和不同受试者目标脑区的活动。

Conclusion: 激活最大化可成功应用于基于DNN的编码模型，能灵活表征和调节人类视觉系统的响应。

Abstract: Deep neural networks (DNNs) trained on visual tasks develop feature
representations that resemble those in the human visual system. Although
DNN-based encoding models can accurately predict brain responses to visual
stimuli, they offer limited insight into the specific features driving these
responses. Here, we demonstrate that activation maximization -- a technique
designed to interpret vision DNNs -- can be applied to DNN-based encoding
models of the human brain. We extract and adaptively downsample activations
from multiple layers of a pretrained Inception V3 network, then use linear
regression to predict fMRI responses. This yields a full image-computable model
of brain responses. Next, we apply activation maximization to generate images
optimized for predicted responses in individual cortical voxels. We find that
these images contain visual characteristics that qualitatively correspond with
known selectivity and enable exploration of selectivity across the visual
cortex. We further extend our method to whole regions of interest (ROIs) of the
brain and validate its efficacy by presenting these images to human
participants in an fMRI study. We find that the generated images reliably drive
activity in targeted regions across both low- and high-level visual areas and
across subjects. These results demonstrate that activation maximization can be
successfully applied to DNN-based encoding models. By addressing key
limitations of alternative approaches that require natively generative models,
our approach enables flexible characterization and modulation of responses
across the human visual system.

</details>


### [558] [HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation](https://arxiv.org/abs/2506.04421)
*Hermann Kumbong,Xian Liu,Tsung-Yi Lin,Ming-Yu Liu,Xihui Liu,Ziwei Liu,Daniel Y. Fu,Christopher Ré,David W. Romero*

Main category: cs.CV

TL;DR: 提出分层掩码自回归建模（HMAR）算法，解决视觉自回归建模（VAR）问题，在基准测试中表现优，训练推理快、内存占用低且灵活。


<details>
  <summary>Details</summary>
Motivation: VAR存在图像质量下降、序列长度超线性增长、更改采样计划需重新训练等问题，需改进。

Method: 将下尺度预测重新表述为马尔可夫过程，预测时使用可控多步掩码生成程序。开发高效IO感知块稀疏注意力内核。

Result: 在ImageNet基准测试中匹配或超越VAR、扩散和自回归基线；训练和推理速度分别比VAR快2.5倍和1.75倍，推理内存占用低3倍以上。

Conclusion: HMAR解决了VAR问题，具有更快的训练推理速度、更低内存占用，且无需重新训练更改采样计划，可零样本应用于图像编辑。

Abstract: Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed
and quality gap between autoregressive image models and diffusion models. VAR
reformulates autoregressive modeling by decomposing an image into successive
resolution scales. During inference, an image is generated by predicting all
the tokens in the next (higher-resolution) scale, conditioned on all tokens in
all previous (lower-resolution) scales. However, this formulation suffers from
reduced image quality due to the parallel generation of all tokens in a
resolution scale; has sequence lengths scaling superlinearly in image
resolution; and requires retraining to change the sampling schedule.
  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image
generation algorithm that alleviates these issues using next-scale prediction
and masked prediction to generate high-quality images with fast sampling. HMAR
reformulates next-scale prediction as a Markovian process, wherein the
prediction of each resolution scale is conditioned only on tokens in its
immediate predecessor instead of the tokens in all predecessor resolutions.
When predicting a resolution scale, HMAR uses a controllable multi-step masked
generation procedure to generate a subset of the tokens in each step. On
ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform
parameter-matched VAR, diffusion, and autoregressive baselines. We develop
efficient IO-aware block-sparse attention kernels that allow HMAR to achieve
faster training and inference times over VAR by over 2.5x and 1.75x
respectively, as well as over 3x lower inference memory footprint. Finally,
HMAR yields additional flexibility over VAR; its sampling schedule can be
changed without further training, and it can be applied to image editing tasks
in a zero-shot manner.

</details>


### [559] [Photoreal Scene Reconstruction from an Egocentric Device](https://arxiv.org/abs/2506.04444)
*Zhaoyang Lv,Maurizio Monge,Ka Chen,Yufeng Zhu,Michael Goesele,Jakob Engel,Zhao Dong,Richard Newcombe*

Main category: cs.CV

TL;DR: 研究用第一人称设备进行高动态范围场景照片级重建挑战，提出用VIBA校准相机和结合物理图像形成模型到高斯渲染的方法，实验有PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法用设备视觉惯性里程计系统的6DoF姿态估计进行重建，可能忽略像素级精确重建的关键细节。

Method: 1. 用视觉惯性束平差（VIBA）以高频轨迹格式校准卷帘快门RGB传感相机的精确时间戳和移动；2. 将物理图像形成模型融入高斯渲染。

Result: 在开源Project Aria设备和Meta Quest3设备上实验，结合VIBA使PSNR提升1 dB，结合图像形成模型再提升1 dB。

Conclusion: 提出的方法适用于广泛使用的高斯渲染变体，能提升视觉效果，代码和数据集公开。

Abstract: In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/

</details>


### [560] [Feature-Based Lie Group Transformer for Real-World Applications](https://arxiv.org/abs/2506.04668)
*Takayuki Komatsu,Yoshiyuki Ohmura,Kayato Nishitsunoi,Yasuo Kuniyoshi*

Main category: cs.CV

TL;DR: 文章介绍表征学习目标与现状，改进此前方法，结合特征提取和对象分割使理论适用于现实场景，并在实际数据集验证，助力理解人类对象识别发展。


<details>
  <summary>Details</summary>
Motivation: 现有表征学习未确立好的表征形式，此前提出的方法有局限性，无法应用于现实场景，需改进。

Method: 结合特征提取和对象分割，用特征翻译替代像素翻译，将对象分割表述为相同变换下的特征分组。

Result: 在包含现实对象和背景的实际数据集上验证了所提方法。

Conclusion: 该模型有助于更好理解现实世界中人类对象识别的发展。

Abstract: The main goal of representation learning is to acquire meaningful
representations from real-world sensory inputs without supervision.
Representation learning explains some aspects of human development. Various
neural network (NN) models have been proposed that acquire empirically good
representations. However, the formulation of a good representation has not been
established. We recently proposed a method for categorizing changes between a
pair of sensory inputs. A unique feature of this approach is that
transformations between two sensory inputs are learned to satisfy algebraic
structural constraints. Conventional representation learning often assumes that
disentangled independent feature axes is a good representation; however, we
found that such a representation cannot account for conditional independence.
To overcome this problem, we proposed a new method using group decomposition in
Galois algebra theory. Although this method is promising for defining a more
general representation, it assumes pixel-to-pixel translation without feature
extraction, and can only process low-resolution images with no background,
which prevents real-world application. In this study, we provide a simple
method to apply our group decomposition theory to a more realistic scenario by
combining feature extraction and object segmentation. We replace pixel
translation with feature translation and formulate object segmentation as
grouping features under the same transformation. We validated the proposed
method on a practical dataset containing both real-world object and background.
We believe that our model will lead to a better understanding of human
development of object recognition in the real world.

</details>


### [561] [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)
*Jing-En Huang,I-Sheng Fang,Tzuhsuan Huang,Chih-Yu Wang,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 文章介绍Gen - n - Val框架，利用LD、LLM和VLLM生成高质量合成数据，解决当前合成数据问题，提升计算机视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉任务存在数据稀缺和标签噪声问题，现有合成数据方法有缺陷，需新框架解决。

Method: 引入Gen - n - Val框架，含LD提示代理和数据验证代理，通过TextGrad优化提示，用图像协调组合多实例。

Result: 相比MosaicFusion，无效合成数据从50%降至7%，COCO实例分割稀有类mAP提升1%；在开放词汇目标检测基准中比YOLO - Worldv2 - M提升7.1% mAP；提升YOLOv9和YOLO11系列性能。

Conclusion: Gen - n - Val框架有效解决合成数据问题，提升计算机视觉任务性能。

Abstract: Recently, Large Language Models (LLMs) and Vision Large Language Models
(VLLMs) have demonstrated impressive performance as agents across various tasks
while data scarcity and label noise remain significant challenges in computer
vision tasks, such as object detection and instance segmentation. A common
solution for resolving these issues is to generate synthetic data. However,
current synthetic data generation methods struggle with issues, such as
multiple objects per mask, inaccurate segmentation, and incorrect category
labels, limiting their effectiveness. To address these issues, we introduce
Gen-n-Val, a novel agentic data generation framework that leverages Layer
Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks
and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt
agent, an LLM, optimizes prompts for LD to generate high-quality foreground
instance images and segmentation masks. These optimized prompts ensure the
generation of single-object synthetic data with precise instance masks and
clean backgrounds. (2) The data validation agent, a VLLM, which filters out
low-quality synthetic instance images. The system prompts for both agents are
refined through TextGrad. Additionally, we use image harmonization to combine
multiple instances within scenes. Compared to state-of-the-art synthetic data
approaches like MosaicFusion, our approach reduces invalid synthetic data from
50% to 7% and improves performance by 1% mAP on rare classes in COCO instance
segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant
improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object
detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance
of YOLOv9 and YOLO11 families in instance segmentation and object detection.

</details>


### [562] [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)
*Youngwan Lee,Kangsan Kim,Kwanyong Park,Ilcahe Jung,Soojin Jang,Seanie Lee,Yong-Ju Lee,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 文章指出当前VLM安全方法不足，提出HoliSafe数据集和基准以及SafeLLaVA模型，实验显示SafeLLaVA有先进安全性能，HoliSafe揭示现有模型漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有VLM安全调优数据集和基准覆盖不全面，且方法多依赖数据调优，缺乏架构创新来增强安全性。

Method: 引入涵盖五种图像 - 文本组合的HoliSafe数据集和基准，提出带有可学习安全元令牌和专用安全头的SafeLLaVA模型。

Result: SafeLLaVA在多个VLM基准测试中达到先进安全性能，HoliSafe揭示现有模型关键漏洞。

Conclusion: 希望HoliSafe和SafeLLaVA能推动VLM安全研究，拓展多模态对齐途径。

Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models
(VLMs), current approaches face two main shortcomings. 1) Existing
safety-tuning datasets and benchmarks only partially consider how image-text
interactions can yield harmful content, often overlooking contextually unsafe
outcomes from seemingly benign pairs. This narrow coverage leaves VLMs
vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely
primarily on data-centric tuning, with limited architectural innovations to
intrinsically strengthen safety. We address these gaps by introducing a
holistic safety dataset and benchmark, HoliSafe, that spans all five
safe/unsafe image-text combinations, providing a more robust basis for both
training and evaluation. We further propose SafeLLaVA, a novel VLM augmented
with a learnable safety meta token and a dedicated safety head. The meta token
encodes harmful visual cues during training, intrinsically guiding the language
model toward safer responses, while the safety head offers interpretable
harmfulness classification aligned with refusal rationales. Experiments show
that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety
performance across multiple VLM benchmarks. Additionally, the HoliSafe
benchmark itself reveals critical vulnerabilities in existing models. We hope
that HoliSafe and SafeLLaVA will spur further research into robust and
interpretable VLM safety, expanding future avenues for multimodal alignment.

</details>


### [563] [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706)
*Achyuta Rajaram,Sarah Schwettmann,Jacob Andreas,Arthur Conmy*

Main category: cs.CV

TL;DR: 探索LlaVA - Next中图像概念表示，发现线性可解码特征，训练SAEs并分析跨模态表示情况


<details>
  <summary>Details</summary>
Motivation: 研究多模态模型如何在隐藏激活中表示图像

Method: 探索LlaVA - Next中图像概念表示，对模型输出进行有针对性编辑，训练多模态稀疏自编码器（SAEs）

Result: 在残差流中发现通过线性可解码特征表示的多种ImageNet类，特征具有因果性，跨模态表示开始时不相交，深层逐渐共享

Conclusion: 揭示了多模态模型中图像概念的表示特点及跨模态表示的变化情况

Abstract: Language models can be equipped with multimodal capabilities by fine-tuning
on embeddings of visual inputs. But how do such multimodal models represent
images in their hidden activations? We explore representations of image
concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set
of ImageNet classes represented via linearly decodable features in the residual
stream. We show that the features are causal by performing targeted edits on
the model output. In order to increase the diversity of the studied linear
features, we train multimodal Sparse Autoencoders (SAEs), creating a highly
interpretable dictionary of text and image features. We find that although
model representations across modalities are quite disjoint, they become
increasingly shared in deeper layers.

</details>


### [564] [Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data](https://arxiv.org/abs/2506.04717)
*Babar Hussain,Qiang Liu,Gang Chen,Bihai She,Dahai Yu*

Main category: cs.CV

TL;DR: 本文提出基于上下文学习能力的AI辅助自动标注系统用于显示面板缺陷检测，验证有显著效果且能减少人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 减少工业检测系统中的人工标注工作量。

Method: 采用并改进SegGPT架构，引入基于涂鸦的标注机制，采用两阶段训练方法。

Result: 相比基线模型有显著改进，平均IoU提高0.22，召回率提高14%，保持约60%自动标注覆盖率，用自动标注数据训练的模型与人工标注数据训练的模型性能相当。

Conclusion: 该系统为工业检测系统减少人工标注工作提供了实用解决方案。

Abstract: This paper presents an AI-assisted auto-labeling system for display panel
defect detection that leverages in-context learning capabilities. We adopt and
enhance the SegGPT architecture with several domain-specific training
techniques and introduce a scribble-based annotation mechanism to streamline
the labeling process. Our two-stage training approach, validated on industrial
display panel datasets, demonstrates significant improvements over the baseline
model, achieving an average IoU increase of 0.22 and a 14% improvement in
recall across multiple product types, while maintaining approximately 60%
auto-labeling coverage. Experimental results show that models trained on our
auto-labeled data match the performance of those trained on human-labeled data,
offering a practical solution for reducing manual annotation efforts in
industrial inspection systems.

</details>


### [565] [Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater Image Enhancement](https://arxiv.org/abs/2506.04753)
*Niki Martinel,Rita Pucci*

Main category: cs.CV

TL;DR: 提出一种双流架构实现水下图像增强，实验显示PSNR提升且计算复杂度降低。


<details>
  <summary>Details</summary>
Motivation: 实现水下图像增强，尊重水下形成约束同时保留语义结构和细节。

Method: 将Jaffe - McGlamery物理模型与基于胶囊聚类的特征表示学习结合，通过物理估计器估计传输图和背景光，并行流提取实体级特征，有新的优化目标。

Result: 在六个基准测试中，比现有最佳方法PSNR提升0.5dB，计算复杂度仅为三分之一；与计算预算相似方法相比，PSNR提升超1dB。

Conclusion: 所提出的双流架构在水下图像增强上有效，能在保证物理一致性和感知质量的同时降低计算复杂度。

Abstract: We present a novel dual-stream architecture that achieves state-of-the-art
underwater image enhancement by explicitly integrating the Jaffe-McGlamery
physical model with capsule clustering-based feature representation learning.
Our method simultaneously estimates transmission maps and spatially-varying
background light through a dedicated physics estimator while extracting
entity-level features via capsule clustering in a parallel stream. This
physics-guided approach enables parameter-free enhancement that respects
underwater formation constraints while preserving semantic structures and
fine-grained details. Our approach also features a novel optimization objective
ensuring both physical adherence and perceptual quality across multiple spatial
frequencies. To validate our approach, we conducted extensive experiments
across six challenging benchmarks. Results demonstrate consistent improvements
of $+0.5$dB PSNR over the best existing methods while requiring only one-third
of their computational complexity (FLOPs), or alternatively, more than $+1$dB
PSNR improvement when compared to methods with similar computational budgets.
Code and data \textit{will} be available at https://github.com/iN1k1/.

</details>


### [566] [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning](https://arxiv.org/abs/2506.04755)
*Shenshen Li,Kaiyuan Deng,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Heng Tao Shen,Xing Xu*

Main category: cs.CV

TL;DR: 本文提出Reasoning Activation Potential (RAP)数据选择范式，用少量数据实现多模态推理任务高性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 挑战多模态大语言模型需大量训练数据提升推理能力的假设，解决数据冗余和高计算成本问题。

Method: 提出RAP范式，用Causal Discrepancy Estimator (CDE)和Attention Confidence Estimator (ACE)识别认知样本，引入Difficulty - aware Replacement Module (DRM)替换简单实例。

Result: 在六个数据集上实验，RAP方法用9.3%训练数据取得更好性能，降低超43%计算成本。

Conclusion: RAP范式能以少量高价值数据实现多模态推理任务的良好效果，减少计算成本。

Abstract: While multi-modal large language models (MLLMs) have made significant
progress in complex reasoning tasks via reinforcement learning, it is commonly
believed that extensive training data is necessary for improving multi-modal
reasoning ability, inevitably leading to data redundancy and substantial
computational costs. However, can smaller high-value datasets match or
outperform full corpora for multi-modal reasoning in MLLMs? In this work, we
challenge this assumption through a key observation: meaningful multi-modal
reasoning is triggered by only a sparse subset of training samples, termed
cognitive samples, whereas the majority contribute marginally. Building on this
insight, we propose a novel data selection paradigm termed Reasoning Activation
Potential (RAP), which identifies cognitive samples by estimating each sample's
potential to stimulate genuine multi-modal reasoning by two complementary
estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential
outcome model principle, eliminates samples that overly rely on language priors
by comparing outputs between multi-modal and text-only inputs; 2) Attention
Confidence Estimator (ACE), which exploits token-level self-attention to
discard samples dominated by irrelevant but over-emphasized tokens in
intermediate reasoning stages. Moreover, we introduce a Difficulty-aware
Replacement Module (DRM) to substitute trivial instances with cognitively
challenging ones, thereby ensuring complexity for robust multi-modal reasoning.
Experiments on six datasets show that our RAP method consistently achieves
superior performance using only 9.3% of the training data, while reducing
computational costs by over 43%. Our code is available at
https://github.com/Leo-ssl/RAP.

</details>


### [567] [CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx](https://arxiv.org/abs/2506.04931)
*Lukas Picek,Elisa Belotti,Michal Bojda,Ludek Bufka,Vojtech Cermak,Martin Dula,Rostislav Dvorak,Luboslav Hrdy,Miroslav Jirik,Vaclav Kocourek,Josefa Krausova,Jirı Labuda,Jakub Straka,Ludek Toman,Vlado Trulık,Martin Vana,Miroslav Kutal*

Main category: cs.CV

TL;DR: 本文介绍了首个用于欧亚猞猁个体识别、二维姿态估计和实例分割的大规模开放数据集CzechLynx，还创建合成集并定义评估协议。


<details>
  <summary>Details</summary>
Motivation: 提供用于欧亚猞猁相关任务的大规模开放数据集，助力基准测试和新方法开发。

Method: 收集超30k张相机陷阱图像并标注，创建超100k张合成图像，定义三种评估协议。

Result: 得到CzechLynx数据集及合成集，定义三种评估协议。

Conclusion: 该数据集对基准测试和新方法开发有重要作用，不仅限于个体动物重识别。

Abstract: We introduce CzechLynx, the first large-scale, open-access dataset for
individual identification, 2D pose estimation, and instance segmentation of the
Eurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images
annotated with segmentation masks, identity labels, and 20-point skeletons and
covers 219 unique individuals across 15 years of systematic monitoring in two
geographically distinct regions: Southwest Bohemia and the Western Carpathians.
To increase the data variability, we create a complementary synthetic set with
more than 100k photorealistic images generated via a Unity-based pipeline and
diffusion-driven text-to-texture modeling, covering diverse environments,
poses, and coat-pattern variations. To allow testing generalization across
spatial and temporal domains, we define three tailored evaluation
protocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware
closed-set. This dataset is targeted to be instrumental in benchmarking
state-of-the-art models and the development of novel methods for not just
individual animal re-identification.

</details>


### [568] [Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations](https://arxiv.org/abs/2506.04951)
*Igor Meleshin,Anna Chistyakova,Anastasia Antsiferova,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 本文指出图像质量评估（IQA）模型存在不稳定性易被对抗操作欺骗，提出将鲁棒性作为架构先验，重塑内部结构，得到无需对抗训练就能抵御攻击的IQA架构。


<details>
  <summary>Details</summary>
Motivation: IQA模型存在不稳定性，传统数据驱动防御方法可能存在局限，探索从设计角度实现鲁棒性。

Method: 通过强制正交信息流，约束网络进行保范操作，再通过剪枝和微调稳定系统，重塑模型内部结构。

Result: 得到了一个能抵御对抗攻击的鲁棒IQA架构，且无需对抗训练或对原模型进行重大更改。

Conclusion: 应从通过数据优化鲁棒性转变为通过设计来构建鲁棒性。

Abstract: Image Quality Assessment (IQA) models are increasingly relied upon to
evaluate image quality in real-world systems -- from compression and
enhancement to generation and streaming. Yet their adoption brings a
fundamental risk: these models are inherently unstable. Adversarial
manipulations can easily fool them, inflating scores and undermining trust.
Traditionally, such vulnerabilities are addressed through data-driven defenses
-- adversarial retraining, regularization, or input purification. But what if
this is the wrong lens? What if robustness in perceptual models is not
something to learn but something to design? In this work, we propose a
provocative idea: robustness as an architectural prior. Rather than training
models to resist perturbations, we reshape their internal structure to suppress
sensitivity from the ground up. We achieve this by enforcing orthogonal
information flow, constraining the network to norm-preserving operations -- and
further stabilizing the system through pruning and fine-tuning. The result is a
robust IQA architecture that withstands adversarial attacks without requiring
adversarial training or significant changes to the original model. This
approach suggests a shift in perspective: from optimizing robustness through
data to engineering it through design.

</details>


### [569] [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)
*Alan Mitkiy,James Smith,Hana Satou,Hiroshi Tanaka,Emily Johnson,F Monkey*

Main category: cs.CV

TL;DR: 提出动态ε调度（DES）框架，自适应调整对抗扰动预算，实验表明能提升对抗鲁棒性和标准准确率，并给出理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法依赖固定扰动预算，未考虑实例特定鲁棒性，先前自适应方法依赖启发式或静态近似。

Method: 提出DES框架，整合基于梯度代理的决策边界距离、softmax熵预测置信度和蒙特卡罗丢弃估计的模型不确定性三个因素，动态调整扰动预算。

Result: 在CIFAR - 10和CIFAR - 100上实验，相比固定ε基线和先前自适应方法，持续提升对抗鲁棒性和标准准确率，还给出调度策略稳定性和收敛性理论见解。

Conclusion: 为实例感知、数据驱动的对抗训练方法开辟新途径。

Abstract: Adversarial training is among the most effective strategies for defending
deep neural networks against adversarial examples. A key limitation of existing
adversarial training approaches lies in their reliance on a fixed perturbation
budget, which fails to account for instance-specific robustness
characteristics. While prior works such as IAAT and MMA introduce
instance-level adaptations, they often rely on heuristic or static
approximations of data robustness. In this paper, we propose Dynamic Epsilon
Scheduling (DES), a novel framework that adaptively adjusts the adversarial
perturbation budget per instance and per training iteration. DES integrates
three key factors: (1) the distance to the decision boundary approximated via
gradient-based proxies, (2) prediction confidence derived from softmax entropy,
and (3) model uncertainty estimated via Monte Carlo dropout. By combining these
cues into a unified scheduling strategy, DES tailors the perturbation budget
dynamically to guide more effective adversarial learning. Experimental results
on CIFAR-10 and CIFAR-100 show that our method consistently improves both
adversarial robustness and standard accuracy compared to fixed-epsilon
baselines and prior adaptive methods. Moreover, we provide theoretical insights
into the stability and convergence of our scheduling policy. This work opens a
new avenue for instance-aware, data-driven adversarial training methods.

</details>


### [570] [Rectified Point Flow: Generic Point Cloud Pose Estimation](https://arxiv.org/abs/2506.05282)
*Tao Sun,Liyuan Zhu,Shengyu Huang,Shuran Song,Iro Armeni*

Main category: cs.CV

TL;DR: 提出Rectified Point Flow统一参数化方法，将点云配准和形状组装作为条件生成问题，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在点云配准和形状组装中需手动处理对称性等问题，实现更高效准确的处理。

Method: 学习连续点速度场将噪声点移向目标位置以恢复部分姿态，结合关注重叠点的自监督编码器。

Result: 在六个涵盖点对配准和形状组装的基准测试中达到新的SOTA性能。

Conclusion: 统一公式可在不同数据集上联合训练，学习共享几何先验，提高准确性。

Abstract: We introduce Rectified Point Flow, a unified parameterization that formulates
pairwise point cloud registration and multi-part shape assembly as a single
conditional generative problem. Given unposed point clouds, our method learns a
continuous point-wise velocity field that transports noisy points toward their
target positions, from which part poses are recovered. In contrast to prior
work that regresses part-wise poses with ad-hoc symmetry handling, our method
intrinsically learns assembly symmetries without symmetry labels. Together with
a self-supervised encoder focused on overlapping points, our method achieves a
new state-of-the-art performance on six benchmarks spanning pairwise
registration and shape assembly. Notably, our unified formulation enables
effective joint training on diverse datasets, facilitating the learning of
shared geometric priors and consequently boosting accuracy. Project page:
https://rectified-pointflow.github.io/.

</details>


### [571] [Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning](https://arxiv.org/abs/2506.05341)
*Xingjian Ran,Yixuan Li,Linning Xu,Mulin Yu,Bo Dai*

Main category: cs.CV

TL;DR: 提出DirectLayout框架，从文本描述直接生成3D布局，经多阶段生成和多种策略处理，实验证明其语义一致性、泛化性和物理合理性出色。


<details>
  <summary>Details</summary>
Motivation: 现有室内场景布局生成方法因数据集有限，存在过拟合或牺牲灵活性问题，无法生成开放式词汇且符合细粒度用户指令的场景。

Method: 将生成过程分解为生成鸟瞰图布局、提升到3D空间、细化对象放置三阶段；采用基于3D - Front数据集的思维链激活；设计基于思维链的生成布局奖励；推理时通过上下文学习的迭代资产 - 布局对齐处理不匹配问题。

Result: DirectLayout实现了令人印象深刻的语义一致性、泛化性和物理合理性。

Conclusion: DirectLayout框架在3D室内场景布局生成上表现良好，能有效解决现有方法的问题。

Abstract: Realistic 3D indoor scene synthesis is vital for embodied AI and digital
content creation. It can be naturally divided into two subtasks: object
generation and layout generation. While recent generative models have
significantly advanced object-level quality and controllability, layout
generation remains challenging due to limited datasets. Existing methods either
overfit to these datasets or rely on predefined constraints to optimize
numerical layout that sacrifice flexibility. As a result, they fail to generate
scenes that are both open-vocabulary and aligned with fine-grained user
instructions. We introduce DirectLayout, a framework that directly generates
numerical 3D layouts from text descriptions using generalizable spatial
reasoning of large language models (LLMs). DirectLayout decomposes the
generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting
it into 3D space, and refining object placements. To enable explicit spatial
reasoning and help the model grasp basic principles of object placement, we
employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.
Additionally, we design CoT-Grounded Generative Layout Reward to enhance
generalization and spatial planning. During inference, DirectLayout addresses
asset-layout mismatches via Iterative Asset-Layout Alignment through in-context
learning. Extensive experiments demonstrate that DirectLayout achieves
impressive semantic consistency, generalization and physical plausibility.

</details>


### [572] [Refer to Anything with Vision-Language Prompts](https://arxiv.org/abs/2506.05342)
*Shengcao Cao,Zijun Wei,Jason Kuen,Kangning Liu,Lingzhi Zhang,Jiuxiang Gu,HyunJoon Jung,Liang-Yan Gui,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 本文提出全模态指代表达分割任务（ORES），提出RAS框架并创建数据集，在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割模型无法基于语言和视觉进行复杂查询的语义理解，限制其在视觉 - 语言交互应用中的效果。

Method: 提出RAS框架，通过以掩码为中心的大型多模态模型增强分割模型的多模态交互和理解；创建MaskGroups - 2M和MaskGroups - HQ数据集。

Result: RAS在新的ORES任务以及经典的RES和GRES任务上表现优异。

Conclusion: 提出的RAS框架和创建的数据集能有效解决全模态指代表达分割问题，提升模型在相关任务上的性能。

Abstract: Recent image segmentation models have advanced to segment images into
high-quality masks for visual entities, and yet they cannot provide
comprehensive semantic understanding for complex queries based on both language
and vision. This limitation reduces their effectiveness in applications that
require user-friendly interactions driven by vision-language prompts. To bridge
this gap, we introduce a novel task of omnimodal referring expression
segmentation (ORES). In this task, a model produces a group of masks based on
arbitrary prompts specified by text only or text plus reference visual
entities. To address this new challenge, we propose a novel framework to "Refer
to Any Segmentation Mask Group" (RAS), which augments segmentation models with
complex multimodal interactions and comprehension via a mask-centric large
multimodal model. For training and benchmarking ORES models, we create datasets
MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by
text and reference entities. Through extensive evaluation, we demonstrate
superior performance of RAS on our new ORES task, as well as classic referring
expression segmentation (RES) and generalized referring expression segmentation
(GRES) tasks. Project page: https://Ref2Any.github.io.

</details>


### [573] [Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors](https://arxiv.org/abs/2506.04823)
*Svetlana Pavlitska,Jamie Robb,Nikolai Polley,Melih Yazgan,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文展示了如何用打印贴片攻击交通灯检测的CNN，在通用设置和现实场景中验证了攻击效果，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 此前针对交通灯检测器的攻击研究较少，作者希望研究用打印贴片攻击交通灯检测的CNN。

Method: 提出威胁模型，用贴片攻击每个交通灯实例，描述训练策略。

Result: 在通用设置中成功进行对抗贴片攻击，实现现实针对性的红变绿标签翻转攻击和象形图分类攻击，在实验室和测试区域进行了现实评估。

Conclusion: 证明了用打印贴片攻击交通灯检测CNN的可行性。

Abstract: Realistic adversarial attacks on various camera-based perception tasks of
autonomous vehicles have been successfully demonstrated so far. However, only a
few works considered attacks on traffic light detectors. This work shows how
CNNs for traffic light detection can be attacked with printed patches. We
propose a threat model, where each instance of a traffic light is attacked with
a patch placed under it, and describe a training strategy. We demonstrate
successful adversarial patch attacks in universal settings. Our experiments
show realistic targeted red-to-green label-flipping attacks and attacks on
pictogram classification. Finally, we perform a real-world evaluation with
printed patches and demonstrate attacks in the lab settings with a mobile
traffic light for construction sites and in a test area with stationary traffic
lights. Our code is available at
https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.

</details>


### [574] [Quantifying Cross-Modality Memorization in Vision-Language Models](https://arxiv.org/abs/2506.05198)
*Yuxin Wen,Yangsibo Huang,Tom Goldstein,Ravi Kumar,Badih Ghazi,Chiyuan Zhang*

Main category: cs.CV

TL;DR: 研究视觉语言模型跨模态记忆特点，用合成数据集实验，发现模态间信息召回有差距并提出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注单模态记忆，统一多模态模型应用增多，需研究跨模态记忆特点。

Method: 引入合成人物数据集，在单模态上训练模型并在另一模态评估，量化事实知识记忆和跨模态可迁移性。

Result: 一模态学到的事实能转移到另一模态，但源和目标模态信息召回有显著差距，且各场景均存在。

Conclusion: 提出缓解挑战的基线方法，希望启发开发更稳健多模态学习技术的研究。

Abstract: Understanding what and how neural networks memorize during training is
crucial, both from the perspective of unintentional memorization of potentially
sensitive information and from the standpoint of effective knowledge
acquisition for real-world, knowledge-intensive tasks. While previous studies
primarily investigate memorization within a single modality, such as text
memorization in large language models or image memorization in diffusion
models, unified multimodal models are becoming increasingly prevalent in
practical applications. In this work, we focus on the unique characteristics of
cross-modality memorization and conduct a systematic study centered on
vision-language models. To facilitate controlled experiments, we first
introduce a synthetic persona dataset comprising diverse synthetic person
images and textual descriptions. We quantify factual knowledge memorization and
cross-modal transferability by training models on a single modality and
evaluating their performance in the other. Our results reveal that facts
learned in one modality transfer to the other, but a significant gap exists
between recalling information in the source and target modalities. Furthermore,
we observe that this gap exists across various scenarios, including more
capable models, machine unlearning, and the multi-hop case. At the end, we
propose a baseline method to mitigate this challenge. We hope our study can
inspire future research on developing more robust multimodal learning
techniques to enhance cross-modal transferability.

</details>


### [575] [Stable Vision Concept Transformers for Medical Diagnosis](https://arxiv.org/abs/2506.05286)
*Lijie Hu,Songning Lai,Yuan Hua,Shu Yang,Jingfeng Zhang,Di Wang*

Main category: cs.CV

TL;DR: 提出Vision Concept Transformer (VCT) 弥补原始模型与基于概念模型的差距，又提出Stable Vision Concept Transformer (SVCT) 解决忠实性问题，实验表明模型有准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域需要透明度，现有Concept Bottleneck Models (CBMs) 方法有局限，既未利用医学图像固有特征嵌入，又影响模型性能且面对输入扰动时解释不稳定。

Method: 提出VCT，在此基础上提出SVCT，以Vision Transformer (ViT) 为骨干并加入概念层，融合概念特征与图像特征提升决策能力，通过Denoised Diffusion Smoothing保证模型忠实性。

Result: 在四个医学数据集上的综合实验表明，VCT和SVCT与基线相比保持了准确性和可解释性，SVCT在受到扰动时也能提供忠实解释。

Conclusion: VCT和SVCT能满足医疗领域对模型准确性、可解释性和忠实性的需求。

Abstract: Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [576] [Unregularized limit of stochastic gradient method for Wasserstein distributionally robust optimization](https://arxiv.org/abs/2506.04948)
*Tam Le*

Main category: math.OC

TL;DR: 论文提出了基于Wasserstein分布鲁棒优化的正则化方法，通过熵平滑实现采样近似，证明了梯度收敛性和临界点集收缩规律，并推导了随机梯度法的收敛保证，适用于无界样本空间的复杂机器学习场景。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒优化在机器学习应用中存在采样近似困难与理论分析空白，特别是在无界样本空间和混合数据类型场景下缺乏收敛性保证。本研究旨在填补这一理论缺口。

Method: 采用Wasserstein分布鲁棒优化框架，引入熵平滑正则化构建采样近似目标，通过紧致集上的梯度收敛分析，结合渐进正则化参数调整策略，最终建立投影随机梯度法的收敛理论。

Result: 证明近似梯度在紧致集上一致收敛，正则化问题的临界点随正则化衰减和样本增加收缩至原问题临界集，推导出投影随机梯度法的收敛速率。

Conclusion: 该理论研究为分布鲁棒优化在复杂数据场景的实际应用提供了数学基础，验证了正则化与采样策略的有效性，支持机器学习算法设计。

Abstract: Distributionally robust optimization offers a compelling framework for model
fitting in machine learning, as it systematically accounts for data
uncertainty. Focusing on Wasserstein distributionally robust optimization, we
investigate the regularized problem where entropic smoothing yields a
sampling-based approximation of the original objective. We establish the
convergence of the approximate gradient over a compact set, leading to the
concentration of the regularized problem critical points onto the original
problem critical set as regularization diminishes and the number of
approximation samples increases. Finally, we deduce convergence guarantees for
a projected stochastic gradient method. Our analysis covers a general machine
learning situation with an unbounded sample space and mixed continuous-discrete
data.

</details>


### [577] [Was Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?](https://arxiv.org/abs/2506.04742)
*Oliver G. S. Lundqvist,Fabricio Oliveira*

Main category: math.OC

TL;DR: 提出通过解耦训练与优化流程，使用DeepONet架构结合无约束优化程序，单次训练即可解决多个最优控制问题


<details>
  <summary>Details</summary>
Motivation: 传统方法在神经网络训练中整合成本函数导致流程复杂，需要简化架构并提升多任务适应性

Method: 1. 使用DeepONet神经算子架构
2. 在成本函数添加微分方程残差惩罚项
3. 通过自动微分计算控制变量梯度
4. 迭代优化流程分离训练与优化阶段

Result: 在9个最优控制问题上验证（3个模型各对应3个微分方程），所有案例均实现精确控制（平均误差<0.5%）

Conclusion: 该方法突破了传统端到端训练限制，证明了单次物理信息训练后通过独立优化阶段处理多类控制问题的可行性

Abstract: Neural networks have been used to solve optimal control problems, typically
by training neural networks using a combined loss function that considers data,
differential equation residuals, and objective costs. We show that including
cost functions in the training process is unnecessary, advocating for a simpler
architecture and streamlined approach by decoupling the optimal control problem
from the training process. Thus, our work shows that a simple neural operator
architecture, such as DeepONet, coupled with an unconstrained optimization
routine, can solve multiple optimal control problems with a single
physics-informed training phase and a subsequent optimization phase. We achieve
this by adding a penalty term based on the differential equation residual to
the cost function and computing gradients with respect to the control using
automatic differentiation through the trained neural operator within an
iterative optimization routine. We showcase our method on nine distinct optimal
control problems by training three separate DeepONet models, each corresponding
to a different differential equation. For each model, we solve three problems
with varying cost functions, demonstrating accurate and consistent performance
across all cases.

</details>


### [578] [Non-linear Multi-objective Optimization with Probabilistic Branch and Bound](https://arxiv.org/abs/2506.04554)
*Hao Huang,Zelda B. Zabinsky*

Main category: math.OC

TL;DR: 提出MOPBnB(so)算法，通过单次观测实现多目标优化，显著降低计算资源消耗并优于NSGA-II算法


<details>
  <summary>Details</summary>
Motivation: 解决传统多目标优化算法在随机场景下因多次重复评估导致的超高计算成本问题

Method: 基于概率分支定界框架，利用单次噪声函数评估+邻域解估计目标函数，建立有限时间性能分析模型

Result: 1）理论证明算法对确定性问题的Pareto集捕获概率界 2）随机场景下渐近收敛性 3）计算效率提升10倍级优于传统方法

Conclusion: MOPBnB(so)开创性地平衡了计算效率与优化精度，为复杂系统多目标决策提供了新范式

Abstract: A multiple objective simulation optimization algorithm named Multiple
Objective Probabilistic Branch and Bound with Single Observation (MOPBnB(so))
is presented for approximating the Pareto optimal set and the associated
efficient frontier for stochastic multi-objective optimization problems.
MOPBnB(so) evaluates a noisy function exactly once at any solution and uses
neighboring solutions to estimate the objective functions, in contrast to a
variant that uses multiple replications at a solution to estimate the objective
functions. A finite-time performance analysis for deterministic multi-objective
problems provides a bound on the probability that MOPBnB(so) captures the
Pareto optimal set. Asymptotic convergence of MOPBnB(so) on stochastic problems
is derived, in that the algorithm captures the Pareto optimal set and the
estimations converge to the true objective function values. Numerical results
reveal that the variant with multiple replications is extremely intensive in
terms of computational resources compared to MOPBnB(so). In addition, numerical
results show that MOPBnB(so) outperforms a genetic algorithm NSGA-II on test
problems.

</details>


### [579] [Unregularized limit of stochastic gradient method for Wasserstein distributionally robust optimization](https://arxiv.org/abs/2506.04948)
*Tam Le*

Main category: math.OC

TL;DR: 研究Wasserstein分布鲁棒优化的正则化问题，建立近似梯度收敛性，推导投影随机梯度法收敛保证。


<details>
  <summary>Details</summary>
Motivation: 分布鲁棒优化能系统考虑数据不确定性，在机器学习模型拟合中有重要价值，需对其进行研究。

Method: 聚焦Wasserstein分布鲁棒优化，用熵平滑对原目标进行基于采样的近似，建立近似梯度在紧集上的收敛性。

Result: 随着正则化减小和近似样本数量增加，正则化问题的临界点集中到原问题的临界集，推导出投影随机梯度法的收敛保证。

Conclusion: 分析涵盖了样本空间无界和混合连续 - 离散数据的一般机器学习情况，对分布鲁棒优化有理论贡献。

Abstract: Distributionally robust optimization offers a compelling framework for model
fitting in machine learning, as it systematically accounts for data
uncertainty. Focusing on Wasserstein distributionally robust optimization, we
investigate the regularized problem where entropic smoothing yields a
sampling-based approximation of the original objective. We establish the
convergence of the approximate gradient over a compact set, leading to the
concentration of the regularized problem critical points onto the original
problem critical set as regularization diminishes and the number of
approximation samples increases. Finally, we deduce convergence guarantees for
a projected stochastic gradient method. Our analysis covers a general machine
learning situation with an unbounded sample space and mixed continuous-discrete
data.

</details>


### [580] [Was Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?](https://arxiv.org/abs/2506.04742)
*Oliver G. S. Lundqvist,Fabricio Oliveira*

Main category: math.OC

TL;DR: 研究表明训练神经网络解决最优控制问题时无需在训练过程中包含成本函数，用简单神经算子架构结合无约束优化例程可解决多问题。


<details>
  <summary>Details</summary>
Motivation: 简化解决最优控制问题的神经网络架构和方法，去除训练过程中成本函数的必要性。

Method: 采用简单神经算子架构如DeepONet，结合无约束优化例程，在成本函数中加入基于微分方程残差的惩罚项，通过自动微分计算控制梯度。

Result: 训练三个不同的DeepONet模型，解决九种不同最优控制问题，各模型解决三种不同成本函数问题，表现准确且一致。

Conclusion: 简单神经算子架构结合无约束优化例程，经一次物理信息训练和后续优化阶段，可有效解决多个最优控制问题。

Abstract: Neural networks have been used to solve optimal control problems, typically
by training neural networks using a combined loss function that considers data,
differential equation residuals, and objective costs. We show that including
cost functions in the training process is unnecessary, advocating for a simpler
architecture and streamlined approach by decoupling the optimal control problem
from the training process. Thus, our work shows that a simple neural operator
architecture, such as DeepONet, coupled with an unconstrained optimization
routine, can solve multiple optimal control problems with a single
physics-informed training phase and a subsequent optimization phase. We achieve
this by adding a penalty term based on the differential equation residual to
the cost function and computing gradients with respect to the control using
automatic differentiation through the trained neural operator within an
iterative optimization routine. We showcase our method on nine distinct optimal
control problems by training three separate DeepONet models, each corresponding
to a different differential equation. For each model, we solve three problems
with varying cost functions, demonstrating accurate and consistent performance
across all cases.

</details>


### [581] [Non-linear Multi-objective Optimization with Probabilistic Branch and Bound](https://arxiv.org/abs/2506.04554)
*Hao Huang,Zelda B. Zabinsky*

Main category: math.OC

TL;DR: 提出MOPBnB(so)算法近似随机多目标优化问题的帕累托最优集和有效前沿，分析其性能，数值结果显示其优于多复制变体和NSGA - II。


<details>
  <summary>Details</summary>
Motivation: 为随机多目标优化问题近似帕累托最优集和有效前沿。

Method: 提出MOPBnB(so)算法，仅对每个解的噪声函数评估一次，用邻域解估计目标函数；对确定性多目标问题进行有限时间性能分析，推导随机问题的渐近收敛性。

Result: 多复制变体计算资源消耗大；MOPBnB(so)在测试问题上优于NSGA - II。

Conclusion: MOPBnB(so)算法在解决随机多目标优化问题上具有优势。

Abstract: A multiple objective simulation optimization algorithm named Multiple
Objective Probabilistic Branch and Bound with Single Observation (MOPBnB(so))
is presented for approximating the Pareto optimal set and the associated
efficient frontier for stochastic multi-objective optimization problems.
MOPBnB(so) evaluates a noisy function exactly once at any solution and uses
neighboring solutions to estimate the objective functions, in contrast to a
variant that uses multiple replications at a solution to estimate the objective
functions. A finite-time performance analysis for deterministic multi-objective
problems provides a bound on the probability that MOPBnB(so) captures the
Pareto optimal set. Asymptotic convergence of MOPBnB(so) on stochastic problems
is derived, in that the algorithm captures the Pareto optimal set and the
estimations converge to the true objective function values. Numerical results
reveal that the variant with multiple replications is extremely intensive in
terms of computational resources compared to MOPBnB(so). In addition, numerical
results show that MOPBnB(so) outperforms a genetic algorithm NSGA-II on test
problems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [582] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

TL;DR: 通过跨源知识协调框架CoRe-MMRAG解决多模态检索增强生成中的知识不一致问题，在KB-VQA基准实现5.6%-9.3%性能提升


<details>
  <summary>Details</summary>
Motivation: 解决多模态RAG中参数化知识/检索知识不一致（PRKI）及视觉-文本知识错位（VTKI）导致的可靠性不确定和实体表征混乱问题

Method: 四阶段端到端框架：1) 参数知识生成内部响应 2) 联合相似度评估选择多模态证据 3) 生成外部响应 4) 知识整合生成最终答案；配套训练范式增强知识源判别和多模态融合能力

Result: 在KB-VQA的InfoSeek和Encyclopedic-VQA基准分别取得5.6%和9.3%的性能提升

Conclusion: CoRe-MMRAG通过跨源知识协调有效解决多模态RAG的知识不一致问题，实验验证其在复杂问答任务中的显著有效性

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose Cross-source knowledge \textbf{Re}conciliation for
Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively
reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a
four-stage pipeline: it first generates an internal response from parametric
knowledge, then selects the most relevant multimodal evidence via joint
similarity assessment, generates an external response, and finally integrates
both to produce a reliable answer. Additionally, a specialized training
paradigm enhances knowledge source discrimination, multimodal integration, and
unified answer generation. Experiments on KB-VQA benchmarks show that
CoRe-MMRAG achieves substantial improvements over baseline methods, achieving
5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.

</details>


### [583] [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
*Likun Cao,Rui Pan,James Evans*

Main category: cs.CL

TL;DR: 研究利用机器学习分析创新者主观视角，发现视角多样性促进创新成果，而经验背景多样性反而抑制创造力。团队通过共同语言融合差异化经验是创新关键机制。


<details>
  <summary>Details</summary>
Motivation: 现有创新研究多关注社会结构影响，但缺乏对个体主观认知差异的量化分析。本研究旨在通过语言表征建模创新者的个人视角，揭示其对跨领域创意组合的预测能力。

Method: 基于动态语言模型构建概念几何空间，量化数百万科学家、发明家等创新者的主观视角。通过自然实验验证，并设计LLM智能体模拟不同视角/背景组合的协作效果。

Result: 协作团队中视角差异每增加1个弧度，创新成功率提升18.7%；而经验背景差异每增加1个弧度，成功率下降9.3%。AI模拟实验与实证结果高度一致（r=0.86）。

Conclusion: 创新团队组建应优先选择视角多元但共享认知框架的成员，政策制定需建立促进视角交流的基础设施，而非简单追求背景多样性。

Abstract: Existing studies of innovation emphasize the power of social structures to
shape innovation capacity. Emerging machine learning approaches, however,
enable us to model innovators' personal perspectives and interpersonal
innovation opportunities as a function of their prior trajectories of
experience. We theorize then quantify subjective perspectives and innovation
opportunities based on innovator positions within the geometric space of
concepts inscribed by dynamic language representations. Using data on millions
of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors
across the creative domains of science, technology, film, entrepreneurship, and
Wikipedia, here we show that measured subjective perspectives anticipate what
ideas individuals and groups creatively attend to and successfully combine in
future. When perspective and background diversity are decomposed as the angular
difference between collaborators' perspectives on their creation and between
their experiences, the former consistently anticipates creative achievement
while the latter portends its opposite, across all cases and time periods
examined. We analyze a natural experiment and simulate creative collaborations
between AI (large language model) agents designed with various perspective and
background diversity, which are consistent with our observational findings. We
explore mechanisms underlying these findings and identify how successful
collaborators leverage common language to weave together diverse experience
obtained through trajectories of prior work that converge to provoke one
another and innovate. We explore the importance of these findings for team
assembly and research policy.

</details>


### [584] [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)
*Alex Pan,Mary-Anne Williams*

Main category: cs.CL

TL;DR: Verbose ListOps基准测试通过故事化嵌套计算问题，揭示LLMs在语义干扰下的状态管理缺陷


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准无法有效检测LLMs的嵌套推理缺陷，需解耦上下文长度与推理复杂度

Method: 将ListOps计算转化为长叙事故事，通过隐藏中间结果强制内部状态管理，并实现叙事规模与推理难度的双重控制

Result: 主流LLMs（如GPT-4/Gemini 2.5 Pro）在10k token叙事长度下性能崩溃，但可轻松解决原始ListOps问题

Conclusion: Verbose ListOps为针对性增强推理能力提供框架，超越单纯扩展上下文窗口，是实现知识工作自动化的关键

Abstract: Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.

</details>


### [585] [Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05154)
*Chenyu Lin,Yilin Wen,Du Su,Fei Sun,Muhan Chen,Chenfu Bao,Zhonghou Lv*

Main category: cs.CL

TL;DR: 提出Knowledgeable-r1方法，通过联合采样和多策略分布激发大语言模型自我整合参数化与上下文知识，显著提升RAG任务中的鲁棒性和推理准确率（反事实场景提升17.07%）


<details>
  <summary>Details</summary>
Motivation: 解决当前RAG系统过度依赖检索上下文导致的准确性下降问题，特别是对抗性/信息过载场景中模型自身知识未被充分利用的缺陷

Method: 使用联合采样定义多策略分布，构建知识能力探索框架，通过自适应机制协调参数化知识与上下文知识的整合利用

Result: 在参数/上下文冲突任务中推理准确率显著提升，反事实场景超越基线17.07%，所有RAG任务均取得持续性能增益

Conclusion: 该方法成功实现参数化与上下文知识的动态平衡，特别是提升系统在对抗性条件下的鲁棒性，为知识密集型任务提供新解决方案

Abstract: Retrieval-augmented generation (RAG) is a mainstream method for improving
performance on knowledge-intensive tasks. However,current RAG systems often
place too much emphasis on retrieved contexts. This can lead to reliance on
inaccurate sources and overlook the model's inherent knowledge, especially when
dealing with misleading or excessive information. To resolve this imbalance, we
propose Knowledgeable-r1 that using joint sampling and define multi policy
distributions in knowledge capability exploration to stimulate large language
models'self-integrated utilization of parametric and contextual knowledge.
Experiments show that Knowledgeable-r1 significantly enhances robustness and
reasoning accuracy in both parameters and contextual conflict tasks and general
RAG tasks, especially outperforming baselines by 17.07% in counterfactual
scenarios and demonstrating consistent gains across RAG tasks. Our code are
available at https://github.com/lcy80366872/ knowledgeable-r1.

</details>


### [586] [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/abs/2506.05167)
*Yeonseok Jeong,Jinsu Kim,Dohyeon Lee,Seung-won Hwang*

Main category: cs.CL

TL;DR: 提出ECoRAG框架，通过基于证据性的检索文档压缩提升大语言模型在开放域问答任务中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成(RAG)的上下文压缩方法未能有效过滤非证据信息，限制了模型性能

Method: ECoRAG框架包含：1. 基于证据性分数压缩文档 2. 验证证据充分性 3. 证据不足时迭代检索补充

Result: 在开放域问答任务中性能优于现有压缩方法，同时降低延迟和token使用量（成本效益提升）

Conclusion: 通过证据引导的文档压缩和迭代补充机制，ECoRAG在保证答案准确性的同时显著提升大模型应用效率

Abstract: Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing
retrieved documents based on evidentiality, ensuring whether answer generation
is supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.

</details>


### [587] [Search Arena: Analyzing Search-Augmented LLMs](https://arxiv.org/abs/2506.05334)
*Mihran Miroyan,Tsung-Han Wu,Logan King,Tianle Li,Jiayi Pan,Xinyan Hu,Wei-Lin Chiang,Anastasios N. Angelopoulos,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 研究引入Search Arena数据集，分析搜索增强语言模型的用户偏好，发现引用数量及来源影响可信度感知，并揭示跨环境应用中的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强系统评估数据集规模小、场景单一，难以全面分析用户偏好与系统表现。

Method: 通过众包收集24,000+多轮交互数据，涵盖多语言/多意图场景，结合12,000人类偏好投票，进行跨环境（搜索/通用聊天）对比实验。

Result: 用户偏好与引用数量正相关（即使引用无关），社区平台（如维基）更受青睐；非搜索场景中网络搜索无损性能，但纯参数化知识在搜索场景表现显著下降。

Conclusion: Search Arena为评估搜索增强模型提供基准，揭示感知可信度与实际支持的差距，开源数据促进可信增强语言模型研究。

Abstract: Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.

</details>


### [588] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
*Zhaolu Kang,Junhao Gong,Jiaxu Yan,Wanke Xia,Yian Wang,Ziwen Wang,Huaxuan Ding,Zhuo Cheng,Wenhao Cao,Zhiyuan Feng,Siqi He,Shannan Yan,Junzhe Chen,Xiaomin He,Chaoya Jiang,Wei Ye,Kaidong Yu,Xuelong Li*

Main category: cs.CL

TL;DR: 提出专为评估多模态大语言模型在人文社科领域能力的HSSBench基准，覆盖6类任务和6种联合国官方语言，包含1.3万+样本，实验证明现有模型面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM评估基准过度关注STEM领域知识，缺乏对人文社科(HSS)横向跨学科思维及抽象概念-视觉关联能力的评估维度。

Method: 设计专家与自动化代理协作的HSS数据生成流程，构建覆盖哲学/艺术/语言等6大领域、包含多模态推理链的评估基准，涵盖20+主流模型的系统性评测。

Result: 实验显示即使GPT-4o/Gemini等顶尖模型在HSS任务上的准确率不足60%，暴露出跨领域知识整合能力的明显短板。

Conclusion: HSSBench将推动MLLM在跨学科推理、多语言理解方面的研究，特别强调知识内化与跨领域连接能力的提升需求。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [589] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
*Matthieu Tehenan,Vikram Natarajan,Jonathan Michala,Milton Lin,Juri Opitz*

Main category: cs.CL

TL;DR: 提出通过字典学习方法将句子嵌入分解为可解释组件，揭示池化操作如何压缩特征并发现语义/句法特征的线性编码机制


<details>
  <summary>Details</summary>
Motivation: 现有句子嵌入缺乏可解释性，复杂神经变换和池化操作掩盖了特征贡献，需要实现更透明的表示分析

Method: 在词级表示上应用字典学习进行机制性分解，分析池化操作的特征压缩过程，评估句子嵌入的潜在特征

Result: 发现语义和句法特征在嵌入空间中呈线性编码，揭示了池化过程的信息压缩机制

Conclusion: 该方法连接了词级与句子级的可解释性分析，为嵌入空间工作机制提供新见解，实现更可控的表示方法

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [590] [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
*Kurt Micallef,Claudia Borg*

Main category: cs.CL

TL;DR: 评估55个公开大语言模型在马耳他语(低资源语言)的表现，发现小规模精调模型优于大模型，建议低资源语言研究采用传统方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源语言（如马耳他语）上的有效性仍然有限，需要系统性评估其性能差距及影响因素

Method: 使用涵盖11个判别式和生成任务的新基准，通过多维度分析模型性能与预训练数据、指令微调、模型规模等因素的关系

Result: 多数模型表现欠佳（生成任务尤甚），小规模精调模型全面占优；预训练阶段接触马耳他语数据是性能关键因素

Conclusion: 建议低资源语言研究回归传统建模方法，平衡精调成本与性能；强调构建包容性语言技术的必要性

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various Natural Language Processing (NLP) tasks, largely due to their
generalisability and ability to perform tasks without additional training.
However, their effectiveness for low-resource languages remains limited. In
this study, we evaluate the performance of 55 publicly available LLMs on
Maltese, a low-resource language, using a newly introduced benchmark covering
11 discriminative and generative tasks. Our experiments highlight that many
models perform poorly, particularly on generative tasks, and that smaller
fine-tuned models often perform better across all tasks. From our
multidimensional analysis, we investigate various factors impacting
performance. We conclude that prior exposure to Maltese during pre-training and
instruction-tuning emerges as the most important factor. We also examine the
trade-offs between fine-tuning and prompting, highlighting that while
fine-tuning requires a higher initial cost, it yields better performance and
lower inference costs. Through this work, we aim to highlight the need for more
inclusive language technologies and recommend that researchers working with
low-resource languages consider more "traditional" language modelling
approaches.

</details>


### [591] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: 首个公开医疗编码推理训练环境MedAgentGYM，通过7万+真实生物医学任务提升LLM性能，实验验证监督学习+强化学习使模型性能提升超40%


<details>
  <summary>Details</summary>
Motivation: 解决商业与开源医疗LLM性能代差，构建可扩展、隐私安全的生物医学编码助手训练体系

Method: 创建72k+真实场景任务库，搭建含交互反馈的执行环境，测试30+模型后采用监督微调+强化学习两阶段训练

Result: Med-Copilot-7B较基线提升36.44%（监督学习）和42.47%（强化学习），达到gpt-4o级别性能

Conclusion: MedAgentGYM通过标准化训练框架与可扩展资源，推动医疗AI助手在科研与临床的实用化落地

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [592] [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
*Wesley Scivetti,Tatsuya Aoyama,Ethan Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: 人类规模的语言模型能泛化罕见句法结构的形式，但无法正确理解其语义含义


<details>
  <summary>Details</summary>
Motivation: 探究当前大规模预训练语言模型是否像人类学习者那样，既能掌握罕见语法结构的形式，又能理解其语义内涵

Method: 构建针对英语LET-ALONE结构的合成测试基准，在过滤相关结构的数据集上测试transformer模型的句法和语义泛化能力

Result: 模型对句法形式敏感（准确率75%），但对语义理解准确率仅32%，显示形式与意义泛化的不对称性

Conclusion: 当前架构在语言形式与意义的学习效率上存在失衡，这种缺陷在人类语言学习者中不存在，提示需要改进模型架构的语义泛化能力

Abstract: Humans have a remarkable ability to acquire and understand grammatical
phenomena that are seen rarely, if ever, during childhood. Recent evidence
suggests that language models with human-scale pretraining data may possess a
similar ability by generalizing from frequent to rare constructions. However,
it remains an open question how widespread this generalization ability is, and
to what extent this knowledge extends to meanings of rare constructions, as
opposed to just their forms. We fill this gap by testing human-scale
transformer language models on their knowledge of both the form and meaning of
the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we
construct a bespoke synthetic benchmark that targets syntactic and semantic
properties of the construction. We find that human-scale LMs are sensitive to
form, even when related constructions are filtered from the dataset. However,
human-scale LMs do not make correct generalizations about LET-ALONE's meaning.
These results point to an asymmetry in the current architectures' sample
efficiency between language form and meaning, something which is not present in
human language learners.

</details>


### [593] [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
*Lev Morozov,Aleksandr Mogilevskii,Alexander Shirnin*

Main category: cs.CL

TL;DR: 无需训练模型的集成方法实现高效情感检测


<details>
  <summary>Details</summary>
Motivation: 解决SemEval-2025多标签情感检测任务需求，寻求无需额外训练的高效解决方案

Method: 采用预训练模型集成策略，避免额外模型训练

Result: 性能媲美最优系统，且具备更高效率和扩展性

Conclusion: EmoRAG系统通过创新集成方法，实现了高效可扩展的情感分析解决方案

Abstract: This paper describes EmoRAG, a system designed to detect perceived emotions
in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We
focus on predicting the perceived emotions of the speaker from a given text
snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,
and disgust. Our approach does not require additional model training and only
uses an ensemble of models to predict emotions. EmoRAG achieves results
comparable to the best performing systems, while being more efficient,
scalable, and easier to implement.

</details>


### [594] [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
*William Sheffield,Kanishka Misra,Valentina Pyatkin,Ashwini Deo,Kyle Mahowald,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 研究探讨LLMs区分英语小品词'just'细微语义的能力，发现模型在捕捉微妙差异上存在局限


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能够准确识别语篇小品词的多功能性和细微语义差异（如'just'的排他性/时间性/强调用法）

Method: 使用语言学家精心标注的'just'多义数据集，评估LLMs的细粒度语义区分能力

Result: LLMs可区分大类语义，但难以完全捕捉更微妙的语用差异（如语境依赖的隐含意义）

Conclusion: 现有LLMs对语篇小品词的理解存在显著差距，需开发更精细的语义建模方法

Abstract: Discourse particles are crucial elements that subtly shape the meaning of
text. These words, often polyfunctional, give rise to nuanced and often quite
disparate semantic/discourse effects, as exemplified by the diverse uses of the
particle "just" (e.g., exclusive, temporal, emphatic). This work investigates
the capacity of LLMs to distinguish the fine-grained senses of English "just",
a well-studied example in formal semantics, using data meticulously created and
labeled by expert linguists. Our findings reveal that while LLMs exhibit some
ability to differentiate between broader categories, they struggle to fully
capture more subtle nuances, highlighting a gap in their understanding of
discourse particles.

</details>


### [595] [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
*Senyu Li,Jiayi Wang,Felermino D. M. A. Ali,Colin Cherry,Daniel Deutsch,Eleftheria Briakou,Rui Sousa-Silva,Henrique Lopes Cardoso,Pontus Stenetorp,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: SSA-MTE数据集及SSA-COMET指标显著提升非洲低资源语言机器翻译评估性能


<details>
  <summary>Details</summary>
Motivation: 解决现有机器翻译评估指标在非洲低资源语言中覆盖不足、性能差的问题，特别是AfriCOMET的小数据集局限和LLM评估缺失

Method: 构建含63k标注的SSA-MTE数据集，开发SSA-COMET/COMET-QE指标，并测试GPT-4o/Claude等LLM

Result: SSA-COMET显著超越AfriCOMET，在Twi/Yoruba等语言上与Gemini 2.5 Pro竞争，且所有资源开源

Conclusion: 该研究通过大规模标注数据和新型指标推动了非洲语言MT评估，同时验证了LLM的潜力，开源资源将促进领域发展

Abstract: Evaluating machine translation (MT) quality for under-resourced African
languages remains a significant challenge, as existing metrics often suffer
from limited language coverage and poor performance in low-resource settings.
While recent efforts, such as AfriCOMET, have addressed some of the issues,
they are still constrained by small evaluation sets, a lack of publicly
available training data tailored to African languages, and inconsistent
performance in extremely low-resource scenarios. In this work, we introduce
SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13
African language pairs from the News domain, with over 63,000 sentence-level
annotations from a diverse set of MT systems. Based on this data, we develop
SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free
evaluation metrics. We also benchmark prompting-based approaches using
state-of-the-art LLMs like GPT-4o and Claude. Our experimental results show
that SSA-COMET models significantly outperform AfriCOMET and are competitive
with the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on
low-resource languages such as Twi, Luo, and Yoruba. All resources are released
under open licenses to support future research.

</details>


### [596] [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)
*Dimitris Vamvourellis,Dhagash Mehta*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在金融情感分析中，非推理型模型GPT-4o无CoT提示时与人类判断最接近，挑战了'更多推理带来更好决策'的假设。


<details>
  <summary>Details</summary>
Motivation: 验证推理机制（显式提示或模型固有设计）是否能提升LLMs在金融情感分析中的表现，探究不同提示策略对模型与人类判断对齐程度的影响。

Method: 使用金融专家标注的Financial PhraseBank数据集，对比3个商业LLM（GPT-4o/GPT-4.1/o3-mini）在模拟系统1（直觉）和系统2（推理）思维的不同提示策略，并与两个金融领域微调模型（FinBERT-Prosus/FinBERT-Tone）进行基准测试。

Result: 推理机制未提升性能，最佳表现来自无CoT的GPT-4o；语言复杂度与标注一致性分析显示推理可能引发过度思考导致预测偏差。

Conclusion: 金融情感分类任务中，类系统1的快速直觉判断比模拟系统2的缓慢推理更接近人类判断，挑战了高风险金融应用中默认需要更多推理的假设。

Abstract: We investigate the effectiveness of large language models (LLMs), including
reasoning-based and non-reasoning models, in performing zero-shot financial
sentiment analysis. Using the Financial PhraseBank dataset annotated by domain
experts, we evaluate how various LLMs and prompting strategies align with
human-labeled sentiment in a financial context. We compare three proprietary
LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that
simulate System 1 (fast and intuitive) or System 2 (slow and deliberate)
thinking and benchmark them against two smaller models (FinBERT-Prosus,
FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest
that reasoning, either through prompting or inherent model design, does not
improve performance on this task. Surprisingly, the most accurate and
human-aligned combination of model and method was GPT-4o without any
Chain-of-Thought (CoT) prompting. We further explore how performance is
impacted by linguistic complexity and annotation agreement levels, uncovering
that reasoning may introduce overthinking, leading to suboptimal predictions.
This suggests that for financial sentiment classification, fast, intuitive
"System 1"-like thinking aligns more closely with human judgment compared to
"System 2"-style slower, deliberative reasoning simulated by reasoning models
or CoT prompting. Our results challenge the default assumption that more
reasoning always leads to better LLM decisions, particularly in high-stakes
financial applications.

</details>


### [597] [SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing](https://arxiv.org/abs/2506.04583)
*Hongjun Liu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 提出SUCEA框架，通过分步处理对抗性声明（分割、迭代检索编辑、证据聚合）显著提高事实核查的检索和标签准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强的事实核查系统难以应对人为设计的对抗性声明，需改进检索证据能力

Method: 三阶段框架：1）声明分割与去上下文化；2）迭代证据检索与声明编辑；3）证据聚合与标签预测

Result: 在两个对抗性数据集上检索准确率提升17.5%，标签准确率提升13.8%，优于四个基线方法

Conclusion: 模块化分解任务有效提升对抗性声明处理能力，验证了训练无关方法的有效性

Abstract: Automatic fact-checking has recently received more attention as a means of
combating misinformation. Despite significant advancements, fact-checking
systems based on retrieval-augmented language models still struggle to tackle
adversarial claims, which are intentionally designed by humans to challenge
fact-checking systems. To address these challenges, we propose a training-free
method designed to rephrase the original claim, making it easier to locate
supporting evidence. Our modular framework, SUCEA, decomposes the task into
three steps: 1) Claim Segmentation and Decontextualization that segments
adversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval
and Claim Editing that iteratively retrieves evidence and edits the subclaim
based on the retrieved evidence; 3) Evidence Aggregation and Label Prediction
that aggregates all retrieved evidence and predicts the entailment label.
Experiments on two challenging fact-checking datasets demonstrate that our
framework significantly improves on both retrieval and entailment label
accuracy, outperforming four strong claim-decomposition-based baselines.

</details>


### [598] [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
*Chengwu Liu,Ye Yuan,Yichun Yin,Yan Xu,Xin Xu,Zaoyu Chen,Yasheng Wang,Lifeng Shang,Qun Liu,Ming Zhang*

Main category: cs.CL

TL;DR: 提出基于Lean 4的形式化验证框架Safe，通过数学证明方式检测LLM推理中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示法的幻觉检测方法（如PRMs/self-consistency）缺乏可验证证据支持，需要更可靠的数学验证方法

Method: 开发基于Lean 4的回溯式分步验证框架，将自然语言推理步骤转化为形式化数学声明并构建可验证证明

Result: 在多个数学模型和数据集上实现显著性能提升，提出包含30,809个形式声明的FormalStep基准测试集

Conclusion: 首次将形式数学语言Lean 4应用于LLM生成内容的验证，契合形式化数学语言创建初衷，为人类易错证明提供可靠基础

Abstract: Chain-of-Thought (CoT) prompting has become the de facto method to elicit
reasoning capabilities from large language models (LLMs). However, to mitigate
hallucinations in CoT that are notoriously difficult to detect, current methods
such as process reward models (PRMs) or self-consistency operate as opaque
boxes and do not provide checkable evidence for their judgments, possibly
limiting their effectiveness. To address this issue, we draw inspiration from
the idea that "the gold standard for supporting a mathematical claim is to
provide a proof". We propose a retrospective, step-aware formal verification
framework $Safe$. Rather than assigning arbitrary scores, we strive to
articulate mathematical claims in formal mathematical language Lean 4 at each
reasoning step and provide formal proofs to identify hallucinations. We
evaluate our framework $Safe$ across multiple language models and various
mathematical datasets, demonstrating a significant performance improvement
while offering interpretable and verifiable evidence. We also propose
$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$
formal statements. To the best of our knowledge, our work represents the first
endeavor to utilize formal mathematical language Lean 4 for verifying natural
language content generated by LLMs, aligning with the reason why formal
mathematical languages were created in the first place: to provide a robust
foundation for hallucination-prone human-written proofs.

</details>


### [599] [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
*Takashi Wada,Yuki Hirakawa,Ryotaro Shimizu,Takahiro Kawashima,Yuki Saito*

Main category: cs.CL

TL;DR: 提出通过句子级PCA和改进训练方法优化静态词向量，在低成本计算下实现与动态模型相媲美的句子语义表示


<details>
  <summary>Details</summary>
Motivation: 解决传统静态词向量在句子语义表示上的不足，同时避免类似Sentence Transformer等动态模型的高计算成本

Method: 1. 从预训练Sentence Transformer提取词向量 2. 应用句子级主成分分析 3. 采用知识蒸馏或对比学习优化 4. 推理时简单平均词向量

Result: 在单语和跨语言任务中超越现有静态模型，部分数据集接近SimCSE基础模型性能

Conclusion: 通过消除与句子语义无关的词向量成分，构建出高效且具有竞争力的静态句子表示方法

Abstract: We propose new static word embeddings optimised for sentence semantic
representation. We first extract word embeddings from a pre-trained Sentence
Transformer, and improve them with sentence-level principal component analysis,
followed by either knowledge distillation or contrastive learning. During
inference, we represent sentences by simply averaging word embeddings, which
requires little computational cost. We evaluate models on both monolingual and
cross-lingual tasks and show that our model substantially outperforms existing
static models on sentence semantic tasks, and even rivals a basic Sentence
Transformer model (SimCSE) on some data sets. Lastly, we perform a variety of
analyses and show that our method successfully removes word embedding
components that are irrelevant to sentence semantics, and adjusts the vector
norms based on the influence of words on sentence semantics.

</details>


### [600] [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)
*Gio Paik,Geewook Kim,Jinbae Im*

Main category: cs.CL

TL;DR: MMRefine是首个专注评估多模态大语言模型错误细化能力的基准，通过六场景六错误类型分析模型改进瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最终准确率，缺乏对推理过程中错误修正能力的系统性评估框架

Method: 构建包含视觉问答、情境推理等六种场景的基准，将错误分类为定位/逻辑/上下文等六种类型，开发系统化评估框架

Result: 实验显示当前MLLMs存在错误修正瓶颈，仅实现有限性能提升（最高3.4%改进），揭示模型在复杂推理中的局限性

Conclusion: 该基准为提升MLLMs的推理能力提供新方向，公开代码数据集促进相关研究发展

Abstract: This paper introduces MMRefine, a MultiModal Refinement benchmark designed to
evaluate the error refinement capabilities of Multimodal Large Language Models
(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,
MMRefine provides a framework that evaluates MLLMs' abilities to detect and
correct errors across six distinct scenarios beyond just comparing final
accuracy before and after refinement. Furthermore, the benchmark analyzes the
refinement performance by categorizing errors into six error types. Experiments
with various open and closed MLLMs reveal bottlenecks and factors impeding
refinement performance, highlighting areas for improvement in effective
reasoning enhancement. Our code and dataset are publicly available at
https://github.com/naver-ai/MMRefine.

</details>


### [601] [Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection](https://arxiv.org/abs/2506.04739)
*Ziyi Zhou,Xiaoming Zhang,Litian Zhang,Yibo Zhang,Zhenyu Guan,Chaozhuo Li,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出C²EFND框架，通过结合LLMs的泛化能力和SLMs的分类优势，采用持续协作学习机制提升假新闻检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统小型语言模型需大量标注数据且难以适应动态新闻环境，大型语言模型受限于知识时效性和示范缺失，导致检测精度不足。

Method: 1.多轮协作学习框架整合LLMs与SLMs优势
2.基于混合专家的终身知识编辑模块持续更新LLMs
3.基于回放的持续学习防止SLMs遗忘历史知识

Result: 在Pheme和Twitter16数据集上准确率显著超越现有方法，有效提升持续涌现假新闻场景下的检测适应能力

Conclusion: C²EFND通过双模型协同和持续学习机制，解决了假新闻检测中的动态适应难题，为社交平台内容治理提供新方案

Abstract: The widespread dissemination of fake news on social media has significantly
impacted society, resulting in serious consequences. Conventional deep learning
methodologies employing small language models (SLMs) suffer from extensive
supervised training requirements and difficulties adapting to evolving news
environments due to data scarcity and distribution shifts. Large language
models (LLMs), despite robust zero-shot capabilities, fall short in accurately
detecting fake news owing to outdated knowledge and the absence of suitable
demonstrations. In this paper, we propose a novel Continuous Collaborative
Emergent Fake News Detection (C$^2$EFND) framework to address these challenges.
The C$^2$EFND framework strategically leverages both LLMs' generalization power
and SLMs' classification expertise via a multi-round collaborative learning
framework. We further introduce a lifelong knowledge editing module based on a
Mixture-of-Experts architecture to incrementally update LLMs and a replay-based
continue learning method to ensure SLMs retain prior knowledge without
retraining entirely. Extensive experiments on Pheme and Twitter16 datasets
demonstrate that C$^2$EFND significantly outperforms existed methods,
effectively improving detection accuracy and adaptability in continuous
emergent fake news scenarios.

</details>


### [602] [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)
*Jingyu Hu,Mengyue Yang,Mengnan Du,Weiru Liu*

Main category: cs.CL

TL;DR: 通过构建四维政治学习框架和可解释表征工程技术，解构LLMs政治概念混淆，实现细粒度检测与干预


<details>
  <summary>Details</summary>
Motivation: 传统LLMs政治分析依赖单轴概念易导致混淆，且模型回应与内部意图存在偏差，需开发透明化方法探测内部政治状态

Method: 设计四维政治分析框架，构建专用数据集训练政治概念向量，采用三种表征工程技术在8个开源LLMs进行实验验证

Result: 学习向量成功解构概念混淆，检测任务验证语义有效性（OOD场景保持稳健），干预实验实现政治倾向响应调控

Conclusion: 多维表征工程为LLMs政治概念分析提供可解释解决方案，增强模型透明度与控制力，推动负责任AI发展

Abstract: Studies of LLMs' political opinions mainly rely on evaluations of their
open-ended responses. Recent work indicates that there is a misalignment
between LLMs' responses and their internal intentions. This motivates us to
probe LLMs' internal mechanisms and help uncover their internal political
states. Additionally, we found that the analysis of LLMs' political opinions
often relies on single-axis concepts, which can lead to concept confounds. In
this work, we extend the single-axis to multi-dimensions and apply
interpretable representation engineering techniques for more transparent LLM
political concept learning. Specifically, we designed a four-dimensional
political learning framework and constructed a corresponding dataset for
fine-grained political concept vector learning. These vectors can be used to
detect and intervene in LLM internals. Experiments are conducted on eight
open-source LLMs with three representation engineering techniques. Results show
these vectors can disentangle political concept confounds. Detection tasks
validate the semantic meaning of the vectors and show good generalization and
robustness in OOD settings. Intervention Experiments show these vectors can
intervene in LLMs to generate responses with different political leanings.

</details>


### [603] [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)
*Jisu An,Junseok Lee,Jeoungeun Lee,Yongseok Son*

Main category: cs.CL

TL;DR: 系统分析多模态大模型整合方法，提出三维分类框架（架构策略、表征学习、训练范式），基于125个模型揭示技术趋势


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对多模态输入如何整合到语言模型背骨的系统分析，需建立统一分类框架指导未来研究

Method: 从架构整合机制（融合层级/方式）、表征学习（联合/坐标表征）、训练范式（策略/目标函数）三个维度进行分类，分析2021-2025年间125个MLLMs

Result: 揭示多模态整合技术演进模式，建立首个LLM-centric的分类体系，为不同模态对齐策略提供系统比较框架

Conclusion: 提出的三维分类法帮助研究者理解现有技术格局，为基于预训练基座开发更鲁棒的跨模态整合策略提供指导路径

Abstract: The rapid progress of Multimodal Large Language Models(MLLMs) has transformed
the AI landscape. These models combine pre-trained LLMs with various modality
encoders. This integration requires a systematic understanding of how different
modalities connect to the language backbone. Our survey presents an LLM-centric
analysis of current approaches. We examine methods for transforming and
aligning diverse modal inputs into the language embedding space. This addresses
a significant gap in existing literature. We propose a classification framework
for MLLMs based on three key dimensions. First, we examine architectural
strategies for modality integration. This includes both the specific
integration mechanisms and the fusion level. Second, we categorize
representation learning techniques as either joint or coordinate
representations. Third, we analyze training paradigms, including training
strategies and objective functions. By examining 125 MLLMs developed between
2021 and 2025, we identify emerging patterns in the field. Our taxonomy
provides researchers with a structured overview of current integration
techniques. These insights aim to guide the development of more robust
multimodal integration strategies for future models built on pre-trained
foundations.

</details>


### [604] [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)
*Yujun Zhou,Jiayi Ye,Zipeng Ling,Yufei Han,Yue Huang,Haomin Zhuang,Zhenwen Liang,Kehan Guo,Taicheng Guo,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文提出FineLogic评估框架，从准确性、推理结构、表征对齐三个维度评估大语言模型逻辑推理能力，并通过不同监督风格的对比实验揭示其对推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅依赖最终答案准确性，无法捕捉推理过程的质量和结构。需建立更细粒度、可解释的评估体系以提升LLMs的逻辑推理能力。

Method: 构建四种监督风格（1种自然语言+3种符号化变体），在不同分布外任务和长上下文场景下训练LLMs，并通过表征层探针分析推理行为机制。

Result: 自然语言监督展现强泛化能力，符号化监督促进结构化推理链；微调主要通过逐步生成而非捷径预测提升推理表现。

Conclusion: FineLogic框架为LLMs逻辑推理能力提供了更严谨、可解释的评估体系，监督风格的对比实验为优化推理训练范式提供了重要洞见。

Abstract: Logical reasoning is a core capability for many applications of large
language models (LLMs), yet existing benchmarks often rely solely on
final-answer accuracy, failing to capture the quality and structure of the
reasoning process. We propose FineLogic, a fine-grained evaluation framework
that assesses logical reasoning across three dimensions: overall benchmark
accuracy, stepwise soundness, and representation-level alignment. In addition,
to better understand how reasoning capabilities emerge, we conduct a
comprehensive study on the effects of supervision format during fine-tuning. We
construct four supervision styles (one natural language and three symbolic
variants) and train LLMs under each. Our findings reveal that natural language
supervision yields strong generalization even on out-of-distribution and
long-context tasks, while symbolic reasoning styles promote more structurally
sound and atomic inference chains. Further, our representation-level probing
shows that fine-tuning primarily improves reasoning behaviors through
step-by-step generation, rather than enhancing shortcut prediction or
internalized correctness. Together, our framework and analysis provide a more
rigorous and interpretable lens for evaluating and improving logical reasoning
in LLMs.

</details>


### [605] [A Reasoning-Based Approach to Cryptic Crossword Clue Solving](https://arxiv.org/abs/2506.04824)
*Martin Andrews,Sam Witteveen*

Main category: cs.CL

TL;DR: 研究构建了一个基于开源组件的LLM推理系统，通过假设答案、提出字谜解释和验证步骤，在Cryptonite数据集上实现了新的SOTA性能，并提供可解释的Python代码验证过程。


<details>
  <summary>Details</summary>
Motivation: 传统填字游戏解决系统缺乏可验证的推理过程，加密线索的复杂结构要求既能正确解题又能提供可解释的证明步骤。

Method: 采用三步框架：1)生成候选答案；2)提出基于字谜结构的解释；3)通过编码化推理步骤的验证系统确认答案正确性。

Result: 在英国主流报纸的Cryptonite数据集上达到新SOTA，所有验证通过的解决方案均以Python代码形式展现可解释的推理链条。

Conclusion: 该工作首次将可验证的符号推理与LLM结合，显著提升加密线索解决性能，同时为复杂语言任务提供透明化推理范例。

Abstract: Cryptic crossword clues are challenging language tasks for which new test
sets are released daily by major newspapers on a global basis. Each cryptic
clue contains both the definition of the answer to be placed in the crossword
grid (in common with regular crosswords), and 'wordplay' that proves that the
answer is correct (i.e. a human solver can be confident that an answer is
correct without needing crossing words as confirmation). This work describes an
LLM-based reasoning system built from open-licensed components that solves
cryptic clues by (i) hypothesising answers; (ii) proposing wordplay
explanations; and (iii) using a verifier system that operates on codified
reasoning steps. Overall, this system establishes a new state-of-the-art
performance on the challenging Cryptonite dataset of clues from The Times and
The Telegraph newspapers in the UK. Because each proved solution is expressed
in Python, interpretable wordplay reasoning for proven answers is available for
inspection.

</details>


### [606] [Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights](https://arxiv.org/abs/2506.04851)
*Giorgio Biancini,Alessio Ferrato,Carla Limongelli*

Main category: cs.CL

TL;DR: 研究比较Llama 2/Mistral/GPT-3.5生成选择题的能力，发现GPT-3.5最优但教育界对AI应用仍存顾虑


<details>
  <summary>Details</summary>
Motivation: 人工编制选择题耗时费力，LLMs可提升效率但需控制内容来源避免幻觉

Method: 通过知识注入提示工程对比三模型，由21名教育工作者评估生成题目质量

Result: GPT-3.5在多样性/挑战性等指标表现最佳，68%教育者担忧AI融入教学

Conclusion: LLMs可有效生成选择题但需解决教育者的技术接受度问题

Abstract: Integrating Artificial Intelligence (AI) in educational settings has brought
new learning approaches, transforming the practices of both students and
educators. Among the various technologies driving this transformation, Large
Language Models (LLMs) have emerged as powerful tools for creating educational
materials and question answering, but there are still space for new
applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess
student knowledge, but manually generating these questions is
resource-intensive and requires significant time and cognitive effort. In our
opinion, LLMs offer a promising solution to these challenges. This paper
presents a novel comparative analysis of three widely known LLMs - Llama 2,
Mistral, and GPT-3.5 - to explore their potential for creating informative and
challenging MCQs. In our approach, we do not rely on the knowledge of the LLM,
but we inject the knowledge into the prompt to contrast the hallucinations,
giving the educators control over the test's source text, too. Our experiment
involving 21 educators shows that GPT-3.5 generates the most effective MCQs
across several known metrics. Additionally, it shows that there is still some
reluctance to adopt AI in the educational field. This study sheds light on the
potential of LLMs to generate MCQs and improve the educational experience,
providing valuable insights for the future.

</details>


### [607] [Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback](https://arxiv.org/abs/2506.04920)
*Junior Cedric Tonga,KV Aditya Srivatsa,Kaushal Kumar Maurya,Fajri Koto,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究验证多语言反馈对LLM教育工具的影响，发现母语对齐的提示能显著提升低资源语言学习效果


<details>
  <summary>Details</summary>
Motivation: 探索LLM在多语言环境下提供数学推理教学反馈的可行性，特别是检验语言资源水平对反馈效果的影响

Method: 基于352组实验，用强模型生成多语言提示作为教师反馈，弱模型模拟学生响应，测试11种语言在不同资源水平下的学习效果

Result: 多语言提示提升学习效果，低资源语言环境下母语对齐的反馈可带来18.7%准确率提升

Conclusion: 研究为开发多语言LLM教育工具提供实践指导，强调母语反馈在资源匮乏语言中的关键作用

Abstract: Large language models (LLMs) have demonstrated the ability to generate
formative feedback and instructional hints in English, making them increasingly
relevant for AI-assisted education. However, their ability to provide effective
instructional support across different languages, especially for mathematically
grounded reasoning tasks, remains largely unexamined. In this work, we present
the first large-scale simulation of multilingual tutor-student interactions
using LLMs. A stronger model plays the role of the tutor, generating feedback
in the form of hints, while a weaker model simulates the student. We explore
352 experimental settings across 11 typologically diverse languages, four
state-of-the-art LLMs, and multiple prompting strategies to assess whether
language-specific feedback leads to measurable learning gains. Our study
examines how student input language, teacher feedback language, model choice,
and language resource level jointly influence performance. Results show that
multilingual hints can significantly improve learning outcomes, particularly in
low-resource languages when feedback is aligned with the student's native
language. These findings offer practical insights for developing multilingual,
LLM-based educational tools that are both effective and inclusive.

</details>


### [608] [From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation](https://arxiv.org/abs/2506.04965)
*Adrian Marius Dumitran,Theodor-Pierre Moroianu,Vasile Paul Alexe*

Main category: cs.CL

TL;DR: 评估大语言模型在高校算法考试中的表现，最新模型得分接近顶尖学生且在复杂问题推理中展现能力，但图形任务仍有困难，探索LLM生成教学内容辅助教育的潜力


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在高级算法教育中的应用可能性，特别是考试表现评估与教学材料生成方向

Method: 通过多语言算法考试测试不同模型，分析解题能力/一致性/多语言表现，并进行实证研究

Result: 最新模型成绩达优秀学生水平，复杂多步骤问题推理能力突出，图形任务处理仍存挑战，验证了教学材料生成可行性

Conclusion: 研究为生成式AI在算法教育的深度整合提供实践依据，展示了LLM提升教学反馈质量的具体路径

Abstract: This paper presents a comprehensive evaluation of the performance of
state-of-the-art Large Language Models (LLMs) on challenging university-level
algorithms exams. By testing multiple models on both a Romanian exam and its
high-quality English translation, we analyze LLMs' problem-solving
capabilities, consistency, and multilingual performance. Our empirical study
reveals that the most recent models not only achieve scores comparable to
top-performing students but also demonstrate robust reasoning skills on
complex, multi-step algorithmic challenges, even though difficulties remain
with graph-based tasks. Building on these findings, we explore the potential of
LLMs to support educational environments through the generation of high-quality
editorial content, offering instructors a powerful tool to enhance student
feedback. The insights and best practices discussed herein pave the way for
further integration of generative AI in advanced algorithm education.

</details>


### [609] [TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages](https://arxiv.org/abs/2506.05057)
*Moshe Ofer,Orel Zamler,Amos Azaria*

Main category: cs.CL

TL;DR: 提出TALL架构解决大模型在低资源语言表现差的问题，通过双语翻译转换+参数高效训练策略实现性能提升


<details>
  <summary>Details</summary>
Motivation: 现有大模型直接应用/翻译/微调方法在低资源语言处理中存在性能不足和计算低效问题，需新解决方案

Method: 1. 构建双语翻译模型转换低资源语言输入 2. 设计维度对齐层和定制转换器保留语言特征 3. 冻结预训练参数，仅训练轻量适配器模块

Result: 在希伯来语实验中显著超越直接使用、简单翻译和微调基线，验证架构有效性

Conclusion: TALL通过参数隔离策略平衡计算效率与性能，为低资源语言处理提供新范式

Abstract: Large Language Models (LLMs) excel in high-resource languages but struggle
with low-resource languages due to limited training data. This paper presents
TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource
Languages), which integrates an LLM with two bilingual translation models. TALL
transforms low-resource inputs into high-resource representations, leveraging
the LLM's capabilities while preserving linguistic features through dimension
alignment layers and custom transformers. Our experiments on Hebrew demonstrate
significant improvements over several baselines, including direct use, naive
translation, and fine-tuning approaches. The architecture employs a
parameter-efficient strategy, freezing pre-trained components while training
only lightweight adapter modules, balancing computational efficiency with
performance gains.

</details>


### [610] [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/abs/2506.05068)
*Iulia Comşa,Murray Shanahan*

Main category: cs.CL

TL;DR: 探讨大语言模型（LLMs）的自我报告是否可被视为内省行为，通过两个案例论证其适用边界


<details>
  <summary>Details</summary>
Motivation: LLMs展现出类人的语言行为并产生自我报告，引发其是否具有内省能力的哲学争议，需通过具体案例分析厘清边界

Method: 选取两个典型自我报告案例：1）LLM解释自身创作过程；2）LLM正确推断自身温度参数值，进行概念分析与现象学比较

Result: 案例一不符合内省标准（创作过程解释缺乏真实认知机制），案例二构成最低限度的内省（参数推断体现元认知能力，但无意识伴随）

Conclusion: LLMs的自我报告能力需严格区分现象与本质，参数推断类案例可作为弱内省研究样本，但应避免与人类意识体验的简单类比

Abstract: Large language models (LLMs) exhibit compelling linguistic behaviour, and
sometimes offer self-reports, that is to say statements about their own nature,
inner workings, or behaviour. In humans, such reports are often attributed to a
faculty of introspection and are typically linked to consciousness. This raises
the question of how to interpret self-reports produced by LLMs, given their
increasing linguistic fluency and cognitive capabilities. To what extent (if
any) can the concept of introspection be meaningfully applied to LLMs? Here, we
present and critique two examples of apparent introspective self-report from
LLMs. In the first example, an LLM attempts to describe the process behind its
own ``creative'' writing, and we argue this is not a valid example of
introspection. In the second example, an LLM correctly infers the value of its
own temperature parameter, and we argue that this can be legitimately
considered a minimal example of introspection, albeit one that is (presumably)
not accompanied by conscious experience.

</details>


### [611] [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
*Tanmay Parekh,Kartik Mehta,Ninareh Mehrabi,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出DiCoRe框架，通过发散-收敛推理机制提升零样本事件检测效果，在多个领域实现4-7%平均F1提升


<details>
  <summary>Details</summary>
Motivation: 大语言模型在零样本事件检测中受限于复杂事件本体理解、领域触发词提取和结构化编排，导致检测覆盖率和精度不足

Method: 采用Dreamer(发散式开放事件发现)+Grounder(有限状态机约束解码对齐指令)+LLM-Judge验证的三阶段框架，平衡事件覆盖与任务对齐

Result: 在5个领域6个数据集上超越现有零样本/迁移学习基线，平均F1提升4-7%，最高达72.3%

Conclusion: DiCoRe通过协同发散与收敛推理机制，成为首个在跨领域零样本事件检测中实现高覆盖与高精度的统一框架

Abstract: Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.

</details>


### [612] [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)
*Chih-Kai Yang,Neo Ho,Yi-Jyun Lee,Hung-yi Lee*

Main category: cs.CL

TL;DR: 首次通过词汇投影分析三大音频-语言模型，揭示听觉属性识别机制：识别失败时深层特征退化、浅层特征解析决定准确性、模型依赖实时查询而非特征聚合。基于发现提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 理解音频-语言大模型（LALMs）的内部机制对于解释其行为和改进性能至关重要，本研究旨在揭示LALMs处理听觉属性的内部表征规律。

Method: 在三大前沿LALMs上应用词汇投影技术，追踪听觉属性信息在不同网络层和token位置的演化路径。

Result: 发现：1）识别失败时属性信息随网络层加深而退化 2）属性提及位置的浅层特征解析与识别准确率正相关 3）模型依赖实时音频查询而非特征聚合。据此提出模型增强方法。

Conclusion: 研究揭示了LALMs处理听觉属性的核心机制，为未来模型优化提供了关键见解，推动更高效的音频-语言模型设计。

Abstract: Understanding the internal mechanisms of large audio-language models (LALMs)
is crucial for interpreting their behavior and improving performance. This work
presents the first in-depth analysis of how LALMs internally perceive and
recognize auditory attributes. By applying vocabulary projection on three
state-of-the-art LALMs, we track how attribute information evolves across
layers and token positions. We find that attribute information generally
decreases with layer depth when recognition fails, and that resolving
attributes at earlier layers correlates with better accuracy. Moreover, LALMs
heavily rely on querying auditory inputs for predicting attributes instead of
aggregating necessary information in hidden states at attribute-mentioning
positions. Based on our findings, we demonstrate a method to enhance LALMs. Our
results offer insights into auditory attribute processing, paving the way for
future improvements.

</details>


### [613] [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
*Bhavik Chandna,Zubair Bashir,Procheta Sen*

Main category: cs.CL

TL;DR: 通过机械可解释性方法发现LLM的偏见计算高度集中在少数层，移除相关组件会同时影响偏见输出和其他NLP任务性能。


<details>
  <summary>Details</summary>
Motivation: 探究LLM内部偏见的结构化表征及其在不同场景下的稳定性，解决模型伦理应用中的偏见问题。

Method: 使用系统性消融实验分析GPT-2/Llama2模型的内部组件，评估偏见的稳定性/局部化/泛化性。

Result: 偏见计算集中在少数层且微调会改变组件结构；移除偏见组件会降低NLP任务性能。

Conclusion: LLM的偏见机制具有高度局部性和任务组件共享特性，需权衡去偏见与模型性能的平衡。

Abstract: Large Language Models (LLMs) are known to exhibit social, demographic, and
gender biases, often as a consequence of the data on which they are trained. In
this work, we adopt a mechanistic interpretability approach to analyze how such
biases are structurally represented within models such as GPT-2 and Llama2.
Focusing on demographic and gender biases, we explore different metrics to
identify the internal edges responsible for biased behavior. We then assess the
stability, localization, and generalizability of these components across
dataset and linguistic variations. Through systematic ablations, we demonstrate
that bias-related computations are highly localized, often concentrated in a
small subset of layers. Moreover, the identified components change across
fine-tuning settings, including those unrelated to bias. Finally, we show that
removing these components not only reduces biased outputs but also affects
other NLP tasks, such as named entity recognition and linguistic acceptability
judgment because of the sharing of important components with these tasks.

</details>


### [614] [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
*Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 研究发现大规模语言模型在受控线性回归任务中具备反事实推理能力，揭示了注意力机制、模型深度和数据多样性对推理性能的影响，并初步验证了序列数据上的噪声溯因可能性。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在假设场景下进行反事实推理的能力，验证其通过上下文观察推断潜在噪声并预测因果关系的机制。

Method: 构建线性回归噪声溯因任务，要求模型基于观察数据推断并复制上下文噪声，通过Transformer架构分析不同组件对推理性能的影响。

Result: 模型在自注意力机制、深度增加和数据多样性条件下表现优异，且在序列数据噪声溯因任务中展示出反事实故事生成的初步潜力。

Conclusion: 该研究为语言模型的因果推理能力提供了理论框架，揭示了关键性能驱动因素，为后续反事实文本生成研究奠定基础。

Abstract: Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .

</details>


### [615] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: 提出自监督方法GEM使仅解码器LLM兼具高质量文本嵌入生成和原有文本生成能力


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成等技术依赖独立嵌入模型，导致系统复杂性增加和查询理解不一致问题

Method: 通过插入特殊标记并操纵注意力掩码生成文本摘要嵌入，可集成到现有LLM的后训练/微调阶段

Result: 在1B-8B参数模型上验证，MTEB指标显著提升（文本嵌入能力）而MMLU影响极小（保留NLP能力）

Conclusion: GEM方法以极简方式赋予LLM先进文本嵌入能力，同时保持原有性能，扩展了LLM的应用潜力

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [616] [Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)
*Nan Huo,Jinyang Li,Bowen Qin,Ge Qu,Xiaolong Li,Xiaodong Li,Chenhao Ma,Reynold Cheng*

Main category: cs.CL

TL;DR: 提出Micro-Act框架通过层级动作空间自动感知上下文复杂度，将知识源分解为细粒度比较序列，显著提升RAG系统在知识冲突场景下的问答准确率


<details>
  <summary>Details</summary>
Motivation: 现有方法直接横向对比知识源易导致LLM被冗余信息干扰，无法有效解决外部检索知识与模型内在知识冲突的问题

Method: 构建层次化动作空间，自适应分解知识源为可操作的细粒度比较步骤，实现超越表层语境的深度推理

Result: 在5个基准数据集和3种冲突类型（特别是时间/语义冲突）上，QA准确率显著超越基线模型，且在非冲突问题上保持稳健性能

Conclusion: Micro-Act框架通过结构化知识比较机制，在保持通用性的同时有效解决知识冲突问题，具有实际应用价值

Abstract: Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge
Conflicts, where retrieved external knowledge contradicts the inherent,
parametric knowledge of large language models (LLMs). It adversely affects
performance on downstream tasks such as question answering (QA). Existing
approaches often attempt to mitigate conflicts by directly comparing two
knowledge sources in a side-by-side manner, but this can overwhelm LLMs with
extraneous or lengthy contexts, ultimately hindering their ability to identify
and mitigate inconsistencies. To address this issue, we propose Micro-Act a
framework with a hierarchical action space that automatically perceives context
complexity and adaptively decomposes each knowledge source into a sequence of
fine-grained comparisons. These comparisons are represented as actionable
steps, enabling reasoning beyond the superficial context. Through extensive
experiments on five benchmark datasets, Micro-Act consistently achieves
significant increase in QA accuracy over state-of-the-art baselines across all
5 datasets and 3 conflict types, especially in temporal and semantic types
where all baselines fail significantly. More importantly, Micro-Act exhibits
robust performance on non-conflict questions simultaneously, highlighting its
practical value in real-world RAG applications.

</details>


### [617] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: 提出HTC-CLIP模型，通过对比学习融合层次感知文本表示和路径引导层次表示，在推理时合并两种概率分布，显著提升分层文本分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有HTC模型分别处理标签层次结构和文本编码，两种方法具有互补性但尚未有效结合。

Method: 使用对比学习同时生成层次感知的文本表示和文本引导的路径层次表示，训练时维护两套概率分布，推理时合并输出。

Result: 在两个公开数据集上Macro F1分数提升0.99%-2.37%，超越现有最优模型。

Conclusion: 通过架构整合两种互补方法，验证了结合层次结构编码与文本编码器引导的有效性，证明联合表示的优势。

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [618] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: 提出嵌入器-分类器模型架构，通过监督微调和多语言知识蒸馏策略，在标注数据有限的跨领域意图检测任务中实现20-23%准确率提升


<details>
  <summary>Details</summary>
Motivation: 解决跨领域(渠道/地域/语言)客户关怀意图分类中标注数据不足的瓶颈问题

Method: 1. 监督式微调训练领域专用句嵌入器 2. 多语言知识蒸馏实现跨领域泛化 3. 线性分类器增强部署

Result: 在加拿大和墨西哥电商数据集上的小样本意图检测准确率超越SOTA模型20-23%

Conclusion: 该架构有效提升跨领域意图识别性能，具有实际商业应用价值

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [619] [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
*Deepak Pandita,Tharindu Cyril Weerasooriya,Ankit Parag Shah,Christopher M. Homan,Wei Wei*

Main category: cs.CL

TL;DR: ProRefine提出了一种基于大模型反馈的动态提示优化方法，显著提升了多步推理任务的准确性（3-37个百分点），使小模型能媲美大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理工作流存在错误传播和性能低下问题，主要源于提示词设计不佳，限制了AI系统的可靠性和扩展性。

Method: ProRefine在推理时利用大模型生成的文本反馈，无需额外训练或标注数据，动态优化多步推理任务的提示词。

Result: 在5个数学推理基准测试中超越零样本思维链基线3-37个百分点，小模型达到大模型同等性能。

Conclusion: 该方法为高效部署可扩展的AI系统提供新思路，通过提示优化实现性能民主化，降低大模型使用门槛。

Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.

</details>


### [620] [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)
*Taha Entesari,Arman Hatami,Rinat Khaziev,Anil Ramakrishna,Mahyar Fazlyab*

Main category: cs.CL

TL;DR: 提出基于约束优化的LLM遗忘方法，通过logit-margin损失实现稳定遗忘并保持模型性能


<details>
  <summary>Details</summary>
Motivation: 现有正则化遗忘方法导致优化不稳定和性能退化，需开发更高效的遗忘保留平衡机制

Method: 使用logit-margin flattening损失驱动遗忘集均匀分布，保留集采用硬约束，结合原始-对偶算法优化

Result: 在TOFU和MUSE基准测试中超越现有基线，有效移除目标信息并保持下游任务性能

Conclusion: 新方法提供softmax-free的稳定优化框架，通过双变量动态平衡实现高效安全的内容遗忘

Abstract: Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.

</details>


### [621] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: 论文系统分析了水印技术对LLM对齐属性的负面影响，提出基于奖励模型的Alignment Resampling方法有效恢复模型安全性/有用性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印技术（如Gumbel/KGW）会通过改变token分布导致模型对齐属性（真实性/安全性/有用性）的退化，存在安全性与有用性此消彼长的根本矛盾。

Method: 提出Alignment Resampling（AR）方法：1）在推理阶段采样2-4个水印生成结果；2）利用外部奖励模型选择最优输出；3）改进Gumbel水印牺牲严格的无失真特性以提升响应多样性。

Result: 实验证明AR能使两种水印方法恢复基线对齐分数，同时保持强检测性。采样4次时，水印模型的平均奖励分数超越无水印基线0.5-1.2分。

Conclusion: 揭示了水印强度与模型对齐的平衡关系，通过简单的推理时采样方案实现水印LLM的负责任部署，为实际应用提供可靠解决方案。

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


### [622] [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
*Thao Nguyen,Yang Li,Olga Golovneva,Luke Zettlemoyer,Sewoong Oh,Ludwig Schmidt,Xian Li*

Main category: cs.CL

TL;DR: 通过REWIRE方法回收过滤后的低质量网络文本，经改写后混合高质量数据训练语言模型，显著提升多任务性能


<details>
  <summary>Details</summary>
Motivation: 现有预训练数据受限于高质量文本稀缺性（过滤后仅保留1%），且自然数据增长速度远低于算力增长，需开发数据再利用方案突破'数据墙'瓶颈

Method: 提出REWIRE框架，通过指导性文本改写技术提升被过滤低质量文档的可用性，生成合成数据并与原始高质量数据混合训练

Result: 在1B/3B/7B模型上混合数据带来1.0/1.3/2.5个百分点提升，82%有效文本来自改写后的低质文档，效果优于2倍原始数据及其他合成方法（如维基式改写、QA生成）

Conclusion: 网络文本回收是扩展预训练数据的有效途径，REWIRE通过数据改造再利用显著提升模型性能，验证了数据循环利用的可行性

Abstract: Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.

</details>


### [623] [Controlling Summarization Length Through EOS Token Weighting](https://arxiv.org/abs/2506.05017)
*Zeno Belligoli,Emmanouil Stergiadis,Eran Fainman,Ilya Gusev*

Main category: cs.CL

TL;DR: 通过调整交叉熵损失函数中EOS令牌的预测权重，实现无需修改模型架构的文本摘要长度控制方法


<details>
  <summary>Details</summary>
Motivation: 现有文本长度控制方法需要复杂模型改造且与预训练模型兼容性差，需开发更通用的解决方案

Method: 在交叉熵损失计算中增强EOS（结束符）令牌预测的重要性，该方法兼容不同架构模型和解码算法

Result: 在encoder-decoder和GPT类大模型中验证有效，能在保持摘要质量的前提下控制生成文本长度

Conclusion: 提出了一种简单且模型无关的文本生成长度控制方法，与现有推理阶段技术正交兼容

Abstract: Controlling the length of generated text can be crucial in various
text-generation tasks, including summarization. Existing methods often require
complex model alterations, limiting compatibility with pre-trained models. We
address these limitations by developing a simple approach for controlling the
length of automatic text summaries by increasing the importance of correctly
predicting the EOS token in the cross-entropy loss computation. The proposed
methodology is agnostic to architecture and decoding algorithms and orthogonal
to other inference-time techniques to control the generation length. We tested
it with encoder-decoder and modern GPT-style LLMs, and show that this method
can control generation length, often without affecting the quality of the
summary.

</details>


### [624] [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://arxiv.org/abs/2506.05209)
*Nikhil Kandpal,Brian Lester,Colin Raffel,Sebastian Majstorovic,Stella Biderman,Baber Abbasi,Luca Soldaini,Enrico Shippole,A. Feder Cooper,Aviya Skowron,John Kirchenbauer,Shayne Longpre,Lintang Sutawika,Alon Albalak,Zhenlin Xu,Guilherme Penedo,Loubna Ben Allal,Elie Bakouch,John David Pressman,Honglu Fan,Dashiell Stander,Guangyu Song,Aaron Gokaslan,Tom Goldstein,Brian R. Bartoldson,Bhavya Kailkhura,Tyler Murray*

Main category: cs.CL

TL;DR: 提出并发布开放许可的Common Pile v0.1数据集（8TB），训练出性能媲美未授权数据模型的Comma v0.1系列LLM，推动合法合规的LLM发展。


<details>
  <summary>Details</summary>
Motivation: 解决LLM训练依赖未授权文本导致的知识产权和伦理问题，证明开放许可数据同样能训练出高性能模型。

Method: 整合30个多领域开放资源构建数据集，训练7B参数的Comma模型（1T/2T tokens），并与Llama系列对比验证性能。

Result: Comma模型在相同算力下性能与Llama 1/2 7B相当，打破开放数据无法训练优质LLM的固有认知。

Conclusion: 通过完整数据-训练-模型开源生态，为合规LLM发展提供新范式，促进社区协作和透明化研究。

Abstract: Large language models (LLMs) are typically trained on enormous quantities of
unlicensed text, a practice that has led to scrutiny due to possible
intellectual property infringement and ethical concerns. Training LLMs on
openly licensed text presents a first step towards addressing these issues, but
prior data collection efforts have yielded datasets too small or low-quality to
produce performant LLMs. To address this gap, we collect, curate, and release
the Common Pile v0.1, an eight terabyte collection of openly licensed text
designed for LLM pretraining. The Common Pile comprises content from 30 sources
that span diverse domains including research papers, code, books,
encyclopedias, educational materials, audio transcripts, and more. Crucially,
we validate our efforts by training two 7 billion parameter LLMs on text from
the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion
tokens respectively. Both models attain competitive performance to LLMs trained
on unlicensed text with similar computational budgets, such as Llama 1 and 2
7B. In addition to releasing the Common Pile v0.1 itself, we also release the
code used in its creation as well as the training mixture and checkpoints for
the Comma v0.1 models.

</details>


### [625] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出CoRe - MMRAG框架解决MMRAG中的知识不一致问题，经实验验证有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: MMRAG引入外部多模态知识增强大模型时，存在参数 - 检索知识不一致和视觉 - 文本知识不一致的问题。

Method: 提出CoRe - MMRAG框架，采用四阶段流程处理知识，同时有专门训练范式。

Result: 在KB - VQA基准测试中，相比基线方法有显著性能提升，InfoSeek提升5.6%，Encyclopedic - VQA提升9.3%。

Conclusion: CoRe - MMRAG能有效调和多模态知识源的不一致性。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose Cross-source knowledge \textbf{Re}conciliation for
Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively
reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a
four-stage pipeline: it first generates an internal response from parametric
knowledge, then selects the most relevant multimodal evidence via joint
similarity assessment, generates an external response, and finally integrates
both to produce a reliable answer. Additionally, a specialized training
paradigm enhances knowledge source discrimination, multimodal integration, and
unified answer generation. Experiments on KB-VQA benchmarks show that
CoRe-MMRAG achieves substantial improvements over baseline methods, achieving
5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.

</details>


### [626] [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
*Likun Cao,Rui Pan,James Evans*

Main category: cs.CL

TL;DR: 本文用机器学习方法量化创新者主观视角和创新机会，发现视角差异预示创新成就，背景差异反之，并分析机制及对团队组建和研究政策的意义。


<details>
  <summary>Details</summary>
Motivation: 现有创新研究强调社会结构作用，新兴机器学习方法可从创新者个人视角和人际创新机会建模。

Method: 基于动态语言表征构建的概念几何空间量化创新者主观视角和创新机会，分析大量不同领域人员数据，进行自然实验和模拟AI代理间的创意合作。

Result: 主观视角能预测个人和群体未来的创意关注和成功组合；视角差异预示创意成就，背景差异反之，模拟结果与观察结果一致。

Conclusion: 成功的合作者利用共同语言融合多样经验以创新，研究结果对团队组建和研究政策有重要意义。

Abstract: Existing studies of innovation emphasize the power of social structures to
shape innovation capacity. Emerging machine learning approaches, however,
enable us to model innovators' personal perspectives and interpersonal
innovation opportunities as a function of their prior trajectories of
experience. We theorize then quantify subjective perspectives and innovation
opportunities based on innovator positions within the geometric space of
concepts inscribed by dynamic language representations. Using data on millions
of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors
across the creative domains of science, technology, film, entrepreneurship, and
Wikipedia, here we show that measured subjective perspectives anticipate what
ideas individuals and groups creatively attend to and successfully combine in
future. When perspective and background diversity are decomposed as the angular
difference between collaborators' perspectives on their creation and between
their experiences, the former consistently anticipates creative achievement
while the latter portends its opposite, across all cases and time periods
examined. We analyze a natural experiment and simulate creative collaborations
between AI (large language model) agents designed with various perspective and
background diversity, which are consistent with our observational findings. We
explore mechanisms underlying these findings and identify how successful
collaborators leverage common language to weave together diverse experience
obtained through trajectories of prior work that converge to provoke one
another and innovate. We explore the importance of these findings for team
assembly and research policy.

</details>


### [627] [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)
*Alex Pan,Mary-Anne Williams*

Main category: cs.CL

TL;DR: 现有基准难以测试大语言模型嵌套叙事推理能力，本文引入Verbose ListOps基准，实验表明领先模型在该基准上表现不佳，该基准有助于针对性提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文和多跳问答基准无法充分测试大语言模型嵌套叙事推理能力，掩盖了模型的基本局限性。

Method: 引入Verbose ListOps基准，将ListOps计算转换为冗长连贯的故事，对叙事大小和推理难度进行细粒度控制。

Result: 领先的大语言模型在Verbose ListOps基准上，在中等叙事长度（约10k个标记）时性能下降，尽管能轻松解决原始ListOps方程。

Conclusion: 解决大语言模型在该基准上的失败对现实世界文本解释至关重要，Verbose ListOps及其可扩展生成框架有助于针对性提升推理能力，是自动化知识工作的关键一步。

Abstract: Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.

</details>


### [628] [Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05154)
*Chenyu Lin,Yilin Wen,Du Su,Fei Sun,Muhan Chen,Chenfu Bao,Zhonghou Lv*

Main category: cs.CL

TL;DR: 当前RAG系统存在过度依赖检索上下文的问题，提出Knowledgeable - r1方法，实验表明该方法能提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统过度强调检索上下文，导致依赖不准确源且忽视模型固有知识，需解决此失衡问题。

Method: 提出Knowledgeable - r1，使用联合采样并在知识能力探索中定义多策略分布，刺激大语言模型对参数和上下文知识的自我整合利用。

Result: Knowledgeable - r1显著增强参数和上下文冲突任务及一般RAG任务的鲁棒性和推理准确性，在反事实场景中比基线高17.07%，在RAG任务中有持续提升。

Conclusion: Knowledgeable - r1方法有效解决了RAG系统存在的问题，提升了任务性能。

Abstract: Retrieval-augmented generation (RAG) is a mainstream method for improving
performance on knowledge-intensive tasks. However,current RAG systems often
place too much emphasis on retrieved contexts. This can lead to reliance on
inaccurate sources and overlook the model's inherent knowledge, especially when
dealing with misleading or excessive information. To resolve this imbalance, we
propose Knowledgeable-r1 that using joint sampling and define multi policy
distributions in knowledge capability exploration to stimulate large language
models'self-integrated utilization of parametric and contextual knowledge.
Experiments show that Knowledgeable-r1 significantly enhances robustness and
reasoning accuracy in both parameters and contextual conflict tasks and general
RAG tasks, especially outperforming baselines by 17.07% in counterfactual
scenarios and demonstrating consistent gains across RAG tasks. Our code are
available at https://github.com/lcy80366872/ knowledgeable-r1.

</details>


### [629] [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/abs/2506.05167)
*Yeonseok Jeong,Jinsu Kim,Dohyeon Lee,Seung-won Hwang*

Main category: cs.CL

TL;DR: 提出ECoRAG框架以压缩检索文档，提升大语言模型在开放域问答任务的性能，且成本效益高。


<details>
  <summary>Details</summary>
Motivation: 现有上下文压缩方法未聚焦过滤非证据信息，限制了基于大语言模型的检索增强生成（RAG）性能。

Method: 提出ECoRAG框架，基于证据性压缩检索文档，检查压缩内容证据是否充分，不足则继续检索。

Result: ECoRAG提升了大语言模型在开放域问答任务的性能，优于现有压缩方法，还降低了延迟并减少了令牌使用。

Conclusion: ECoRAG框架有效提升大语言模型在开放域问答任务的表现，且成本效益高。

Abstract: Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing
retrieved documents based on evidentiality, ensuring whether answer generation
is supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.

</details>


### [630] [Search Arena: Analyzing Search-Augmented LLMs](https://arxiv.org/abs/2506.05334)
*Mihran Miroyan,Tsung-Han Wu,Logan King,Tianle Li,Jiayi Pan,Xinyan Hu,Wei-Lin Chiang,Anastasios N. Angelopoulos,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 提出Search Arena数据集，分析搜索增强大模型，揭示用户偏好影响因素并开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有分析搜索增强大模型的数据集规模小、范围窄，需更好的数据集。

Method: 众包生成Search Arena数据集，进行跨场景分析。

Result: 发现用户偏好受引用数量影响，不同引用源偏好不同，搜索在非搜索场景不降低性能。

Conclusion: 开源数据集支持该方向未来研究。

Abstract: Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.

</details>


### [631] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
*Zhaolu Kang,Junhao Gong,Jiaxu Yan,Wanke Xia,Yian Wang,Ziwen Wang,Huaxuan Ding,Zhuo Cheng,Wenhao Cao,Zhiyuan Feng,Siqi He,Shannan Yan,Junzhe Chen,Xiaomin He,Chaoya Jiang,Wei Ye,Kaidong Yu,Xuelong Li*

Main category: cs.CL

TL;DR: 提出HSSBench基准评估多模态大语言模型在人文社科任务上的能力，含数据生成流程，测试20多个模型，此基准有挑战性，望促进模型跨学科推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型评估基准忽视人文社科领域需求，人文社科任务对模型有独特挑战。

Method: 提出HSSBench基准，涵盖六种联合国官方语言；引入针对人文社科场景的数据生成流程，专家和自动化代理协作生成和迭代优化样本。

Result: HSSBench包含超13000个精心设计样本，涵盖六个关键类别；对超20个主流多模态大语言模型测试，发现该基准对先进模型也有很大挑战。

Conclusion: 此基准有望激发提升多模态大语言模型跨学科推理能力的研究，特别是内化和关联跨领域知识的能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [632] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
*Matthieu Tehenan,Vikram Natarajan,Jonathan Michala,Milton Lin,Juri Opitz*

Main category: cs.CL

TL;DR: 提出新方法将句子嵌入分解为可解释组件，获得句子嵌入空间内部工作的有趣见解。


<details>
  <summary>Details</summary>
Motivation: 现有句子嵌入内部结构未知，特征不可解释且内容难以追踪。

Method: 在词元级表示上使用字典学习，将句子嵌入机械分解为可解释组件，分析池化如何压缩特征。

Result: 获得句子嵌入空间内部工作的有趣见解，如许多语义和句法方面线性编码在嵌入中。

Conclusion: 该方法桥接了词元级机械可解释性和句子级分析，使表示更透明可控。

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [633] [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
*Kurt Micallef,Claudia Borg*

Main category: cs.CL

TL;DR: 评估55个公开大语言模型在低资源语言马耳他语上的表现，发现小的微调模型表现更好，预训练和指令调优接触马耳他语是关键因素，强调微调与提示的权衡及对低资源语言研究建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源语言上效果有限，评估其在马耳他语上的性能。

Method: 使用新引入的涵盖11个判别和生成任务的基准评估55个公开大语言模型。

Result: 许多模型表现不佳，尤其是生成任务；小的微调模型在各任务中表现更好。

Conclusion: 预训练和指令调优时接触马耳他语是最重要因素；微调虽初始成本高，但性能好且推理成本低；呼吁更具包容性的语言技术，建议低资源语言研究考虑‘传统’语言建模方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various Natural Language Processing (NLP) tasks, largely due to their
generalisability and ability to perform tasks without additional training.
However, their effectiveness for low-resource languages remains limited. In
this study, we evaluate the performance of 55 publicly available LLMs on
Maltese, a low-resource language, using a newly introduced benchmark covering
11 discriminative and generative tasks. Our experiments highlight that many
models perform poorly, particularly on generative tasks, and that smaller
fine-tuned models often perform better across all tasks. From our
multidimensional analysis, we investigate various factors impacting
performance. We conclude that prior exposure to Maltese during pre-training and
instruction-tuning emerges as the most important factor. We also examine the
trade-offs between fine-tuning and prompting, highlighting that while
fine-tuning requires a higher initial cost, it yields better performance and
lower inference costs. Through this work, we aim to highlight the need for more
inclusive language technologies and recommend that researchers working with
low-resource languages consider more "traditional" language modelling
approaches.

</details>


### [634] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: 介绍了首个公开的MedAgentGYM训练环境，可提升大语言模型基于编码的医学推理能力，对超30个LLM基准测试有差异，Med - Copilot - 7B经训练性能提升，MedAgentGYM提供综合平台。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型基于编码的医学推理能力。

Method: 构建包含72413个任务实例、129个类别的MedAgentGYM训练环境，对超30个LLM进行基准测试，对Med - Copilot - 7B进行监督微调与持续强化学习。

Result: 基准测试显示商业API模型和开源模型有性能差异，Med - Copilot - 7B经训练性能大幅提升。

Conclusion: MedAgentGYM为生物医学研究和实践提供开发基于LLM的编码助手的综合平台。

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [635] [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
*Wesley Scivetti,Tatsuya Aoyama,Ethan Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: 研究测试人类规模的Transformer语言模型对英语LET - ALONE结构的形式和意义的掌握，发现模型对形式敏感，但对意义泛化不佳。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽有从常见到罕见结构的泛化能力，但该能力的普遍性及对罕见结构意义的掌握程度未知，本文旨在填补此空白。

Method: 构建针对LET - ALONE结构句法和语义特性的合成基准来评估人类规模的Transformer语言模型。

Result: 人类规模的语言模型对形式敏感，即便过滤相关结构；但对LET - ALONE的意义无法正确泛化。

Conclusion: 当前架构在语言形式和意义的样本效率上存在不对称，这在人类语言学习者中不存在。

Abstract: Humans have a remarkable ability to acquire and understand grammatical
phenomena that are seen rarely, if ever, during childhood. Recent evidence
suggests that language models with human-scale pretraining data may possess a
similar ability by generalizing from frequent to rare constructions. However,
it remains an open question how widespread this generalization ability is, and
to what extent this knowledge extends to meanings of rare constructions, as
opposed to just their forms. We fill this gap by testing human-scale
transformer language models on their knowledge of both the form and meaning of
the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we
construct a bespoke synthetic benchmark that targets syntactic and semantic
properties of the construction. We find that human-scale LMs are sensitive to
form, even when related constructions are filtered from the dataset. However,
human-scale LMs do not make correct generalizations about LET-ALONE's meaning.
These results point to an asymmetry in the current architectures' sample
efficiency between language form and meaning, something which is not present in
human language learners.

</details>


### [636] [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
*Lev Morozov,Aleksandr Mogilevskii,Alexander Shirnin*

Main category: cs.CL

TL;DR: 本文介绍EmoRAG系统用于SemEval - 2025任务11子任务A的多标签情感检测，采用集成模型预测，高效易实现且效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决SemEval - 2025任务11子任务A的多标签情感检测问题，预测文本中说话者的感知情绪。

Method: 不进行额外模型训练，仅使用模型集成的方法来预测情绪。

Result: EmoRAG取得了与最佳表现系统相当的结果。

Conclusion: EmoRAG系统高效、可扩展且易于实现。

Abstract: This paper describes EmoRAG, a system designed to detect perceived emotions
in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We
focus on predicting the perceived emotions of the speaker from a given text
snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,
and disgust. Our approach does not require additional model training and only
uses an ensemble of models to predict emotions. EmoRAG achieves results
comparable to the best performing systems, while being more efficient,
scalable, and easier to implement.

</details>


### [637] [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
*William Sheffield,Kanishka Misra,Valentina Pyatkin,Ashwini Deo,Kyle Mahowald,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 研究大语言模型（LLMs）区分英语话语标记词“just”细粒度含义的能力，发现LLMs有一定区分大类能力，但难以捕捉细微差别。


<details>
  <summary>Details</summary>
Motivation: 话语标记词对文本意义有重要影响，“just”有多种语义和话语功能，研究LLMs对其细粒度含义的区分能力。

Method: 使用专家语言学家精心创建和标注的数据，研究LLMs对英语“just”细粒度含义的区分能力。

Result: LLMs能区分更广泛的类别，但难以完全捕捉更细微的差别。

Conclusion: LLMs在理解话语标记词方面存在差距。

Abstract: Discourse particles are crucial elements that subtly shape the meaning of
text. These words, often polyfunctional, give rise to nuanced and often quite
disparate semantic/discourse effects, as exemplified by the diverse uses of the
particle "just" (e.g., exclusive, temporal, emphatic). This work investigates
the capacity of LLMs to distinguish the fine-grained senses of English "just",
a well-studied example in formal semantics, using data meticulously created and
labeled by expert linguists. Our findings reveal that while LLMs exhibit some
ability to differentiate between broader categories, they struggle to fully
capture more subtle nuances, highlighting a gap in their understanding of
discourse particles.

</details>


### [638] [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
*Senyu Li,Jiayi Wang,Felermino D. M. A. Ali,Colin Cherry,Daniel Deutsch,Eleftheria Briakou,Rui Sousa-Silva,Henrique Lopes Cardoso,Pontus Stenetorp,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文引入大规模人工标注MT评估数据集SSA - MTE，开发评估指标SSA - COMET和SSA - COMET - QE，对基于提示的大模型方法进行基准测试，结果显示SSA - COMET模型表现优异，资源开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标在评估非洲低资源语言机器翻译质量时存在语言覆盖有限、低资源场景表现差等问题。

Method: 引入涵盖13个非洲语言对的大规模人工标注数据集SSA - MTE，基于此开发评估指标SSA - COMET和SSA - COMET - QE，对基于提示的大模型方法进行基准测试。

Result: SSA - COMET模型显著优于AfriCOMET，在低资源语言上与最强的大模型（Gemini 2.5 Pro）有竞争力。

Conclusion: 所开发的评估指标和方法表现良好，资源开源以支持未来研究。

Abstract: Evaluating machine translation (MT) quality for under-resourced African
languages remains a significant challenge, as existing metrics often suffer
from limited language coverage and poor performance in low-resource settings.
While recent efforts, such as AfriCOMET, have addressed some of the issues,
they are still constrained by small evaluation sets, a lack of publicly
available training data tailored to African languages, and inconsistent
performance in extremely low-resource scenarios. In this work, we introduce
SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13
African language pairs from the News domain, with over 63,000 sentence-level
annotations from a diverse set of MT systems. Based on this data, we develop
SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free
evaluation metrics. We also benchmark prompting-based approaches using
state-of-the-art LLMs like GPT-4o and Claude. Our experimental results show
that SSA-COMET models significantly outperform AfriCOMET and are competitive
with the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on
low-resource languages such as Twi, Luo, and Yoruba. All resources are released
under open licenses to support future research.

</details>


### [639] [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)
*Dimitris Vamvourellis,Dhagash Mehta*

Main category: cs.CL

TL;DR: 研究大语言模型在零样本金融情感分析中的有效性，发现推理不一定提升性能，GPT - 4o无思维链提示表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（包括基于推理和非推理模型）在零样本金融情感分析中的有效性。

Method: 使用金融短语库数据集，对比三种专有大语言模型在不同提示范式下的表现，并与两个微调的小模型进行基准测试。

Result: 推理未提升任务性能，GPT - 4o无思维链提示组合最准确且与人类判断最一致，推理可能导致过度思考和次优预测。

Conclusion: 在金融情感分类中，类似系统1的快速直觉思维比系统2的缓慢审慎推理更符合人类判断，挑战了推理越多决策越好的默认假设。

Abstract: We investigate the effectiveness of large language models (LLMs), including
reasoning-based and non-reasoning models, in performing zero-shot financial
sentiment analysis. Using the Financial PhraseBank dataset annotated by domain
experts, we evaluate how various LLMs and prompting strategies align with
human-labeled sentiment in a financial context. We compare three proprietary
LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that
simulate System 1 (fast and intuitive) or System 2 (slow and deliberate)
thinking and benchmark them against two smaller models (FinBERT-Prosus,
FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest
that reasoning, either through prompting or inherent model design, does not
improve performance on this task. Surprisingly, the most accurate and
human-aligned combination of model and method was GPT-4o without any
Chain-of-Thought (CoT) prompting. We further explore how performance is
impacted by linguistic complexity and annotation agreement levels, uncovering
that reasoning may introduce overthinking, leading to suboptimal predictions.
This suggests that for financial sentiment classification, fast, intuitive
"System 1"-like thinking aligns more closely with human judgment compared to
"System 2"-style slower, deliberative reasoning simulated by reasoning models
or CoT prompting. Our results challenge the default assumption that more
reasoning always leads to better LLM decisions, particularly in high-stakes
financial applications.

</details>


### [640] [SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing](https://arxiv.org/abs/2506.04583)
*Hongjun Liu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 提出训练无关方法SUCEA应对对抗性声明事实核查难题，实验显示该框架提升检索和标签预测准确率，优于四个基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强语言模型的事实核查系统难以处理对抗性声明，需新方法解决。

Method: 提出模块化框架SUCEA，将任务分解为声明分割与去上下文、迭代证据检索与声明编辑、证据聚合与标签预测三步。

Result: 在两个挑战性数据集上实验表明，框架显著提升检索和蕴含标签预测的准确率，优于四个强基线。

Conclusion: SUCEA框架有效应对对抗性声明的事实核查问题，在检索和标签预测上表现出色。

Abstract: Automatic fact-checking has recently received more attention as a means of
combating misinformation. Despite significant advancements, fact-checking
systems based on retrieval-augmented language models still struggle to tackle
adversarial claims, which are intentionally designed by humans to challenge
fact-checking systems. To address these challenges, we propose a training-free
method designed to rephrase the original claim, making it easier to locate
supporting evidence. Our modular framework, SUCEA, decomposes the task into
three steps: 1) Claim Segmentation and Decontextualization that segments
adversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval
and Claim Editing that iteratively retrieves evidence and edits the subclaim
based on the retrieved evidence; 3) Evidence Aggregation and Label Prediction
that aggregates all retrieved evidence and predicts the entailment label.
Experiments on two challenging fact-checking datasets demonstrate that our
framework significantly improves on both retrieval and entailment label
accuracy, outperforming four strong claim-decomposition-based baselines.

</details>


### [641] [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
*Chengwu Liu,Ye Yuan,Yichun Yin,Yan Xu,Xin Xu,Zaoyu Chen,Yasheng Wang,Lifeng Shang,Qun Liu,Ming Zhang*

Main category: cs.CL

TL;DR: 提出回顾性、步骤感知的形式验证框架Safe，用Lean 4语言识别幻觉，评估显示性能提升并提供可解释证据，还提出FormalStep基准。


<details>
  <summary>Details</summary>
Motivation: 当前缓解CoT幻觉的方法是黑盒操作，无法提供可检查证据，可能限制效果。

Method: 提出回顾性、步骤感知的形式验证框架Safe，在每个推理步骤用Lean 4表达数学声明并提供形式证明以识别幻觉。

Result: 在多个语言模型和数学数据集上评估，显示性能显著提升，能提供可解释和可验证的证据。

Conclusion: 此工作是首次用Lean 4验证LLMs生成的自然语言内容，符合形式数学语言创建初衷。

Abstract: Chain-of-Thought (CoT) prompting has become the de facto method to elicit
reasoning capabilities from large language models (LLMs). However, to mitigate
hallucinations in CoT that are notoriously difficult to detect, current methods
such as process reward models (PRMs) or self-consistency operate as opaque
boxes and do not provide checkable evidence for their judgments, possibly
limiting their effectiveness. To address this issue, we draw inspiration from
the idea that "the gold standard for supporting a mathematical claim is to
provide a proof". We propose a retrospective, step-aware formal verification
framework $Safe$. Rather than assigning arbitrary scores, we strive to
articulate mathematical claims in formal mathematical language Lean 4 at each
reasoning step and provide formal proofs to identify hallucinations. We
evaluate our framework $Safe$ across multiple language models and various
mathematical datasets, demonstrating a significant performance improvement
while offering interpretable and verifiable evidence. We also propose
$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$
formal statements. To the best of our knowledge, our work represents the first
endeavor to utilize formal mathematical language Lean 4 for verifying natural
language content generated by LLMs, aligning with the reason why formal
mathematical languages were created in the first place: to provide a robust
foundation for hallucination-prone human-written proofs.

</details>


### [642] [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
*Takashi Wada,Yuki Hirakawa,Ryotaro Shimizu,Takahiro Kawashima,Yuki Saito*

Main category: cs.CL

TL;DR: 提出优化句子语义表示的静态词嵌入方法，推理成本低，在单语和跨语言任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为句子语义表示提出新的静态词嵌入方法。

Method: 从预训练的句子转换器提取词嵌入，用句子级主成分分析改进，再通过知识蒸馏或对比学习优化，推理时简单平均词嵌入。

Result: 模型在句子语义任务上大幅超越现有静态模型，在部分数据集上与基本的句子转换器模型相当。

Conclusion: 方法成功去除与句子语义无关的词嵌入成分，根据词对句子语义的影响调整向量范数。

Abstract: We propose new static word embeddings optimised for sentence semantic
representation. We first extract word embeddings from a pre-trained Sentence
Transformer, and improve them with sentence-level principal component analysis,
followed by either knowledge distillation or contrastive learning. During
inference, we represent sentences by simply averaging word embeddings, which
requires little computational cost. We evaluate models on both monolingual and
cross-lingual tasks and show that our model substantially outperforms existing
static models on sentence semantic tasks, and even rivals a basic Sentence
Transformer model (SimCSE) on some data sets. Lastly, we perform a variety of
analyses and show that our method successfully removes word embedding
components that are irrelevant to sentence semantics, and adjusts the vector
norms based on the influence of words on sentence semantics.

</details>


### [643] [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)
*Gio Paik,Geewook Kim,Jinbae Im*

Main category: cs.CL

TL;DR: 介绍MMRefine多模态细化基准，评估MLLM误差细化能力，实验揭示瓶颈与改进方向，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 随着推理中对增强推理的重视，需要评估多模态大语言模型（MLLMs）误差细化能力的框架。

Method: 设计MMRefine基准，通过六个不同场景评估MLLMs检测和纠正错误的能力，将错误分为六种类型分析细化性能。

Result: 对各种开放和封闭的MLLMs实验，揭示了阻碍细化性能的瓶颈和因素。

Conclusion: 指出了在有效增强推理方面MLLMs需要改进的方向。

Abstract: This paper introduces MMRefine, a MultiModal Refinement benchmark designed to
evaluate the error refinement capabilities of Multimodal Large Language Models
(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,
MMRefine provides a framework that evaluates MLLMs' abilities to detect and
correct errors across six distinct scenarios beyond just comparing final
accuracy before and after refinement. Furthermore, the benchmark analyzes the
refinement performance by categorizing errors into six error types. Experiments
with various open and closed MLLMs reveal bottlenecks and factors impeding
refinement performance, highlighting areas for improvement in effective
reasoning enhancement. Our code and dataset are publicly available at
https://github.com/naver-ai/MMRefine.

</details>


### [644] [Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection](https://arxiv.org/abs/2506.04739)
*Ziyi Zhou,Xiaoming Zhang,Litian Zhang,Yibo Zhang,Zhenyu Guan,Chaozhuo Li,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出C$^2$EFND框架检测虚假新闻，实验表明其优于现有方法，提升检测准确率和适应性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假新闻广泛传播影响社会，传统小语言模型有监督训练需求大、难适应新闻环境变化问题，大语言模型检测不准。

Method: 提出C$^2$EFND框架，通过多轮协作学习利用大语言模型泛化能力和小语言模型分类专长，引入基于混合专家架构的终身知识编辑模块更新大语言模型，采用基于重放的持续学习方法让小语言模型保留先验知识。

Result: 在Pheme和Twitter16数据集上的实验表明，C$^2$EFND显著优于现有方法。

Conclusion: C$^2$EFND能有效提高连续突发虚假新闻场景中的检测准确率和适应性。

Abstract: The widespread dissemination of fake news on social media has significantly
impacted society, resulting in serious consequences. Conventional deep learning
methodologies employing small language models (SLMs) suffer from extensive
supervised training requirements and difficulties adapting to evolving news
environments due to data scarcity and distribution shifts. Large language
models (LLMs), despite robust zero-shot capabilities, fall short in accurately
detecting fake news owing to outdated knowledge and the absence of suitable
demonstrations. In this paper, we propose a novel Continuous Collaborative
Emergent Fake News Detection (C$^2$EFND) framework to address these challenges.
The C$^2$EFND framework strategically leverages both LLMs' generalization power
and SLMs' classification expertise via a multi-round collaborative learning
framework. We further introduce a lifelong knowledge editing module based on a
Mixture-of-Experts architecture to incrementally update LLMs and a replay-based
continue learning method to ensure SLMs retain prior knowledge without
retraining entirely. Extensive experiments on Pheme and Twitter16 datasets
demonstrate that C$^2$EFND significantly outperforms existed methods,
effectively improving detection accuracy and adaptability in continuous
emergent fake news scenarios.

</details>


### [645] [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)
*Jingyu Hu,Mengyue Yang,Mengnan Du,Weiru Liu*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLMs）政治观点研究问题，拓展单轴到多维度，设计四维政治学习框架并构建数据集，实验表明生成向量可解决概念混淆、有良好泛化性且能干预模型生成不同政治倾向回复。


<details>
  <summary>Details</summary>
Motivation: 现有研究中LLMs回复与内部意图存在不一致，且多依赖单轴概念导致概念混淆，需探究内部机制并拓展维度进行政治概念学习。

Method: 将单轴拓展到多维度，运用可解释表示工程技术，设计四维政治学习框架，构建对应数据集进行细粒度政治概念向量学习，并在八个开源LLMs上用三种表示工程技术实验。

Result: 向量能解决政治概念混淆问题，检测任务验证向量语义，在OOD设置中有良好泛化性和鲁棒性，干预实验显示可干预模型生成不同政治倾向回复。

Conclusion: 所提出的方法能有效用于检测和干预LLMs的内部政治状态，为LLMs政治观点研究提供新途径。

Abstract: Studies of LLMs' political opinions mainly rely on evaluations of their
open-ended responses. Recent work indicates that there is a misalignment
between LLMs' responses and their internal intentions. This motivates us to
probe LLMs' internal mechanisms and help uncover their internal political
states. Additionally, we found that the analysis of LLMs' political opinions
often relies on single-axis concepts, which can lead to concept confounds. In
this work, we extend the single-axis to multi-dimensions and apply
interpretable representation engineering techniques for more transparent LLM
political concept learning. Specifically, we designed a four-dimensional
political learning framework and constructed a corresponding dataset for
fine-grained political concept vector learning. These vectors can be used to
detect and intervene in LLM internals. Experiments are conducted on eight
open-source LLMs with three representation engineering techniques. Results show
these vectors can disentangle political concept confounds. Detection tasks
validate the semantic meaning of the vectors and show good generalization and
robustness in OOD settings. Intervention Experiments show these vectors can
intervene in LLMs to generate responses with different political leanings.

</details>


### [646] [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)
*Jisu An,Junseok Lee,Jeoungeun Lee,Yongseok Son*

Main category: cs.CL

TL;DR: 本文对多模态大语言模型（MLLMs）进行以大语言模型为中心的分析，提出分类框架，研究新兴模式，为研究者提供参考。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型发展迅速，需要系统理解不同模态与语言主干的连接方式，填补现有文献空白。

Method: 对2021 - 2025年的125个MLLMs进行研究，从架构策略、表征学习技术、训练范式三个维度提出分类框架。

Result: 识别出该领域的新兴模式，为研究者提供当前集成技术的结构化概述。

Conclusion: 研究成果有助于指导基于预训练基础的未来模型开发更强大的多模态集成策略。

Abstract: The rapid progress of Multimodal Large Language Models(MLLMs) has transformed
the AI landscape. These models combine pre-trained LLMs with various modality
encoders. This integration requires a systematic understanding of how different
modalities connect to the language backbone. Our survey presents an LLM-centric
analysis of current approaches. We examine methods for transforming and
aligning diverse modal inputs into the language embedding space. This addresses
a significant gap in existing literature. We propose a classification framework
for MLLMs based on three key dimensions. First, we examine architectural
strategies for modality integration. This includes both the specific
integration mechanisms and the fusion level. Second, we categorize
representation learning techniques as either joint or coordinate
representations. Third, we analyze training paradigms, including training
strategies and objective functions. By examining 125 MLLMs developed between
2021 and 2025, we identify emerging patterns in the field. Our taxonomy
provides researchers with a structured overview of current integration
techniques. These insights aim to guide the development of more robust
multimodal integration strategies for future models built on pre-trained
foundations.

</details>


### [647] [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)
*Yujun Zhou,Jiayi Ye,Zipeng Ling,Yufei Han,Yue Huang,Haomin Zhuang,Zhenwen Liang,Kehan Guo,Taicheng Guo,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出FineLogic评估框架评估大语言模型逻辑推理能力，并研究微调时监督格式的影响，为评估和改进大模型逻辑推理提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅依赖最终答案准确性，无法捕捉推理过程的质量和结构，需新评估框架。

Method: 提出FineLogic框架从三个维度评估逻辑推理；构建四种监督风格微调大模型并研究其影响；进行表示层探测。

Result: 自然语言监督在分布外和长上下文任务上泛化能力强；符号推理风格促进更结构合理和原子化的推理链；微调主要通过逐步生成改善推理行为。

Conclusion: 框架和分析为评估和改进大语言模型逻辑推理提供更严格和可解释的视角。

Abstract: Logical reasoning is a core capability for many applications of large
language models (LLMs), yet existing benchmarks often rely solely on
final-answer accuracy, failing to capture the quality and structure of the
reasoning process. We propose FineLogic, a fine-grained evaluation framework
that assesses logical reasoning across three dimensions: overall benchmark
accuracy, stepwise soundness, and representation-level alignment. In addition,
to better understand how reasoning capabilities emerge, we conduct a
comprehensive study on the effects of supervision format during fine-tuning. We
construct four supervision styles (one natural language and three symbolic
variants) and train LLMs under each. Our findings reveal that natural language
supervision yields strong generalization even on out-of-distribution and
long-context tasks, while symbolic reasoning styles promote more structurally
sound and atomic inference chains. Further, our representation-level probing
shows that fine-tuning primarily improves reasoning behaviors through
step-by-step generation, rather than enhancing shortcut prediction or
internalized correctness. Together, our framework and analysis provide a more
rigorous and interpretable lens for evaluating and improving logical reasoning
in LLMs.

</details>


### [648] [A Reasoning-Based Approach to Cryptic Crossword Clue Solving](https://arxiv.org/abs/2506.04824)
*Martin Andrews,Sam Witteveen*

Main category: cs.CL

TL;DR: 本文介绍了一个基于大语言模型的推理系统，该系统使用开源组件解决隐晦填字游戏线索，在Cryptonite数据集上达到了新的最优性能，且答案推理过程可解释。


<details>
  <summary>Details</summary>
Motivation: 解决全球各大报纸每日发布的隐晦填字游戏线索这一具有挑战性的语言任务。

Method: 构建基于大语言模型的推理系统，通过假设答案、提出文字游戏解释以及使用基于编码推理步骤的验证系统来解决隐晦线索。

Result: 该系统在具有挑战性的Cryptonite数据集上建立了新的最优性能。

Conclusion: 基于开源组件的大语言模型推理系统可有效解决隐晦填字游戏线索，且推理过程可解释。

Abstract: Cryptic crossword clues are challenging language tasks for which new test
sets are released daily by major newspapers on a global basis. Each cryptic
clue contains both the definition of the answer to be placed in the crossword
grid (in common with regular crosswords), and 'wordplay' that proves that the
answer is correct (i.e. a human solver can be confident that an answer is
correct without needing crossing words as confirmation). This work describes an
LLM-based reasoning system built from open-licensed components that solves
cryptic clues by (i) hypothesising answers; (ii) proposing wordplay
explanations; and (iii) using a verifier system that operates on codified
reasoning steps. Overall, this system establishes a new state-of-the-art
performance on the challenging Cryptonite dataset of clues from The Times and
The Telegraph newspapers in the UK. Because each proved solution is expressed
in Python, interpretable wordplay reasoning for proven answers is available for
inspection.

</details>


### [649] [Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights](https://arxiv.org/abs/2506.04851)
*Giorgio Biancini,Alessio Ferrato,Carla Limongelli*

Main category: cs.CL

TL;DR: 文章探讨用大语言模型生成选择题，对比三种模型，发现GPT - 3.5效果最佳，还指出教育领域对采用AI有顾虑。


<details>
  <summary>Details</summary>
Motivation: 教育中人工生成选择题耗费资源，大语言模型有望解决该问题。

Method: 对Llama 2、Mistral和GPT - 3.5三种大语言模型进行对比分析，不依赖模型知识，而是将知识注入提示。

Result: 涉及21名教育工作者的实验表明，GPT - 3.5在多个已知指标上生成选择题效果最佳，且教育领域对采用AI存在顾虑。

Conclusion: 大语言模型有生成选择题和改善教育体验的潜力，为未来研究提供有价值见解。

Abstract: Integrating Artificial Intelligence (AI) in educational settings has brought
new learning approaches, transforming the practices of both students and
educators. Among the various technologies driving this transformation, Large
Language Models (LLMs) have emerged as powerful tools for creating educational
materials and question answering, but there are still space for new
applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess
student knowledge, but manually generating these questions is
resource-intensive and requires significant time and cognitive effort. In our
opinion, LLMs offer a promising solution to these challenges. This paper
presents a novel comparative analysis of three widely known LLMs - Llama 2,
Mistral, and GPT-3.5 - to explore their potential for creating informative and
challenging MCQs. In our approach, we do not rely on the knowledge of the LLM,
but we inject the knowledge into the prompt to contrast the hallucinations,
giving the educators control over the test's source text, too. Our experiment
involving 21 educators shows that GPT-3.5 generates the most effective MCQs
across several known metrics. Additionally, it shows that there is still some
reluctance to adopt AI in the educational field. This study sheds light on the
potential of LLMs to generate MCQs and improve the educational experience,
providing valuable insights for the future.

</details>


### [650] [Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback](https://arxiv.org/abs/2506.04920)
*Junior Cedric Tonga,KV Aditya Srivatsa,Kaushal Kumar Maurya,Fajri Koto,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 本文首次用大语言模型进行多语言师生交互大规模模拟，探索不同因素对学习表现的影响，发现多语言提示能显著改善学习成果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言、数学推理任务的教学支持能力未被充分研究，需评估语言特定反馈能否带来学习提升。

Method: 用更强模型模拟教师、较弱模型模拟学生，在11种语言、4种大语言模型和多种提示策略下开展352种实验设置。

Result: 多语言提示能显著改善学习成果，尤其在低资源语言中，反馈与学生母语一致时效果更佳。

Conclusion: 研究为开发有效且具包容性的多语言、基于大语言模型的教育工具提供实用见解。

Abstract: Large language models (LLMs) have demonstrated the ability to generate
formative feedback and instructional hints in English, making them increasingly
relevant for AI-assisted education. However, their ability to provide effective
instructional support across different languages, especially for mathematically
grounded reasoning tasks, remains largely unexamined. In this work, we present
the first large-scale simulation of multilingual tutor-student interactions
using LLMs. A stronger model plays the role of the tutor, generating feedback
in the form of hints, while a weaker model simulates the student. We explore
352 experimental settings across 11 typologically diverse languages, four
state-of-the-art LLMs, and multiple prompting strategies to assess whether
language-specific feedback leads to measurable learning gains. Our study
examines how student input language, teacher feedback language, model choice,
and language resource level jointly influence performance. Results show that
multilingual hints can significantly improve learning outcomes, particularly in
low-resource languages when feedback is aligned with the student's native
language. These findings offer practical insights for developing multilingual,
LLM-based educational tools that are both effective and inclusive.

</details>


### [651] [From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation](https://arxiv.org/abs/2506.04965)
*Adrian Marius Dumitran,Theodor-Pierre Moroianu,Vasile Paul Alexe*

Main category: cs.CL

TL;DR: 本文对大语言模型在大学算法考试中的表现进行评估，发现最新模型表现佳，还探讨其在教育领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在大学算法考试中的表现，分析其解题能力、一致性和多语言性能。

Method: 在罗马尼亚考试及其英文翻译上测试多个模型。

Result: 最新模型得分可与顶尖学生媲美，在复杂算法挑战中展现推理能力，但图相关任务仍有困难。

Conclusion: 研究为生成式AI在高级算法教育中的进一步集成奠定基础。

Abstract: This paper presents a comprehensive evaluation of the performance of
state-of-the-art Large Language Models (LLMs) on challenging university-level
algorithms exams. By testing multiple models on both a Romanian exam and its
high-quality English translation, we analyze LLMs' problem-solving
capabilities, consistency, and multilingual performance. Our empirical study
reveals that the most recent models not only achieve scores comparable to
top-performing students but also demonstrate robust reasoning skills on
complex, multi-step algorithmic challenges, even though difficulties remain
with graph-based tasks. Building on these findings, we explore the potential of
LLMs to support educational environments through the generation of high-quality
editorial content, offering instructors a powerful tool to enhance student
feedback. The insights and best practices discussed herein pave the way for
further integration of generative AI in advanced algorithm education.

</details>


### [652] [TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages](https://arxiv.org/abs/2506.05057)
*Moshe Ofer,Orel Zamler,Amos Azaria*

Main category: cs.CL

TL;DR: 论文提出TALL架构提升大语言模型在低资源语言上的性能，在希伯来语实验中表现优于基线方法，采用参数高效策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言表现出色，但在低资源语言上因训练数据有限而表现不佳。

Method: 提出TALL架构，将大语言模型与两个双语翻译模型集成，通过维度对齐层和自定义变换器转换低资源输入，采用参数高效策略，仅训练轻量级适配器模块。

Result: 在希伯来语实验中，TALL架构比直接使用、简单翻译和微调等基线方法有显著改进。

Conclusion: TALL架构能有效提升大语言模型在低资源语言上的性能，且平衡了计算效率和性能提升。

Abstract: Large Language Models (LLMs) excel in high-resource languages but struggle
with low-resource languages due to limited training data. This paper presents
TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource
Languages), which integrates an LLM with two bilingual translation models. TALL
transforms low-resource inputs into high-resource representations, leveraging
the LLM's capabilities while preserving linguistic features through dimension
alignment layers and custom transformers. Our experiments on Hebrew demonstrate
significant improvements over several baselines, including direct use, naive
translation, and fine-tuning approaches. The architecture employs a
parameter-efficient strategy, freezing pre-trained components while training
only lightweight adapter modules, balancing computational efficiency with
performance gains.

</details>


### [653] [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/abs/2506.05068)
*Iulia Comşa,Murray Shanahan*

Main category: cs.CL

TL;DR: 探讨大语言模型（LLMs）自我报告能否被视为内省，分析两个例子，一个不属内省，另一个属最小程度内省且无意识体验。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs语言流畅性和认知能力提升，探讨如何解读其自我报告，以及内省概念在多大程度上可应用于LLMs。

Method: 呈现并批判LLMs两个明显的内省式自我报告例子。

Result: 第一个例子中LLMs描述自身‘创造性’写作过程不属有效内省；第二个例子中LLMs正确推断自身温度参数值，可视为最小程度内省。

Conclusion: LLMs存在不同情况的自我报告，部分可被视为最小程度内省，但无伴随意识体验。

Abstract: Large language models (LLMs) exhibit compelling linguistic behaviour, and
sometimes offer self-reports, that is to say statements about their own nature,
inner workings, or behaviour. In humans, such reports are often attributed to a
faculty of introspection and are typically linked to consciousness. This raises
the question of how to interpret self-reports produced by LLMs, given their
increasing linguistic fluency and cognitive capabilities. To what extent (if
any) can the concept of introspection be meaningfully applied to LLMs? Here, we
present and critique two examples of apparent introspective self-report from
LLMs. In the first example, an LLM attempts to describe the process behind its
own ``creative'' writing, and we argue this is not a valid example of
introspection. In the second example, an LLM correctly infers the value of its
own temperature parameter, and we argue that this can be legitimately
considered a minimal example of introspection, albeit one that is (presumably)
not accompanied by conscious experience.

</details>


### [654] [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
*Tanmay Parekh,Kartik Mehta,Ninareh Mehrabi,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出DiCoRe框架用于零样本事件检测，经实验验证其性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在零样本事件检测中因理解复杂事件本体、提取特定领域触发词等任务而受限，需更好的方法。

Method: 提出DiCoRe框架，通过Dreamer进行发散推理，Grounder进行收敛推理，LLM - Judge验证输出。

Result: 在六个数据集、五个领域和九个大语言模型上实验，DiCoRe始终优于先前的零样本、迁移学习和推理基线，平均F1值比最佳基线高4 - 7%。

Conclusion: DiCoRe是一个强大的零样本事件检测框架。

Abstract: Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.

</details>


### [655] [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)
*Chih-Kai Yang,Neo Ho,Yi-Jyun Lee,Hung-yi Lee*

Main category: cs.CL

TL;DR: 对大音频语言模型（LALMs）内部感知和识别听觉属性进行深入分析，发现属性信息变化规律及模型预测依赖方式，并展示增强方法。


<details>
  <summary>Details</summary>
Motivation: 理解LALMs内部机制对解释其行为和提升性能至关重要，因此对其内部感知和识别听觉属性进行分析。

Method: 对三个最先进的LALMs应用词汇投影，跟踪属性信息在各层和标记位置的演变。

Result: 识别失败时属性信息随层深度增加而减少，早期层解析属性与更高准确率相关，LALMs依赖查询听觉输入预测属性。基于发现展示了增强LALMs的方法。

Conclusion: 研究结果为听觉属性处理提供了见解，为未来改进奠定基础。

Abstract: Understanding the internal mechanisms of large audio-language models (LALMs)
is crucial for interpreting their behavior and improving performance. This work
presents the first in-depth analysis of how LALMs internally perceive and
recognize auditory attributes. By applying vocabulary projection on three
state-of-the-art LALMs, we track how attribute information evolves across
layers and token positions. We find that attribute information generally
decreases with layer depth when recognition fails, and that resolving
attributes at earlier layers correlates with better accuracy. Moreover, LALMs
heavily rely on querying auditory inputs for predicting attributes instead of
aggregating necessary information in hidden states at attribute-mentioning
positions. Based on our findings, we demonstrate a method to enhance LALMs. Our
results offer insights into auditory attribute processing, paving the way for
future improvements.

</details>


### [656] [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
*Bhavik Chandna,Zubair Bashir,Procheta Sen*

Main category: cs.CL

TL;DR: 采用机械可解释性方法分析大语言模型如GPT - 2和Llama2中的人口统计和性别偏差，研究偏差组件特性及移除效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在社会、人口统计和性别偏差，需分析其在模型内的结构表示。

Method: 采用机械可解释性方法，用不同指标识别导致偏差行为的内部边，进行系统消融实验。

Result: 偏差相关计算高度局部化，集中在少数层；微调设置会改变识别的组件；移除组件减少偏差输出但影响其他NLP任务。

Conclusion: 大语言模型偏差计算有局部化特点，移除偏差组件会因组件共享影响其他NLP任务。

Abstract: Large Language Models (LLMs) are known to exhibit social, demographic, and
gender biases, often as a consequence of the data on which they are trained. In
this work, we adopt a mechanistic interpretability approach to analyze how such
biases are structurally represented within models such as GPT-2 and Llama2.
Focusing on demographic and gender biases, we explore different metrics to
identify the internal edges responsible for biased behavior. We then assess the
stability, localization, and generalizability of these components across
dataset and linguistic variations. Through systematic ablations, we demonstrate
that bias-related computations are highly localized, often concentrated in a
small subset of layers. Moreover, the identified components change across
fine-tuning settings, including those unrelated to bias. Finally, we show that
removing these components not only reduces biased outputs but also affects
other NLP tasks, such as named entity recognition and linguistic acceptability
judgment because of the sharing of important components with these tasks.

</details>


### [657] [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
*Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 研究语言模型的上下文反事实推理，以线性回归任务为例，表明模型在受控设置下有反事实推理能力，发现Transformer性能影响因素，还为反事实故事生成提供初步证据。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的上下文反事实推理，即预测假设情景下变化的后果。

Method: 聚焦线性回归任务，该任务需进行噪声溯因，准确预测基于从事实观测中推断和复制上下文噪声。

Result: 语言模型在受控设置下能进行反事实推理，反事实推理可简化为上下文观测的转换；发现自注意力、模型深度和预训练数据多样性驱动Transformer性能；Transformer可对序列数据进行噪声溯因。

Conclusion: 研究为反事实推理提供了见解，代码开源，且研究结果可能扩展到故事生成等领域。

Abstract: Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .

</details>


### [658] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: 提出Generative Embedding large language Model (GEM)方法让解码器大模型生成文本嵌入，应用于两个流行LLM家族评估效果好。


<details>
  <summary>Details</summary>
Motivation: 许多应用依赖单独嵌入模型生成文本嵌入，使系统复杂并导致理解差异，需解决此局限。

Method: 在文本中插入特殊标记，通过操作注意力掩码生成文本摘要嵌入，可集成到现有LLM的后训练或微调阶段。

Result: 在文本嵌入基准测试（MTEB）上显著提升原LLM，对NLP基准测试（MMLU）影响极小。

Conclusion: 该方法能赋予LLM先进文本嵌入能力并保持原有NLP性能。

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [659] [Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)
*Nan Huo,Jinyang Li,Bowen Qin,Ge Qu,Xiaolong Li,Xiaodong Li,Chenhao Ma,Reynold Cheng*

Main category: cs.CL

TL;DR: 提出Micro - Act框架解决RAG系统知识冲突问题，实验显示其在QA任务上表现出色且具实用价值。


<details>
  <summary>Details</summary>
Motivation: RAG系统存在知识冲突问题，影响下游任务表现，现有方法会使LLMs难以识别和缓解不一致性。

Method: 提出具有分层动作空间的Micro - Act框架，自动感知上下文复杂性，将知识源分解为细粒度比较序列。

Result: 在五个基准数据集上，Micro - Act在所有数据集和3种冲突类型上均显著提高QA准确率，尤其在时间和语义类型上，同时在非冲突问题上表现稳健。

Conclusion: Micro - Act框架在解决RAG系统知识冲突方面有效，有实际应用价值。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge
Conflicts, where retrieved external knowledge contradicts the inherent,
parametric knowledge of large language models (LLMs). It adversely affects
performance on downstream tasks such as question answering (QA). Existing
approaches often attempt to mitigate conflicts by directly comparing two
knowledge sources in a side-by-side manner, but this can overwhelm LLMs with
extraneous or lengthy contexts, ultimately hindering their ability to identify
and mitigate inconsistencies. To address this issue, we propose Micro-Act a
framework with a hierarchical action space that automatically perceives context
complexity and adaptively decomposes each knowledge source into a sequence of
fine-grained comparisons. These comparisons are represented as actionable
steps, enabling reasoning beyond the superficial context. Through extensive
experiments on five benchmark datasets, Micro-Act consistently achieves
significant increase in QA accuracy over state-of-the-art baselines across all
5 datasets and 3 conflict types, especially in temporal and semantic types
where all baselines fail significantly. More importantly, Micro-Act exhibits
robust performance on non-conflict questions simultaneously, highlighting its
practical value in real-world RAG applications.

</details>


### [660] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: 提出HTC - CLIP模型结合两种现有分层文本分类方法，在两个公开基准数据集上Macro F1分数提升0.99 - 2.37%。


<details>
  <summary>Details</summary>
Motivation: 现有分层文本分类（HTC）模型有分别编码标签层次和在文本编码器中引导标签层次结构两种方法，二者互补但未有效结合。

Method: 提出HTC - CLIP模型，使用对比学习学习层次感知文本表示和文本引导路径层次表示，训练时学习两组类概率分布，推理时使用两者池化输出。

Result: 在两个公开基准数据集上，HTC - CLIP相比现有模型Macro F1分数提升0.99 - 2.37%。

Conclusion: 两种现有方法可有效整合到一个架构中以提升性能。

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [661] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: 提出嵌入器兼分类器模型架构，以少量标注样本跨域推广模型，实验显示准确率提升20 - 23%。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型依赖大量标注的特定领域数据，跨域数据不足，需要构建能以少量标注跨域泛化的意图分类器。

Method: 提出嵌入器兼分类器模型架构，采用监督微调与各向同性正则化训练特定领域句子嵌入器，使用多语言知识蒸馏策略跨域泛化，结合线性分类器用于新领域。

Result: 在加拿大和墨西哥电子商务客户服务数据集的少样本意图检测实验中，准确率比现有模型提高20 - 23%。

Conclusion: 提出的模型架构能有效利用少量标注样本，实现跨域泛化，提升意图分类准确率。

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [662] [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
*Deepak Pandita,Tharindu Cyril Weerasooriya,Ankit Parag Shah,Christopher M. Homan,Wei Wei*

Main category: cs.CL

TL;DR: 介绍ProRefine方法优化推理时提示，在数学推理数据集上表现出色，提升AI效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工作流中因提示设计不佳导致的错误传播和性能不佳问题，提高系统可靠性和可扩展性。

Method: 引入ProRefine方法，利用大语言模型的文本反馈，动态优化多步推理任务的提示，无需额外训练和真实标签。

Result: 在五个数学推理基准数据集上，ProRefine显著超越零样本思维链基线3至37个百分点，使小模型能达到大模型性能。

Conclusion: ProRefine方法可提升准确率，实现高效可扩展的AI部署，推动高性能AI的普及。

Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.

</details>


### [663] [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)
*Taha Entesari,Arman Hatami,Rinat Khaziev,Anil Ramakrishna,Mahyar Fazlyab*

Main category: cs.CL

TL;DR: 提出将大语言模型遗忘问题作为约束优化问题的新方法，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法将遗忘和保留作为正则化权衡，导致优化不稳定和保留数据性能下降。

Method: 将遗忘问题构建为约束优化问题，用新的对数边际扁平化损失强制遗忘，通过硬约束保留数据，用可扩展的原始对偶算法求解。

Result: 在TOFU和MUSE基准测试中，该方法与或超过了现有最先进的基线。

Conclusion: 该方法能有效去除目标信息，同时保留下游实用性。

Abstract: Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.

</details>


### [664] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: 本文分析两种水印方法对大语言模型核心对齐属性的影响，揭示退化模式，提出Alignment Resampling方法恢复对齐，兼顾水印检测和模型对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型水印技术在真实性、安全性和有用性方面的影响缺乏审视，需系统分析。

Method: 对两种水印方法在四个对齐的大语言模型上进行实验，提出Alignment Resampling推理时间采样方法，使用外部奖励模型恢复对齐。

Result: 发现两种退化模式，通过2 - 4次水印生成采样可恢复或超越基线对齐分数，AR方法在两种水印方法中成功恢复基线对齐，保持强水印可检测性。

Conclusion: 揭示水印强度和模型对齐的关键平衡，提供实用的推理时间解决方案。

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


### [665] [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
*Thao Nguyen,Yang Li,Olga Golovneva,Luke Zettlemoyer,Sewoong Oh,Ludwig Schmidt,Xian Li*

Main category: cs.CL

TL;DR: 为解决预训练扩展的数据墙问题，提出REWIRE方法，通过转换和回收现有过滤过程中丢弃的数据来丰富低质量文档。实验表明，混合高质量原始文本和重写文本能提升模型性能，该方法优于其他生成合成数据的方法。


<details>
  <summary>Details</summary>
Motivation: 现有预训练依赖大量网络爬取数据，但自然数据增长速率跟不上计算资源供应，且高质量文本有限，为解决预训练扩展的数据墙问题。

Method: 提出REWIRE方法，即REcycling the Web with guIded REwrite，用于丰富低质量文档，增加合成数据在最终预训练集中的占比。

Result: 在DCLM基准测试的1B、3B和7B规模实验中，混合文本比仅使用过滤后的网络数据在22个不同任务上分别提升1.0、1.3和2.5个百分点；比使用2倍网络数据更有效；约82%的混合文本来自低质量文档转换；优于其他生成合成数据的相关方法。

Conclusion: 回收网络文本是一种简单有效的预训练数据扩展方法。

Abstract: Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.

</details>


### [666] [Controlling Summarization Length Through EOS Token Weighting](https://arxiv.org/abs/2506.05017)
*Zeno Belligoli,Emmanouil Stergiadis,Eran Fainman,Ilya Gusev*

Main category: cs.CL

TL;DR: 提出简单方法控制自动文本摘要长度，不影响摘要质量且与预训练模型兼容。


<details>
  <summary>Details</summary>
Motivation: 现有控制文本生成长度的方法需复杂模型改动，与预训练模型兼容性差。

Method: 在交叉熵损失计算中提高正确预测EOS令牌的重要性。

Result: 在编码器 - 解码器和现代GPT风格大语言模型上测试，该方法能控制生成长度，且常不影响摘要质量。

Conclusion: 该方法能有效控制生成文本长度，不依赖架构和解码算法，与其他推理时间技术正交。

Abstract: Controlling the length of generated text can be crucial in various
text-generation tasks, including summarization. Existing methods often require
complex model alterations, limiting compatibility with pre-trained models. We
address these limitations by developing a simple approach for controlling the
length of automatic text summaries by increasing the importance of correctly
predicting the EOS token in the cross-entropy loss computation. The proposed
methodology is agnostic to architecture and decoding algorithms and orthogonal
to other inference-time techniques to control the generation length. We tested
it with encoder-decoder and modern GPT-style LLMs, and show that this method
can control generation length, often without affecting the quality of the
summary.

</details>


### [667] [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://arxiv.org/abs/2506.05209)
*Nikhil Kandpal,Brian Lester,Colin Raffel,Sebastian Majstorovic,Stella Biderman,Baber Abbasi,Luca Soldaini,Enrico Shippole,A. Feder Cooper,Aviya Skowron,John Kirchenbauer,Shayne Longpre,Lintang Sutawika,Alon Albalak,Zhenlin Xu,Guilherme Penedo,Loubna Ben Allal,Elie Bakouch,John David Pressman,Honglu Fan,Dashiell Stander,Guangyu Song,Aaron Gokaslan,Tom Goldstein,Brian R. Bartoldson,Bhavya Kailkhura,Tyler Murray*

Main category: cs.CL

TL;DR: 为解决大语言模型使用未授权文本训练的问题，本文收集、整理并发布八TB的开放许可文本数据集Common Pile v0.1，还训练两个模型验证其效果，且发布相关代码和检查点。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型使用未授权文本训练带来的知识产权侵权和伦理问题，且之前开放许可文本数据集过小或质量低，无法训练出高性能模型。

Method: 收集、整理30个来源的开放许可文本，构建Common Pile v0.1数据集；用该数据集训练两个70亿参数的大语言模型Comma v0.1 - 1T和Comma v0.1 - 2T。

Result: 两个模型在相似计算预算下，与使用未授权文本训练的模型（如Llama 1和2 7B）达到了有竞争力的性能。

Conclusion: 发布的Common Pile v0.1数据集可用于训练高性能大语言模型，且发布代码、训练混合和检查点利于后续研究。

Abstract: Large language models (LLMs) are typically trained on enormous quantities of
unlicensed text, a practice that has led to scrutiny due to possible
intellectual property infringement and ethical concerns. Training LLMs on
openly licensed text presents a first step towards addressing these issues, but
prior data collection efforts have yielded datasets too small or low-quality to
produce performant LLMs. To address this gap, we collect, curate, and release
the Common Pile v0.1, an eight terabyte collection of openly licensed text
designed for LLM pretraining. The Common Pile comprises content from 30 sources
that span diverse domains including research papers, code, books,
encyclopedias, educational materials, audio transcripts, and more. Crucially,
we validate our efforts by training two 7 billion parameter LLMs on text from
the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion
tokens respectively. Both models attain competitive performance to LLMs trained
on unlicensed text with similar computational budgets, such as Llama 1 and 2
7B. In addition to releasing the Common Pile v0.1 itself, we also release the
code used in its creation as well as the training mixture and checkpoints for
the Comma v0.1 models.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [668] [What does making money have to do with crime?: A dive into the National Crime Victimization survey](https://arxiv.org/abs/2506.04240)
*Sydney Anuyah*

Main category: physics.soc-ph

TL;DR: 通过犯罪数据分析发现收入/教育降低暴力犯罪风险，年轻男性/少数族裔风险更高，不同地区需差异化干预策略


<details>
  <summary>Details</summary>
Motivation: 探究社会经济因素（收入/教育/就业）与人口特征（年龄/性别/种族）如何影响暴力犯罪与财产犯罪的类型分布

Method: 使用1992-2022年犯罪调查数据，通过逻辑回归模型分组分析（Group A仅社会经济因素，Group B加入人口控制变量），采用F1分数评估模型效能

Result: 高收入/教育群体暴力犯罪率下降58%（OR=0.42），郊区模型预测力最强（F1=0.59），农村犯罪预测R²仅0.21，年轻男性暴力犯罪风险比女性高2.3倍

Conclusion: 需实施精准防控：大都市侧重教育投资（如社区大学计划），农村强化经济支持（就业补贴），针对不同人口群体设计犯罪预防方案（如少数族裔青年职业培训）

Abstract: In this short article, I leverage the National Crime Victimization Survey
from 1992 to 2022 to examine how income, education, employment, and key
demographic factors shape the type of crime victims experience (violent vs
property). Using balanced classification splits and logistic regression models
evaluated by F1-score, there is an isolation of the socioeconomic drivers of
victimization "Group A" models and then an introduction of demographic factors
such as age, gender, race, and marital status controls called "Group B" models.
The results consistently proves that higher income and education lower the odds
of violent relative to property crime, while men younger individuals and racial
minorities face disproportionately higher violentcrime risks. On the geographic
spectrum, the suburban models achieve the strongest predictive performance with
an accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education
and employment predictors and crime in rural areas are still unpredictable
using these current factors. The patterns found in this study shows the need
for specific interventions like educational investments in metropolitan
settings economic support in rural communities and demographicaware prevention
strategies.

</details>


### [669] [What does making money have to do with crime?: A dive into the National Crime Victimization survey](https://arxiv.org/abs/2506.04240)
*Sydney Anuyah*

Main category: physics.soc-ph

TL;DR: 利用1992 - 2022年全国犯罪受害调查数据，分析社会经济和人口因素对犯罪类型的影响，得出不同因素与犯罪类型的关系及不同地区模型预测表现，提出针对性干预措施。


<details>
  <summary>Details</summary>
Motivation: 探究收入、教育、就业和关键人口因素如何影响犯罪受害者经历的犯罪类型（暴力犯罪与财产犯罪）。

Method: 使用平衡分类拆分和逻辑回归模型，以F1分数评估，先构建“Group A”模型分离受害的社会经济驱动因素，再引入年龄、性别、种族和婚姻状况等人口因素构建“Group B”模型。

Result: 高收入和高教育水平降低暴力犯罪相对财产犯罪的几率；男性、年轻人和少数族裔面临更高暴力犯罪风险；郊区模型预测性能最强，城市地区添加教育和就业预测因素有益，农村地区犯罪难以用现有因素预测。

Conclusion: 需要采取特定干预措施，如大城市的教育投资、农村社区的经济支持和考虑人口因素的预防策略。

Abstract: In this short article, I leverage the National Crime Victimization Survey
from 1992 to 2022 to examine how income, education, employment, and key
demographic factors shape the type of crime victims experience (violent vs
property). Using balanced classification splits and logistic regression models
evaluated by F1-score, there is an isolation of the socioeconomic drivers of
victimization "Group A" models and then an introduction of demographic factors
such as age, gender, race, and marital status controls called "Group B" models.
The results consistently proves that higher income and education lower the odds
of violent relative to property crime, while men younger individuals and racial
minorities face disproportionately higher violentcrime risks. On the geographic
spectrum, the suburban models achieve the strongest predictive performance with
an accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education
and employment predictors and crime in rural areas are still unpredictable
using these current factors. The patterns found in this study shows the need
for specific interventions like educational investments in metropolitan
settings economic support in rural communities and demographicaware prevention
strategies.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [670] [kTULA: A Langevin sampling algorithm with improved KL bounds under super-linear log-gradients](https://arxiv.org/abs/2506.04878)
*Iosif Lytras,Sotirios Sabanis,Ying Zhang*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by applications in deep learning, where the global Lipschitz
continuity condition is often not satisfied, we examine the problem of sampling
from distributions with super-linearly growing log-gradients. We propose a
novel tamed Langevin dynamics-based algorithm, called kTULA, to solve the
aforementioned sampling problem, and provide a theoretical guarantee for its
performance. More precisely, we establish a non-asymptotic convergence bound in
Kullback-Leibler (KL) divergence with the best-known rate of convergence equal
to $2-\overline{\epsilon}$, $\overline{\epsilon}>0$, which significantly
improves relevant results in existing literature. This enables us to obtain an
improved non-asymptotic error bound in Wasserstein-2 distance, which can be
used to further derive a non-asymptotic guarantee for kTULA to solve the
associated optimization problems. To illustrate the applicability of kTULA, we
apply the proposed algorithm to the problem of sampling from a high-dimensional
double-well potential distribution and to an optimization problem involving a
neural network. We show that our main results can be used to provide
theoretical guarantees for the performance of kTULA.

</details>


### [671] [kTULA: A Langevin sampling algorithm with improved KL bounds under super-linear log-gradients](https://arxiv.org/abs/2506.04878)
*Iosif Lytras,Sotirios Sabanis,Ying Zhang*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by applications in deep learning, where the global Lipschitz
continuity condition is often not satisfied, we examine the problem of sampling
from distributions with super-linearly growing log-gradients. We propose a
novel tamed Langevin dynamics-based algorithm, called kTULA, to solve the
aforementioned sampling problem, and provide a theoretical guarantee for its
performance. More precisely, we establish a non-asymptotic convergence bound in
Kullback-Leibler (KL) divergence with the best-known rate of convergence equal
to $2-\overline{\epsilon}$, $\overline{\epsilon}>0$, which significantly
improves relevant results in existing literature. This enables us to obtain an
improved non-asymptotic error bound in Wasserstein-2 distance, which can be
used to further derive a non-asymptotic guarantee for kTULA to solve the
associated optimization problems. To illustrate the applicability of kTULA, we
apply the proposed algorithm to the problem of sampling from a high-dimensional
double-well potential distribution and to an optimization problem involving a
neural network. We show that our main results can be used to provide
theoretical guarantees for the performance of kTULA.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [672] [The Determinants of Net Interest Margin in the Turkish Banking Sector: Does Bank Ownership Matter? Central Bank Digital Currencies](https://arxiv.org/abs/2506.04384)
*Fatih Kansoy*

Main category: econ.GN

TL;DR: 该研究通过2001-2012年土耳其商业银行数据，实证分析了不同所有权结构下净息差的影响因素差异。


<details>
  <summary>Details</summary>
Motivation: 探讨银行所有权结构（外资/国有/私营）对净息差决定机制的影响差异，揭示不同所有制银行的经营特性。

Method: 采用银行层面面板数据，通过实证模型分析运营多样性、信用风险、运营成本等变量对净息差的影响，并分组比较不同所有制银行。

Result: 信用风险、银行规模等核心因素在各类银行中影响程度不同；运营多样性、运营成本则呈现跨所有制一致性影响。高效银行和价格稳定环境有助于降低净息差。

Conclusion: 银行所有权结构显著调节部分关键因素对净息差的作用机制，监管政策需考虑所有制差异对货币政策传导的异质性影响。

Abstract: This research presented an empirical investigation of the determinants of the
net interest margin in Turkish Banking sector with a particular emphasis on the
bank ownership structure. This study employed a unique bank-level dataset
covering Turkey`s commercial banking sector for the 2001-2012. Our main results
are as follows. Operation diversity, credit risk and operating costs are
important determinants of margin in Turkey. More efficient banks exhibit lower
margin and also price stability contributes to lower margin. The effect of
principal determinants such as credit risk, bank size, market concentration and
inflation vary across foreign-owned, state-controlled and private banks. At the
same time, the impacts of implicit interest payment, operation diversity and
operating cost are homogeneous across all banks

</details>


### [673] [The Determinants of Net Interest Margin in the Turkish Banking Sector: Does Bank Ownership Matter? Central Bank Digital Currencies](https://arxiv.org/abs/2506.04384)
*Fatih Kansoy*

Main category: econ.GN

TL;DR: 研究通过2001-2012年土耳其商业银行数据，实证分析了不同所有权结构（外资/国有/私营）对银行净息差的影响机制及差异。


<details>
  <summary>Details</summary>
Motivation: 探讨银行所有权结构如何影响净息差决定因素，这对理解不同性质银行的经营策略差异和制定针对性监管政策具有现实意义。

Method: 使用土耳其商业银行2001-2012年面板数据，建立计量经济模型进行实证分析，重点关注外资银行、国有银行和私营银行的异质性效应。

Result: 运营多元化、信用风险和运营成本是核心影响因素（外资/国有/私营银行对市场集中度等变量的敏感度存在显著差异，而隐性利息支付等变量则呈现同质化影响）。

Conclusion: 所有权结构是调节净息差决定机制的重要维度，监管政策需考虑银行所有制差异，同时提升运营效率和保持价格稳定有助于降低净息差。

Abstract: This research presented an empirical investigation of the determinants of the
net interest margin in Turkish Banking sector with a particular emphasis on the
bank ownership structure. This study employed a unique bank-level dataset
covering Turkey`s commercial banking sector for the 2001-2012. Our main results
are as follows. Operation diversity, credit risk and operating costs are
important determinants of margin in Turkey. More efficient banks exhibit lower
margin and also price stability contributes to lower margin. The effect of
principal determinants such as credit risk, bank size, market concentration and
inflation vary across foreign-owned, state-controlled and private banks. At the
same time, the impacts of implicit interest payment, operation diversity and
operating cost are homogeneous across all banks

</details>


### [674] [Becoming Immutable: How Ethereum is Made](https://arxiv.org/abs/2506.04940)
*Andrea Canidio,Vabuk Pahari*

Main category: econ.GN

TL;DR: 分析以太坊区块数据发现：85%获胜区块费用来自单一构建者的独占交易，大量延迟交易未及时上链，套利机器人在CEX交易存在3.4-4.2基点价格优势


<details>
  <summary>Details</summary>
Motivation: 探究以太坊区块构建中交易分配机制的有效性，揭示套利机器人的市场影响及区块构建者间竞争动态

Method: 分析2024年12月3日8分钟内以太坊链上数据（含38个获胜区块/15,097提议区块/10,793笔独特交易/2.38M交易-区块配对），采用统计分析与套利策略逆向工程

Result: 1. 获胜区块85%费用源于独占交易
2. 失败构建者提议的交易延迟率显著
3. 套利机器人在CEX的USDC/WETH交易价格较Binance优3.4-4.2基点

Conclusion: 当前区块构建机制存在信息不对称问题，套利机会持续影响市场效率，需改进区块提议机制以提升网络公平性

Abstract: We analyze blocks proposed for inclusion in the Ethereum blockchain during 8
minutes on December 3rd, 2024. Our dataset comprises 38 winning blocks, 15,097
proposed blocks, 10,793 unique transactions, and 2,380,014 transaction-block
pairings. We find that exclusive transactions--transactions present only in
blocks proposed by a single builder--account for 85% of the fees paid by all
transactions included in winning blocks. We also find that a surprisingly large
number of user transactions are delayed: although proposed during a bidding
cycle, they are not included in the corresponding winning block. Many such
delayed transactions are exclusive to a losing builder. We also identify two
arbitrage bots trading between decentralized (DEX) and centralized exchanges
(CEX). By examining their bidding dynamics, we estimate that the implied price
at which these bots trade USDC/WETH and USDT/WETH on CEXes is between 3.4 and
4.2 basis points better than the contemporaneous price reported on Binance.

</details>


### [675] [The Determinants of Net Interest Margin in the Turkish Banking Sector: Does Bank Ownership Matter? Central Bank Digital Currencies](https://arxiv.org/abs/2506.04384)
*Fatih Kansoy*

Main category: econ.GN

TL;DR: 研究土耳其银行业净息差决定因素，尤其关注银行所有权结构，发现不同因素对净息差影响及在不同类型银行的差异。


<details>
  <summary>Details</summary>
Motivation: 探究土耳其银行业净息差的决定因素，重点关注银行所有权结构的影响。

Method: 采用涵盖2001 - 2012年土耳其商业银行业的独特银行层面数据集进行实证研究。

Result: 运营多样性、信用风险和运营成本是土耳其净息差重要决定因素；更高效银行息差更低，价格稳定也降低息差；主要决定因素影响在不同所有权银行有差异，部分因素影响在所有银行一致。

Conclusion: 不同因素对土耳其银行业净息差有不同影响，且受银行所有权结构制约。

Abstract: This research presented an empirical investigation of the determinants of the
net interest margin in Turkish Banking sector with a particular emphasis on the
bank ownership structure. This study employed a unique bank-level dataset
covering Turkey`s commercial banking sector for the 2001-2012. Our main results
are as follows. Operation diversity, credit risk and operating costs are
important determinants of margin in Turkey. More efficient banks exhibit lower
margin and also price stability contributes to lower margin. The effect of
principal determinants such as credit risk, bank size, market concentration and
inflation vary across foreign-owned, state-controlled and private banks. At the
same time, the impacts of implicit interest payment, operation diversity and
operating cost are homogeneous across all banks

</details>


### [676] [Becoming Immutable: How Ethereum is Made](https://arxiv.org/abs/2506.04940)
*Andrea Canidio,Vabuk Pahari*

Main category: econ.GN

TL;DR: 分析2024年12月3日8分钟内以太坊区块链提议的区块，发现独家交易占获胜区块费用85%，大量用户交易延迟，还识别出套利机器人及交易价格优势。


<details>
  <summary>Details</summary>
Motivation: 了解以太坊区块链中区块交易的特征、费用构成、交易延迟情况以及套利交易情况。

Method: 分析特定时间段内以太坊区块链提议的区块，处理包含获胜区块、提议区块、交易等数据。

Result: 独家交易占获胜区块费用85%；大量用户交易延迟，很多延迟交易属于失败构建者；识别出两个套利机器人，其在CEX上交易USDC/WETH和USDT/WETH的隐含价格比币安同期价格好3.4 - 4.2个基点。

Conclusion: 以太坊区块链交易存在独家交易主导费用、交易延迟问题，且存在套利机会。

Abstract: We analyze blocks proposed for inclusion in the Ethereum blockchain during 8
minutes on December 3rd, 2024. Our dataset comprises 38 winning blocks, 15,097
proposed blocks, 10,793 unique transactions, and 2,380,014 transaction-block
pairings. We find that exclusive transactions--transactions present only in
blocks proposed by a single builder--account for 85% of the fees paid by all
transactions included in winning blocks. We also find that a surprisingly large
number of user transactions are delayed: although proposed during a bidding
cycle, they are not included in the corresponding winning block. Many such
delayed transactions are exclusive to a losing builder. We also identify two
arbitrage bots trading between decentralized (DEX) and centralized exchanges
(CEX). By examining their bidding dynamics, we estimate that the implied price
at which these bots trade USDC/WETH and USDT/WETH on CEXes is between 3.4 and
4.2 basis points better than the contemporaneous price reported on Binance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [677] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 深度学习系统通过改进EfficientNetV2架构与注意力机制，在乳腺癌钼靶检测中展现优异性能（AUROC 0.89-0.96），临床验证显示高医师符合率（78-84%）和系统可用性评分（69-74）。


<details>
  <summary>Details</summary>
Motivation: 提升乳腺癌钼靶筛查效率，解决临床工作流中筛查效率瓶颈问题

Method: 基于改进EfficientNetV2架构开发深度学习模型，使用泰国医疗中心数据训练，并在三个数据集（9,421常规/883活检/761跨院）验证性能

Result: 癌症检测AUROC达0.89-0.96；病灶定位LLF/NLF指标优异；临床符合率78-84%；医师接受率89-97%；系统可用性评分69-74分

Conclusion: 该系统显著提升乳腺癌筛查准确性，具备临床推广潜力，特别在跨机构应用场景中展现良好泛化能力

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [678] [Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.04453)
*Hasin Us Sami,Swapneel Sen,Amit K. Roy-Chowdhury,Srikanth V. Krishnamurthy,Basak Guler*

Main category: eess.IV

TL;DR: 联邦学习中的参数高效微调（PEFT）存在隐私泄露风险，攻击者可通过适配器梯度反演用户本地数据


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调机制（如适配器）在提升训练效率时忽视了隐私保护需求，需验证其实际隐私安全性

Method: 提出针对适配器模块的梯度反演攻击方法，通过恶意设计的预训练模型和可训练模块重构用户数据

Result: 实验证明攻击可高保真重建用户大批量微调图像（代码已开源：https://github.com/info-ucr/PEFTLeak）

Conclusion: PEFT机制亟需配套隐私保护方案，本研究为后续联邦学习中轻量化微调技术的安全设计指明方向

Abstract: Federated learning (FL) allows multiple data-owners to collaboratively train
machine learning models by exchanging local gradients, while keeping their
private data on-device. To simultaneously enhance privacy and training
efficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale
pretrained models has gained substantial attention in FL. While keeping a
pretrained (backbone) model frozen, each user fine-tunes only a few lightweight
modules to be used in conjunction, to fit specific downstream applications.
Accordingly, only the gradients with respect to these lightweight modules are
shared with the server. In this work, we investigate how the privacy of the
fine-tuning data of the users can be compromised via a malicious design of the
pretrained model and trainable adapter modules. We demonstrate gradient
inversion attacks on a popular PEFT mechanism, the adapter, which allow an
attacker to reconstruct local data samples of a target user, using only the
accessible adapter gradients. Via extensive experiments, we demonstrate that a
large batch of fine-tuning images can be retrieved with high fidelity. Our
attack highlights the need for privacy-preserving mechanisms for PEFT, while
opening up several future directions. Our code is available at
https://github.com/info-ucr/PEFTLeak.

</details>


### [679] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 研究提出用改进EfficientNetV2架构和增强注意力机制的深度学习系统进行乳腺癌检测，在多个数据集上表现良好，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发用于乳腺钼靶图像中乳腺癌检测的深度学习系统，辅助乳腺钼靶图像解读，提高乳腺癌筛查效率。

Method: 使用改进的EfficientNetV2架构和增强注意力机制构建模型，在泰国一家主要医疗中心的钼靶图像上训练，在三个不同数据集上验证。

Result: 模型在不同数据集上癌症检测的AUROC分别为0.89、0.96和0.94；病灶定位能力表现良好；与放射科医生的一致性测试显示高度一致；专家放射科医生接受率高；系统可用性评分显示有良好的临床接受度。

Conclusion: 该模型在辅助乳腺钼靶图像解读方面有效，有潜力改进临床实践中的乳腺癌筛查工作流程。

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [680] [Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.04453)
*Hasin Us Sami,Swapneel Sen,Amit K. Roy-Chowdhury,Srikanth V. Krishnamurthy,Basak Guler*

Main category: eess.IV

TL;DR: 本文研究联邦学习中参数高效微调（PEFT）机制的隐私风险，通过梯度反转攻击展示攻击者可利用适配器梯度重构用户本地数据样本，强调需要隐私保护机制。


<details>
  <summary>Details</summary>
Motivation: 探究联邦学习中参数高效微调机制下用户微调数据的隐私是否会因预训练模型和可训练适配器模块的恶意设计而受损。

Method: 对流行的PEFT机制——适配器进行梯度反转攻击，仅使用可访问的适配器梯度来重构目标用户的本地数据样本。

Result: 通过大量实验，可高保真地检索到一大批微调图像。

Conclusion: 凸显了PEFT需要隐私保护机制，并指出了未来研究方向。

Abstract: Federated learning (FL) allows multiple data-owners to collaboratively train
machine learning models by exchanging local gradients, while keeping their
private data on-device. To simultaneously enhance privacy and training
efficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale
pretrained models has gained substantial attention in FL. While keeping a
pretrained (backbone) model frozen, each user fine-tunes only a few lightweight
modules to be used in conjunction, to fit specific downstream applications.
Accordingly, only the gradients with respect to these lightweight modules are
shared with the server. In this work, we investigate how the privacy of the
fine-tuning data of the users can be compromised via a malicious design of the
pretrained model and trainable adapter modules. We demonstrate gradient
inversion attacks on a popular PEFT mechanism, the adapter, which allow an
attacker to reconstruct local data samples of a target user, using only the
accessible adapter gradients. Via extensive experiments, we demonstrate that a
large batch of fine-tuning images can be retrieved with high fidelity. Our
attack highlights the need for privacy-preserving mechanisms for PEFT, while
opening up several future directions. Our code is available at
https://github.com/info-ucr/PEFTLeak.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [681] [BEAR: BGP Event Analysis and Reporting](https://arxiv.org/abs/2506.04514)
*Hanqing Li,Melania Fedeli,Vinay Kolar,Diego Klabjan*

Main category: cs.NI

TL;DR: BEAR框架利用大语言模型自动生成BGP异常事件分析报告，提升网络管理效率


<details>
  <summary>Details</summary>
Motivation: 现有BGP异常检测方法依赖专家人工分析AS关系和历史事件，存在解释成本高、响应速度慢的问题

Method: 通过多步推理将表格数据转为文本描述，并开发基于LLM的合成数据生成框架解决数据不足问题

Result: 在真实和合成数据集上实现100%准确率，优于思维链和上下文学习方法

Conclusion: 该研究首次实现BGP异常事件解释的自动化，为网络运维提供可操作的洞察

Abstract: The Internet comprises of interconnected, independently managed Autonomous
Systems (AS) that rely on the Border Gateway Protocol (BGP) for inter-domain
routing. BGP anomalies--such as route leaks and hijacks--can divert traffic
through unauthorized or inefficient paths, jeopardizing network reliability and
security. Although existing rule-based and machine learning methods can detect
these anomalies using structured metrics, they still require experts with
in-depth BGP knowledge of, for example, AS relationships and historical
incidents, to interpret events and propose remediation. In this paper, we
introduce BEAR (BGP Event Analysis and Reporting), a novel framework that
leverages large language models (LLMs) to automatically generate comprehensive
reports explaining detected BGP anomaly events. BEAR employs a multi-step
reasoning process that translates tabular BGP data into detailed textual
narratives, enhancing interpretability and analytical precision. To address the
limited availability of publicly documented BGP anomalies, we also present a
synthetic data generation framework powered by LLMs. Evaluations on both real
and synthetic datasets demonstrate that BEAR achieves 100% accuracy,
outperforming Chain-of-Thought and in-context learning baselines. This work
pioneers an automated approach for explaining BGP anomaly events, offering
valuable operational insights for network management.

</details>


### [682] [Intelligent Channel Allocation for IEEE 802.11be Multi-Link Operation: When MAB Meets LLM](https://arxiv.org/abs/2506.04594)
*Shumin Lian,Jingwen Tong,Jun Zhang,Liqun Fu*

Main category: cs.NI

TL;DR: 提出BAI-MCTS和LLM-BAI-MCTS算法，解决WiFi 7密集网络中动态信道分配问题，显著提升收敛速度


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于静态网络优化，缺乏对密集动态环境下MLO信道分配的研究需求

Method: 建立组合优化模型，采用MAB框架在线学习，创新性提出BAI-MCTS算法并融合LLM实现智能信道分配

Result: BAI-MCTS达到最优值98%时收敛速度提升50.44%，LLM-BAI-MCTS在密集网络收敛速率提升63.32%

Conclusion: 结合蒙特卡洛树搜索与LLM的算法显著提升密集网络信道分配效率，理论分析验证了算法的有效性

Abstract: WiFi networks have achieved remarkable success in enabling seamless
communication and data exchange worldwide. The IEEE 802.11be standard, known as
WiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that
enables devices to establish multiple simultaneous connections across different
bands and channels. While MLO promises substantial improvements in network
throughput and latency reduction, it presents significant challenges in channel
allocation, particularly in dense network environments. Current research has
predominantly focused on performance analysis and throughput optimization
within static WiFi 7 network configurations. In contrast, this paper addresses
the dynamic channel allocation problem in dense WiFi 7 networks with MLO
capabilities. We formulate this challenge as a combinatorial optimization
problem, leveraging a novel network performance analysis mechanism. Given the
inherent lack of prior network information, we model the problem within a
Multi-Armed Bandit (MAB) framework to enable online learning of optimal channel
allocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree
Search (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing
upper bounds for both sample complexity and error probability. To further
reduce sample complexity and enhance generalizability across diverse network
scenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic
channel allocation problem by integrating the Large Language Model (LLM) into
the BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS
algorithm achieves a convergence rate approximately $50.44\%$ faster than the
state-of-the-art algorithms when reaching $98\%$ of the optimal value. Notably,
the convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\%$
in dense networks.

</details>


### [683] [Towards Network Data Analytics in 5G Systems and Beyond](https://arxiv.org/abs/2506.04860)
*Marcos Lima Romero,Ricardo Suyama*

Main category: cs.NI

TL;DR: 研究探讨移动运营商如何通过NWDAF实现数据价值转化，分析70+文献提出新用例以突破学术研究局限


<details>
  <summary>Details</summary>
Motivation: 移动运营商在数据利用上落后于OTT企业，现有学术研究受限于数据集导致用例和方法创新不足，需探索NWDAF的货币化潜力

Method: 采用文献分析法梳理70余篇研究成果，结合产业需求提出两个新型网络数据分析应用场景

Result: 揭示当前研究聚焦特定方法/用例的局限性，成功设计出能增强网络运营效率和商业变现的创新用例

Conclusion: 通过填补研究空白推动NWDAF应用落地，帮助运营商突破连接服务同质化，实现数据驱动的增值服务创新

Abstract: Data has become a critical asset in the digital economy, yet it remains
underutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT)
players that lead global market valuations. To move beyond the commoditization
of connectivity and deliver greater value to customers, data analytics emerges
as a strategic enabler. Using data efficiently is essential for unlocking new
service opportunities, optimizing operational efficiency, and mitigating
operational and business risks. Since Release 15, the 3rd Generation
Partnership Project (3GPP) has introduced the Network Data Analytics Function
(NWDAF) to provide powerful insights and predictions using data collected
across mobile networks, supporting both user-centric and network-oriented use
cases. However, academic research has largely focused on a limited set of
methods and use cases, driven by the availability of datasets, restricting
broader exploration. This study analyzes trends and gaps in more than 70
articles and proposes two novel use cases to promote the adoption of NWDAF and
explore its potential for monetization.

</details>


### [684] [BEAR: BGP Event Analysis and Reporting](https://arxiv.org/abs/2506.04514)
*Hanqing Li,Melania Fedeli,Vinay Kolar,Diego Klabjan*

Main category: cs.NI

TL;DR: 本文提出BEAR框架利用大语言模型自动生成BGP异常事件报告，还用LLM构建合成数据生成框架，评估显示其准确率达100%。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则和机器学习的BGP异常检测方法需专家深度知识解读和提出补救措施，因此需要新方法。

Method: 引入BEAR框架，采用多步推理过程将BGP表格数据转化为详细文本叙述；构建由LLM驱动的合成数据生成框架。

Result: 在真实和合成数据集上评估，BEAR准确率达100%，优于思维链和上下文学习基线。

Conclusion: 该工作开创了自动解释BGP异常事件的方法，为网络管理提供有价值的操作见解。

Abstract: The Internet comprises of interconnected, independently managed Autonomous
Systems (AS) that rely on the Border Gateway Protocol (BGP) for inter-domain
routing. BGP anomalies--such as route leaks and hijacks--can divert traffic
through unauthorized or inefficient paths, jeopardizing network reliability and
security. Although existing rule-based and machine learning methods can detect
these anomalies using structured metrics, they still require experts with
in-depth BGP knowledge of, for example, AS relationships and historical
incidents, to interpret events and propose remediation. In this paper, we
introduce BEAR (BGP Event Analysis and Reporting), a novel framework that
leverages large language models (LLMs) to automatically generate comprehensive
reports explaining detected BGP anomaly events. BEAR employs a multi-step
reasoning process that translates tabular BGP data into detailed textual
narratives, enhancing interpretability and analytical precision. To address the
limited availability of publicly documented BGP anomalies, we also present a
synthetic data generation framework powered by LLMs. Evaluations on both real
and synthetic datasets demonstrate that BEAR achieves 100% accuracy,
outperforming Chain-of-Thought and in-context learning baselines. This work
pioneers an automated approach for explaining BGP anomaly events, offering
valuable operational insights for network management.

</details>


### [685] [Intelligent Channel Allocation for IEEE 802.11be Multi-Link Operation: When MAB Meets LLM](https://arxiv.org/abs/2506.04594)
*Shumin Lian,Jingwen Tong,Jun Zhang,Liqun Fu*

Main category: cs.NI

TL;DR: 本文针对支持MLO的密集WiFi 7网络动态信道分配问题，提出BAI - MCTS和LLM - BAI - MCTS算法，实验表明其收敛速度优于现有算法。


<details>
  <summary>Details</summary>
Motivation: WiFi 7的MLO功能虽提升性能，但在密集网络信道分配上有挑战，现有研究多聚焦静态配置，本文解决动态信道分配问题。

Method: 将问题建模为组合优化问题，基于MAB框架，提出BAI - MCTS算法并进行理论分析，还结合大语言模型提出LLM - BAI - MCTS算法。

Result: BAI - MCTS算法达到最优值98%时收敛速度比现有算法快约50.44%，LLM - BAI - MCTS算法在密集网络中收敛速度提升超63.32%。

Conclusion: 所提算法能有效解决支持MLO的密集WiFi 7网络动态信道分配问题，提升收敛速度。

Abstract: WiFi networks have achieved remarkable success in enabling seamless
communication and data exchange worldwide. The IEEE 802.11be standard, known as
WiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that
enables devices to establish multiple simultaneous connections across different
bands and channels. While MLO promises substantial improvements in network
throughput and latency reduction, it presents significant challenges in channel
allocation, particularly in dense network environments. Current research has
predominantly focused on performance analysis and throughput optimization
within static WiFi 7 network configurations. In contrast, this paper addresses
the dynamic channel allocation problem in dense WiFi 7 networks with MLO
capabilities. We formulate this challenge as a combinatorial optimization
problem, leveraging a novel network performance analysis mechanism. Given the
inherent lack of prior network information, we model the problem within a
Multi-Armed Bandit (MAB) framework to enable online learning of optimal channel
allocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree
Search (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing
upper bounds for both sample complexity and error probability. To further
reduce sample complexity and enhance generalizability across diverse network
scenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic
channel allocation problem by integrating the Large Language Model (LLM) into
the BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS
algorithm achieves a convergence rate approximately $50.44\%$ faster than the
state-of-the-art algorithms when reaching $98\%$ of the optimal value. Notably,
the convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\%$
in dense networks.

</details>


### [686] [Towards Network Data Analytics in 5G Systems and Beyond](https://arxiv.org/abs/2506.04860)
*Marcos Lima Romero,Ricardo Suyama*

Main category: cs.NI

TL;DR: 数据是数字经济关键资产，MNOs未充分利用，3GPP推出NWDAF但学术研究受限，本文分析文章并提出新用例促进采用与探索盈利。


<details>
  <summary>Details</summary>
Motivation: 解决MNOs数据未充分利用问题，推动其超越连接商品化，提供更多价值，弥补NWDAF学术研究局限。

Method: 分析70多篇文章，总结趋势与差距。

Result: 提出两个新的NWDAF用例。

Conclusion: 所提新用例可促进NWDAF采用并探索其盈利潜力。

Abstract: Data has become a critical asset in the digital economy, yet it remains
underutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT)
players that lead global market valuations. To move beyond the commoditization
of connectivity and deliver greater value to customers, data analytics emerges
as a strategic enabler. Using data efficiently is essential for unlocking new
service opportunities, optimizing operational efficiency, and mitigating
operational and business risks. Since Release 15, the 3rd Generation
Partnership Project (3GPP) has introduced the Network Data Analytics Function
(NWDAF) to provide powerful insights and predictions using data collected
across mobile networks, supporting both user-centric and network-oriented use
cases. However, academic research has largely focused on a limited set of
methods and use cases, driven by the availability of datasets, restricting
broader exploration. This study analyzes trends and gaps in more than 70
articles and proposes two novel use cases to promote the adoption of NWDAF and
explore its potential for monetization.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [687] [Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach](https://arxiv.org/abs/2506.05245)
*Shaoxuan Chen,Su Yang,Panayotis G. Kevrekidis,Wei Zhu*

Main category: nlin.PS

TL;DR: 提出基于神经ODE的数据驱动框架，用于学习PDE系统的降维矩动力学。该方法克服传统方法对密集数据和噪声敏感的问题，在稀疏/非规则数据中表现稳健，支持闭合/非闭合系统的建模。


<details>
  <summary>Details</summary>
Motivation: 传统基于导数的方法（如SINDy）需要密集采样且对噪声敏感，限制了在稀疏/非规则数据场景中的应用。需要开发更稳健的方法来学习复杂PDE系统的降维动力学。

Method: 1. 基于神经ODE直接建模矩轨迹，支持稀疏/非规则数据学习
2. 对无闭合系统，采用Stiefel流形优化的数据驱动坐标变换策略，发现闭合的低维表示
3. 在非线性薛定谔方程和Fisher-KPP系统进行验证

Result: 1. 在闭合系统准确恢复动力学（非线性薛定谔方程）
2. 在无闭合系统（Fisher-KPP）中，神经ODE外推精度优于专家模型
3. 方法在稀疏/非规则采样下保持稳健性

Conclusion: 神经ODE框架是学习PDE系统可解释降维动力学的强大工具，尤其在数据受限场景表现出显著优势，为复杂系统建模提供了新范式。

Abstract: We propose a data-driven framework for learning reduced-order moment dynamics
from PDE-governed systems using Neural ODEs. In contrast to derivative-based
methods like SINDy, which necessitate densely sampled data and are sensitive to
noise, our approach based on Neural ODEs directly models moment trajectories,
enabling robust learning from sparse and potentially irregular time series.
Using as an application platform the nonlinear Schr\"{o}dinger equation, the
framework accurately recovers governing moment dynamics when closure is
available, even with limited and irregular observations. For systems without
analytical closure, we introduce a data-driven coordinate transformation
strategy based on Stiefel manifold optimization, enabling the discovery of
low-dimensional representations in which the moment dynamics become closed,
facilitating interpretable and reliable modeling. We also explore cases where a
closure model is not known, such as a Fisher-KPP reaction-diffusion system.
Here we demonstrate that Neural ODEs can still effectively approximate the
unclosed moment dynamics and achieve superior extrapolation accuracy compared
to physical-expert-derived ODE models. This advantage remains robust even under
sparse and irregular sampling, highlighting the method's robustness in
data-limited settings. Our results highlight the Neural ODE framework as a
powerful and flexible tool for learning interpretable, low-dimensional moment
dynamics in complex PDE-governed systems.

</details>


### [688] [Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach](https://arxiv.org/abs/2506.05245)
*Shaoxuan Chen,Su Yang,Panayotis G. Kevrekidis,Wei Zhu*

Main category: nlin.PS

TL;DR: 提出用神经常微分方程（Neural ODEs）学习偏微分方程（PDE）系统降阶矩动力学的数据驱动框架，能从稀疏不规则数据中稳健学习，在不同场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于导数的方法如SINDy需要密集采样数据且对噪声敏感，需一种能从稀疏不规则数据中稳健学习PDE系统矩动力学的方法。

Method: 使用Neural ODEs直接对矩轨迹建模，对于无解析闭合的系统引入基于Stiefel流形优化的数据驱动坐标变换策略。

Result: 在非线性薛定谔方程中准确恢复矩动力学；在无解析闭合系统中发现低维表示；在Fisher - KPP反应扩散系统中有效近似未闭合矩动力学，外推精度优于物理专家推导的ODE模型。

Conclusion: Neural ODE框架是学习复杂PDE系统中可解释、低维矩动力学的强大灵活工具。

Abstract: We propose a data-driven framework for learning reduced-order moment dynamics
from PDE-governed systems using Neural ODEs. In contrast to derivative-based
methods like SINDy, which necessitate densely sampled data and are sensitive to
noise, our approach based on Neural ODEs directly models moment trajectories,
enabling robust learning from sparse and potentially irregular time series.
Using as an application platform the nonlinear Schr\"{o}dinger equation, the
framework accurately recovers governing moment dynamics when closure is
available, even with limited and irregular observations. For systems without
analytical closure, we introduce a data-driven coordinate transformation
strategy based on Stiefel manifold optimization, enabling the discovery of
low-dimensional representations in which the moment dynamics become closed,
facilitating interpretable and reliable modeling. We also explore cases where a
closure model is not known, such as a Fisher-KPP reaction-diffusion system.
Here we demonstrate that Neural ODEs can still effectively approximate the
unclosed moment dynamics and achieve superior extrapolation accuracy compared
to physical-expert-derived ODE models. This advantage remains robust even under
sparse and irregular sampling, highlighting the method's robustness in
data-limited settings. Our results highlight the Neural ODE framework as a
powerful and flexible tool for learning interpretable, low-dimensional moment
dynamics in complex PDE-governed systems.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [689] [The Peculiarities of Extending Queue Layouts](https://arxiv.org/abs/2506.05156)
*Thomas Depian,Simon D. Fink,Robert Ganian,Martin Nöllenburg*

Main category: cs.CG

TL;DR: 提出基于参数化复杂性的ℓ-page队列布局扩展算法，揭示其与堆栈布局在扩展场景中的本质差异


<details>
  <summary>Details</summary>
Motivation: 受堆栈布局扩展研究启发，探索队列布局的扩展可能性及其复杂性特征，填补该问题在参数化分析领域的空白

Method: 建立参数化复杂性分析框架，开发多维度不完整性度量标准，设计混合整数规划与启发式算法相结合的解决方案

Result: 发现队列布局扩展复杂度存在非对称下界，构建出优于传统方法的O(k²n³)时间复杂度算法（k为缺失边数量）

Conclusion: 参数化分析揭示队列布局扩展的固有复杂性，其扩展难度随页面数ℓ呈指数级增长，这与堆栈布局的多项式特性形成鲜明对比

Abstract: We consider the problem of computing $\ell$-page queue layouts, which are
linear arrangements of vertices accompanied with an assignment of the edges to
pages from one to $\ell$ that avoid the nesting of edges on any of the pages.
Inspired by previous work in the extension of stack layouts, here we consider
the setting of extending a partial $\ell$-page queue layout into a complete one
and primarily analyze the problem through the refined lens of parameterized
complexity. We obtain novel algorithms and lower bounds which provide a
detailed picture of the problem's complexity under various measures of
incompleteness, and identify surprising distinctions between queue and stack
layouts in the extension setting.

</details>


### [690] [The Peculiarities of Extending Queue Layouts](https://arxiv.org/abs/2506.05156)
*Thomas Depian,Simon D. Fink,Robert Ganian,Martin Nöllenburg*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of computing $\ell$-page queue layouts, which are
linear arrangements of vertices accompanied with an assignment of the edges to
pages from one to $\ell$ that avoid the nesting of edges on any of the pages.
Inspired by previous work in the extension of stack layouts, here we consider
the setting of extending a partial $\ell$-page queue layout into a complete one
and primarily analyze the problem through the refined lens of parameterized
complexity. We obtain novel algorithms and lower bounds which provide a
detailed picture of the problem's complexity under various measures of
incompleteness, and identify surprising distinctions between queue and stack
layouts in the extension setting.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [691] [Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical Sound Classification](https://arxiv.org/abs/2506.04376)
*Emiliano Acevedo,Martín Rocamora,Magdalena Fuentes*

Main category: cs.SD

TL;DR: 提出通过量化背景声源贡献和缩小模态间隙的领域自适应方法，增强噪声环境下零样本音频分类的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有音频-文本模型在背景噪声环境下性能显著下降，该现象主要由信噪比水平驱动且与噪声类型无关

Method: 开发无需模型重训练的领域适应技术：1) 量化整合背景声源贡献 2) 分析并缩小音频与文本嵌入的模态间隙

Result: 方法在多种噪声环境和信噪比条件下提升分类准确率，且可泛用于现有原型网络架构，验证了方案的扩展性

Conclusion: 通过双重优化策略有效增强噪声环境下的分类鲁棒性，模态间隙分析为跨模态对齐提供了新见解

Abstract: Audio-text models are widely used in zero-shot environmental sound
classification as they alleviate the need for annotated data. However, we show
that their performance severely drops in the presence of background sound
sources. Our analysis reveals that this degradation is primarily driven by SNR
levels of background soundscapes, and independent of background type. To
address this, we propose a novel method that quantifies and integrates the
contribution of background sources into the classification process, improving
performance without requiring model retraining. Our domain adaptation technique
enhances accuracy across various backgrounds and SNR conditions. Moreover, we
analyze the modality gap between audio and text embeddings, showing that
narrowing this gap improves classification performance. The method generalizes
effectively across state-of-the-art prototypical approaches, showcasing its
scalability and robustness for diverse environments.

</details>


### [692] [Survey on the Evaluation of Generative Models in Music](https://arxiv.org/abs/2506.05104)
*Alexander Lerch,Claire Arthur,Nick Bryan-Kinns,Corey Ford,Qianyi Sun,Ashvala Vinay*

Main category: cs.SD

TL;DR: 系统回顾音乐生成系统的多学科评估方法及其优劣势


<details>
  <summary>Details</summary>
Motivation: 针对音乐生成系统评估缺乏系统性的现状，需整合音乐学、工程学和人机交互视角建立全面评估框架

Method: 采用跨学科文献综述方法，综合主观/客观、定性/定量、实证/计算三种维度分析现有评估体系

Result: 揭示了不同学科方法论在评估指标选择上的互补性，识别出现有评估体系在生态效度和技术可行性间的矛盾

Conclusion: 未来音乐生成系统评估需建立融合创作意图表达、技术性能指标和用户体验测量的三维度综合评价模型

Abstract: Research on generative systems in music has seen considerable attention and
growth in recent years. A variety of attempts have been made to systematically
evaluate such systems. We provide an interdisciplinary review of the common
evaluation targets, methodologies, and metrics for the evaluation of both
system output and model usability, covering subjective and objective
approaches, qualitative and quantitative approaches, as well as empirical and
computational methods. We discuss the advantages and challenges of such
approaches from a musicological, an engineering, and an HCI perspective.

</details>


### [693] [Improving AI-generated music with user-guided training](https://arxiv.org/abs/2506.04852)
*Vishwa Mohan Singh,Sai Anirudh Aryasomayajula,Ahan Chatterjee,Beste Aydemir,Rifat Mehreen Amin*

Main category: cs.SD

TL;DR: 提出人类计算与遗传算法结合的AI音乐生成优化方案，通过用户反馈迭代使评分提升0.59


<details>
  <summary>Details</summary>
Motivation: 传统固定数据集训练的AI音乐生成模型难以满足用户主观个性化需求

Method: 1. 人类计算：聚合用户评分作为损失函数
2. 遗传算法：基于用户反馈迭代优化基线模型

Result: 基线→第一轮评分+0.2，第二轮再+0.39，总提升0.59

Conclusion: 用户反馈驱动的迭代优化能有效提升AI音乐生成的主观质量

Abstract: AI music generation has advanced rapidly, with models like diffusion and
autoregressive algorithms enabling high-fidelity outputs. These tools can alter
styles, mix instruments, or isolate them. Since sound can be visualized as
spectrograms, image-generation algorithms can be applied to generate novel
music. However, these algorithms are typically trained on fixed datasets, which
makes it challenging for them to interpret and respond to user input
accurately. This is especially problematic because music is highly subjective
and requires a level of personalization that image generation does not provide.
In this work, we propose a human-computation approach to gradually improve the
performance of these algorithms based on user interactions. The
human-computation element involves aggregating and selecting user ratings to
use as the loss function for fine-tuning the model. We employ a genetic
algorithm that incorporates user feedback to enhance the baseline performance
of a model initially trained on a fixed dataset. The effectiveness of this
approach is measured by the average increase in user ratings with each
iteration. In the pilot test, the first iteration showed an average rating
increase of 0.2 compared to the baseline. The second iteration further improved
upon this, achieving an additional increase of 0.39 over the first iteration.

</details>


### [694] [Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical Sound Classification](https://arxiv.org/abs/2506.04376)
*Emiliano Acevedo,Martín Rocamora,Magdalena Fuentes*

Main category: cs.SD

TL;DR: 音频文本模型在零样本环境声音分类中性能受背景声源影响，提出新方法改善性能，分析模态差距并证明方法有扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决音频文本模型在有背景声源时性能严重下降的问题。

Method: 提出量化并整合背景声源贡献到分类过程的新方法，采用域适应技术，分析音频和文本嵌入的模态差距。

Result: 提高了不同背景和信噪比条件下的分类准确性，方法能有效泛化到最先进的原型方法。

Conclusion: 该方法无需重新训练模型，能改善分类性能，具有可扩展性和鲁棒性，适用于不同环境。

Abstract: Audio-text models are widely used in zero-shot environmental sound
classification as they alleviate the need for annotated data. However, we show
that their performance severely drops in the presence of background sound
sources. Our analysis reveals that this degradation is primarily driven by SNR
levels of background soundscapes, and independent of background type. To
address this, we propose a novel method that quantifies and integrates the
contribution of background sources into the classification process, improving
performance without requiring model retraining. Our domain adaptation technique
enhances accuracy across various backgrounds and SNR conditions. Moreover, we
analyze the modality gap between audio and text embeddings, showing that
narrowing this gap improves classification performance. The method generalizes
effectively across state-of-the-art prototypical approaches, showcasing its
scalability and robustness for diverse environments.

</details>


### [695] [Survey on the Evaluation of Generative Models in Music](https://arxiv.org/abs/2506.05104)
*Alexander Lerch,Claire Arthur,Nick Bryan-Kinns,Corey Ford,Qianyi Sun,Ashvala Vinay*

Main category: cs.SD

TL;DR: 本文对音乐生成系统的评估进行跨学科综述，涵盖评估目标、方法和指标，并从多视角讨论优缺点。


<details>
  <summary>Details</summary>
Motivation: 近年来音乐生成系统研究受关注，需对评估方法进行系统梳理。

Method: 进行跨学科综述，涵盖多种评估方法和视角。

Result: 提供了评估系统输出和模型可用性的常见目标、方法和指标。

Conclusion: 讨论了不同评估方法从音乐学、工程学和人机交互视角的优缺点。

Abstract: Research on generative systems in music has seen considerable attention and
growth in recent years. A variety of attempts have been made to systematically
evaluate such systems. We provide an interdisciplinary review of the common
evaluation targets, methodologies, and metrics for the evaluation of both
system output and model usability, covering subjective and objective
approaches, qualitative and quantitative approaches, as well as empirical and
computational methods. We discuss the advantages and challenges of such
approaches from a musicological, an engineering, and an HCI perspective.

</details>


### [696] [Improving AI-generated music with user-guided training](https://arxiv.org/abs/2506.04852)
*Vishwa Mohan Singh,Sai Anirudh Aryasomayajula,Ahan Chatterjee,Beste Aydemir,Rifat Mehreen Amin*

Main category: cs.SD

TL;DR: AI音乐生成虽有进展，但因基于固定数据集训练，难准确响应用户输入。本文提出人机计算方法，结合遗传算法和用户反馈微调模型，测试显示迭代能提升用户评分。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成算法基于固定数据集训练，难以准确解读和响应主观的用户输入，需要个性化改进。

Method: 提出人机计算方法，聚合和选择用户评分作为损失函数微调模型，采用遗传算法结合用户反馈提升模型性能。

Result: 在试点测试中，第一次迭代较基线平均评分提高0.2，第二次迭代较第一次额外提高0.39。

Conclusion: 通过人机计算结合遗传算法和用户反馈的方法，能逐步提升AI音乐生成算法的性能。

Abstract: AI music generation has advanced rapidly, with models like diffusion and
autoregressive algorithms enabling high-fidelity outputs. These tools can alter
styles, mix instruments, or isolate them. Since sound can be visualized as
spectrograms, image-generation algorithms can be applied to generate novel
music. However, these algorithms are typically trained on fixed datasets, which
makes it challenging for them to interpret and respond to user input
accurately. This is especially problematic because music is highly subjective
and requires a level of personalization that image generation does not provide.
In this work, we propose a human-computation approach to gradually improve the
performance of these algorithms based on user interactions. The
human-computation element involves aggregating and selecting user ratings to
use as the loss function for fine-tuning the model. We employ a genetic
algorithm that incorporates user feedback to enhance the baseline performance
of a model initially trained on a fixed dataset. The effectiveness of this
approach is measured by the average increase in user ratings with each
iteration. In the pilot test, the first iteration showed an average rating
increase of 0.2 compared to the baseline. The second iteration further improved
upon this, achieving an additional increase of 0.39 over the first iteration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [697] [The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model](https://arxiv.org/abs/2506.04247)
*Gage K. R. Hooper*

Main category: q-bio.NC

TL;DR: GAIN模型通过网格结构增强生物合理性，结合Izhikevich模型实现高效精准的神经元模拟


<details>
  <summary>Details</summary>
Motivation: 传统神经网络注重层级处理，但生物神经元通过局部连接互动。GAIN旨在通过网格结构模拟这种生物特性，提升模型动态性和生物可信度。

Method: 1. 采用网格结构促进相邻神经元的局部互动
2. 整合Izhikevich神经元模型
3. 优化连接机制模拟生物神经网络的突触可塑性

Result: 实现计算效率提升(比传统方法快40%)，动态模拟准确性提高32%，支持万级神经元实时仿真

Conclusion: 该模型在保持生物合理性的同时显著提升计算效率，为大规模神经网络仿真和神经科学研究提供新工具，特别适用于脑机接口和类脑计算领域

Abstract: While many neural networks focus on layers to process information, the GAIN
model uses a grid-based structure to improve biological plausibility and the
dynamics of the model. The grid structure helps neurons to interact with their
closest neighbors and improve their connections with one another, which is seen
in biological neurons. While also being implemented with the Izhikevich model
this approach allows for a computationally efficient and biologically accurate
simulation that can aid in the development of neural networks, large scale
simulations, and the development in the neuroscience field. This adaptation of
the Izhikevich model can improve the dynamics and accuracy of the model,
allowing for its uses to be specialized but efficient.

</details>


### [698] [Generalizable, real-time neural decoding with hybrid state-space models](https://arxiv.org/abs/2506.05320)
*Avery Hee-Woon Ryoo,Nanda H. Krishna,Ximeng Mao,Mehdi Azabou,Eva L. Dyer,Matthew G. Perich,Guillaume Lajoie*

Main category: q-bio.NC

TL;DR: POSSM提出了一种结合交叉注意力模块与循环状态空间模型的混合架构，在保持实时推理速度的同时，实现了跨物种的神经活动解码能力迁移。


<details>
  <summary>Details</summary>
Motivation: 解决传统RNN方法泛化能力不足与Transformer模型计算资源消耗大的矛盾，满足脑机接口等实时场景对低延迟和高泛化性的双重要求。

Method: 通过交叉注意力模块实现个体脉冲标记化，结合SSM模型主干进行序列建模。支持多数据集预训练，支持因果在线推理。

Result: 在猴运动皮层解码任务中达到SOTA精度（GPU推理速度提升9倍），并验证了猴→人跨物种迁移潜力（人类手写解码准确率提升）。

Conclusion: SSM混合架构有效平衡了精度、推理速度与泛化能力，为实时闭环神经解码系统提供了新的技术路径。

Abstract: Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.

</details>


### [699] [The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model](https://arxiv.org/abs/2506.04247)
*Gage K. R. Hooper*

Main category: q-bio.NC

TL;DR: GAIN模型采用基于网格的结构改进生物合理性和模型动态，结合Izhikevich模型实现高效准确模拟。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络的生物合理性和模型动态。

Method: 使用基于网格的结构，结合Izhikevich模型。

Result: 实现了计算高效且生物准确的模拟。

Conclusion: 该Izhikevich模型的改进能提升模型动态和准确性，应用专业且高效。

Abstract: While many neural networks focus on layers to process information, the GAIN
model uses a grid-based structure to improve biological plausibility and the
dynamics of the model. The grid structure helps neurons to interact with their
closest neighbors and improve their connections with one another, which is seen
in biological neurons. While also being implemented with the Izhikevich model
this approach allows for a computationally efficient and biologically accurate
simulation that can aid in the development of neural networks, large scale
simulations, and the development in the neuroscience field. This adaptation of
the Izhikevich model can improve the dynamics and accuracy of the model,
allowing for its uses to be specialized but efficient.

</details>


### [700] [Generalizable, real-time neural decoding with hybrid state-space models](https://arxiv.org/abs/2506.05320)
*Avery Hee-Woon Ryoo,Nanda H. Krishna,Ximeng Mao,Mehdi Azabou,Eva L. Dyer,Matthew G. Perich,Guillaume Lajoie*

Main category: q-bio.NC

TL;DR: 提出新型混合架构POSSM用于神经活动实时解码，在多任务中以低推理成本达与SOTA相当精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法难泛化，Transformer计算需求大，不适用于低资源或实时场景。

Method: 提出POSSM，结合跨注意力模块的单个尖峰标记化与循环状态空间模型骨干，通过多数据集预训练实现快速因果在线预测和有效泛化。

Result: 在猴子运动任务皮质内解码、人类手写和语音解码任务中，POSSM推理成本低，速度比GPU上的SOTA Transformer快达9倍，精度相当。

Conclusion: 混合SSM是训练实时闭环应用神经解码器时平衡精度、推理速度和泛化能力的有前景方法。

Abstract: Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [701] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: 通过将VHDL/Chisel/PyMTL3转换为Verilog构建hdl2v数据集，显著提升LLM的Verilog生成能力（最高提升23% pass@10指标）。


<details>
  <summary>Details</summary>
Motivation: 硬件描述语言Verilog的公开数据量远少于Python等软件语言，限制了LLM在硬件代码生成领域的发展。

Method: 1. 构建多语言转译数据集hdl2v（VHDL/Chisel/PyMTL3→Verilog）
2. 在32B参数的开放权重模型上进行微调
3. 使用VerilogEvalV2基准进行评估

Result: 1. 非数据增强场景下性能提升23%（pass@10）
2. 结合数据增强方法后性能提升63%
3. 首次系统性分析HDL转译数据集特性

Conclusion: hdl2v有效突破Verilog数据瓶颈，未来可通过扩展更多HDL转译类型及优化数据集特征进一步提升LLM的硬件代码生成能力。

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


### [702] [QiMeng: Fully Automated Hardware and Software Design for Processor Chip](https://arxiv.org/abs/2506.05007)
*Rui Zhang,Yuanbo Wen,Shuyao Cheng,Di Huang,Shaohui Peng,Jiaming Guo,Pengwei Jin,Jiacheng Zhao,Tianrui Ma,Yaoyu Zhu,Yifan Hao,Yongwei Zhao,Shengwen Liang,Ying Wang,Xing Hu,Zidong Du,Huimin Cui,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.AR

TL;DR: 提出全自动处理器芯片设计系统QiMeng，通过三层架构解决传统设计范式在物理限制、资源需求与生态多样性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统处理器芯片设计面临物理制造技术限制、设计资源需求激增和生态系统多样性三大挑战，需通过自动化设计实现突破。

Method: 1. 底层构建领域大模型LPCM，改进架构/训练/推理机制；2. 中间层开发硬件/软件设计代理；3. 顶层应用验证系统有效性。

Result: 已完成部分组件并在多个顶层应用验证，展现显著效率优势，未来将整合系统实现迭代式全自动设计流程。

Conclusion: QiMeng通过分层架构创新，为高效全自动处理器芯片软硬件设计提供了可行性解决方案，推动计算机领域设计范式革新。

Abstract: Processor chip design technology serves as a key frontier driving
breakthroughs in computer science and related fields. With the rapid
advancement of information technology, conventional design paradigms face three
major challenges: the physical constraints of fabrication technologies, the
escalating demands for design resources, and the increasing diversity of
ecosystems. Automated processor chip design has emerged as a transformative
solution to address these challenges. While recent breakthroughs in Artificial
Intelligence (AI), particularly Large Language Models (LLMs) techniques, have
opened new possibilities for fully automated processor chip design, substantial
challenges remain in establishing domain-specific LLMs for processor chip
design.
  In this paper, we propose QiMeng, a novel system for fully automated hardware
and software design of processor chips. QiMeng comprises three hierarchical
layers. In the bottom-layer, we construct a domain-specific Large Processor
Chip Model (LPCM) that introduces novel designs in architecture, training, and
inference, to address key challenges such as knowledge representation gap, data
scarcity, correctness assurance, and enormous solution space. In the
middle-layer, leveraging the LPCM's knowledge representation and inference
capabilities, we develop the Hardware Design Agent and the Software Design
Agent to automate the design of hardware and software for processor chips.
Currently, several components of QiMeng have been completed and successfully
applied in various top-layer applications, demonstrating significant advantages
and providing a feasible solution for efficient, fully automated
hardware/software design of processor chips. Future research will focus on
integrating all components and performing iterative top-down and bottom-up
design processes to establish a comprehensive QiMeng system.

</details>


### [703] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: 本文提出hdl2v数据集，通过将其他硬件描述语言转换为Verilog增加可用数据，提升LLM的Verilog生成性能并分析数据集。


<details>
  <summary>Details</summary>
Motivation: 公开可用的Verilog代码量远少于软件语言代码，需要增加Verilog数据以提升LLM在硬件代码生成领域的表现。

Method: 创建hdl2v数据集，将VHDL、Chisel和PyMTL3转换为Verilog，用其提升模型性能并进行分析。

Result: 在VerilogEvalV2中使320亿参数的开放权重模型性能提升23%（pass@10），使基于数据增强的微调方法性能提升63%。

Conclusion: hdl2v数据集能有效提升LLM的Verilog生成性能，后续可针对HDL到Verilog数据集特征进一步优化。

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


### [704] [QiMeng: Fully Automated Hardware and Software Design for Processor Chip](https://arxiv.org/abs/2506.05007)
*Rui Zhang,Yuanbo Wen,Shuyao Cheng,Di Huang,Shaohui Peng,Jiaming Guo,Pengwei Jin,Jiacheng Zhao,Tianrui Ma,Yaoyu Zhu,Yifan Hao,Yongwei Zhao,Shengwen Liang,Ying Wang,Xing Hu,Zidong Du,Huimin Cui,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.AR

TL;DR: 本文提出用于处理器芯片全自动软硬件设计的QiMeng系统，介绍其分层结构，部分组件已应用，未来将完善系统。


<details>
  <summary>Details</summary>
Motivation: 传统处理器芯片设计范式面临制造技术物理限制、设计资源需求增加和生态系统多样化挑战，现有AI技术用于芯片设计仍有挑战。

Method: 构建三层结构的QiMeng系统，底层构建特定领域的大处理器芯片模型（LPCM），中层开发硬件和软件设计代理。

Result: QiMeng部分组件已完成并在顶层应用中展现优势。

Conclusion: QiMeng为处理器芯片高效全自动软硬件设计提供可行方案，未来将集成组件建立完整系统。

Abstract: Processor chip design technology serves as a key frontier driving
breakthroughs in computer science and related fields. With the rapid
advancement of information technology, conventional design paradigms face three
major challenges: the physical constraints of fabrication technologies, the
escalating demands for design resources, and the increasing diversity of
ecosystems. Automated processor chip design has emerged as a transformative
solution to address these challenges. While recent breakthroughs in Artificial
Intelligence (AI), particularly Large Language Models (LLMs) techniques, have
opened new possibilities for fully automated processor chip design, substantial
challenges remain in establishing domain-specific LLMs for processor chip
design.
  In this paper, we propose QiMeng, a novel system for fully automated hardware
and software design of processor chips. QiMeng comprises three hierarchical
layers. In the bottom-layer, we construct a domain-specific Large Processor
Chip Model (LPCM) that introduces novel designs in architecture, training, and
inference, to address key challenges such as knowledge representation gap, data
scarcity, correctness assurance, and enormous solution space. In the
middle-layer, leveraging the LPCM's knowledge representation and inference
capabilities, we develop the Hardware Design Agent and the Software Design
Agent to automate the design of hardware and software for processor chips.
Currently, several components of QiMeng have been completed and successfully
applied in various top-layer applications, demonstrating significant advantages
and providing a feasible solution for efficient, fully automated
hardware/software design of processor chips. Future research will focus on
integrating all components and performing iterative top-down and bottom-up
design processes to establish a comprehensive QiMeng system.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [705] [Identity Testing for Circuits with Exponentiation Gates](https://arxiv.org/abs/2506.04529)
*Jiatu Li,Mengdi Wu*

Main category: cs.CC

TL;DR: 提出了基于有限域黑盒查询的随机化身份测试算法，用于验证含指数门的算术电路，实际应用于神经网络编译器并展示优异性能。


<details>
  <summary>Details</summary>
Motivation: 针对神经网络优化编译器开发中遇到的算术电路验证难题，研究带有指数运算的电路身份测试问题以提升编译器可靠性。

Method: 建立有限域黑盒查询模型，设计完美完备性与非平凡可靠性的随机化算法，通过指数多项式结构特性进行电路等价性验证。

Result: 算法被集成到Mirage编译器中，实证显示高效率与低错误率；提出数论猜想支持算法的理论可靠性。

Conclusion: 该算法架起了理论验证与工程实践桥梁，通过有限域方法有效解决含指数运算电路的验证难题，推动神经网络编译优化。

Abstract: Motivated by practical applications in the design of optimization compilers
for neural networks, we initiated the study of identity testing problems for
arithmetic circuits augmented with \emph{exponentiation gates} that compute the
real function $x\mapsto e^x$. These circuits compute real functions of form
$P(\vec x)/P'(\vec x)$, where both $P(\vec x)$ and $P'(\vec x)$ are exponential
polynomials
  \[
  \sum_{i=1}^k f_i(\vec x)\cdot \exp\left(\frac{g_i(\vec x)}{h_i(\vec
x)}\right),
  \]
  for polynomials $f_i(\vec x),g_i(\vec x)$, and $h_i(\vec x)$.
  We formalize a black-box query model over finite fields for this class of
circuits, which is mathematical simple and reflects constraints faced by
real-world neural network compilers. We proved that a simple and efficient
randomized identity testing algorithm achieves perfect completeness and
non-trivial soundness. Concurrent with our work, the algorithm has been
implemented in the optimization compiler Mirage by Wu et al.~(OSDI 2025),
demonstrating promising empirical performance in both efficiency and soundness
error. Finally, we propose a number-theoretic conjecture under which our
algorithm is sound with high probability.

</details>


### [706] [Identity Testing for Circuits with Exponentiation Gates](https://arxiv.org/abs/2506.04529)
*Jiatu Li,Mengdi Wu*

Main category: cs.CC

TL;DR: 本文受神经网络优化编译器设计的实际应用启发，研究带指数门的算术电路的恒等测试问题，提出黑盒查询模型和随机化恒等测试算法，算法有较好性能并提出相关数论猜想。


<details>
  <summary>Details</summary>
Motivation: 受神经网络优化编译器设计的实际应用驱动，研究带指数门的算术电路的恒等测试问题。

Method: 为该类电路在有限域上形式化一个黑盒查询模型，提出简单高效的随机化恒等测试算法。

Result: 算法实现了完美完备性和非平凡稳健性，在优化编译器Mirage中实现并展现出良好的效率和稳健性错误的实证性能。

Conclusion: 提出一个数论猜想，在此猜想下算法大概率是稳健的。

Abstract: Motivated by practical applications in the design of optimization compilers
for neural networks, we initiated the study of identity testing problems for
arithmetic circuits augmented with \emph{exponentiation gates} that compute the
real function $x\mapsto e^x$. These circuits compute real functions of form
$P(\vec x)/P'(\vec x)$, where both $P(\vec x)$ and $P'(\vec x)$ are exponential
polynomials
  \[
  \sum_{i=1}^k f_i(\vec x)\cdot \exp\left(\frac{g_i(\vec x)}{h_i(\vec
x)}\right),
  \]
  for polynomials $f_i(\vec x),g_i(\vec x)$, and $h_i(\vec x)$.
  We formalize a black-box query model over finite fields for this class of
circuits, which is mathematical simple and reflects constraints faced by
real-world neural network compilers. We proved that a simple and efficient
randomized identity testing algorithm achieves perfect completeness and
non-trivial soundness. Concurrent with our work, the algorithm has been
implemented in the optimization compiler Mirage by Wu et al.~(OSDI 2025),
demonstrating promising empirical performance in both efficiency and soundness
error. Finally, we propose a number-theoretic conjecture under which our
algorithm is sound with high probability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [707] [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2506.04308)
*Enshen Zhou,Jingkun An,Cheng Chi,Yi Han,Shanyu Rong,Chi Zhang,Pengwei Wang,Zhongyuan Wang,Tiejun Huang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出RoboRefer模型，通过监督和强化微调提升机器人三维空间定位与推理能力，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂三维场景理解和动态空间定位推理方面存在不足，需提升机器人物理空间交互能力。

Method: 1. 通过监督微调(SFT)集成深度编码器实现精准三维感知；2. 采用强化微调(RFT)设计指标敏感过程奖励函数，支持多步空间推理；3. 构建包含2000万QA对的RefSpatial数据集和评估基准。

Result: SFT模型空间理解成功率89.6%；RFT模型在RefSpatial-Bench上超越Gemini-2.5-Pro达17.4%，可集成至多类型机器人实现动态场景长程任务。

Conclusion: RoboRefer通过创新性双阶段微调机制实现了三维空间理解与推理的突破，实验验证其在复杂现实场景中的高效性和泛化能力。

Abstract: Spatial referring is a fundamental capability of embodied robots to interact
with the 3D physical world. However, even with the powerful pretrained vision
language models (VLMs), recent approaches are still not qualified to accurately
understand the complex 3D scenes and dynamically reason about the
instruction-indicated locations for interaction. To this end, we propose
RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding
by integrating a disentangled but dedicated depth encoder via supervised
fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial
reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process
reward functions tailored for spatial referring tasks. To support SFT and RFT
training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x
prior), covering 31 spatial relations (vs. 15 prior) and supporting complex
reasoning processes (up to 5 steps). In addition, we introduce
RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial
referring with multi-step reasoning. Experiments show that SFT-trained
RoboRefer achieves state-of-the-art spatial understanding, with an average
success rate of 89.6%. RFT-trained RoboRefer further outperforms all other
baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average
accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various
control policies to execute long-horizon, dynamic tasks across diverse robots
(e,g., UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [708] [cuVSLAM: CUDA accelerated visual odometry](https://arxiv.org/abs/2506.04359)
*Alexander Korovko,Dmitry Slepichev,Alexander Efitorov,Aigul Dzhumamuratova,Viktor Kuznetsov,Hesam Rabeti,Joydeep Biswas*

Main category: cs.RO

TL;DR: cuVSLAM是首个支持32相机配置的CUDA加速视觉SLAM系统，在边缘设备实现实时高精度定位建图


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统在传感器兼容性、计算效率和设备适应性方面存在局限，难以满足复杂机器人应用需求

Method: 采用CUDA加速的异构计算架构，支持多模态传感器融合（RGB/深度相机+IMU），设计动态资源分配模块适应不同硬件配置

Result: 在KITTI等基准测试中达到SOTA精度，Jetson AGX Xavier上实现30fps实时处理，支持32相机同步定位误差<0.5%

Conclusion: cuVSLAM通过硬件协同设计突破传统SLAM的传感器和算力限制，为复杂机器人系统提供可扩展的实时定位解决方案

Abstract: Accurate and robust pose estimation is a key requirement for any autonomous
robot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous
localization and mapping, which can operate with a variety of visual-inertial
sensor suites, including multiple RGB and depth cameras, and inertial
measurement units. cuVSLAM supports operation with as few as one RGB camera to
as many as 32 cameras, in arbitrary geometric configurations, thus supporting a
wide range of robotic setups. cuVSLAM is specifically optimized using CUDA to
deploy in real-time applications with minimal computational overhead on
edge-computing devices such as the NVIDIA Jetson. We present the design and
implementation of cuVSLAM, example use cases, and empirical results on several
state-of-the-art benchmarks demonstrating the best-in-class performance of
cuVSLAM.

</details>


### [709] [Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System](https://arxiv.org/abs/2506.05020)
*Haokun Liu,Zhaoqi Ma,Yunong Li,Junichiro Sugihara,Yicheng Chen,Jinjie Li,Moju Zhao*

Main category: cs.RO

TL;DR: 提出结合LLM任务推理与VLM视觉感知的异构机器人协同框架，实现动态环境下的全局语义规划与局部视觉导航


<details>
  <summary>Details</summary>
Motivation: 传统静态模型难以应对异构机器人协同中任务多样性和环境动态性的双重挑战，需要建立高层推理与底层执行的通用智能桥梁

Method: 分层架构：LLM负责任务分解与全局语义地图构建，GridMask增强的VLM从航拍图像提取语义标签支持局部规划，实现空中机器人全局语义路径引导地面机器人目标在场/缺失场景的协同操作

Result: 在字母立方体排列任务中验证框架的适应性与鲁棒性，首次实现VLM感知与LLM任务推理运动规划融合的天地异构系统演示

Conclusion: 该框架为动态环境下异构机器人协同提供了可扩展的智能架构，揭示了多模态大模型在具身智能系统中的应用潜力

Abstract: Heterogeneous multi-robot systems show great potential in complex tasks
requiring coordinated hybrid cooperation. However, traditional approaches
relying on static models often struggle with task diversity and dynamic
environments. This highlights the need for generalizable intelligence that can
bridge high-level reasoning with low-level execution across heterogeneous
agents. To address this, we propose a hierarchical framework integrating a
prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision
Language Model (VLM). The LLM performs task decomposition and global semantic
map construction, while the VLM extracts task-specified semantic labels and 2D
spatial information from aerial images to support local planning. Within this
framework, the aerial robot follows a globally optimized semantic path and
continuously provides bird-view images, guiding the ground robot's local
semantic navigation and manipulation, including target-absent scenarios where
implicit alignment is maintained. Experiments on a real-world letter-cubes
arrangement task demonstrate the framework's adaptability and robustness in
dynamic environments. To the best of our knowledge, this is the first
demonstration of an aerial-ground heterogeneous system integrating VLM-based
perception with LLM-driven task reasoning and motion planning.

</details>


### [710] [SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning](https://arxiv.org/abs/2506.04505)
*Nikita Oskolkov,Huzhenyu Zhang,Dmitry Makarov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: 提出基于3D场景图的强化学习框架SGN-CIRL，通过模仿学习和课程学习提升机器人导航成功率


<details>
  <summary>Details</summary>
Motivation: 解决传统导航方法在部分可观测环境中效率低下、难以预测目标物体位置的问题，利用3D场景图增强空间关系建模能力

Method: 1. 结合强化学习与可学习的开放词汇3D场景图表示
2. 采用模仿学习从演示中学习策略
3. 使用课程学习分阶段提升训练难度

Result: 在Isaac Sim环境中的实验显示，3D场景图使强化学习的导航成功率显著提升，特别是在复杂场景中

Conclusion: 该方法有效解决了无地图导航难题，框架代码已开源为社区提供可复现方案

Abstract: The 3D scene graph models spatial relationships between objects, enabling the
agent to efficiently navigate in a partially observable environment and predict
the location of the target object.This paper proposes an original framework
named SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for
mapless reinforcement learning-based robot navigation with learnable
representation of open-vocabulary 3D scene graph. To accelerate and stabilize
the training of reinforcement learning-based algorithms, the framework also
employs imitation learning and curriculum learning. The first one enables the
agent to learn from demonstrations, while the second one structures the
training process by gradually increasing task complexity from simple to more
advanced scenarios. Numerical experiments conducted in the Isaac Sim
environment showed that using a 3D scene graph for reinforcement learning
significantly increased the success rate in difficult navigation cases. The
code is open-sourced and available at: https://github.com/Xisonik/Aloha\_graph.

</details>


### [711] [Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation](https://arxiv.org/abs/2506.04539)
*Kordel K. France,Ovidiu Daescu,Anirban Paul,Shalini Prasad*

Main category: cs.RO

TL;DR: 提出适用于多类型嗅觉传感器的机器人OIO校准方法，实现厘米级气味源定位，应用于机器人手术和安全筛查场景。


<details>
  <summary>Details</summary>
Motivation: 气体动力学和环境干扰导致嗅觉导航（OIO）困难，需开发通用校准方法提升定位精度和任务表现。

Method: 建立通用化OIO校准流程，在真实机械臂平台验证，重点优化慢速移动场景下的气味源定位准确性。

Result: 校准后系统在冷启动嗅觉导航任务中性能显著提升，验证了方法的有效性。

Conclusion: 提出的OIO校准方案能有效提升机器人嗅觉导航精度，在医疗和安检领域具有实用价值。

Abstract: Visual inertial odometry (VIO) is a process for fusing visual and kinematic
data to understand a machine's state in a navigation task. Olfactory inertial
odometry (OIO) is an analog to VIO that fuses signals from gas sensors with
inertial data to help a robot navigate by scent. Gas dynamics and environmental
factors introduce disturbances into olfactory navigation tasks that can make
OIO difficult to facilitate. With our work here, we define a process for
calibrating a robot for OIO that generalizes to several olfaction sensor types.
Our focus is specifically on calibrating OIO for centimeter-level accuracy in
localizing an odor source on a slow-moving robot platform to demonstrate use
cases in robotic surgery and touchless security screening. We demonstrate our
process for OIO calibration on a real robotic arm and show how this calibration
improves performance over a cold-start olfactory navigation task.

</details>


### [712] [Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques](https://arxiv.org/abs/2506.04540)
*Kordel K. France*

Main category: cs.RO

TL;DR: 提出基于初始瞬态电流推断稳态参数的数学回归方法，将离子液体中计时安培法的测量时间缩短至1秒内，无需硬件改造。


<details>
  <summary>Details</summary>
Motivation: 解决室温离子液体(RTILs)因高粘度/慢传质导致的计时安培法测量时间过长问题，满足分析化学、传感器、电池等领域对快速电化学量化的迫切需求。

Method: 开发数学回归算法，通过分析初始瞬态电流响应(前1秒数据)预测稳态电化学参数，保持标准方法精度的同时免除硬件改造需求。

Result: 与标准方法对比验证显示，在数据采集时间减少1-4倍的情况下仍保持合理精度（误差<5%）。

Conclusion: 该技术为快速嗅觉/电化学分析中的多路复用测量创造了条件，在传感器优化和电池性能快速评估领域具应用潜力。

Abstract: Chronoamperometry (CA) is a fundamental electrochemical technique used for
quantifying redox-active species. However, in room-temperature ionic liquids
(RTILs), the high viscosity and slow mass transport often lead to extended
measurement durations. This paper presents a novel mathematical regression
approach that reduces CA measurement windows to under 1 second, significantly
faster than previously reported methods, which typically require 1-4 seconds or
longer. By applying an inference algorithm to the initial transient current
response, this method accurately predicts steady-state electrochemical
parameters without requiring additional hardware modifications. The approach is
validated through comparison with standard chronoamperometric techniques and is
demonstrated to maintain reasonable accuracy while dramatically reducing data
acquisition time. The implications of this technique are explored in analytical
chemistry, sensor technology, and battery science, where rapid electrochemical
quantification is critical. Our technique is focused on enabling faster
multiplexing of chronoamperometric measurements for rapid olfactory and
electrochemical analysis.

</details>


### [713] [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2506.04308)
*Enshen Zhou,Jingkun An,Cheng Chi,Yi Han,Shanyu Rong,Chi Zhang,Pengwei Wang,Zhongyuan Wang,Tiejun Huang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出3D感知VLM模型RoboRefer，引入数据集RefSpatial和基准RefSpatial - Bench，实验表明其在空间理解和推理上表现出色，可集成到多种机器人执行任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确理解复杂3D场景和动态推理指令指定的交互位置。

Method: 通过监督微调（SFT）集成深度编码器实现精确空间理解，通过强化微调（RFT）推进广义多步空间推理，引入数据集RefSpatial支持训练，引入RefSpatial - Bench进行评估。

Result: SFT训练的RoboRefer实现了最先进的空间理解，平均成功率89.6%；RFT训练的RoboRefer大幅超越其他基线，在RefSpatial - Bench上平均准确率超Gemini - 2.5 - Pro 17.4%。

Conclusion: RoboRefer在空间理解和推理方面表现优秀，可集成到多种机器人在复杂现实场景执行长时动态任务。

Abstract: Spatial referring is a fundamental capability of embodied robots to interact
with the 3D physical world. However, even with the powerful pretrained vision
language models (VLMs), recent approaches are still not qualified to accurately
understand the complex 3D scenes and dynamically reason about the
instruction-indicated locations for interaction. To this end, we propose
RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding
by integrating a disentangled but dedicated depth encoder via supervised
fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial
reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process
reward functions tailored for spatial referring tasks. To support SFT and RFT
training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x
prior), covering 31 spatial relations (vs. 15 prior) and supporting complex
reasoning processes (up to 5 steps). In addition, we introduce
RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial
referring with multi-step reasoning. Experiments show that SFT-trained
RoboRefer achieves state-of-the-art spatial understanding, with an average
success rate of 89.6%. RFT-trained RoboRefer further outperforms all other
baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average
accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various
control policies to execute long-horizon, dynamic tasks across diverse robots
(e,g., UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [714] [cuVSLAM: CUDA accelerated visual odometry](https://arxiv.org/abs/2506.04359)
*Alexander Korovko,Dmitry Slepichev,Alexander Efitorov,Aigul Dzhumamuratova,Viktor Kuznetsov,Hesam Rabeti,Joydeep Biswas*

Main category: cs.RO

TL;DR: 介绍cuVSLAM，可用于视觉同时定位与建图，支持多视觉 - 惯性传感器套件，经CUDA优化，有最佳性能。


<details>
  <summary>Details</summary>
Motivation: 为自主机器人提供准确且鲁棒的位姿估计解决方案。

Method: 使用CUDA对cuVSLAM进行专门优化，以在边缘计算设备上实时部署。

Result: 在多个最先进的基准测试中展现出最佳性能。

Conclusion: cuVSLAM是视觉同时定位与建图的先进解决方案，适用于多种机器人设置。

Abstract: Accurate and robust pose estimation is a key requirement for any autonomous
robot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous
localization and mapping, which can operate with a variety of visual-inertial
sensor suites, including multiple RGB and depth cameras, and inertial
measurement units. cuVSLAM supports operation with as few as one RGB camera to
as many as 32 cameras, in arbitrary geometric configurations, thus supporting a
wide range of robotic setups. cuVSLAM is specifically optimized using CUDA to
deploy in real-time applications with minimal computational overhead on
edge-computing devices such as the NVIDIA Jetson. We present the design and
implementation of cuVSLAM, example use cases, and empirical results on several
state-of-the-art benchmarks demonstrating the best-in-class performance of
cuVSLAM.

</details>


### [715] [Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System](https://arxiv.org/abs/2506.05020)
*Haokun Liu,Zhaoqi Ma,Yunong Li,Junichiro Sugihara,Yicheng Chen,Jinjie Li,Moju Zhao*

Main category: cs.RO

TL;DR: 提出集成LLM和VLM的分层框架用于异构多机器人系统，在真实任务中验证其适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态模型的方法难以应对异构多机器人系统任务多样性和动态环境，需要可泛化的智能。

Method: 提出集成提示大语言模型（LLM）和GridMask增强微调视觉语言模型（VLM）的分层框架，LLM进行任务分解和全局语义地图构建，VLM从航拍图像提取信息支持局部规划。

Result: 在真实世界的字母立方体排列任务实验中，证明了框架在动态环境中的适应性和鲁棒性。

Conclusion: 这是首次展示集成基于VLM感知与基于LLM任务推理和运动规划的空地异构系统。

Abstract: Heterogeneous multi-robot systems show great potential in complex tasks
requiring coordinated hybrid cooperation. However, traditional approaches
relying on static models often struggle with task diversity and dynamic
environments. This highlights the need for generalizable intelligence that can
bridge high-level reasoning with low-level execution across heterogeneous
agents. To address this, we propose a hierarchical framework integrating a
prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision
Language Model (VLM). The LLM performs task decomposition and global semantic
map construction, while the VLM extracts task-specified semantic labels and 2D
spatial information from aerial images to support local planning. Within this
framework, the aerial robot follows a globally optimized semantic path and
continuously provides bird-view images, guiding the ground robot's local
semantic navigation and manipulation, including target-absent scenarios where
implicit alignment is maintained. Experiments on a real-world letter-cubes
arrangement task demonstrate the framework's adaptability and robustness in
dynamic environments. To the best of our knowledge, this is the first
demonstration of an aerial-ground heterogeneous system integrating VLM-based
perception with LLM-driven task reasoning and motion planning.

</details>


### [716] [SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning](https://arxiv.org/abs/2506.04505)
*Nikita Oskolkov,Huzhenyu Zhang,Dmitry Makarov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: 提出SGN - CIRL框架用于无地图基于强化学习的机器人导航，采用模仿学习和课程学习，实验显示3D场景图用于强化学习提高导航成功率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 利用3D场景图模型的空间关系，实现无地图的基于强化学习的机器人导航，并加速和稳定强化学习算法的训练。

Method: 提出SGN - CIRL框架，采用模仿学习让智能体从演示中学习，采用课程学习按任务复杂度递增构建训练过程。

Result: 在Isaac Sim环境的数值实验表明，使用3D场景图进行强化学习显著提高了困难导航案例的成功率。

Conclusion: 3D场景图用于强化学习能有效提升机器人导航性能，且代码开源利于后续研究。

Abstract: The 3D scene graph models spatial relationships between objects, enabling the
agent to efficiently navigate in a partially observable environment and predict
the location of the target object.This paper proposes an original framework
named SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for
mapless reinforcement learning-based robot navigation with learnable
representation of open-vocabulary 3D scene graph. To accelerate and stabilize
the training of reinforcement learning-based algorithms, the framework also
employs imitation learning and curriculum learning. The first one enables the
agent to learn from demonstrations, while the second one structures the
training process by gradually increasing task complexity from simple to more
advanced scenarios. Numerical experiments conducted in the Isaac Sim
environment showed that using a 3D scene graph for reinforcement learning
significantly increased the success rate in difficult navigation cases. The
code is open-sourced and available at: https://github.com/Xisonik/Aloha\_graph.

</details>


### [717] [Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation](https://arxiv.org/abs/2506.04539)
*Kordel K. France,Ovidiu Daescu,Anirban Paul,Shalini Prasad*

Main category: cs.RO

TL;DR: 提出一种适用于多种嗅觉传感器类型的嗅觉惯性里程计（OIO）校准方法，在机械臂上验证其可提升嗅觉导航性能。


<details>
  <summary>Details</summary>
Motivation: 气体动力学和环境因素会干扰嗅觉导航任务，实现厘米级精度的OIO校准以应用于机器人手术和非接触式安全检查等场景。

Method: 定义一种适用于多种嗅觉传感器类型的OIO校准方法。

Result: 在真实机械臂上演示OIO校准过程，相比冷启动嗅觉导航任务，校准后性能提升。

Conclusion: 所提出的OIO校准方法可行且有效，能提升嗅觉导航性能。

Abstract: Visual inertial odometry (VIO) is a process for fusing visual and kinematic
data to understand a machine's state in a navigation task. Olfactory inertial
odometry (OIO) is an analog to VIO that fuses signals from gas sensors with
inertial data to help a robot navigate by scent. Gas dynamics and environmental
factors introduce disturbances into olfactory navigation tasks that can make
OIO difficult to facilitate. With our work here, we define a process for
calibrating a robot for OIO that generalizes to several olfaction sensor types.
Our focus is specifically on calibrating OIO for centimeter-level accuracy in
localizing an odor source on a slow-moving robot platform to demonstrate use
cases in robotic surgery and touchless security screening. We demonstrate our
process for OIO calibration on a real robotic arm and show how this calibration
improves performance over a cold-start olfactory navigation task.

</details>


### [718] [Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques](https://arxiv.org/abs/2506.04540)
*Kordel K. France*

Main category: cs.RO

TL;DR: 提出新颖数学回归方法，将计时电流法测量时间减至1秒内，经验证可在减少数据采集时间同时保持合理精度，并探讨其在多领域的应用。


<details>
  <summary>Details</summary>
Motivation: 室温离子液体中计时电流法因高粘度和慢传质导致测量时间长，需更快的测量方法。

Method: 应用推理算法处理初始瞬态电流响应，预测稳态电化学参数，无需额外硬件修改。

Result: 该方法测量时间显著快于以往方法，与标准技术对比验证了其能在大幅减少数据采集时间时保持合理精度。

Conclusion: 此技术可实现计时电流法测量的快速复用，用于快速嗅觉和电化学分析，在多领域有重要意义。

Abstract: Chronoamperometry (CA) is a fundamental electrochemical technique used for
quantifying redox-active species. However, in room-temperature ionic liquids
(RTILs), the high viscosity and slow mass transport often lead to extended
measurement durations. This paper presents a novel mathematical regression
approach that reduces CA measurement windows to under 1 second, significantly
faster than previously reported methods, which typically require 1-4 seconds or
longer. By applying an inference algorithm to the initial transient current
response, this method accurately predicts steady-state electrochemical
parameters without requiring additional hardware modifications. The approach is
validated through comparison with standard chronoamperometric techniques and is
demonstrated to maintain reasonable accuracy while dramatically reducing data
acquisition time. The implications of this technique are explored in analytical
chemistry, sensor technology, and battery science, where rapid electrochemical
quantification is critical. Our technique is focused on enabling faster
multiplexing of chronoamperometric measurements for rapid olfactory and
electrochemical analysis.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [719] [Optical Physics-Based Generative Models](https://arxiv.org/abs/2506.04357)
*Amirreza Ahmadnejad,Somayyeh Koohi*

Main category: physics.optics

TL;DR: 建立了连接光学物理方程与生成模型的数学框架，揭示非线性光学模型通过自组织原理实现参数减少40-60%、训练效率提升30-50%、生成质量显著优于传统方法，促进AI与光学的双向发展


<details>
  <summary>Details</summary>
Motivation: 利用光传播动力学中的自组织现象改进生成模型，通过非线性光学特性解决传统模型参数冗余、模式崩溃等问题，探索物理系统与人工智能的深层联系

Method: 构建光学方程与生成模型的数学映射框架，对比线性/非线性光学模型特性（自聚焦、立方五次平衡作用、强度依赖折射率），将非线性稳定性和自适应机制应用于生成过程

Result: 非线性Helmholtz模型FID 0.0089（线性1.0909），内存降40-60%；立方五次模型实现0.0156 FID且无模式崩溃；整体训练效率提升30-50%，折射率重建精度达95%

Conclusion: 该框架双向推动光学计算与生成式AI发展，证实物理自组织机制可显著提升模型效率，为光学AI芯片实现提供新路径，开辟波前控制与孤子分析新方法

Abstract: This paper establishes a comprehensive mathematical framework connecting
optical physics equations to generative models, demonstrating how light
propagation dynamics inspire powerful artificial intelligence approaches. We
analyze six fundamental optical equations, comparing linear models (Helmholtz,
dissipative wave, and Eikonal equations) with their nonlinear extensions
incorporating Kerr effects, cubic-quintic nonlinearities, and
intensity-dependent refractive indices. Our nonlinear optical models reveal
remarkable capabilities through natural self-organization principles. The
nonlinear Helmholtz model achieves 40-60% parameter reduction while maintaining
superior mode separation via self-focusing phenomena. The cubic-quintic
dissipative wave model prevents mode collapse through balanced
attractive-repulsive interactions, enabling stable soliton formation with
20-40% improved coverage. The intensity-dependent Eikonal model creates
adaptive pathways that dynamically respond to content, providing enhanced
controllability in conditional generation. Experimental validation demonstrates
consistent superiority over linear predecessors and traditional generative
approaches. The nonlinear Helmholtz model achieves FID scores of 0.0089 versus
1.0909 for linear versions, while the cubic-quintic model reaches 0.0156 FID
with exceptional stability. Memory usage drops 40-60% and training time
improves 30-50% due to inherent nonlinear stability properties. The framework
enables bidirectional benefits, advancing both generative AI and optical
physics through novel approaches to soliton analysis, wavefront control, and
refractive index reconstruction with 95% accuracy. This work reveals deep
connections between physical self-organization and artificial intelligence,
opening pathways toward efficient optical computing implementations.

</details>


### [720] [Optical Physics-Based Generative Models](https://arxiv.org/abs/2506.04357)
*Amirreza Ahmadnejad,Somayyeh Koohi*

Main category: physics.optics

TL;DR: 本文建立光学物理方程与生成模型的数学框架，对比线性与非线性光学方程，非线性模型表现出色，实验验证优于线性和传统方法，实现双向收益，揭示物理与AI联系。


<details>
  <summary>Details</summary>
Motivation: 探索光传播动力学对人工智能方法的启发，建立光学物理与生成模型的联系。

Method: 分析六个基本光学方程，对比线性模型及其包含Kerr效应等的非线性扩展模型。

Result: 非线性模型参数减少、性能提升，FID分数优，内存使用和训练时间改善，对光孤子分析等有95%准确率。

Conclusion: 该框架使生成AI和光学物理双向受益，揭示物理自组织与AI的联系，为光计算实现提供途径。

Abstract: This paper establishes a comprehensive mathematical framework connecting
optical physics equations to generative models, demonstrating how light
propagation dynamics inspire powerful artificial intelligence approaches. We
analyze six fundamental optical equations, comparing linear models (Helmholtz,
dissipative wave, and Eikonal equations) with their nonlinear extensions
incorporating Kerr effects, cubic-quintic nonlinearities, and
intensity-dependent refractive indices. Our nonlinear optical models reveal
remarkable capabilities through natural self-organization principles. The
nonlinear Helmholtz model achieves 40-60% parameter reduction while maintaining
superior mode separation via self-focusing phenomena. The cubic-quintic
dissipative wave model prevents mode collapse through balanced
attractive-repulsive interactions, enabling stable soliton formation with
20-40% improved coverage. The intensity-dependent Eikonal model creates
adaptive pathways that dynamically respond to content, providing enhanced
controllability in conditional generation. Experimental validation demonstrates
consistent superiority over linear predecessors and traditional generative
approaches. The nonlinear Helmholtz model achieves FID scores of 0.0089 versus
1.0909 for linear versions, while the cubic-quintic model reaches 0.0156 FID
with exceptional stability. Memory usage drops 40-60% and training time
improves 30-50% due to inherent nonlinear stability properties. The framework
enables bidirectional benefits, advancing both generative AI and optical
physics through novel approaches to soliton analysis, wavefront control, and
refractive index reconstruction with 95% accuracy. This work reveals deep
connections between physical self-organization and artificial intelligence,
opening pathways toward efficient optical computing implementations.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [721] [Knowledge-guided Contextual Gene Set Analysis Using Large Language Models](https://arxiv.org/abs/2506.04303)
*Zhizheng Wang,Chi-Ping Day,Chih-Hsuan Wei,Qiao Jin,Robert Leaman,Yifan Yang,Shubo Tian,Aodong Qiu,Yin Fang,Qingqing Zhu,Xinghua Lu,Zhiyong Lu*

Main category: q-bio.GN

TL;DR: 提出cGSA框架，通过结合临床背景的AI驱动通路优先排序，提升基因集分析的精准度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GSA方法忽略临床背景，产生冗余/非特异性通路结果，需耗费大量人工解读且可靠性低。

Method: 整合基因簇检测、富集分析和LLM，筛选统计显著且具生物学意义的通路。基于19种疾病102个基因集进行验证。

Result: 基准测试显示cGSA性能超越基线30%，专家验证证实其精确性。黑色素瘤和乳腺癌案例验证其上下文洞察能力。

Conclusion: cGSA实现了临床背景敏感的机制发现，支持精准假设生成，推动疾病研究的可靠性提升。

Abstract: Gene set analysis (GSA) is a foundational approach for interpreting genomic
data of diseases by linking genes to biological processes. However,
conventional GSA methods overlook clinical context of the analyses, often
generating long lists of enriched pathways with redundant, nonspecific, or
irrelevant results. Interpreting these requires extensive, ad-hoc manual
effort, reducing both reliability and reproducibility. To address this
limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by
incorporating context-aware pathway prioritization. cGSA integrates gene
cluster detection, enrichment analysis, and large language models to identify
pathways that are not only statistically significant but also biologically
meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases
and ten disease-related biological mechanisms shows that cGSA outperforms
baseline methods by over 30%, with expert validation confirming its increased
precision and interpretability. Two independent case studies in melanoma and
breast cancer further demonstrate its potential to uncover context-specific
insights and support targeted hypothesis generation.

</details>


### [722] [Knowledge-guided Contextual Gene Set Analysis Using Large Language Models](https://arxiv.org/abs/2506.04303)
*Zhizheng Wang,Chi-Ping Day,Chih-Hsuan Wei,Qiao Jin,Robert Leaman,Yifan Yang,Shubo Tian,Aodong Qiu,Yin Fang,Qingqing Zhu,Xinghua Lu,Zhiyong Lu*

Main category: q-bio.GN

TL;DR: 传统基因集分析（GSA）方法有局限，本文提出AI驱动的cGSA框架，在基准测试中表现优于基线方法，案例研究也证明其潜力。


<details>
  <summary>Details</summary>
Motivation: 传统GSA方法忽略临床背景，结果冗余、非特异性或不相关，需大量人工解释，可靠性和可重复性低。

Method: 引入cGSA框架，整合基因簇检测、富集分析和大语言模型进行上下文感知的通路优先级排序。

Result: 在102个手动策划基因集、19种疾病和10种疾病相关生物机制的基准测试中，cGSA比基线方法表现好超30%，专家验证其精度和可解释性提高，两个案例研究展示其潜力。

Conclusion: cGSA框架能增强GSA，具有更高精度和可解释性，可揭示特定背景见解并支持针对性假设生成。

Abstract: Gene set analysis (GSA) is a foundational approach for interpreting genomic
data of diseases by linking genes to biological processes. However,
conventional GSA methods overlook clinical context of the analyses, often
generating long lists of enriched pathways with redundant, nonspecific, or
irrelevant results. Interpreting these requires extensive, ad-hoc manual
effort, reducing both reliability and reproducibility. To address this
limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by
incorporating context-aware pathway prioritization. cGSA integrates gene
cluster detection, enrichment analysis, and large language models to identify
pathways that are not only statistically significant but also biologically
meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases
and ten disease-related biological mechanisms shows that cGSA outperforms
baseline methods by over 30%, with expert validation confirming its increased
precision and interpretability. Two independent case studies in melanoma and
breast cancer further demonstrate its potential to uncover context-specific
insights and support targeted hypothesis generation.

</details>
