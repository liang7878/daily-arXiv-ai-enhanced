<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 24]
- [stat.ML](#stat.ML) [Total: 11]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 3]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [econ.GN](#econ.GN) [Total: 6]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CY](#cs.CY) [Total: 7]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.CC](#cs.CC) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.SD](#cs.SD) [Total: 7]
- [eess.SY](#eess.SY) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

Main category: cs.AI

TL;DR: 提出高维空间信息项表示和计算的非结合代数框架，可构建长序列稀疏表示，有L和R状态，能复制串行位置曲线。


<details>
  <summary>Details</summary>
Motivation: 解决依赖关联捆绑的模型丢失顺序信息问题，更好表示认知任务所需的顺序信息。

Method: 通过类似乘法的绑定和非结合干扰式捆绑进行计算。

Result: 该框架用L和R两个不同状态表示序列，能复制串行位置曲线。

Conclusion: 非结合代数框架适用于高维空间信息表示和计算，与大脑记忆活动可能相关。

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [2] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill,Tom Jeleniewski,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文引入模块化语义模型和高效知识图谱生成方法，在航空维护领域验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 时间连续动态模型在CPS应用中需整合信息，但缺乏可复用本体工件和方法减少手动实例化工作。

Method: 引入基于标准的模块化语义模型表示微分方程，提出高效知识图谱生成方法。

Result: 复杂电液伺服作动器的微分方程能在知识图谱中形式化表示并与其他生命周期数据关联。

Conclusion: 所提出的工件具有实际应用价值。

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [3] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson,Ahmed Amer,Evan Harris,Preeti Ravindra,Shujun Zhang*

Main category: cs.AI

TL;DR: 本文提出“超我”代理解决代理式AI行为与人类价值观等对齐难题，经评估可大幅降低有害输出，简化个性化AI对齐。


<details>
  <summary>Details</summary>
Motivation: 代理式AI实际部署面临与人类价值观、安全要求和合规需求对齐的挑战，现有对齐方法存在不足。

Method: 引入“超我”代理作为个性化监督机制，通过参考用户选择的“信条宪法”动态引导AI规划，实时合规执行器在执行前验证计划。

Result: 综合基准评估显示，“超我”代理大幅降低有害输出，如对领先大模型Harm分数降低达98.3%，拒绝率接近完美。

Conclusion: 该方法简化个性化AI对齐，使代理式系统更符合个人和文化背景，同时提升安全性。

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [4] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 本文指出基础模型评估中人类基线应更严谨透明，给出建议和清单以促进更有意义的人机性能比较。


<details>
  <summary>Details</summary>
Motivation: 现有基线方法不足以有力衡量和评估人机性能差异，为使机器学习社区、下游用户和政策制定者能解读AI评估，需更严谨透明的人类基线。

Method: 基于测量理论和AI评估文献的元综述得出设计、执行和报告人类基线的框架，合成清单审查115个人类基线研究。

Result: 发现现有基线方法的不足，清单可帮助研究人员开展人类基线研究并报告结果。

Conclusion: 希望推动更严谨的AI评估实践，服务研究界和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [5] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

Main category: cs.AI

TL;DR: 发布NordDRG - AI - Benchmark评估大语言模型处理DRG逻辑能力，展示不同模型表现并确认其能凸显特定领域优劣势。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对医院资金层面（DRG决定报销）的开放基准，需要评估大语言模型在该领域的推理能力。

Method: 发布NordDRG - AI - Benchmark，包含定义表、专家手册和任务提示包，用自动可验证任务评估模型。

Result: 5个先进大语言模型在9个自动可验证任务中表现差异大，o3得分9/9，GPT - 4o和o4 - mini - high得分7/9，Gemini 2.5 Pro和Gemini 2.5 Flash分别得分5/9和3/9。

Conclusion: NordDRG - AI - Benchmark能凸显通用基准中隐藏的领域特定优劣势，为医院资金可信自动化研究提供可复现基线。

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [6] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: 介绍用于历史身份解析的基准数据集ICE - ID，评估多种方法，NARS表现佳，发布数据集和代码促进可复现研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模开放的用于研究现实人群中长期人员实体匹配的表格数据集，需要推进身份解析研究。

Method: 定义身份解析任务，评估手工规则匹配器、ML集成、LLMs及NARS。

Result: NARS简单且有竞争力，在任务中达到SOTA。

Conclusion: 发布ICE - ID和代码可实现纵向设置下身份解析方法的可复现基准测试，为跨学科研究开辟新途径。

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [7] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang,Jiayu Qian,Zegao Peng,Haoyu Zhang,Zhi-An Huang*

Main category: cs.AI

TL;DR: 大推理模型在医学领域推理不佳，提出Med - REFL方法，实验显示有性能提升，表明注重反思质量有益医学AI推理。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在医学领域推理未取得理想效果，关键问题是未充分关注中间反思步骤质量，需解决此挑战。

Method: 提出Med - REFL方法，利用思维树方法将医学问题分解为细粒度推理路径，定量评估各步骤及后续反思，自动构建直接偏好优化数据。

Result: 在MedQA - USMLE基准上平均提升4.11%，将7B/8B模型的SOTA性能再提升4.13%，在多个数据集上有强泛化和鲁棒性。

Conclusion: 优先考虑反思质量可使医学AI应用的推理更准确和可信。

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [8] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi,Yongqing Wang,Fangda Guo,Jiangli Shao,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 为解决GNN模型检测社交机器人时的标签稀缺问题，提出多源图域适应模型BotTrans，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决利用相关社交网络知识进行社交机器人检测时面临的网络异质性和单源迁移效果不佳的问题。

Method: 探索多源域，建立跨源域拓扑，聚合跨域邻居信息，将源 - 目标对相关性与模型优化结合，提出利用目标域语义知识的细化策略。

Result: 在真实数据集上的大量实验表明BotTrans优于现有最先进方法。

Conclusion: BotTrans在目标检测任务无标签时能有效利用多源知识。

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [9] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

Main category: cs.AI

TL;DR: 提出一套可扩展算法用于最小化大规模加权有向图中的反馈弧，在果蝇数据集验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 揭示神经连接组中有生物学意义的前馈结构。

Method: 整合贪心启发式、增益感知局部细化和基于强连通分量的全局结构分析。

Result: 最佳方案比之前的方法提高了前向边权重。

Conclusion: 所提出算法有效，可高效用Python实现并在云端验证。

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [10] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Main category: cs.AI

TL;DR: 人类以因果关系理解世界，当前机器学习系统在这方面较弱。虽引入SCM框架有进展，但该框架未能捕捉人类因果认知特点。应结合人类因果认知特性，让机器学习与因果关系研究创造更强大系统。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统在因果认知方面较弱，构建类人因果能力有助于构建有效且可解释的AI，但SCM框架存在不足，因此要探索人类因果认知特性以推动相关研究。

Method: 分析人类因果认知特性，对比SCM框架与人类因果认知在人类生存环境中的差异，探讨人类因果能力在人类生存环境中的适应性。

Result: 指出SCM框架未能捕捉人类因果认知的一些显著方面，如人类可通过因果类比在相似对象间进行知识泛化，而SCM难以表达此类类比。

Conclusion: 未来机器学习与因果关系研究应利用类人归纳偏差，创建更强大、可控和可解释的系统。

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [11] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

Main category: cs.AI

TL;DR: 提出模式感知复杂性框架，利用结构规律降低计算复杂度，在TSP基准测试中获质量提升。


<details>
  <summary>Details</summary>
Motivation: NP难优化问题在最坏情况下难有效求解，但现实实例有可利用模式，需找到降低计算复杂度的方法。

Method: 提出模式感知复杂性框架，用严格定义、定理和元学习驱动求解器管道，引入PUE指标。

Result: 在TSP基准测试（22到2392个城市）中实现高达79%的解决方案质量提升。

Conclusion: 该方法与理论NP难不同，为模式驱动效率提供统一实用视角。

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [12] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.AI

TL;DR: 提出自反综合信息单元RIIU，证明其特性，在Grid - world测试中表现优于GRU，将意识研究转为实证数学问题。


<details>
  <summary>Details</summary>
Motivation: 人工意识研究缺乏类似感知器的可训练模块，需找到这样的模块推进研究。

Method: 引入RIIU，通过滑动窗口协方差和可微的Auto - Φ代理使RIIU在线最大化局部信息整合，证明其相关特性。

Result: 在八向Grid - world中，四层RIIU代理在执行器故障后13步内恢复超90%奖励，速度是参数匹配GRU的两倍，且保持非零Auto - Φ信号。

Conclusion: RIIU将意识计算缩小到单元规模，把哲学辩论转化为实证数学问题。

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [13] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: 本文介绍用于客户支持对话的隐式推荐系统ImpReSS，评估显示其推荐效果良好，数据和代码可按需共享。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注在客户支持交互中隐式集成推荐，本文旨在解决该问题。

Method: 引入ImpReSS系统，该系统与现有支持聊天机器人协同工作，基于客户支持对话识别推荐相关解决方案产品类别。

Result: 实证评估显示，ImpReSS在不同场景下推荐相关解决方案产品类别的效果良好，如通用问题解决、信息安全支持和网络安全故障排除。

Conclusion: ImpReSS系统能有效在客户支持对话中隐式推荐相关解决方案产品类别，支持业务增长，数据和代码共享利于未来研究。

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [14] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda,Yu Zheng,Ruixian Ma,Mingyang Sun,Devesh Pansare,Fabio Duarte,Paolo Santi*

Main category: cs.AI

TL;DR: 论文引入LocationReasoner基准评估大语言模型在现实选址场景的推理能力，评估发现先进推理模型提升有限，部分策略效果不佳，最后发布该基准。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力主要在数学和代码领域评估，需评估其在复杂现实场景的推理能力。

Method: 引入LocationReasoner基准，包含300多个不同难度查询及约束选址搜索工具的沙盒环境进行评估。

Result: 先进推理模型在现实场景相比非推理模型提升有限，OpenAI o4有30%选址任务失败，部分策略因过度推理效果更差。

Conclusion: 指出大语言模型在整体和非线性推理的关键局限，发布LocationReasoner促进其在现实决策任务中发展。

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [15] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago,Ghada Zamzmi,Brandon Eich,Jana G. Delfino*

Main category: cs.AI

TL;DR: 提出评估可解释AI特征的框架，含四个评估标准并开发计分卡，以乳腺病变检测为例说明，旨在推动可解释性特征价值讨论并改进AI医疗设备。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI设备解释质量的技术，需提出评估框架。

Method: 基于一致性、合理性、保真度和有用性四个标准构建评估框架，开发计分卡，以Ablation CAM和Eigen CAM为例评估解释热力图。

Result: 通过案例研究评估了前三个标准在临床相关场景中的情况。

Conclusion: 提出的框架可评估AI模型解释质量，有助于推动可解释性特征价值讨论和改进AI医疗设备开发与评估。

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [16] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo,Deshendran Moodley*

Main category: cs.AI

TL;DR: 提出从基于本体的知识图谱和多模态电子病历数据构建贝叶斯网络用于可解释疾病风险预测的新方法，以房颤为例验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 多模态电子病历数据用于疾病风险预测需适配特定医疗场景和患者群体，且风险预测系统要处理数据不确定性和保证可解释性。

Method: 提出从基于本体的知识图谱和多模态电子病历数据构建贝叶斯网络的方法。

Result: 以房颤为例，结合真实电子病历数据，表明该方法能平衡通用医学知识和患者特定情境，有效处理不确定性，可解释性高且预测性能好。

Conclusion: 所提方法可用于可解释疾病风险预测，能应对现有挑战。

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [17] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David,Yair Meidan,Ido Hersko,Daniel Varnovitzky,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: 提出ProfiLLM框架用于聊天机器人交互中的隐式动态用户画像，在ITSec领域验证有效，还提供相关方法、分类法、代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有聊天机器人难以根据用户特征个性化回复，尤其在ITSec等专业领域，现有个性化方法适应性不足。

Method: 提出ProfiLLM框架，包括可适应多领域的分类法和基于大语言模型的用户画像方法，在ITSec领域应用并评估。

Result: ProfiLLM[ITSec]能快速准确推断ITSec画像，单次提示后缩小实际与预测分数差距55 - 65%。

Conclusion: ProfiLLM框架有效，还提供相关资源支持后续研究。

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [18] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta,Bhabesh Mali,Chandan Karfa*

Main category: cs.AI

TL;DR: 本文介绍了使用大语言模型引导的蒙特卡罗树搜索的SystemVerilog断言生成框架SANGAM，其分三阶段生成SVAs，结果显示优于近期方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理领域的进展为硬件断言生成技术带来新可能，期望实现从行业级规范自动生成SVAs。

Method: 采用三阶段方法，第一阶段用多模态规范处理，第二阶段用蒙特卡罗树自精炼算法，第三阶段结合推理痕迹生成SVA断言。

Result: 框架SANGAM能生成强大的SVAs集合，在评估过程中表现优于近期方法。

Conclusion: 所提出的SANGAM框架在自动生成SVAs方面具有有效性和优势。

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [19] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

Main category: cs.AI

TL;DR: 多模态机器智能系统出现机器幻景类认知异常，本文列举部分错误，强调需明确界定和系统评估，理解其对提升可靠性和构建生态重要。


<details>
  <summary>Details</summary>
Motivation: 随着多模态机器智能系统发展，出现机器幻景这类新的认知异常，需对其进行研究。

Method: 文章列举机器幻景包含的多种错误形式，如妄想、幻觉等。

Result: 明确提出机器幻景包含多种类似但非人类或动物易错性的错误形式。

Conclusion: 必须明确界定和系统评估这些失败，理解机器幻景对提升机器智能可靠性和构建智能生态至关重要。

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [20] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov,Akhil Bagaria,Ziyan Luo,George Konidaris,Doina Precup,Marlos C. Machado*

Main category: cs.AI

TL;DR: 本文探讨分层强化学习（HRL）在复杂环境决策中的应用，分析其益处、方法，指出挑战与适用领域。


<details>
  <summary>Details</summary>
Motivation: 在复杂开放环境中开发智能体是AI的重大挑战，HRL有潜力但对良好结构定义及适用问题不明确，需明确其益处和影响。

Method: 从决策基本挑战角度分析HRL益处，介绍从在线经验、离线数据集学习及利用大语言模型发现时间结构的方法。

Result: 明确了HRL的益处，涵盖了发现时间结构的方法家族。

Conclusion: 指出时间结构发现的挑战和特别适合的领域。

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [21] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Ilya Ilyankou,Junyuan Liu,Lu Yin,Weiming Huang,Natchapon Jongwiriyanurak*

Main category: cs.AI

TL;DR: 文章探索CaLLiPer框架用于个体移动预测的位置嵌入，实验表明其优于基线模型，突出多模态归纳位置嵌入潜力并公开代码数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法在编码空间信息、整合语义上下文和适应新位置方面存在局限，需要新方法提升个体移动预测能力。

Method: 应用CaLLiPer框架，通过对比学习融合空间坐标和兴趣点语义特征进行位置嵌入。

Result: 在四个公开移动数据集的传统和归纳设置实验中，CaLLiPer始终优于强基线模型，在归纳场景表现出色。

Conclusion: 多模态、归纳位置嵌入有潜力提升人类移动预测系统能力。

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [22] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles,Rattandeep Singh,Isaac Song Zhou Yu*

Main category: cs.AI

TL;DR: 提出新表单填写基准，指出基线模型准确率低，推出辅助工具FieldFinder提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决纯图像领域无OCR等条件下表单填写的难题，应对计算机代理所需的多能力要求。

Method: 提出含432个字段、55个文档和3个任务的表单填写基准，开发辅助工具FieldFinder。

Result: 基线VLA准确率多数不到1%，GUI代理得分10.6 - 68.0%，使用FieldFinder后模型在所有研究条件下性能持平或提升，最高从2%提至56%。

Conclusion: FieldFinder能有效提升模型在表单填写任务中的性能。

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [23] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

Main category: cs.AI

TL;DR: 本文提出用微调llama - 3.2 - 1b作为相关评分器，提升RAG检索文档相关性验证的精度，代码开源。


<details>
  <summary>Details</summary>
Motivation: RAG中确保检索文档与查询的相关性是挑战，为降低相关评分器计算需求，需要轻量级小语言模型。

Method: 微调llama - 3.2 - 1b作为相关评分器。

Result: 将精度从0.1301显著提升至0.7750，精度与llama - 3.1 - 70b相当。

Conclusion: 微调的llama - 3.2 - 1b可有效作为RAG中的相关评分器，提升检索文档相关性验证效果。

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [24] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin,Shai Vardi,Vidyanand Choudhary*

Main category: cs.AI

TL;DR: 本文全面研究大语言模型在多架构和领域中的位置偏差，发现新偏差，提出分类框架，揭示偏差影响并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 之前研究未系统剖析大语言模型驱动比较中的位置顺序偏差，也未将其与潜在偏好结构关联，本文旨在填补这一空白。

Method: 对多个大语言模型架构和领域的位置偏差进行全面调查，引入框架对成对偏好进行分类。

Result: 发现强且一致的顺序效应、新的中心性偏差、质量依赖的偏差转移、对特定名称的偏差；顺序效应会使模型选择较差选项，位置偏差通常强于性别偏差。

Conclusion: 大语言模型不仅继承类人偏差，还存在人类决策中未见的失败模式，提出针对性缓解策略以减少顺序驱动的偏差。

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [25] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang,Yang Chen,Toru Takisaka,Kaiqi Zhao,Weidong Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 本文提出SCRL框架解决情境约束下的顺序资源分配问题，经实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决情境约束下顺序资源分配这一现实应用中的重大挑战。

Method: 将情境约束形式化为逻辑蕴涵，开发动态惩罚约束违规的新算法，提出概率选择机制克服传统约束强化学习方法的局限。

Result: 在医疗资源分配和农药分配两个场景的实验中，SCRL在满足约束的同时保持高资源效率，优于现有基线。

Conclusion: SCRL在现实世界中上下文敏感的决策任务中有应用潜力。

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [26] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang,Aitong Wu,Yao Lu,Guangda Sun*

Main category: cs.AI

TL;DR: 本文提出协作可编辑模型CoEM解决垂直领域大语言模型训练难题，在金融场景验证有显著效果。


<details>
  <summary>Details</summary>
Motivation: 垂直领域大语言模型训练依赖大量标注数据和计算资源，阻碍快速发展和迭代。

Method: 引入CoEM，构建候选知识池，通过用户-模型交互对话、用户评分和归因分析确定高价值知识片段，通过上下文提示注入进行轻量级领域适配。

Result: 在金融信息场景收集约120名用户的15k条反馈，经用户评分验证，在特定领域生成方面有显著改进，避免传统微调工作流的时间和计算开销。

Conclusion: CoEM能让大语言模型利用高价值知识生成更准确和特定领域的内容，可有效解决垂直领域大语言模型训练难题。

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [27] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying,Daniel Xu,Alicia Zhang,Katherine M. Collins,Max H. Siegel,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 提出神经符号模型整合多源信息进行推理，通过猜物游戏评估，模型与人类判断强相关。


<details>
  <summary>Details</summary>
Motivation: 探究如何灵活整合多源信息来理解未知世界。

Method: 提出神经符号模型，用神经网络解析多模态输入，用贝叶斯模型整合信息评估假设，通过“盒子里有什么”游戏评估。

Result: 该模型与人类判断强相关，单模态消融模型和大型多模态神经模型基线相关性差。

Conclusion: 所提出的神经符号模型在整合多源信息进行推理方面表现良好，优于对比模型。

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [28] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li,Siqi Liu,Bochao Zou,Jiansheng Chen,Huimin Ma*

Main category: cs.AI

TL;DR: 现有评估机器心智理论方法有局限，研究用基于内部机制方法对多模态大语言模型心智理论进行可解释性评估，构建数据集，发现注意力头可区分信息并提出提升方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估机器心智理论的方法主要针对单模态模型，且将模型视为黑盒，缺乏对内部机制的解释性探索。

Method: 构建多模态心智理论测试数据集GridToM，分析多模态大模型中注意力头区分跨视角认知信息的能力，提出通过调整注意力头方向提升模型心智理论表现的轻量级免训练方法。

Result: 注意力头能区分跨视角认知信息，调整注意力头方向可显著提升模型表现出的心智理论能力。

Conclusion: 基于内部机制的方法能对多模态大语言模型的心智理论进行可解释性评估，且提出的方法可提升模型相关能力。

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [29] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Main category: cs.AI

TL;DR: 提出基于因果哲学的AI抽象因果推理测试，在大语言模型上验证，提出更具普适性的神经元图中原因定义，认为结果展示了未来哲学研究的发展方向。


<details>
  <summary>Details</summary>
Motivation: 开展AI抽象因果推理测试，评估AI在因果判断上的能力。

Method: 基于因果哲学中D. Lewis推广的神经元图设计测试，并在先进大语言模型（ChatGPT、DeepSeek和Gemini）上进行测试。

Result: 这些聊天机器人能正确识别文献中激烈争论的因果案例，提出了比以往更具普适性的神经元图中原因的定义。

Conclusion: 结果表明未来哲学研究可能是人类与人工智能专业知识的相互作用。

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [30] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.AI

TL;DR: 研究基于AST的混合图表示在基于GNN的代码克隆检测中的有效性，比较多种混合表示在不同GNN架构下的表现。


<details>
  <summary>Details</summary>
Motivation: 代码克隆增加软件维护成本和安全风险，基于AST的深度学习代码克隆检测缺乏语义深度，虽有研究用语义图丰富表示，但不同表示的有效性和与机器学习技术的兼容性待研究。

Method: 开展全面实证研究，系统比较多种混合表示（CFG、DFG、FA - AST）在多个GNN架构下的表现。

Result: 混合表示对GNN影响不同，AST+CFG+DFG能提升卷积和注意力模型准确性，FA - AST常带来结构复杂性损害性能，GMN用标准AST表示就表现优异。

Conclusion: 揭示了不同基于AST的混合图表示在GNN代码克隆检测中的效果差异，GMN在跨代码相似度检测上有优势，减少对丰富结构的依赖。

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [31] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen,Zihan Liu,Shun Zheng,Zhijian Xu,Shengyu Ye,Zhirong Wu,Xiao Liang,Yang Wang,Junjie Li,Ziming Miao,Jiang Bian,Mao Yang*

Main category: cs.AI

TL;DR: 本文指出RLVR在Pass@K指标下的悖论，提出新指标CoT - Pass@K，证实RLVR能促进正确推理泛化。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在Pass@K指标下表现不佳的矛盾，明确RLVR在提升大语言模型推理能力方面的作用。

Method: 引入新评估指标CoT - Pass@K，为RLVR建立新理论基础，分析训练动态。

Result: 使用CoT - Pass@K指标发现RLVR能激励正确推理泛化，且增强的推理能力在训练早期出现并能平滑泛化。

Conclusion: 清晰阐述了RLVR的作用，提供更可靠评估方法，证实其提升机器推理的潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [32] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Qifan Zheng,Wenxin Li*

Main category: cs.AI

TL;DR: 提出Mxplainer算法学习麻将黑盒AI参数，提供可解释性并解释决策过程


<details>
  <summary>Details</summary>
Motivation: 人们需内化AI技能提升自身能力，但现有麻将AI多为黑盒，难以获取见解

Method: 引入可转化为等效神经网络的参数化搜索算法Mxplainer学习黑盒代理参数

Result: 在AI和人类玩家数据实验中，学习的参数能提供对代理特征和玩法的可理解见解，框架可局部解释黑盒代理决策过程

Conclusion: Mxplainer算法可有效为麻将黑盒AI提供可解释性

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [33] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Main category: cs.AI

TL;DR: 文章聚焦ARC-AGI挑战，利用深度学习范式，结合测试时训练等方法，提升ARC性能，取得当前最佳成绩并强调推理系统关键要素。


<details>
  <summary>Details</summary>
Motivation: 应对ARC-AGI挑战，提升AI系统在ARC上的表现。

Method: 结合测试时训练，提出从预训练大模型训练ARC的方法、TTFT和AIRV技术。

Result: AIRV使准确率提升260%，TTFT再提升300%，早期版本获竞赛第一，最终版本ARC私有测试集得分58%。

Conclusion: 指出陌生领域强大推理系统的关键要素，强调提升广泛感知推理的核心机制。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [34] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng,Siqi Wang,Chuzhao Zhu,Li Li*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的ADRD框架用于自动驾驶决策，实验显示该框架性能优越，有实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 解决如何构建可解释的自动驾驶决策系统的问题。

Method: 提出ADRD框架，包含信息模块、代理模块和测试模块，先通过信息模块聚合信息，再由代理模块生成策略，经测试模块迭代优化。

Result: ADRD在自动驾驶决策任务中表现优越，在可解释性、响应速度和驾驶性能上比传统强化学习和先进的基于大语言模型的方法更具优势。

Conclusion: 该框架能理解复杂驾驶场景，基于规则的透明决策系统有广阔前景，首次结合大语言模型和基于规则的系统用于自动驾驶决策，有实际部署潜力。

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [35] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan,Feng Shen,Fujuan Li,Yanjin Sun,Yan Li,Shiwen Zhang*

Main category: cs.AI

TL;DR: 本文提出RALA - DPO方法用于航空理论培训，结合DPO和RAG技术，实验表明该方法能提高专业航空知识回答的准确性并实现零成本知识更新。


<details>
  <summary>Details</summary>
Motivation: 现有航空培训系统因教员数量有限、网络答案不准确导致效率低，基础预训练模型无法准确回答专业领域问题，传统SFT存在数据覆盖不足问题。

Method: 提出Retrieval - Augmented LLM Alignment via Direct Preference Optimization (RALA - DPO)，选用开源预训练大模型Qwen，通过基于DPO的领域对齐进行适配，同时采用Retrieval - Augmented Generation (RAG)技术。

Result: RALA - DPO能提高专业航空知识回答的准确性，集成RAG机制可进一步提高答案准确性并实现零成本知识更新。

Conclusion: RALA - DPO方法在航空理论培训中能有效提高回答准确性，集成RAG机制有更好效果。

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [36] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 现有大语言模型微调工作忽视安全对齐能力退化，本文提出SEAT方法，实验显示其能在保留微调性能的同时显著提升模型承认无知的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调工作主要关注保留特定数据或任务，忽视了安全对齐中模型承认无知能力的退化，导致幻觉等不良行为。

Method: 提出SEAT方法，包含约束激活漂移的稀疏训练和对抗知识纠缠的实体扰动方法与KL散度正则化。

Result: SEAT在保留微调性能的同时，在保留无知意识方面显著优于基线。

Conclusion: SEAT为大语言模型微调提供了更鲁棒的解决方案。

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [37] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang,Zhilong Song,Jiawei Chen,Mingli Song,Sheng Zhou,linjun sun,Xiaogang Ouyang,Chun Chen,Can Wang*

Main category: cs.AI

TL;DR: 提出新数据集GUI - Robust用于GUI代理评估，采用半自动化构建范式降低标注成本，评估显示现有GUI代理在异常场景性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理数据集在理想条件下构建，忽略现实中的多样异常，需要开发能综合评估的数据集。

Method: 引入包含七种常见异常的GUI - Robust数据集，采用半自动化数据集构建范式，借助RPA工具收集用户动作序列，用MLLMs生成描述。

Result: 半自动化范式使标注时间成本降低超19倍，评估显示现有GUI代理在异常场景性能大幅下降。

Conclusion: 强调GUI代理鲁棒性的重要性，有望启发相关研究。

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [38] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman,Melanie Schranz*

Main category: cs.AI

TL;DR: 本文对比传统群体算法与大语言模型驱动的群体，研究现代AI中群体概念的重新定义，评估大语言模型在群体系统中的适用性，指出其机遇与局限。


<details>
  <summary>Details</summary>
Motivation: 探讨在现代人工智能中，大语言模型作为协作代理时，群体概念里的去中心化、可扩展性和涌现性如何被重新定义。

Method: 使用Boids和蚁群优化算法实现并对比传统群体算法和大语言模型驱动的群体，评估延迟、资源使用和行为准确性，评估云基和本地大语言模型在群体代理中的适用性。

Result: 大语言模型虽有强大推理和抽象能力，但在计算和协调方面引入新限制，挑战传统群体设计概念。

Conclusion: 强调将大语言模型集成到群体系统中的机遇和局限，讨论现代AI研究中群体定义的演变。

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [39] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang,Junru Yu,Jun Huang,Qiong Wu,Ljubo Vacic,Changyin Sun*

Main category: cs.AI

TL;DR: 提出用于自动驾驶汽车的安全优先类人决策框架SF - HLDM，可高效决策。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在时变交通流中面临挑战，数据驱动方法有迁移性差和搜索成本高的问题。

Method: 提出SF - HLDM框架，集成分层渐进框架，含时空注意力机制、社会合规估计模块和深度进化强化学习模型。

Result: 框架使自动驾驶智能体动态调整决策参数，保持安全裕度并遵循合适驾驶行为。

Conclusion: 该框架可让自动驾驶汽车安全、舒适、高效且社交兼容地行驶，决策具有可解释性和灵活性。

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [40] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang,YeongHwan Shin,Doyeon Kim,Kyu-Hwan Jung,Meong Hi Son*

Main category: cs.AI

TL;DR: 本文提出Doppelgänger方法展示代理被劫持风险，定义PACAT级别评估对抗转移攻击脆弱性，提出CAT提示来对抗Doppelgänger方法，实验表明前者有危害，后者可有效防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型下提示工程虽方便但引发对提示安全性、鲁棒性和行为一致性的担忧，且要防止提示被暴露。

Method: 提出Doppelgänger方法展示风险，定义PACAT级别评估脆弱性，提出CAT提示进行对抗。

Result: Doppelgänger方法会破坏代理一致性并暴露内部信息，CAT提示能有效防御对抗攻击。

Conclusion: Doppelgänger方法存在风险，CAT提示可有效应对此类对抗攻击。

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [41] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier*

Main category: cs.AI

TL;DR: 提出QUEST框架用于商业文档表格提取，通过质量评估模型和多样性措施提升性能，实验效果好且适合商业文档。


<details>
  <summary>Details</summary>
Motivation: 自动化商业文档表格提取因标注稀疏和多阶段管道易出错而具挑战性，现有半监督学习方法依赖的置信度分数不能反映提取质量。

Method: 提出QUEST框架，引入质量评估模型预测F1分数引导伪标签选择，用多样性措施减少确认偏差。

Result: 在专有商业数据集和DocILE基准上，QUEST提高了F1分数并减少了空预测。

Conclusion: 框架的可解释质量评估和对标注稀缺的鲁棒性使其适合商业文档。

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [42] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth,Lennart Baur,Derian Boer,Stefan Kramer*

Main category: cs.AI

TL;DR: 提出增强符号机器学习方案，让其访问神经嵌入，实验显示该方法在F1分数上优于其他基线方法，且应用场景广泛。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI系统如LTN和DeepProbLog在简单场景下效率低，尤其是判别式机器学习和常量多的领域。

Method: 增强符号机器学习方案，使其能访问神经嵌入，以TILDE和常量嵌入为例，还可根据符号理论细化嵌入。

Result: 在三个真实世界领域的实验中，该方法在F1分数上优于所有其他基线方法。

Conclusion: 该方法简单有效，可扩展到实例间相似性、类比推理或命题化等场景。

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [43] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi,Andreas Zufle*

Main category: cs.AI

TL;DR: 本文倡导构建新的空间基础模型，以支持跨地域和场景的时空数据可扩展和可迁移分析，提出研究方向并指出其应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在处理移动性数据的时空和语义复杂性挑战上存在局限，需要可推广的时空数据基础模型以支持跨地域和场景分析。

Method: 倡导构建集成地理位置语义和多尺度人类移动性的空间基础模型，从建模离散兴趣点转向理解‘场所’，识别关键差距并提出聚焦建模场所和实现高效学习的研究方向。

Result: 未提及具体研究结果。

Conclusion: 旨在指导开发可扩展、上下文感知的下一代地理空间智能模型，解锁多领域强大应用，实现更智能、响应式的空间决策。

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [44] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu,Xinzhe Juan,Yimin Wang,Ling Yang,Xuan Qi,Tongcheng Zhang,Jiacheng Guo,Yifu Lu,Zixin Yao,Hongru Wang,Shilong Liu,Xun Jiang,Liu Leqi,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出训练无关的AgentDistill框架，利用MCP实现大语言模型智能体知识蒸馏，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有智能体蒸馏方法难以训练学生智能体在新环境中动态规划和行动，LLM智能体蒸馏待探索。

Method: 提出AgentDistill框架，通过直接复用教师智能体自主生成的MCP实现知识转移。

Result: 在生物医学和数学基准测试中，基于小语言模型的学生智能体表现与使用大LLM的先进系统相当。

Conclusion: 框架在构建可扩展且经济高效的智能体方面有效。

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [45] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng,Dongping Chen,Mingyang Fu,Tianyi Zhou*

Main category: cs.AI

TL;DR: 论文针对大推理模型推理链冗长问题，提出简洁性和充分性原则，引入 LC - R1 方法，实验表明能显著缩短序列长度且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在产生不必要且冗长推理链的问题，核心是“无效思考”，需解决此效率问题。

Method: 提出简洁性和充分性原则，引入基于 GRPO 的 LC - R1 后训练方法，结合长度奖励和压缩奖励。

Result: 在多个推理基准测试中，LC - R1 使序列长度显著减少约 50%，准确率仅下降约 2%，在帕累托前沿取得优先高压缩的有利权衡点。

Conclusion: LC - R1 具有鲁棒性，为开发更强大且计算高效的大推理模型提供有价值的见解。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [46] [Higher-Oder Splitting Schemes for Fluids with Variable Viscosity](https://arxiv.org/abs/2506.14424)
*Richard Schussnig,Niklas Fehn,Douglas Ramalho Queiroz Pacheco,Martin Kronbichler*

Main category: cs.CE

TL;DR: 本文研究不可压缩变粘度流动的Navier - Stokes方程无矩阵高阶间断Galerkin离散化，对比多种求解方案并通过数值算例评估。


<details>
  <summary>Details</summary>
Motivation: 研究生物医学或工业应用中变粘度不可压缩流的Navier - Stokes方程求解，对比不同方案性能。

Method: 对比鞍点块系统的线性化变体和基于投影的分裂时间积分方案，扩展双分裂方法，采用无矩阵$hp$ - 多重网格求解器的高阶DG方法和加速非线性求解器变体。

Result: 通过一系列数值算例验证了方案的时空精度和预条件子性能，在后向台阶基准测试中展示了效率。

Conclusion: 文中提出的方案在求解变粘度不可压缩流的Navier - Stokes方程方面有一定性能和效率。

Abstract: This article investigates matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The viscosity field may be prescribed analytically or governed by a rheological law, as often found in biomedical or industrial applications. We compare several linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of their computational performance. Compared to the velocity-pressure block-system for the former, the splitting scheme allows solving a sequence of simple problems such as mass, convection-diffusion and Poisson equations. We investigate under which conditions the improved temporal stability of fully implicit schemes and resulting expensive nonlinear solves outperform the splitting schemes and linearized variants that are stable under hyperbolic time step restrictions.
  The key aspects of this work are i) the extension of the dual splitting method originally proposed by G.E. Karniadakis et al. (J. Comput. Phys. 97, 414-443, 1991) towards non-constant viscosity, ii) a higher-order DG method for incompressible flows with variable viscosity, iii) accelerated nonlinear solver variants and suitable linearizations adopting a matrix-free $hp$-multigrid solver, and iv) a detailed comparison of the monolithic and projection-based solvers in terms of their (non-)linear solver performance.
  The presented schemes are evaluated in a series of numerical examples verifying their spatial and temporal accuracy, and the preconditioner performance under increasing viscosity contrasts, while their efficiency is showcased in the backward-facing step benchmark.

</details>


### [47] [Active Digital Twins via Active Inference](https://arxiv.org/abs/2506.14453)
*Matteo Torzoni,Domenico Maisto,Andrea Manzoni,Francesco Donnarumma,Giovanni Pezzulo,Alberto Corigliano*

Main category: cs.CE

TL;DR: 本文引入基于主动推理的主动数字孪生范式，以解决传统数字孪生适应性问题，并在铁路桥梁健康监测与预测性维护中评估该框架。


<details>
  <summary>Details</summary>
Motivation: 传统数字孪生主要依赖被动数据同化，在不确定和动态环境中适应性受限。

Method: 将主动数字孪生的演化表述为部分可观测马尔可夫决策过程，主动推理代理通过贝叶斯更新不断完善生成模型，通过优化过程进行决策，动态规划行动以最小化预期自由能。

Result: 该方法使数字孪生具有新的自主性和弹性，具备卓越的自发探索能力。

Conclusion: 所提出的主动数字孪生框架在铁路桥梁健康监测和预测性维护评估中展现出优势。

Abstract: Digital twins are transforming engineering and applied sciences by enabling real-time monitoring, simulation, and predictive analysis of physical systems and processes. However, conventional digital twins rely primarily on passive data assimilation, which limits their adaptability in uncertain and dynamic environments. This paper introduces the active digital twin paradigm, based on active inference. Active inference is a neuroscience-inspired, Bayesian framework for probabilistic reasoning and predictive modeling that unifies inference, decision-making, and learning under a unique, free energy minimization objective. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the active inference agent continuously refines its generative model through Bayesian updates and forecasts future states and observations. Decision-making emerges from an optimization process that balances pragmatic exploitation (maximizing goal-directed utility) and epistemic exploration or information gain (actively resolving uncertainty). Actions are dynamically planned to minimize expected free energy, which quantifies both the divergence between predicted and preferred future observations, and the epistemic value of expected information gain about hidden states. This approach enables a new level of autonomy and resilience in digital twins, offering superior spontaneous exploration capabilities. The proposed framework is assessed on the health monitoring and predictive maintenance of a railway bridge.

</details>


### [48] [A Machine Learning Framework for Climate-Resilient Insurance and Real Estate Decisions](https://arxiv.org/abs/2506.14638)
*Lang Qin,Yuejin Xie,Daili Hua,Xuhui Meng*

Main category: cs.CE

TL;DR: 提出SSC - 保险模型和TOA - 保护模型应对极端天气对保险和房地产行业的威胁，给出模型准确率、阈值等结果，为相关方提供气候风险管理工具。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件威胁保险和房地产行业，产生盈利与房主负担的冲突，需要解决办法。

Method: 提出SSC - 保险模型，整合SMOTE、SVM和C - D - C算法评估天气影响；开发TOA - 保护模型，使用TOPSIS - ORM和AHP确定建筑保护优先级。

Result: SSC - 保险模型在浙江准确率88.3%，在爱尔兰79.6%，确定保险可行性临界阈值43%；TOA - 保护模型中文化价值权重0.3383，南浔古镇案例可保概率65.32%，保护得分0.512。

Conclusion: 本研究为保险公司、开发商和政策制定者提供了可持续管理气候风险的实用工具。

Abstract: Extreme weather events increasingly threaten the insurance and real estate industries, creating conflicts between profitability and homeowner burdens. To address this, we propose the SSC-Insurance Model, which integrates SMOTE, SVM, and C-D-C algorithms to evaluate weather impacts on policies and investments. Our model achieves 88.3% accuracy in Zhejiang and 79.6% in Ireland, identifying a critical threshold (43% weather increase) for insurance viability. Additionally, we develop the TOA-Preservation Model using TOPSIS-ORM and AHP to prioritize building protection, with cultural value scoring highest (weight: 0.3383). Case studies on Nanxun Ancient Town show a 65.32% insurability probability and a protection score of 0.512. This work provides actionable tools for insurers, developers, and policymakers to manage climate risks sustainably.

</details>


### [49] [Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand](https://arxiv.org/abs/2506.14768)
*Ozan Solmaz,Lioba Heimbach,Yann Vonlanthen,Roger Wattenhofer*

Main category: cs.CE

TL;DR: 研究Layer 2 rollups的乐观MEV，发现其在不同链上的占比、交易特点及与相关因素的关联。


<details>
  <summary>Details</summary>
Motivation: Layer 2 rollups的MEV动态研究不足，需对乐观MEV进行定义和量化。

Method: 采用多阶段识别流程分析Arbitrum、Base和Optimism链上数据，使用OLS回归分析。

Result: 2025年Q1，乐观MEV在Base和Optimism上占链上gas超50%，在Arbitrum占7%；消耗超一半链上gas但支付不到四分之一总gas费；不同链成功率、代码复用模式有差异，与ETH波动率等因素有关。

Conclusion: Layer 2协议参数独特地鼓励了投机性MEV。

Abstract: Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40 billion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet their MEV dynamics remain understudied. We address this gap by defining and quantifying optimistic MEV, a form of speculative, on-chain cyclic arbitrage whose detection and execution logic reside largely on-chain in smart contracts. As a result of their speculative nature and lack of off-chain opportunity verification, optimistic MEV transactions frequently fail to execute a profitable arbitrage.
  Applying our multi-stage identification pipeline to Arbitrum, Base, and Optimism, we find that in Q1 2025, optimistic MEV accounts for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, driven mainly by "interaction" probes (on-chain computations searching for arbitrage). This speculative probing keeps blocks on Base and Optimism persistently full. Despite consuming over half of on-chain gas, optimistic MEV transactions pay less than one quarter of total gas fees. Cross-network comparison reveals divergent success rates, differing patterns of code reuse, and sensitivity to varying sequencer ordering and block production times. Finally, OLS regressions link optimistic MEV trade count to ETH volatility, retail trading activity, and DEX aggregator usage, showing how Layer 2 protocol parameters uniquely encourage speculative MEV.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [LLM-Driven Data Generation and a Novel Soft Metric for Evaluating Text-to-SQL in Aviation MRO](https://arxiv.org/abs/2506.13785)
*Patrick Sutanto,Jonathan Kenrick,Max Lorenz,Joan Santoso*

Main category: cs.DB

TL;DR: 论文针对大语言模型在文本转SQL任务应用中的评估指标僵化和领域特定评估数据集稀缺问题，提出新的软指标和数据合成方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于文本转SQL任务可促进数据访问民主化，但传统评估指标僵化和领域特定评估数据集稀缺阻碍了进展。

Method: 引入基于F1分数的软指标量化生成与真实SQL结果的信息重叠，提出基于大语言模型的管道从数据库模式合成现实的问题 - SQL对。

Result: 实验表明，提出的软指标比严格准确率能提供更有洞察力的性能分析，数据生成技术能有效创建领域特定基准。

Conclusion: 这些贡献为专业环境下评估和推进文本转SQL系统提供了强大框架。

Abstract: The application of Large Language Models (LLMs) to text-to-SQL tasks promises to democratize data access, particularly in critical industries like aviation Maintenance, Repair, and Operation (MRO). However, progress is hindered by two key challenges: the rigidity of conventional evaluation metrics such as execution accuracy, which offer coarse, binary feedback, and the scarcity of domain-specific evaluation datasets. This paper addresses these gaps. To enable more nuanced assessment, we introduce a novel F1-score-based 'soft' metric that quantifies the informational overlap between generated and ground-truth SQL results. To address data scarcity, we propose an LLM-driven pipeline that synthesizes realistic question-SQL pairs from database schemas. We demonstrate our contributions through an empirical evaluation on an authentic MRO database. Our experiments show that the proposed soft metric provides more insightful performance analysis than strict accuracy, and our data generation technique is effective in creating a domain-specific benchmark. Together, these contributions offer a robust framework for evaluating and advancing text-to-SQL systems in specialized environments.

</details>


### [51] [Sketched Sum-Product Networks for Joins](https://arxiv.org/abs/2506.14034)
*Brian Tsan,Abylay Amanbayev,Asoke Datta,Florin Rusu*

Main category: cs.DB

TL;DR: 提出用Sum - Product Networks动态近似草图以解决多连接基数估计问题，为查询优化提供实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有草图通常为预定义查询选择构造，阻碍其在新查询中的应用，需更通用解决方案。

Method: 用Sum - Product Networks分解和建模多元分布，将单变量分布表示为草图并组合，实现Fast - AGMS和Bound Sketch方法。

Result: 能高效近似任意查询选择的草图，并应用于连接基数估计。

Conclusion: 为将草图应用于查询优化提供了实用替代方案。

Abstract: Sketches have shown high accuracy in multi-way join cardinality estimation, a critical problem in cost-based query optimization. Accurately estimating the cardinality of a join operation -- analogous to its computational cost -- allows the optimization of query execution costs in relational database systems. However, although sketches have shown high efficacy in query optimization, they are typically constructed specifically for predefined selections in queries that are assumed to be given a priori, hindering their applicability to new queries. As a more general solution, we propose for Sum-Product Networks to dynamically approximate sketches on-the-fly. Sum-Product Networks can decompose and model multivariate distributions, such as relations, as linear combinations of multiple univariate distributions. By representing these univariate distributions as sketches, Sum-Product Networks can combine them element-wise to efficiently approximate the sketch of any query selection. These approximate sketches can then be applied to join cardinality estimation. In particular, we implement the Fast-AGMS and Bound Sketch methods, which have successfully been used in prior work, despite their costly construction. By accurately approximating them instead, our work provides a practical alternative to apply these sketches to query optimization.

</details>


### [52] [HARMONY: A Scalable Distributed Vector Database for High-Throughput Approximate Nearest Neighbor Search](https://arxiv.org/abs/2506.14707)
*Qian Xu,Feng Zhang,Chengxi Li,Lei Cao,Zheng Chen,Jidong Zhai,Xiaoyong Du*

Main category: cs.DB

TL;DR: 介绍分布式近似最近邻搜索系统Harmony，采用多粒度分区策略和早停剪枝机制，实验表明其性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有分布式向量数据库在处理高维向量的近似最近邻搜索时存在负载不均衡和高通信开销问题。

Method: 引入Harmony系统，采用结合基于维度和基于向量的多粒度分区策略，还融入早停剪枝机制。

Result: 在不同真实数据集上实验，Harmony在四节点上平均吞吐量达4.63倍，偏斜工作负载下性能比传统分布提升58%。

Conclusion: Harmony系统有效解决了现有分布式向量数据库的问题，性能优于领先的分布式向量数据库。

Abstract: Approximate Nearest Neighbor Search (ANNS) is essential for various data-intensive applications, including recommendation systems, image retrieval, and machine learning. Scaling ANNS to handle billions of high-dimensional vectors on a single machine presents significant challenges in memory capacity and processing efficiency. To address these challenges, distributed vector databases leverage multiple nodes for the parallel storage and processing of vectors. However, existing solutions often suffer from load imbalance and high communication overhead, primarily due to traditional partition strategies that fail to effectively distribute the workload. In this paper, we introduce Harmony, a distributed ANNS system that employs a novel multi-granularity partition strategy, combining dimension-based and vector-based partition. This strategy ensures a balanced distribution of computational load across all nodes while effectively minimizing communication costs. Furthermore, Harmony incorporates an early-stop pruning mechanism that leverages the monotonicity of distance computations in dimension-based partition, resulting in significant reductions in both computational and communication overhead. We conducted extensive experiments on diverse real-world datasets, demonstrating that Harmony outperforms leading distributed vector databases, achieving 4.63 times throughput on average in four nodes and 58% performance improvement over traditional distribution for skewed workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [DAGs for the Masses](https://arxiv.org/abs/2506.13998)
*Michael Anoprenko,Andrei Tonkikh,Alexander Spiegelman,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 提出稀疏DAG架构以解决传统DAG协议扩展性问题，应用于Bullshark协议展示了更好的扩展性和容错性。


<details>
  <summary>Details</summary>
Motivation: 传统DAG协议在维护图的开销成为瓶颈时扩展性有限，需要实现基于DAG协议的大规模部署。

Method: 提出稀疏DAG架构，每个节点仅包含对上一轮随机节点的固定数量引用；提出稀疏版Bullshark协议。

Result: 协议能容忍最多f < n/3的拜占庭故障，与确定性版本容错能力相同；稀疏DAG减少传输元数据，实现更高效网络利用和更好扩展性。

Conclusion: 提出的稀疏方法可应用于任何以图结构维护系统更新和因果关系的协议。

Abstract: A recent approach to building consensus protocols on top of Directed Acyclic Graphs (DAGs) shows much promise due to its simplicity and stable throughput. However, as each node in the DAG typically includes a linear number of references to the nodes in the previous round, prior DAG protocols only scale up to a certain point when the overhead of maintaining the graph becomes the bottleneck.
  To enable large-scale deployments of DAG-based protocols, we propose a sparse DAG architecture, where each node includes only a constant number of references to random nodes in the previous round. We present a sparse version of Bullshark -- one of the most prominent DAG-based consensus protocols -- and demonstrate its improved scalability.
  Remarkably, unlike other protocols that use random sampling to reduce communication complexity, we manage to avoid sacrificing resilience: the protocol can tolerate up to $f<n/3$ Byzantine faults (where $n$ is the number of participants), same as its less scalable deterministic counterpart. The proposed ``sparse'' methodology can be applied to any protocol that maintains disseminated system updates and causal relations between them in a graph-like structure. Our simulations show that the considerable reduction of transmitted metadata in sparse DAGs results in more efficient network utilization and better scalability.

</details>


### [54] [Déjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse](https://arxiv.org/abs/2506.14107)
*Jinwoo Hwang,Daeun Kim,Sangyeop Lee,Yoonsung Kim,Guseul Heo,Hojoon Kim,Yunseok Jeong,Tadiwos Meaza,Eunhyeok Park,Jeongseob Ahn,Jongse Park*

Main category: cs.DC

TL;DR: 介绍了视频语言查询引擎Déjà Vu，可加速基于ViT的视频语言模型，评估显示能在误差范围内提升嵌入生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的视频语言模型处理大规模视频时推理计算量大，难以在现实部署，需解决方案集成到可扩展视频数据管理系统。

Method: 引入Déjà Vu，核心是ReuseViT模型检测帧间重用机会，还集成内存计算联合压缩技术将计算节省转化为性能提升。

Result: 在三个视频语言任务评估中，Déjà Vu在2%误差范围内使嵌入生成加速达2.64倍。

Conclusion: Déjà Vu显著增强了视频语言模型在大规模视频分析中的实用性。

Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces Déjà Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, Déjà Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that Déjà Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.

</details>


### [55] [The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic Demonstration of Miner-Centric Propagation Topologies](https://arxiv.org/abs/2506.14197)
*Dr Craig S Wright*

Main category: cs.DC

TL;DR: 运用复杂图理论研究BTC和BSV网络结构，表明家用全节点无法参与或影响传播拓扑，对共识传播不重要。


<details>
  <summary>Details</summary>
Motivation: 正式研究BTC和BSV的网络结构，探究家用全节点在其中的作用。

Method: 使用复杂图理论，借助无标度网络和小世界连通性等模型，采用模拟支持的指标和特征值中心性分析。

Result: 传播图由紧密相连的矿工群体主导，全节点处于边缘，被排除在所有交易到区块的包含路径之外。

Conclusion: 全节点对共识传播既不关键也无实际作用。

Abstract: This paper formally examines the network structure of Bitcoin CORE (BTC) and Bitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that home-hosted full nodes are incapable of participating in or influencing the propagation topology. Leveraging established models such as scale-free networks and small-world connectivity, we demonstrate that the propagation graph is dominated by a densely interconnected miner clique, while full nodes reside on the periphery, excluded from all transaction-to-block inclusion paths. Using simulation-backed metrics and eigenvalue centrality analysis, we confirm that full nodes are neither critical nor operationally relevant for consensus propagation.

</details>


### [56] [A Novel Indicator for Quantifying and Minimizing Information Utility Loss of Robot Teams](https://arxiv.org/abs/2506.14237)
*Xiyu Zhao,Qimei Cui,Wei Ni,Quan Z. Sheng,Abbas Jamalipour,Guoshun Nan,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

TL;DR: 提出信息效用损失（LoIU）指标，用半去中心化多智能体深度确定性策略梯度框架优化传输调度和资源分配，仿真显示信息新鲜度和效用提升98%。


<details>
  <summary>Details</summary>
Motivation: 团队内机器人信息及时交换受无线容量限制，信息传递不及时会导致估计误差影响协作。

Method: 提出LoIU指标量化信息新鲜度和效用；用信念分布估计LoIU，优化传输调度和资源分配；开发半去中心化多智能体深度确定性策略梯度框架。

Result: 仿真验证该方法有效，与其他方法相比，信息新鲜度和效用提升98%。

Conclusion: 所提方法能有效提升机器人团队内信息的新鲜度和效用。

Abstract: The timely exchange of information among robots within a team is vital, but it can be constrained by limited wireless capacity. The inability to deliver information promptly can result in estimation errors that impact collaborative efforts among robots. In this paper, we propose a new metric termed Loss of Information Utility (LoIU) to quantify the freshness and utility of information critical for cooperation. The metric enables robots to prioritize information transmissions within bandwidth constraints. We also propose the estimation of LoIU using belief distributions and accordingly optimize both transmission schedule and resource allocation strategy for device-to-device transmissions to minimize the time-average LoIU within a robot team. A semi-decentralized Multi-Agent Deep Deterministic Policy Gradient framework is developed, where each robot functions as an actor responsible for scheduling transmissions among its collaborators while a central critic periodically evaluates and refines the actors in response to mobility and interference. Simulations validate the effectiveness of our approach, demonstrating an enhancement of information freshness and utility by 98%, compared to alternative methods.

</details>


### [57] [Concepts for designing modern C++ interfaces for MPI](https://arxiv.org/abs/2506.14610)
*C. Nicole Avans,Alfredo A. Correa,Sayan Ghosh,Matthias Schimek,Joseph Schuchart,Anthony Skjellum,Evan D. Suggs,Tim Niklas Uhl*

Main category: cs.DC

TL;DR: MPI社区致力于构建高级C++接口，本文聚焦接口三方面基础问题并指出MPI规范不一致。


<details>
  <summary>Details</summary>
Motivation: 随着现代C++异构编程模型等发展，MPI社区标准化高级C++接口角色在演变，需解决MPI语义与惯用C++间的不协调问题。

Method: 聚焦高级接口的类型系统、对象生命周期和通信缓冲区三个基本方面，识别MPI规范中的不一致。

Result: 文中未提及具体成果。

Conclusion: 提出的解决方案可能不完善，希望与更广泛社区交流想法和关注问题。

Abstract: Since the C++ bindings were deleted in 2008, the Message Passing Interface (MPI) community has revived efforts in building high-level modern C++ interfaces. Such interfaces are either built to serve specific scientific application needs (with limited coverage to the underlying MPI functionalities), or as an exercise in general-purpose programming model building, with the hope that bespoke interfaces can be broadly adopted to construct a variety of distributed-memory scientific applications. However, with the advent of modern C++-based heterogeneous programming models, GPUs and widespread Machine Learning (ML) usage in contemporary scientific computing, the role of prospective community-standardized high-level C++ interfaces to MPI is evolving. The success of such an interface clearly will depend on providing robust abstractions and features adhering to the generic programming principles that underpin the C++ programming language, without compromising on either performance and portability, the core principles upon which MPI was founded. However, there is a tension between idiomatic C++ handling of types and lifetimes, and, MPI's loose interpretation of object lifetimes/ownership and insistence on maintaining global states.
  Instead of proposing "yet another" high-level C++ interface to MPI, overlooking or providing partial solutions to work around the key issues concerning the dissonance between MPI semantics and idiomatic C++, this paper focuses on the three fundamental aspects of a high-level interface: type system, object lifetimes and communication buffers, also identifying inconsistencies in the MPI specification. Presumptive solutions can be unrefined, and we hope the broader MPI and C++ communities will engage with us in productive exchange of ideas and concerns.

</details>


### [58] [Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)](https://arxiv.org/abs/2506.14630)
*Rúben Adão,Zhongjie Wu,Changjun Zhou,Oana Balmau,João Paulo,Ricardo Macedo*

Main category: cs.DC

TL;DR: 介绍了并发和工作负载感知的存储中间件Keigo，可提升分层存储设备上LSM KVS性能，评估显示其能显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在分层存储设备上部署LSM KVS时，没有一种通用的数据放置方式能优化所有工作负载，需要利用不同存储设备优势。

Method: 引入并发感知数据放置、持久只读缓存和基于上下文的I/O差异化三种技术，根据并行性、I/O带宽和容量在不同设备放置文件。

Result: 使用合成和真实工作负载评估，相比通用存储系统和专用LSM KVS，写负载下生产级LSM吞吐量最多提高4倍，读负载最多提高18倍。

Conclusion: Keigo可跨不同LSM移植，适应动态工作负载，无需大量分析，能让生产KVS从异构存储设置中受益。

Abstract: We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no one-size-fits-all placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques - concurrency-aware data placement, persistent read-only caching, and context-based I/O differentiation. Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4x for write- and 18x for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.

</details>


### [59] [Resource Optimization with MPI Process Malleability for Dynamic Workloads in HPC Clusters](https://arxiv.org/abs/2506.14743)
*Sergio Iserte,Iker Martín-Álvarez,Krzysztof Rojek,José I. Aliaga,Maribel Castillo,Weronika Folwarska,Antonio J. Peña*

Main category: cs.DC

TL;DR: 本文扩展DMR框架，集成新方法增强动态资源管理能力，评估显示可提升资源利用率和工作负载效率。


<details>
  <summary>Details</summary>
Motivation: 解决动态资源管理技术在生产环境中因标准化、互操作性和可用性问题导致采用受限的问题。

Method: 扩展DMR框架，集成Proteo框架MaM模块的新方法，探索新的可塑性策略。

Result: 在超级计算机上评估，动态资源管理比静态资源分配减少40%工作负载完成时间，提高超20%资源利用率。

Conclusion: 所提出的增强方法能提高资源利用率和工作负载效率。

Abstract: Dynamic resource management is essential for optimizing computational efficiency in modern high-performance computing (HPC) environments, particularly as systems scale. While research has demonstrated the benefits of malleability in resource management systems (RMS), the adoption of such techniques in production environments remains limited due to challenges in standardization, interoperability, and usability. Addressing these gaps, this paper extends our prior work on the Dynamic Management of Resources (DMR) framework, which provides a modular and user-friendly approach to dynamic resource allocation. Building upon the original DMRlib reconfiguration runtime, this work integrates new methodology from the Malleability Module (MaM) of the Proteo framework, further enhancing reconfiguration capabilities with new spawning strategies and data redistribution methods. In this paper, we explore new malleability strategies in HPC dynamic workloads, such as merging MPI communicators and asynchronous reconfigurations, which offer new opportunities for dramatically reducing memory overhead. The proposed enhancements are rigorously evaluated on a world-class supercomputer, demonstrating improved resource utilization and workload efficiency. Results show that dynamic resource management can reduce the workload completion time by 40% and increase the resource utilization by over 20%, compared to static resource allocation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [60] [Spectral partitioning of graphs into compact, connected regions](https://arxiv.org/abs/2506.13982)
*Ewan Davies,Ryan Job,Maxine Kampbell,Hannah Kim,Hyojin Seo*

Main category: cs.DS

TL;DR: 本文定义并研究用于将图划分为指定数量连通部分的光谱重组算法SpecReCom，给出其变体BalSpecReCom，通过实验表明该算法比RevReCom分区更紧凑。


<details>
  <summary>Details</summary>
Motivation: 解决将图划分为指定数量连通部分，并满足部分权重或顶点数近似平衡等约束的问题。

Method: 定义SpecReCom算法和其变体BalSpecReCom算法。

Result: 通过对56×56网格图和科罗拉多州平面图的研究，该算法比RevReCom等替代算法实现更紧凑的分区。

Conclusion: SpecReCom算法在图分区问题上比替代算法更有效。

Abstract: We define and study a spectral recombination algorithm, SpecReCom, for partitioning a graph into a given number of connected parts. It is straightforward to introduce additional constraints such as the requirement that the weight (or number of vertices) in each part is approximately balanced, and we exemplify this by stating a variant, BalSpecReCom, of the SpecReCom algorithm. We provide empirical evidence that the algorithm achieves more compact partitions than alternatives such as RevReCom by studying a $56\times 56$ grid graph and a planar graph obtained from the state of Colorado.

</details>


### [61] [glass: ordered set data structure for client-side order books](https://arxiv.org/abs/2506.13991)
*Viktor Krapivensky*

Main category: cs.DS

TL;DR: 提出基于trie的有序集实现，有多项创新，相比C++的std::map容器有显著加速。


<details>
  <summary>Details</summary>
Motivation: 有序集抽象数据类型常用红黑树等实现，为优化市场数据处理，提出基于trie的新实现。

Method: 利用缓存路径、哈希表、硬件加速操作、订单簿特定特性等方法实现有序集。

Result: 相比C++的std::map容器，修改操作加速6x - 20x，查找操作加速30x，真实市场数据加速9x - 15x，迭代加速2x - 3x。

Conclusion: 展示了基于trie的有序集实现及其优势。

Abstract: The "ordered set" abstract data type with operations "insert", "erase", "find", "min", "max", "next" and "prev" is ubiquitous in computer science. It is usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We present our implementation of ordered set based on a trie. It only supports integer keys (as opposed to keys of any strict weakly ordered type) and is optimized for market data, namely for what we call sequential locality. The following is the list of what we believe to be novelties:
  * Cached path to exploit sequential locality, and fast truncation thereof on erase operation;
  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on any operation to speed up key lookup (up to a pre-leaf node);
  * Hardware-accelerated "find next/previous set bit" operations with BMI2 instruction set extension on x86-64;
  * Order book-specific features: the preemption principle and the tree restructure operation that prevent the tree from consuming too much memory.
  We achieve the following speedups over C++'s standard std::map container: 6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market data, and a more modest 2x-3x speedup on iteration. In this paper, we discuss our implementation.

</details>


### [62] [An Exact and Efficient Sampler for Dynamic Discrete Distributions](https://arxiv.org/abs/2506.14062)
*Lilith Orion Hafner,Adriano Meligrana*

Main category: cs.DS

TL;DR: 本文提出EBUS算法解决动态离散分布采样问题，具有O(1)采样和更新成本，比先前方法更高效。


<details>
  <summary>Details</summary>
Motivation: 现有动态离散分布采样方法如Alias方法更新成本高，其他方法存在数值问题，不适用于频繁更新场景。

Method: 提出EBUS（Exact BUcket Sampling）算法，通过有界精度和指数的b进制数更新采样器。

Result: 使用IEEE 64 - 位浮点数实现该方法，实验表明比先前不精确方法的几种实现更高效。

Conclusion: EBUS是首个具有O(1)采样和更新成本的精确算法，能有效解决动态离散分布采样问题。

Abstract: Sampling from a dynamic discrete distribution involves sampling from a dynamic set of weighted elements, where elements can be added or removed at any stage of the sampling process. Although efficient for static sets, the Alias method becomes impractical in dynamic settings due to the need to reconstruct the sampler after each update, which incurs a computational cost proportional to the size of the distribution, making it unsuitable for applications requiring frequent insertions, deletions, or weight adjustments. To address this limitation, different approaches have been studied, such as the Forest of Trees method and the BUcket Sampling (BUS) method. However, all previous methods suffered from numerical issues which can bias the sampling process. In this paper, we describe EBUS (Exact BUcket Sampling), the first exact algorithm with $O(1)$ sampling and update cost. The sampler can be updated by base-$b$ numbers with bounded precision and exponent, and sample the distribution of its elements exactly and efficiently. We provide also a state of the art implementation of the method using IEEE 64-bit floating point numbers which we empirically show to be more efficient than several implementations of previous inexact methods.

</details>


### [63] [Simpler, Better, Faster, Stronger: Revisiting a Successful Reduction Rule for Dominating Set](https://arxiv.org/abs/2506.14564)
*Lukas Geis,Alexander Leonhardt,Johannes Meintrup,Ulrich Meyer,Manuel Penschuck*

Main category: cs.DS

TL;DR: 提出线性时间算法改进Rule1并对支配集问题化简规则提供见解，还提出扩展算法进一步修剪图。


<details>
  <summary>Details</summary>
Motivation: 原Rule1执行成本高，希望改进以更高效处理支配集问题。

Method: 提出线性时间算法实现并超越原Rule1，重铸其为单次线性时间搜索，还提出算法框架扩展。

Result: 得到关于化简规则和支配集问题交互的有趣结构见解，算法易实现且实用。

Conclusion: 提出的算法高效且实用，能改进对支配集问题实例的处理。

Abstract: DominatingSet is a classical NP-complete problem and also known to be W[2]-hard. Thus, there is little hope for small kernels on general graphs. However, in practice, reduction rules to heuristically shrink instances are used. In this context, Rule1 by Alber et. al. is quite successful, yet at times somewhat expensive to execute. We propose a linear time algorithm implementing and surpassing the original Rule1 formulation. Our discussions and proofs yield interesting structural insights into the reduction rule and its interplay with the DominatingSet problem. For instance, while the original formulation warrants repeated invocations of an $\mathcal{O}(n^3)$ time algorithm, we recast it to allow a single search run in linear time. Then, we propose simple, but practically significant, extensions to our algorithmic framework to prune the graph even further. The algorithm is easy to implement and highly practical.

</details>


### [64] [Compressing Suffix Trees by Path Decompositions](https://arxiv.org/abs/2506.14734)
*Ruben Becker,Davide Cenzato,Travis Gagie,Sung-Hwan Kim,Ragnar Groot Koerkamp,Giovanni Manzini,Nicola Prezza*

Main category: cs.DS

TL;DR: 本文重新审视后缀树的路径压缩，提出新压缩方法，其索引更小且查找更快。


<details>
  <summary>Details</summary>
Motivation: 改进经典后缀树路径压缩方法，寻找更高效的压缩方式。

Method: 将后缀字典树分解为节点到叶子的路径，用指向字符串后缀的指针表示路径，对索引数组按对应文本前缀的共字典序排序。

Result: 得到的数组支持缓存高效的模式匹配查询，索引比r - index更小且查找速度快很多。

Conclusion: 更谨慎地选择指针能带来一种优雅、简单且高效的后缀树压缩方法。

Abstract: In classic suffix trees, path compression works by replacing unary suffix trie paths with pairs of pointers to $T$, which must be available in the form of some random access oracle at query time. In this paper, we revisit path compression and show that a more careful choice of pointers leads to a new elegant, simple, and remarkably efficient way to compress the suffix tree. We begin by observing that an alternative way to path-compress the suffix trie of $T$ is to decompose it into a set of (disjoint) node-to-leaf paths and then represent each path as a pointer $i$ to one of the string's suffixes $T[i,n]$. At this point, we show that the array $A$ of such indices $i$, sorted by the colexicographic order of the corresponding text prefixes $T[1,i]$, possesses the following properties: (i) it supports \emph{cache-efficient} pattern matching queries via simple binary search on $A$ and random access on $T$, and (ii) it contains a number of entries being proportional to the size of the \emph{compressed text}. Of particular interest is the path decomposition given by the colexicographic rank of $T$'s prefixes. The resulting index is smaller and orders of magnitude faster than the $r$-index on the task of locating all occurrences of a query pattern.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [65] [Dividing Conflicting Items Fairly](https://arxiv.org/abs/2506.14149)
*Ayumi Igarashi,Pasin Manurangsi,Hirotaka Yoneda*

Main category: cs.GT

TL;DR: 研究冲突约束下不可分物品分配问题，扩展了最大EF1分配存在性结果，给出计算算法，指出多代理情况的反例和NP难问题，结果适用于任务分配。


<details>
  <summary>Details</summary>
Motivation: 扩展Kumar等人（2024）关于区间图和两个单调估值代理的最大EF1分配存在性结果到任意图。

Method: 提出多项式时间算法计算加性估值的分配，伪多项式时间算法计算单调估值的分配，给出反例和证明NP难。

Result: 证明任意图在两个单调估值代理时有最大EF1分配，三个代理时可能不存在，确定n≥3个代理时判定分配存在性为NP难，结果适用于任务分配。

Conclusion: 在冲突约束下不可分物品分配问题上有重要进展，明确了不同情况下最大EF1分配的存在性和计算复杂度。

Abstract: We study the allocation of indivisible goods under conflicting constraints, represented by a graph. In this framework, vertices correspond to goods and edges correspond to conflicts between a pair of goods. Each agent is allocated an independent set in the graph. In a recent work of Kumar et al. (2024), it was shown that a maximal EF1 allocation exists for interval graphs and two agents with monotone valuations. We significantly extend this result by establishing that a maximal EF1 allocation exists for \emph{any graph} when the two agents have monotone valuations. To compute such an allocation, we present a polynomial-time algorithm for additive valuations, as well as a pseudo-polynomial time algorithm for monotone valuations. Moreover, we complement our findings by providing a counterexample demonstrating a maximal EF1 allocation may not exist for three agents with monotone valuations; further, we establish NP-hardness of determining the existence of such allocations for every fixed number $n \geq 3$ of agents. All of our results for goods also apply to the allocation of chores.

</details>


### [66] [Infinite lexicographic products of positional objectives](https://arxiv.org/abs/2506.14544)
*Antonio Casares,Pierre Ohlmann,Michał Skrzypczak,Igor Walukiewicz*

Main category: cs.GT

TL;DR: 本文研究无限图上无限时长游戏的位置确定性，提出无限字典积概念并拓展前人结果，还得到奇偶目标相关完备性及位置语言结果。


<details>
  <summary>Details</summary>
Motivation: 在无限图上无限时长游戏的位置确定性研究中，拓展已有关于前缀无关目标位置性在有限字典积下性质的结果。

Method: 提出由任意序数索引的两种不同的无限字典积概念，并进行相关证明。

Result: 证明无限字典积也能保持位置性，拓展了单玩家位置确定性结果，得出最大和最小奇偶目标的完备性，得到对应位置语言的完备性及关于并集和中性字母的新见解。

Conclusion: 通过新的无限字典积概念拓展了位置确定性研究成果，在奇偶目标和位置语言方面有新发现。

Abstract: This paper contributes to the study of positional determinacy of infinite duration games played on potentially infinite graphs. Recently, [Ohlmann, TheoretiCS 2023] established that positionality of prefix-independent objectives is preserved by finite lexicographic products. We propose two different notions of infinite lexicographic products indexed by arbitrary ordinals, and extend Ohlmann's result by proving that they also preserve positionality. In the context of one-player positionality, this extends positional determinacy results of [Grädel and Walukiewicz, Logical Methods in Computer Science 2006] to edge-labelled games and arbitrarily many priorities for both Max-Parity and Min-Parity. Moreover, we show that the Max-Parity objectives over countable ordinals are complete for the infinite levels of the difference hierarchy over $Σ^0_2$ and that Min-Parity is complete for the class $Σ^0_3$. We obtain therefore positional languages that are complete for all those levels, as well as new insights about closure under unions and neutral letters.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
*Anvi Alex Eponon,Moein Shahiki-Tash,Ildar Batyrshin,Christian E. Maldonado-Sifuentes,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.IR

TL;DR: 本文提出一种基于问题的知识编码方法，无需微调或传统分块即可改进RAG系统，在单跳和多跳检索任务中表现出色，具有多种优势。


<details>
  <summary>Details</summary>
Motivation: 改进检索增强生成（RAG）系统，避免微调与传统分块方法的局限。

Method: 用生成的跨越词汇和语义空间的问题对文本内容编码，结合自定义句法重排序方法；引入‘paper - cards’。

Result: 单跳检索中Recall@3达0.84，超传统分块方法60%；‘paper - cards’使BM25检索MRR@3从0.56提升到0.85；多跳任务中重排序方法F1得分超分块和微调基线。

Conclusion: 该方法无需微调，减少检索延迟等，是可扩展且高效的RAG替代方案。

Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.

</details>


### [68] [XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2506.13782)
*Ke Wang,Bo Pan,Yingchaojie Feng,Yuwei Wu,Jieyi Chen,Minfeng Zhu,Wei Chen*

Main category: cs.IR

TL;DR: 提出可视化分析框架及原型系统XGraphRAG，提升GraphRAG可解释性和可访问性，评估证明其有效性和可用性，代码开源。


<details>
  <summary>Details</summary>
Motivation: GraphRAG信息处理流程复杂、大模型调用多，开发者难分析其在自身数据集上的有效性，限制了可解释性和可访问性。

Method: 提出可视化分析框架，开发包含交互式可视化的原型系统XGraphRAG。

Result: 评估证明了方法的有效性和可用性。

Conclusion: 所提方法能帮助RAG开发者分析GraphRAG，且代码开源利于进一步研究。

Abstract: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.

</details>


### [69] [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
*Junting Zhou,Wang Li,Yiyan Liao,Nengyuan Zhang,Tingjia Miaoand Zhihui Qi,Yuhan Wu,Tong Yang*

Main category: cs.IR

TL;DR: 提出首个评估大语言模型学术研究复杂信息检索能力的数据集AcademicBrowse，介绍其特点并期望提升模型学术检索表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用搜索场景，无法满足学术搜索特定需求，如文献追踪、数据库支持等。

Method: 设计具有学术实用性、高难度、简洁评估和广泛覆盖特点的AcademicBrowse数据集。

Result: 创建了AcademicBrowse数据集，涵盖至少15个学科，数据可在指定链接获取。

Conclusion: 通过AcademicBrowse可更精确衡量和促进大语言模型在复杂学术信息检索任务中的性能提升。

Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse

</details>


### [70] [Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network](https://arxiv.org/abs/2506.13787)
*Yanjun Dai,Haoyang Feng,Yuan Gao*

Main category: cs.IR

TL;DR: 提出解耦的时间分层图神经网络DTH - GNN，在AUC和对数损失上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有图模型难以捕捉匿名用户交互网络的多尺度时间、语义和高阶依赖特征，难以描述匿名行为复杂模式。

Method: 引入时间边分解；构建分层异质聚合；制定反馈感知对比规则。

Result: DTH - GNN的AUC较最佳基线模型提高8.2%，对数损失提高5.7%。

Conclusion: DTH - GNN在处理匿名用户交互网络进行参与度推断等方面表现良好。

Abstract: While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.

</details>


### [71] [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
*Rahul Seetharaman,Kaustubh D. Dhole,Aman Bansal*

Main category: cs.IR

TL;DR: 介绍基于大语言模型的重排器InsertRank，利用词法信号提升检索性能，在多个基准测试中优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型推理能力进行重排的方法仍有改进空间，用户复杂查询需求增加。

Method: 引入InsertRank，在重排时利用BM25分数等词法信号。

Result: InsertRank在BRIGHT和R2MED基准测试中提升了检索效果，在多个大语言模型家族中表现一致良好，如Deepseek - R1在两基准测试中得分超先前方法。

Conclusion: InsertRank能有效提升检索性能，在不同大语言模型上均适用。

Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.

</details>


### [72] [RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](https://arxiv.org/abs/2506.14412)
*Tim Cofala,Oleh Astappiev,William Xion,Hailay Teklehaymanot*

Main category: cs.IR

TL;DR: 本文围绕LiveRAG 2025挑战，探索RAG解决方案，最终用InstructRAG、Pinecone检索器和BGE重排器组合的方案参赛获第四名。


<details>
  <summary>Details</summary>
Motivation: 通过结合外部非参数源增强大语言模型，提高事实正确性和减少幻觉，参与LiveRAG 2025挑战以最大化DataMorgana问答对的准确性。

Method: 探索不同的检索器组合和RAG解决方案，最终使用InstructRAG结合Pinecone检索器和BGE重排器。

Result: 解决方案的正确性得分为1.13，忠实度得分为0.55，在SIGIR 2025 LiveRAG挑战中获得第四名。

Conclusion: 所采用的InstructRAG、Pinecone检索器和BGE重排器的组合方案在挑战中取得了较好成绩。

Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.

</details>


### [73] [Similarity = Value? Consultation Value Assessment and Alignment for Personalized Search](https://arxiv.org/abs/2506.14437)
*Weicong Qin,Yi Xu,Weijie Yu,Teng Shi,Chenglei Shen,Ming He,Jianping Fan,Xiao Zhang,Jun Xu*

Main category: cs.IR

TL;DR: 提出评估框架和价值感知个性化搜索模型VAPS，实验显示其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似性的方法无法捕捉咨询对个性化的真实价值，需要更好的方法。

Method: 提出咨询价值评估框架，从场景范围价值、后验行动价值和时间衰减价值三个新视角评估历史咨询；引入VAPS模型，通过咨询 - 用户行动交互模块和明确目标选择纳入高价值咨询。

Result: 在公共和商业数据集的检索和排序任务中，VAPS始终优于基线模型。

Conclusion: 提出的评估框架和VAPS模型能有效提升个性化搜索性能。

Abstract: Personalized search systems in e-commerce platforms increasingly involve user interactions with AI assistants, where users consult about products, usage scenarios, and more. Leveraging consultation to personalize search services is trending. Existing methods typically rely on semantic similarity to align historical consultations with current queries due to the absence of 'value' labels, but we observe that semantic similarity alone often fails to capture the true value of consultation for personalization. To address this, we propose a consultation value assessment framework that evaluates historical consultations from three novel perspectives: (1) Scenario Scope Value, (2) Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce VAPS, a value-aware personalized search model that selectively incorporates high-value consultations through a consultation-user action interaction module and an explicit objective that aligns consultations with user actions. Experiments on both public and commercial datasets show that VAPS consistently outperforms baselines in both retrieval and ranking tasks.

</details>


### [74] [Vela: Scalable Embeddings with Voice Large Language Models for Multimodal Retrieval](https://arxiv.org/abs/2506.14445)
*Ruofan Hu,Yan Xia,Minjie Hong,Jieming Zhu,Bo Chen,Xiaoda Yang,Minghui Fang,Tao Jin*

Main category: cs.IR

TL;DR: 介绍Vela框架适配多模态大语言模型生成通用多模态嵌入，实验显示其在文本 - 音频检索任务中优于传统CLAP模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在声学领域表示多模态信息能力待探索。

Method: 使用特制提示和上下文学习示例，提出单模态训练方法，仅在文本对上训练模型。

Result: Vela在标准文本 - 音频检索任务中表现优于传统CLAP模型，新基准测试显示CLAP模型在处理长文本和复杂检索任务有局限，Vela表现更优。

Conclusion: Vela框架能有效适配多模态大语言模型，在文本 - 音频检索任务中有出色表现。

Abstract: Multimodal large language models (MLLMs) have seen substantial progress in recent years. However, their ability to represent multimodal information in the acoustic domain remains underexplored. In this work, we introduce Vela, a novel framework designed to adapt MLLMs for the generation of universal multimodal embeddings. By leveraging MLLMs with specially crafted prompts and selected in-context learning examples, Vela effectively bridges the modality gap across various modalities. We then propose a single-modality training approach, where the model is trained exclusively on text pairs. Our experiments show that Vela outperforms traditional CLAP models in standard text-audio retrieval tasks. Furthermore, we introduce new benchmarks that expose CLAP models' limitations in handling long texts and complex retrieval tasks. In contrast, Vela, by harnessing the capabilities of MLLMs, demonstrates robust performance in these scenarios. Our code will soon be available.

</details>


### [75] [RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.14516)
*Kun Ran,Shuoqi Sun,Khoi Nguyen Dinh Anh,Damiano Spina,Oleg Zendel*

Main category: cs.IR

TL;DR: 本文介绍RMIT - ADM+S参加SIGIR 2025 LiveRAG挑战赛情况，采用GRAG方法，经系统评估，系统在私榜取得成绩并入围前四。


<details>
  <summary>Details</summary>
Motivation: 参加SIGIR 2025 LiveRAG挑战赛，展示有效解决方案。

Method: 采用Generation - Retrieval - Augmented Generation (GRAG)方法，结合假设答案检索和基于大语言模型的重排步骤，用Grid of Points (GoP)框架和N - way ANOVA进行系统评估。

Result: 系统在私榜相关性得分为1.199，忠实度得分为0.477。

Conclusion: 系统入围LiveRAG 2025挑战赛前四，证明GRAG方法有效。

Abstract: This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG Challenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies on generating a hypothetical answer that is used in the retrieval phase, alongside the original question. GRAG also incorporates a pointwise large language model (LLM)-based re-ranking step prior to final answer generation. We describe the system architecture and the rationale behind our design choices. In particular, a systematic evaluation using the Grid of Points (GoP) framework and N-way ANOVA enabled comparison across multiple configurations, including query variant generation, question decomposition, rank fusion strategies, and prompting techniques for answer generation. Our system achieved a Relevance score of 1.199 and a Faithfulness score of 0.477 on the private leaderboard, placing among the top four finalists in the LiveRAG 2025 Challenge.

</details>


### [76] [A Systematic Replicability and Comparative Study of BSARec and SASRec for Sequential Recommendation](https://arxiv.org/abs/2506.14692)
*Chiara D'Ercoli,Giulia Di Teodoro,Federico Siciliano*

Main category: cs.IR

TL;DR: 研究对比SASRec和BSARec两个序列推荐系统，用EasyRec重实现模型对比，结果显示BSARec表现更好，但性能提升没原作者说的高，强调实现细节重要性。


<details>
  <summary>Details</summary>
Motivation: 对比SASRec和BSARec，检验BSARec中频率增强元素对推荐效果的提升。

Method: 用EasyRec的通用基础结构重新实现模型以进行公平、可复现的比较。

Result: BSARec通过加入频率增强的偏置项确实优于SASRec，但性能提升不如原作者所述。

Conclusion: 提供现有方法概述，强调实现细节对性能比较的重要性。

Abstract: This study aims at comparing two sequential recommender systems: Self-Attention based Sequential Recommendation (SASRec), and Beyond Self-Attention based Sequential Recommendation (BSARec) in order to check the improvement frequency enhancement - the added element in BSARec - has on recommendations. The models in the study, have been re-implemented with a common base-structure from EasyRec, with the aim of obtaining a fair and reproducible comparison. The results obtained displayed how BSARec, by including bias terms for frequency enhancement, does indeed outperform SASRec, although the increases in performance obtained, are not as high as those presented by the authors. This work aims at offering an overview on existing methods, and most importantly at underlying the importance of implementation details for performance comparison.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Main category: cs.LG

TL;DR: 本文提出LittleBit方法用于极端大语言模型压缩，在亚1位量化上表现优越，能实现大幅内存减少和速度提升，为资源受限环境部署LLMs提供可能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临内存和计算成本挑战，现有量化方法在亚1位体制下性能退化严重。

Method: 使用潜在矩阵分解将权重表示为低秩形式并二值化因子，集成多尺度补偿机制，采用Dual - SVID进行稳定的量化感知训练初始化和集成残差补偿来减轻误差。

Result: 实现近31倍内存减少，如Llama2 - 13B压缩至0.9GB以下；0.1 BPW在Llama2 - 7B上性能超领先方法的0.7 BPW；内核级基准测试显示比FP16有5倍速度提升潜力。

Conclusion: LittleBit建立了更优的尺寸 - 性能权衡，为在资源受限环境部署强大的大语言模型铺平道路。

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [78] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: 提出基于粒子的变分推理方法BSVGD处理多峰分布，给出收敛理论保证并做数值实验和性能比较。


<details>
  <summary>Details</summary>
Motivation: 设计适用于多峰分布的变分推理方法。

Method: 在经典SVGD算法基础上加入随机分支机制得到BSVGD算法。

Result: 给出分布收敛的理论保证，通过数值实验验证算法适用性，用Wasserstein距离和计算时间对比BSVGD和SVGD性能。

Conclusion: 未明确提及，但可推测BSVGD在处理多峰分布上有一定优势。

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [79] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: 提出移动知识编辑框架MobiEdit，能在商用移动设备上高效实现大语言模型个性化，相比先前方法在内存、能耗和延迟上有显著优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理个性化或未见查询时会产生幻觉，现有知识编辑方法因资源密集型反向传播难以在本地设备运行。

Method: MobiEdit用量化前向梯度估计替代全精度反向传播，引入提前停止机制和前缀缓存优化。

Result: 能在商用移动设备上实时编辑3B参数模型，相比先前方法内存减少7.6倍、能耗减少14.7倍、延迟减少3.6倍。

Conclusion: MobiEdit能在商用移动设备上高效实现大语言模型个性化知识编辑。

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [80] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [81] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: 介绍JobShopLib库解决作业车间调度问题，通过实验证明其效用，GNN模型取得近最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法用简单启发式规则，深度学习模型训练需定制多因素且缺乏模块化库，研究耗时。

Method: 引入JobShopLib模块化库，可定制多因素，通过模仿学习训练调度器。

Result: 一个模型仅用个体操作特征就优于多种基于图的调度器，GNN模型在大规模问题上取得近最优结果。

Conclusion: 开发此类模型有很大改进空间，JobShopLib为未来实验提供必要工具。

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [82] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Main category: cs.LG

TL;DR: 该研究整合糖尿病相关数据集，用增强的袋装集成回归模型（EBMBag+）预测美国城市糖尿病患病率，结果显示EBMBag+性能最佳。


<details>
  <summary>Details</summary>
Motivation: 糖尿病会引发多种并发症，准确的州级预测对医疗规划和干预很重要，但分析所需数据常不完整，故需有效预测方法。

Method: 先进行数据工程整合2011 - 2021年糖尿病相关数据集，再引入EBMBag+进行时间序列预测，并与多个基线模型对比。

Result: EBMBag+表现最佳，MAE为0.41，RMSE为0.53，MAPE为4.01，R2为0.9。

Conclusion: EBMBag+模型在预测美国城市糖尿病患病率上有良好效果，可用于相关医疗规划和干预。

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [83] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Main category: cs.LG

TL;DR: 介绍了一种名为ReinDSplit的新框架，用强化学习动态调整DNN分割点，在昆虫分类数据集上验证其有效性，还能用于异构环境。


<details>
  <summary>Details</summary>
Motivation: 传统SL框架的一刀切策略在农业生态系统中存在局限，导致性能问题，需新方法解决。

Method: 引入ReinDSplit框架，用Q - learning代理作为自适应协调器，将分割层选择构建为有限状态马尔可夫决策过程。

Result: 在三个昆虫分类数据集上，使用MobileNetV2时ReinDSplit达到94.31%的准确率。

Conclusion: ReinDSplit在农业及异构环境中实现了资源效率、隐私和可扩展性的平衡，带来范式转变。

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [84] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: 提出混合元学习框架用于非线性动力系统预测和异常检测，结合多模型，实验表明表现优于单一模型，可用于早期缺陷识别和预测性监测。


<details>
  <summary>Details</summary>
Motivation: 针对具有非平稳和随机行为的非线性动力系统，解决缺乏完整物理模型时的预测和异常检测问题。

Method: 集成物理启发模拟器，使用CNN - LSTM、VAE、Isolation Forests和DA - RNN等模型，通过元学习器组合模型。

Result: 基于模拟实验，混合集成在异常定位、泛化和对非线性偏差的鲁棒性方面优于单一模型。

Conclusion: 该框架为非线性系统提供了广泛的数据驱动方法，可用于多种场景的早期缺陷识别和预测性监测。

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [85] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: 本文提出DP - Ditto，在差分隐私保护下扩展Ditto，分析隐私、收敛性和公平性权衡，给出收敛上界和最优聚合次数，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法如Ditto中，客户端隐私担忧及本地模型扰动会影响个性化学习的收敛和性能公平性，需改进。

Method: 提出DP - Ditto，对Ditto在差分隐私保护下进行扩展，分析隐私保证、模型收敛和性能分布公平性的权衡，给出收敛上界和最优全局聚合次数。

Result: 实验验证分析结果，DP - Ditto在公平性上比DP扰动的现有PFL模型高32.71%，准确率高9.66%。

Conclusion: DP - Ditto能在差分隐私保护下有效平衡隐私、收敛性和性能分布公平性，且可联合优化收敛和公平性。

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [86] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Main category: cs.LG

TL;DR: 本文针对现有概念方法缺乏统计严谨性的问题，提出假设检验框架和概念分解方法，该方法有理论保证且表现更好，还能缓解数据中的虚假线索。


<details>
  <summary>Details</summary>
Motivation: 当前基于概念的嵌入解释方法缺乏统计严谨性，难以验证概念和比较技术。

Method: 引入假设检验框架量化CLIP嵌入空间中的旋转敏感结构，提出事后概念分解方法。

Result: 该方法发现的概念有理论保证，重建误差优于其他技术，能平衡重建精度和概念可解释性，在去除虚假背景概念后，最差组准确率提高22.6%。

Conclusion: 提出的基于概念的分解算法有效，能缓解数据中的虚假线索。

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [87] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出可进化条件扩散方法，用黑盒、不可微多物理模型引导生成过程，在两个AI for Science场景验证有效。


<details>
  <summary>Details</summary>
Motivation: 让黑盒、不可微多物理模型有效用于引导生成过程，促进自主科学发现。

Method: 将引导表述为优化问题，通过更新去噪分布的描述性统计来优化期望的适应度函数，从概率进化角度推导进化引导方法。

Result: 在流体拓扑自动设计和超表面两个AI for Science场景验证，能有效生成更满足特定优化目标的设计。

Conclusion: 该方法是一种有效的基于引导的扩散方法，可利用大量黑盒、不可微多物理数值模型。

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [88] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 介绍开源T-REX框架评估RL - TSC方法在动态、事故场景下的性能，发现不同方法特点，强调RL - TSC研究需考虑鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习交通信号控制（RL - TSC）在现实世界干扰（如交通事件）下的鲁棒性研究不足。

Method: 引入T - REX框架模拟事故场景，提出一套评估鲁棒性的指标，在合成和真实网络上进行实验。

Result: 独立值基和分散压力基方法在稳定交通和同质网络中收敛快、泛化性好，但在事故场景性能下降；分层协调方法在大规模、不规则网络中性能更稳定，但收敛慢、训练复杂。

Conclusion: RL - TSC研究需要进行鲁棒性感知设计和评估，T - REX提供了一个开放、标准和可重复的平台。

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [89] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Main category: cs.LG

TL;DR: 研究常见机器学习模型再训练技术的能耗和准确性，发现仅用最新数据再训练可降能耗25%，按需再训练可降40%，为从业者提供节能建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统需定期再训练维护，但再训练能耗大，需研究可持续应用的再训练技术。

Method: 研究常见再训练技术的能耗，从能效和准确性两方面对比不同再训练技术。

Result: 仅用最新数据再训练比用所有数据降低能耗达25%；按需而非按固定时间表再训练，有可靠数据变化检测器时可降低能耗达40%。

Conclusion: 研究为机器学习从业者设计可持续软件系统提供更节能的再训练技术建议。

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [90] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Main category: cs.LG

TL;DR: 开发了结合多模态时空数据的SatHealth数据集，实验表明环境信息可提升AI模型性能，还部署了网页应用，为医疗研究提供资源。


<details>
  <summary>Details</summary>
Motivation: 公共和人群健康研究缺乏长期细粒度时空数据，多数现有研究未纳入环境数据，限制模型性能和应用。

Method: 开发SatHealth数据集，结合多模态时空数据，在区域公共卫生建模和个人疾病风险预测两个用例下进行实验。

Result: 生活环境信息能显著提升AI模型在各种任务上的性能和时空泛化能力。

Conclusion: 通过网页应用和发布的代码管道，为医疗研究纳入环境数据提供有价值角度和资源，建立环境健康信息学未来研究的基础框架。

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [91] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Main category: cs.LG

TL;DR: 本文提出类似PMD的StaQ算法，仅保留最后M个Q函数，具有理论保证，性能稳定且有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有Policy Mirror Descent（PMD）框架的闭式解在实践中难以处理，需要改进。

Method: 提出并分析仅保留最后M个Q函数的类似PMD算法。

Result: 得到的StaQ算法有强理论保证，与深度强化学习基线有竞争力，且性能波动小。

Conclusion: 为完全稳定的深度强化学习算法铺平道路，为Policy Mirror Descent实验提供测试平台。

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [92] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 本文提出用S6模型解决AD在ICRL应用中的瓶颈，经实验证明Mamba模型在AD中优于Transformer模型，扩展AD上下文可提升ICRL性能。


<details>
  <summary>Details</summary>
Motivation: 现有AD方法受Transformer注意力机制限制，实验瓶颈于其二次复杂度，且仅适用于简单离散短时间环境，需改进。

Method: 采用S6模型，通过四个复杂连续的元强化学习环境进行实验。

Result: 基于S6层构建的Mamba模型在AD中总体优于Transformer模型，扩展AD上下文可提升ICRL性能。

Conclusion: S6模型能解决AD在ICRL应用中的瓶颈，扩展AD上下文能使ICRL有竞争力。

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [93] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Main category: cs.LG

TL;DR: 本文提出基于规则系统特征贡献估计框架，含可视化策略和度量指标，经实验验证有新洞察和良好性能。


<details>
  <summary>Details</summary>
Motivation: 规则模型复杂时，难以辨别关键特征、理解其交互和比较不同规则集特征贡献，需解决此问题。

Method: 提出综合框架，包括图特征可视化策略、特征重要性度量和规则集距离度量，在两个临床数据集和四种规则方法上实验。

Result: 能在数据集和特定类别层面发现临床特征组合预测价值新洞察，比较分析显示在15个公共基准上性能有竞争力且更稳健。

Conclusion: 该方法有助于识别新风险因素等，提升诊断准确性，代码可在GitHub获取。

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [94] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出图信息变压器算子（GITO）架构学习不规则几何和非均匀网格上的偏微分方程系统，含HGT和TNO模块，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习复杂的不规则几何和非均匀网格上的偏微分方程系统。

Method: GITO架构包含HGT和TNO模块，HGT结合GNN和变压器编码特征，TNO用线性复杂度注意力层映射输入到预测。

Result: 在基准PDE任务上，GITO性能优于现有基于变压器的神经算子。

Conclusion: GITO为工程应用中高效、与网格无关的替代求解器铺平道路。

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [95] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 研究用稀疏自编码器实现大语言模型特征恢复，提出新统计框架和训练算法，有理论保证且实证表现优。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器训练算法缺乏数学保证，有超参数敏感和不稳定等问题。

Method: 提出特征恢复统计框架，引入基于‘偏差自适应’的训练算法，开发改进的经验变体GBA。

Result: 理论证明算法能正确恢复单语义特征，GBA在应用于15亿参数大语言模型时性能优于基准方法。

Conclusion: 提供首个有理论恢复保证的SAE算法，是揭开SAE训练神秘面纱的基础一步，推动AI系统机制可解释性发展。

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [96] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Main category: cs.LG

TL;DR: 文章对螺丝紧固过程监测开展少样本学习研究，引入标签感知采样器，对比不同范式和骨干网络，发现轻量级CNN架构结合简单度量学习在数据稀缺时更优，还发布相关资源促进研究。


<details>
  <summary>Details</summary>
Motivation: 少样本学习在工业时间序列数据应用未充分探索，标注新缺陷成本高，需研究其在螺丝紧固过程监测中的应用。

Method: 使用2300样本的多变量扭矩数据集，引入标签感知采样器，研究基于度量的原型网络和基于梯度的MAML两种范式，搭配三种骨干网络。

Result: 在10-shot、3-way评估中，InceptionTime + 原型网络组合表现最佳，度量学习优于MAML，标签感知采样有额外提升。

Conclusion: 数据稀缺时，轻量级CNN架构结合简单度量学习收敛快、泛化好，挑战大基础模型总是更优的假设。

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [97] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Main category: cs.LG

TL;DR: 提出并研究Hierarchical Ego Graph Neural Networks (HEGNNs)，从逻辑上刻画其节点分类器，实验验证其可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 受图同构测试的Individualization - Refinement范式启发，扩展图神经网络的表达能力。

Method: 提出HEGNNs，用分级混合逻辑对其节点分类器进行逻辑刻画，将其分离能力与其他模型和算法关联。

Result: 实验证实HEGNNs实际可行，与传统GNN架构相比有优势。

Conclusion: HEGNNs是图神经网络的有效扩展，在理论和实践上均有价值。

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [98] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: 针对图生成任务，指出当前方法不足，提出基于MRF和最优传输位移设计概率路径的BWFlow框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法独立建模节点和边、线性插值的方式未考虑图的非欧结构和互连模式，影响采样收敛，需构建更好的概率路径。

Method: 将图表示为MRF参数化的连接系统，利用MRF对象间的最优传输位移设计概率路径，引入BWFlow框架。

Result: BWFlow框架可适应连续和离散流匹配算法，在普通图生成和2D/3D分子生成实验中表现出有竞争力的性能、稳定的训练和有保证的采样收敛。

Conclusion: BWFlow框架在图生成中有效，能尊重图的底层几何结构，提供概率路径中的平滑速度。

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [99] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Main category: cs.LG

TL;DR: 本文建立Koopman算子逼近与线性循环神经网络的联系，提出SKOLR方法，实验显示其性能出色。


<details>
  <summary>Details</summary>
Motivation: Koopman算子一般是无限维的，目标是学习能产生可处理的有限维Koopman算子逼近的测量函数。

Method: 建立Koopman算子逼近与线性RNN的联系，提出SKOLR方法，将输入信号的可学习谱分解与MLP作为测量函数，通过高度并行的线性RNN堆栈实现结构化Koopman算子。

Result: 在各种预测基准和动力系统的数值实验中，该方法表现出色。

Conclusion: 基于Koopman理论的精简设计能带来卓越性能。

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [100] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Main category: cs.LG

TL;DR: 研究RLVR推理模型解决新问题机制，发现主要途径，通过多规模模型验证，利用自然语言指导提升性能，提出新在线训练算法Guide并验证效果。


<details>
  <summary>Details</summary>
Motivation: 探究用可验证奖励的强化学习训练的推理模型解决新问题的过程。

Method: 在不同规模模型上对超50万个推理问题进行实验，基于发现提出Guide算法，包括其变体并进行消融分析和理论分析。

Result: RLVR通过压缩pass@k和能力增益提升性能，学习新问题主要靠自蒸馏；利用自然语言指导可提升pass@k；Guide - GRPO在7B和32B模型上有最多4%的宏平均提升。

Conclusion: 研究了RLVR解决新问题的机制，提出的Guide算法能提升模型泛化能力。

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [101] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Main category: cs.LG

TL;DR: 提出DiffusionBlocks训练框架，可提升内存效率，实验表明在减少内存同时有出色表现，为有限资源下训练大模型提供途径。


<details>
  <summary>Details</summary>
Motivation: 端到端反向传播训练大神经网络存在内存瓶颈，限制了对先进AI研究的可及性。

Method: 提出DiffusionBlocks框架，将神经网络块解释为连续时间扩散过程中的去噪操作，把网络划分为可独立训练的块，并基于等累积概率质量优化噪声水平分配。

Result: 在图像生成和语言建模任务实验中，内存减少与块数量成正比，且性能更优。

Conclusion: DiffusionBlocks为有限计算资源下大规模神经网络训练的普及提供了有前景的途径。

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [102] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 现有AI模型缺乏适应性，研究表明各种适应方法可视为校正近似后验，更准确后验能实现更快适应，并举例说明。


<details>
  <summary>Details</summary>
Motivation: 当前最好的AI模型缺乏适应性，且不清楚机器如何像人类和动物一样自然学习适应。

Method: 使用Khan和Rue（2023）的贝叶斯学习规则的双视角，将适应过程中的干扰用过去数据的自然梯度失配来表征。

Result: 发现各种适应方法可视为不同的“校正”近似后验方式，更准确的后验会带来更小的校正，意味着更快的适应。

Conclusion: 后验校正可作为机器快速学习适应的自然机制。

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [103] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: 本文引入事后可解释性框架分析弹性决策变换器（EDTs）中内在动机如何塑造学习嵌入，揭示不同内在动机变体产生不同表征结构，解释其提升策略学习的原因。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明将内在动机机制融入EDTs可提升探索任务表现，但对其改进背后的表征机制尚未探索，因此开展研究。

Method: 引入系统的事后可解释性框架，通过对嵌入属性（包括协方差结构、向量大小和正交性）进行统计分析。

Result: 不同内在动机变体创建了根本不同的表征结构，发现嵌入指标与性能之间存在特定环境的相关模式。

Conclusion: 内在动机不仅是简单的探索奖励，还作为表征先验以生物学上合理的方式塑造嵌入几何，创建特定环境的组织结构以促进更好的决策。

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [104] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: 本文使用IVON变分算法解决贝叶斯方法在LoRA微调中的问题，在大模型上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法用于LoRA微调时对其他指标提升有限、增加计算开销且需额外技巧，需解决这些问题。

Method: 采用IVON变分算法及简单的后验剪枝技术。

Result: 在十亿规模大模型上进行大量实验，如微调Llama - 3.2 - 3B模型，相比AdamW准确率提升1.3%，ECE降低5.4%，优于AdamW和其他贝叶斯方法。

Conclusion: 使用IVON进行变分学习可有效改进LoRA微调。

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [105] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 文章指出先前成员推理攻击（MIAs）研究忽略攻击间差异，通过新框架研究这些差异，揭示潜在原因及影响，并提出集成框架用于更强大攻击和隐私评估。


<details>
  <summary>Details</summary>
Motivation: 先前MIA研究关注性能指标，忽略不同攻击间差异，而这些差异对MIA作为隐私评估工具的可靠性和完整性有重要影响。

Method: 通过基于覆盖和稳定性分析的新框架系统研究攻击差异，提出包含三种策略的集成框架。

Result: 实验揭示了MIAs中的显著差异、潜在原因及对隐私评估的广泛影响。

Conclusion: 提出的集成框架能构建更强大攻击，为隐私评估提供更稳健和全面的方法。

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [106] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Main category: cs.LG

TL;DR: 本文提出从第一性原理设计节点级图基础模型的方法，通过研究对称性构建网络，在29个真实数据集验证，有良好零样本性能和随训练图数量增加的改进效果。


<details>
  <summary>Details</summary>
Motivation: 现有图机器学习架构针对性强、适用性受限，需构建能跨任意图和特征泛化的图基础模型。

Method: 系统研究图基础模型需遵循的对称性，刻画对节点和标签排列等变、对特征排列不变的线性变换空间，构建网络并证明其是满足对称性的多集通用近似器，用该网络层得到节点属性预测模型类。

Result: 在29个真实世界节点分类数据集上进行大量实验，展现出强零样本经验性能，且随训练图数量增加性能持续提升。

Conclusion: 所提出的设计图基础模型的方法有效，能实现良好的节点级任务性能。

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [107] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [108] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: 提出HAELT深度学习框架用于高频股票价格预测，在苹果公司数据测试中表现良好，有金融预测和算法交易潜力。


<details>
  <summary>Details</summary>
Motivation: 解决高频股票价格预测中存在的非平稳性、噪声和波动性问题。

Method: 提出HAELT框架，结合基于ResNet的降噪模块、时间自注意力机制和混合LSTM - Transformer核心，并根据近期表现自适应集成各组件。

Result: 在2024年1月至2025年5月苹果公司每小时数据的测试集上，HAELT取得最高F1分数，能有效识别价格涨跌。

Conclusion: HAELT在金融预测和算法交易方面有稳健且实用的潜力。

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [109] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Main category: cs.LG

TL;DR: 提出QCL - MixNet解决专家系统中表格数据类别不平衡问题，实验显示其性能优于基线模型，确立新基准。


<details>
  <summary>Details</summary>
Motivation: 传统方法解决专家系统中类别不平衡表格数据分类存在过拟合、标签噪声和泛化性差等问题。

Method: 提出QCL - MixNet框架，包含量子纠缠启发层、样本感知混合策略和混合损失函数。

Result: 在18个真实不平衡数据集上，QCL - MixNet在macro - F1和召回率上优于20个基线模型，消融实验验证各组件重要性。

Conclusion: QCL - MixNet可作为专家系统表格数据不平衡处理新基准，理论分析证明其性能。

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [110] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Main category: cs.LG

TL;DR: 本文提出时空反演框架用于温室气体排放实时识别与量化，验证显示其准确性高且运行速度快，适用于工业排放监测等任务。


<details>
  <summary>Details</summary>
Motivation: 解决瞬态大气条件下温室气体排放实时识别与量化这一环境监测中的关键挑战。

Method: 引入时空反演框架，将计算流体动力学（CFD）的深度学习替代模型嵌入序贯蒙特卡罗算法，用多层感知器替代高成本数值求解器。

Result: 在Chilbolton甲烷释放数据集上验证，准确性与全CFD求解器和高斯羽流模型相当，运行速度快几个数量级，在模拟受阻流场景实验中表现出鲁棒性。

Conclusion: 该工作兼顾物理保真度和计算可行性，为工业排放监测和其他对时间敏感的时空反演任务提供可扩展解决方案。

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [111] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: 引入AssistedDS基准评估大语言模型在表格预测任务中利用领域知识的能力，发现模型存在不足，指明研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否像人类数据科学家一样批判性地利用外部领域知识。

Method: 引入AssistedDS基准，包含合成数据集和Kaggle竞赛数据，配有相关文档，评估模型辨别和应用有益与有害领域知识的能力。

Result: 大语言模型常不加批判地采用信息，对抗性内容会损害性能，有益指导难抵消负面影响，在Kaggle数据集中处理数据存在多种错误。

Conclusion: 当前模型在批判性评估和利用专家知识方面存在巨大差距，需开展相关研究以开发更强大、知识感知的自动数据科学系统。

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [112] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Main category: cs.LG

TL;DR: 研究带参学习（最佳臂识别），探索带参可学习性的通用理论，揭示结构化带参学习局限。


<details>
  <summary>Details</summary>
Motivation: 在真实奖励函数f属于已知函数类F的假设下，寻求类似PAC框架的带参可学习性通用理论，解答哪类函数F可学习以及如何学习的问题。

Method: 通过对比经典学习理论结果，针对两个问题分别进行证明，一是证明无法用类似维度的量识别可学习类，二是构建奖励函数类证明计算困难性。

Result: 发现用类似维度量识别可学习类对带参学习失败；构建的奖励函数类存在计算困难性，但对标准算法操作有高效算法；还研究了噪声下学习等额外主题。

Conclusion: 揭示了带参学习的局限性，表明计算困难性是带参学习任务固有的。

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


### [113] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Main category: cs.LG

TL;DR: 当前长序列训练在开源领域有挑战，本文提出ALST方法，能支持多种HF模型的长序列训练，大幅提升训练长度并开源。


<details>
  <summary>Details</summary>
Motivation: 解决开源领域长序列训练系统支持不足，现有方法在训练大模型长序列时存在内存问题，导致训练难以进行的问题。

Method: 提出Arctic Long Sequence Training (ALST)，结合注意力无关的单GPU和多GPU内存优化。

Result: ALST能支持Meta的Llama 8B模型在不同GPU配置下训练更长序列，如在4节点集群上超1500万，比基线提升超400倍。

Conclusion: ALST能有效支持多种HF模型的长序列训练，且完全兼容HF模型并已开源。

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [114] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Main category: cs.LG

TL;DR: 研究发现大语言模型去学习后存在可检测的痕迹，简单分类器可根据文本输出判断，实验表明不同输入下检测准确率高，存在逆向工程遗忘信息风险。


<details>
  <summary>Details</summary>
Motivation: 去学习在保护数据隐私等方面重要，但发现去学习后存在新的漏洞——去学习痕迹检测。

Method: 发现去学习在模型行为和内部表示留下“指纹”，用简单监督分类器根据文本输出判断，分析痕迹在中间激活层的嵌入和传播，进行大量实验。

Result: 遗忘相关提示检测准确率超90%，遗忘无关输入下大模型仍有高可检测性。

Conclusion: 去学习会留下可测量痕迹，当模型被识别为已去学习时，存在逆向工程遗忘信息的新风险。

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [115] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Main category: cs.LG

TL;DR: 提出逆弹性物理信息神经网络（IE - PINN）从含噪位移数据重建弹性参数，通过多架构集成、两阶段估计策略及其他创新方法提升性能，实验证明其在强噪声下有效。


<details>
  <summary>Details</summary>
Motivation: 现有逆弹性问题估计技术存在不稳定、对噪声敏感和难恢复绝对尺度杨氏模量等局限，需新方法解决。

Method: 提出IE - PINN，集成三种神经网络架构分别建模位移、应变和弹性分布；采用两阶段估计策略；运用位置编码、正弦激活函数和顺序预训练协议。

Result: 广泛数值实验表明IE - PINN能有效克服现有方法局限，在强噪声条件下实现准确的绝对尺度弹性估计。

Conclusion: 该方法在临床成像诊断和力学表征等含噪测量场景有很大应用潜力。

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [116] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Main category: cs.LG

TL;DR: 提出新负载均衡损失解决稀疏混合专家模型路由问题，实验显示收敛更快且冗余更低。


<details>
  <summary>Details</summary>
Motivation: 现有负载均衡机制在训练时会导致路由行为不一致，使模型学习冗余知识。

Method: 引入一种保留令牌关系结构的新型负载均衡损失，鼓励训练中相似输入选择一致专家。

Result: 将新损失应用于路由器，比流行负载均衡损失收敛快36%且冗余更低。

Conclusion: 新型负载均衡损失有效解决了现有机制问题，提升了模型性能。

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [117] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Main category: cs.LG

TL;DR: 提出ScIReN框架结合可解释神经和基于过程的推理，在两个任务中表现优于黑盒网络且有科学可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络不遵循科学定律且是黑盒，基于过程的模型有自由参数设定问题，现有工作在发现参数与输入特征可解释关系上有挑战。

Method: 提出ScIReN框架，用可解释编码器预测有科学意义的潜在参数，通过可微的基于过程的解码器预测输出变量，使用硬sigmoid约束层限制参数范围。

Result: 在模拟土壤有机碳流动和植物生态系统呼吸两个任务中，ScIReN预测精度优于黑盒网络。

Conclusion: ScIReN在保证预测精度的同时能提供科学可解释性，可推断潜在科学机制及其与输入特征的关系。

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [118] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Main category: cs.LG

TL;DR: 为解决大语言生成模型幻觉问题，提出部分反馈下选择性生成的在线学习算法，理论和实证证明其能控制FDR并保持选择效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言生成模型存在幻觉问题，且缺少部分反馈下选择性生成的学习方法。

Method: 将选择性生成问题转化为多臂老虎机问题，利用转换引理连接两者，同时利用选择性生成中臂的独特结构解锁反馈。

Result: 在不同数据环境下，该算法能控制期望的FDR，保持合理的选择效率。

Conclusion: 提出的在线学习算法在部分反馈下对选择性生成有效，可用于控制大语言生成模型的幻觉。

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [119] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Main category: cs.LG

TL;DR: 提出CVDP基准，含783个问题，涵盖13个任务类别，揭示当前模型能力差距，强调持续研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 推动硬件设计和验证领域的大语言模型和智能体研究。

Method: 创建CVDP基准，涵盖多种任务类别，以非智能体和智能体形式提供问题，使用开源工具和模型评分基础设施进行评估。

Result: 当前最先进模型在代码生成上的pass@1不超过34%，智能体任务尤其困难。

Conclusion: 当前模型能力存在显著差距，需要持续开展研究以实现稳健的现实硬件设计自动化。

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [120] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Main category: cs.LG

TL;DR: 本文探讨时间序列基础模型（TSFMs）微调问题，提出多尺度微调框架MSFT，实验表明其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 当前有效微调TSFMs到特定下游任务的方法未被充分研究，朴素微调无法充分发挥TSFMs能力，常导致过拟合和性能不佳。

Method: 从因果角度分析微调过程，提出将多尺度建模明确融入微调过程的多尺度微调框架MSFT。

Result: 在三种不同骨干模型上的实验结果显示，使用MSFT微调的TSFMs性能优于朴素和典型参数高效微调方法，也超过了最先进的深度学习方法。

Conclusion: MSFT是一个简单且通用的框架，能有效提升TSFMs在特定下游任务中的性能。

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [121] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Main category: cs.LG

TL;DR: 研究稀疏Transformer从学习和泛化角度，发现输入依赖的稀疏注意力模型收敛快、泛化好，输入无关则无此优势，并给出理论解释。


<details>
  <summary>Details</summary>
Motivation: 从学习和泛化而非效率角度研究稀疏Transformer。

Method: 实证研究一系列注意力机制，建立标准softmax稳定性与损失函数Lipschitz性质的联系，分析稀疏性对其影响。

Result: 输入依赖的稀疏注意力模型收敛快、泛化好，输入无关的无此优势，理论证明输入无关稀疏注意力无益处，输入依赖在一定条件下有改善保证。

Conclusion: 集中模型的“语义焦点”（输入依赖的稀疏注意力）可加速学习，且能从理论和实证验证其优势。

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [122] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Main category: cs.LG

TL;DR: 本文探讨构建图基础模型，提出用随机游走表示节点，开发上下文预测损失，展示模型在图数据处理和推理的潜力。


<details>
  <summary>Details</summary>
Motivation: 自然语言领域有基础模型，探究能否为图构建类似模型。

Method: 用随机游走表示节点，使Transformer从序列中提取节点表示；开发上下文预测损失并分析其表达能力。

Result: 展示了模型预训练及其在下游任务的适应性。

Conclusion: 模型有作为处理和推理图结构数据基础的潜力。

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [123] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Main category: cs.LG

TL;DR: 本文对七种GNN架构和30种损失函数组合进行大规模评估，发现在归纳学习中混合损失函数性能更优，GIN架构平均性能最高等。


<details>
  <summary>Details</summary>
Motivation: 此前缺乏对GNN模型和多种损失函数在不同任务上协同工作的大规模评估，本文旨在填补这一空白。

Method: 对七种GNN架构和30种单及混合损失函数进行评估，涵盖归纳和直推设置，使用三个真实数据集和21种评估指标。

Result: 归纳学习中混合损失函数性能更优，GIN架构平均性能最高，GAT在特定混合损失下有专长，MPNN架构表现落后。

Conclusion: 混合损失函数有利于多目标优化，不同GNN架构搭配合适损失函数能满足特定任务需求。

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [124] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Main category: cs.LG

TL;DR: 提出可扩展归纳的基于对比学习的GNN（CLGNN）用于准确高效的TBC预测，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有TBC计算昂贵、分布不平衡，现有GNN方法无法处理不平衡或忽略时间依赖，导致TBC预测不准确。

Method: 构建实例图，用双聚合编码结构和时间特征，引入KContrastNet缓解类不平衡，ValueNet估计TBC值，支持多最优路径定义。

Result: CLGNN比现有精确TBC计算方法提速663.7倍，优于静态和时态GNN基线。

Conclusion: CLGNN在TBC预测上有效且高效。

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [125] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 本文挑战模型训练流程中改进可向下游传播的假设，研究专家微调对模型再利用的影响，发现长时间微调会导致合并性能下降，提出任务依赖的激进早停策略可提升再利用性能。


<details>
  <summary>Details</summary>
Motivation: 挑战模型训练流程中改进可向下游传播的假设，研究专家微调对模型再利用的影响。

Method: 研究专家模型长时间微调对模型再利用的影响，分析性能下降原因，提出任务依赖的激进早停策略。

Result: 长时间微调专家模型会导致合并性能下降，下游结果变差，原因是记住少量困难样本并在合并时遗忘；任务依赖的激进早停策略可显著提升再利用性能。

Conclusion: 任务依赖的激进早停策略可显著改善模型再利用性能。

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [126] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Main category: cs.LG

TL;DR: 本文提出决策树布尔逻辑表示法解决预测等价问题，并应用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 决策树存在预测等价问题，即相同决策边界有不同评估过程，导致模型选择困难。

Method: 提出无预测等价性且忠实于底层决策边界的布尔逻辑表示法，并应用于下游机器学习任务。

Result: 决策树对特征值测试时缺失情况有惊人鲁棒性，解决预测等价对量化变量重要性的影响，提出优化预测成本的算法。

Conclusion: 所提出的布尔逻辑表示法有效解决决策树预测等价问题，并能应用于多个下游任务。

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [127] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Main category: cs.LG

TL;DR: 常见基准低估了编程表示泛化能力，简单修改神经策略训练流程可使其在OOD问题上与编程策略一样有效泛化。


<details>
  <summary>Details</summary>
Motivation: 指出常用基准低估编程表示泛化能力的问题。

Method: 分析四篇文献实验，对神经策略训练流程进行简单修改，如采用更简单架构、使用能产生更安全策略的奖励函数，还提议创建突出OOD泛化所需概念的基准问题。

Result: 简单修改后，神经策略在OOD问题上能和编程策略一样有效泛化。

Conclusion: 常用基准低估编程表示泛化能力，简单修改神经策略训练流程可提升其泛化能力，同时应创建合适的基准问题。

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [128] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: 本文研究多智能体强化学习在LAG战斗环境中的表现，评估HAPPO和HASAC算法，发现HASAC在无武器任务中表现好，HAPPO在导弹战斗场景适应性强。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在部分可观测、合作竞争的战斗环境LAG中的表现。

Method: 描述LAG环境设置，评估HAPPO（基于PPO的策略内分层变体）和HASAC（基于软演员 - 评论家的策略外方法）两种算法，分析训练稳定性、奖励进展和智能体间协调能力。

Result: HASAC在无武器的简单协调任务中表现良好，HAPPO在涉及导弹战斗的更动态和复杂场景中适应性更强。

Conclusion: 研究结果揭示了多智能体环境中策略内和策略外方法的权衡。

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [129] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: 本文将柯尔莫哥洛夫 - 阿诺尔德表示定理应用于生成建模，提出可解释、易设计且高效的生成模型，并引入扩展方法平衡多种性能。


<details>
  <summary>Details</summary>
Motivation: 将经典表示定理与现代概率建模相结合，解决生成模型的灵活性和先验 - 后验不匹配问题，同时平衡训练稳定性、推理速度和生成质量。

Method: 通过逆变换采样将柯尔莫哥洛夫 - 阿诺尔德表示定理的内部函数解释为概率空间之间的马尔可夫核，将柯尔莫哥洛夫 - 阿诺尔德网络生成器与独立基于能量的先验耦合，通过最大似然训练；引入基于混合分布和朗之万蒙特卡罗方法的可扩展扩展。

Result: 实现了可解释、易设计且高效的生成模型，可快速推理，能结合先验知识提高学习效率和样本质量，学习到的先验可恢复和可视化。

Conclusion: 研究成果连接了经典表示定理与现代概率建模，平衡了训练稳定性、推理速度以及生成的质量和多样性。

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [130] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 提出神经网络OOD检测特征构建理论，引入随机特征与新损失函数，优化得到特征，可恢复现有特征属性并预测新函数，提供通用框架。


<details>
  <summary>Details</summary>
Motivation: 构建神经网络的OOD检测特征。

Method: 引入含KL散度和信息瓶颈的信息论损失函数，通过变分过程优化损失获取OOD特征。

Result: 能恢复现有OOD特征属性，预测出在OOD基准上表现更优的新塑造函数。

Conclusion: 该理论为构建多种具有清晰可解释性的新特征提供通用框架。

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [131] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Main category: cs.LG

TL;DR: 提出TriGuard安全评估框架，结合多种方法揭示模型准确性与可解释性不匹配，实验展示其能发现推理脆弱性，熵正则训练可减少解释漂移。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在对抗和分布偏移下可靠性的问题，确保模型评估的安全性。

Method: 提出TriGuard框架，结合形式化鲁棒性验证、归因熵量化显著性集中程度、新颖的归因漂移分数衡量解释稳定性。

Result: 揭示模型准确性和可解释性不匹配，发现模型推理中的细微脆弱性，熵正则训练可减少解释漂移且不牺牲性能。

Conclusion: TriGuard推动了鲁棒、可解释模型评估的发展。

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [132] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 本文提出利用大语言模型（LLMs）估计图的同质性，指导谱图神经网络（SGNNs）的多项式谱滤波器设计，实验表明该框架在多种图结构上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 谱图神经网络（SGNNs）在标签稀缺时学习的滤波器可能次优，性能下降，探索大语言模型（LLMs）能否帮助克服SGNNs的局限性并提升其性能。

Method: 提出一种利用LLMs估计图同质性的方法，将估计的同质性用于自适应指导多项式谱滤波器的设计，引入轻量级流水线，让LLM生成同质性先验并注入滤波器系数。

Result: 在基准数据集上的广泛实验表明，所提出的LLM驱动的SGNN框架在同质性和异质性设置下均始终优于现有基线，且计算和经济成本极低。

Conclusion: 利用LLMs估计图同质性并指导滤波器设计的方法有效提升了SGNNs在不同图结构上的表现力和适应性。

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [133] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Main category: cs.LG

TL;DR: 研究大语言模型能否逃避潜在空间监控器，引入RL - Obfuscation方法进行实验，发现token级监控器易受攻击，整体监控器更稳健，对抗策略有泛化性，模型会内部改变token含义。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否能学会逃避潜在空间监控器。

Method: 引入RL - Obfuscation方法，通过强化学习对7B到14B参数的大语言模型进行微调，以绕过潜在空间监控器，同时保持输出连贯性，并对一系列监控器评估逃避成功率。

Result: token级潜在空间监控器易受攻击，整体监控器如最大池化或基于注意力的探测器更稳健；对抗策略能泛化到同类型未见过的监控器；模型能学会在内部赋予token不同含义。

Conclusion: 大语言模型可以通过微调学习逃避潜在空间监控器，不同类型监控器的抗逃避能力有差异，对抗策略有一定泛化性。

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [134] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Main category: cs.LG

TL;DR: 文章针对现有L2O方法在OOD场景缺乏性能和鲁棒性理论证明的问题，给出证明并提出新模型，新模型在InD和OOD场景表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有L2O方法在OOD场景缺乏性能和鲁棒性的理论证明。

Method: 先证明L2O模型在InD实例上有统一收敛率的充分条件，提出将OOD问题与InD问题对齐的方法，提出具有简洁梯度特征构建和基于梯度的历史建模方法的L2O模型。

Result: 数值模拟显示新模型在InD和OOD场景优于现有基线模型，收敛速度最高提升10倍。

Conclusion: 所提方法解决了现有L2O方法在OOD场景的理论缺失问题，且新模型性能更优。

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [135] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Main category: cs.LG

TL;DR: 论文聚焦双重不平衡数据集的公平性问题，先分析去偏局限性，再提出多标准解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法在数据收集不平衡时效果不佳，尤其针对标签和敏感属性组都不平衡的双重不平衡数据集。

Method: 先对双重不平衡数据集去偏进行探索性分析，再提出基于多标准的解决方案来确定标签和敏感属性最合适的采样和分布。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [136] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Main category: cs.LG

TL;DR: 本文提出优化方法解决有创机械通气设置难题，改进离线强化学习算法并引入新奖励函数，结果显示有助于提升患者安全和个性化肺支持。


<details>
  <summary>Details</summary>
Motivation: 有创机械通气设置复杂且易出错，现有离线强化学习方法难以处理混合动作空间，离散化动作空间存在诸多问题。

Method: 在动作空间缩减的先前工作基础上进行优化，使SOTA离线RL算法直接在混合动作空间运行，引入基于通气自由天数和生理目标的奖励函数。

Result: AI辅助的机械通气优化有助于提升患者安全和实现个性化肺支持。

Conclusion: 该研究是迈向智能、数据驱动的重症监护解决方案的重要进展。

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [137] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Main category: cs.LG

TL;DR: 本文指出残差网络和前馈网络性能差距持续存在，提出残差网络处于不同函数空间的解释，并通过实验表明残差连接有超越优化的性能优势。


<details>
  <summary>Details</summary>
Motivation: 解释残差网络和前馈网络性能差距持续存在的原因。

Method: 设计控制的训练后比较，将泛化性能与可训练性分离。

Result: 可变深度架构（类似ResNets）始终优于固定深度网络，即便优化不太可能产生差异。

Conclusion: 残差连接具有超越优化的性能优势，存在与自然数据结构一致的更深层归纳偏置。

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [138] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 扩展自解释原型变分模型进行离群分布（OOD）检测，引入新损失函数，评估显示该方法有用且优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 理解深度机器学习模型的决策过程和可靠性对其在安全相关应用中的采用至关重要，需要更好的方法进行OOD检测。

Method: 扩展自解释原型变分模型，应用变分自编码器学习潜在空间，用高斯混合分布定义分布内区域，引入新的限制损失函数。

Result: 在常见OOD检测基准和铁路应用的大规模数据集上的评估显示该方法优于先前方法。

Conclusion: 所提出的方法在OOD检测方面是有用且有效的。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [139] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Main category: cs.LG

TL;DR: 提出HiLight分层强化学习框架用于大规模交通信号控制，实验表明其在大规模场景有优势。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大规模交通网络信号控制中，集中式有可扩展性问题，分布式缺乏统一目标，网络效率受限。

Method: 提出HiLight框架，含高层Meta - Policy和低层Sub - Policy，引入对抗训练机制提升全局规划与局部执行的一致性。

Result: 在合成和真实世界基准测试及构建的大规模曼哈顿网络上评估，HiLight在大规模场景优势显著，在不同规模标准基准测试中也有竞争力。

Conclusion: HiLight是一种有效的大规模交通信号控制方法。

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [140] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Main category: cs.LG

TL;DR: 分析医疗预测任务，指出子组级评估对医疗ML模型的重要性


<details>
  <summary>Details</summary>
Motivation: 现实医疗数据集存在问题致模型性能有差异，引发公平性担忧

Method: 分析多个医疗预测任务，进行子组级性能分析

Result: 可清晰识别模型性能差异

Conclusion: 子组级评估对医疗ML模型开发和部署有实际意义，关乎公平与透明

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [141] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Main category: cs.LG

TL;DR: 论文提出交互层框架和基于模型的ACDA算法，以处理强化学习中不可观测的时变延迟，在多种基准环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习中MDP假设在现实动态环境中因交互延迟常不成立，现有方法保守处理延迟不确定性。

Method: 引入交互层框架，让智能体生成可能的未来动作矩阵；基于此开发ACDA算法，动态调整延迟模式。

Result: 在多种运动基准环境中，该方法显著优于现有方法。

Conclusion: 所提的交互层框架和ACDA算法能有效处理强化学习中的不可观测时变延迟。

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [142] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Main category: cs.LG

TL;DR: 提出新的技能发现目标和条件自动编码器，在多下游任务表现优


<details>
  <summary>Details</summary>
Motivation: 解决无监督强化学习中基于熵的探索在大规模状态空间表现不佳、基于互信息估计的赋能方法在状态探索的局限性问题

Method: 提出新的技能发现目标，构建带软模块化的条件自动编码器，基于自动编码器制定内在奖励

Result: 通过实验表明该方法能学习有意义技能，在下游任务中表现优越

Conclusion: 提出的方法有效解决相关问题，在多任务中有良好表现

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [143] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出“模型MoE - 化”策略MoORE，解决多任务场景下任务冲突和遗忘问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模基础模型在多任务场景中存在的任务冲突和遗忘问题。

Method: 对预训练模型权重矩阵进行SVD，引入可学习路由器调整奇异值，得到MoORE，对右奇异向量施加可学习正交变换。

Result: 实验表明MoORE在多个数据集上始终优于现有多任务适应方法。

Conclusion: MoORE在抗冲突和抗遗忘方面具有优越性。

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [144] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Main category: cs.LG

TL;DR: 本文简化双曲神经网络关键操作，提升运行时间和性能，使双曲神经网络更适用于更多应用。


<details>
  <summary>Details</summary>
Motivation: 双曲神经网络虽能低失真嵌入复杂数据，但存在计算效率和高精度任务表现不佳的问题。

Method: 简化双曲神经网络中的关键操作。

Result: 在计算速度和预测精度上取得显著提升。

Conclusion: 简化的双曲操作使双曲神经网络更适用于更广泛的应用。

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [145] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Main category: cs.LG

TL;DR: 本文研究上下文老虎机中脱策略学习（OPL）问题，提出HyPeR方法，利用二级奖励应对部分观察奖励，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当奖励部分观察时，OPL有效性严重下降，仅依靠二级奖励学习策略效果不佳，因此研究利用二级奖励最大化目标奖励的OPL问题。

Method: 提出Hybrid Policy Optimization for Partially - Observed Reward (HyPeR)方法，结合部分观察的目标奖励和二级奖励进行有效OPL。

Result: 对合成数据和真实数据的实证评估表明，HyPeR在各种场景下优于现有方法。

Conclusion: HyPeR方法能有效利用二级奖励，在部分观察奖励的挑战性场景中实现有效OPL，同时兼顾两个目标对优化目标奖励也有好处。

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [146] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Main category: cs.LG

TL;DR: 本文使用无标记多光子显微镜（MPM）图像训练卷积神经网络（CNN）对免疫细胞类型进行分类，取得可靠结果，未来有望提升无标记MPM特异性用于体内内窥镜检查。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在其他光学成像技术中广泛用于预测特定目标注释，但很少用于MPM，希望通过深度学习提升无标记MPM的特异性。

Method: 使用一系列不同免疫细胞类型的无标记MPM图像数据集，训练卷积神经网络（CNN）基于无标记自发荧光（AF）输入对细胞类型进行分类。

Result: 低复杂度的SqueezeNet架构在混合样本二分类和分离样本六分类中取得可靠的免疫细胞分类结果，扰动测试证实模型不受细胞外环境干扰，两个输入AF通道对分类同等重要。

Conclusion: 此类预测性深度学习模型未来可直接在未染色图像中检测特定免疫细胞，提升无标记MPM的特异性，在体内内窥镜检查中有巨大潜力。

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [147] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Main category: cs.LG

TL;DR: 研究数据集蒸馏中记忆信息的传递，发现学生模型用教师软标签可在未观测记忆数据上获非平凡准确率，且与温度相关。


<details>
  <summary>Details</summary>
Motivation: 了解现代神经网络在数据集蒸馏中记忆信息的传递情况，此前这方面认知不足。

Method: 考虑有限随机独立同分布数据集，排除泛化可能；研究不同温度、网络容量、架构和数据集组成下的情况。

Result: 学生模型在未直接观测的记忆数据上能获得非平凡准确率，在某些情况下可达完美准确率；足够软标签可使学生模型匹配教师模型预测；现象与温度密切相关。

Conclusion: 数据集蒸馏中记忆信息可传递，且现象与温度有关，在不同网络和数据设置下持续存在。

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [148] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Main category: cs.LG

TL;DR: 文章提出堆叠集成学习方法提高专业人士抑郁症分类预测准确性，实验结果表明模型性能高，凸显集成学习在心理健康分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法难以应对抑郁症预测的复杂性，需要开发准确且通用的预测模型。

Method: 提出基于堆叠的集成学习方法，集成多个基础学习器与逻辑回归中介模型，使用从Kaggle收集的抑郁症专业数据集。

Result: 模型在训练数据上准确率达99.64%，测试数据上达98.75%，精确率、召回率和F1分数均超98%。

Conclusion: 集成学习在心理健康分析中有效，有用于早期检测和干预策略的潜力。

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [149] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Main category: cs.LG

TL;DR: 本文揭示零阶优化（ZOO）与单步策略优化（PO）的联系，提出新算法ZoAR，理论和实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有流行ZOO方法的潜在机制及与强化学习等优化范式的联系未完全阐明。

Method: 建立ZOO与单步PO的联系，提出结合PO启发的方差减少技术的新算法ZoAR。

Result: 理论分析表明新算法能减少方差、增强收敛，实证研究显示其在收敛速度和最终性能上显著优于其他方法。

Conclusion: 为理解ZOO提供新理论视角，基于与PO的联系提供实用算法改进。

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [150] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Main category: cs.LG

TL;DR: 研究指出超网络架构能结合外部因素提升全球电力消耗预测模型准确性，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测结合外部因素虽能提升家庭层面模型准确性，但会降低全局预测模型性能，需解决此问题。

Method: 收集两年内6000多个卢森堡家庭的用电数据及对应外部因素数据，比较各种预测模型。

Result: 超网络方法在结合外部因素时优于现有方法，减少预测误差，实现最高准确性并保留全局模型优势。

Conclusion: 超网络架构可有效利用外部因素，通过为每个消费者调整模型权重，提升全球电力消耗预测模型的准确性。

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [151] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: 提出FAMR框架用于深度图像分类器事后遗忘，理论分析关联其解与基于影响函数的再训练近似，实验验证有效性，可扩展用于概念和风格擦除。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统依赖受隐私法规约束的数据，从训练模型中选择性遗忘特定信息很重要，在图像分类中需移除特定训练样本等影响。

Method: 引入FAMR框架，将遗忘建模为约束优化问题，最小化遗忘集上的均匀预测损失，并通过l2惩罚使模型参数接近原值。

Result: 在CIFAR - 10和ImageNet - 100的类遗忘任务中表现有效，保留性能强且计算开销小，可自然推广到概念和风格擦除。

Conclusion: FAMR为视觉模型提供了可扩展且可验证的高效事后遗忘途径。

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [152] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Main category: cs.LG

TL;DR: 通过工业AI用例研究揭示当前AI研究方法不足，鼓励研究者评估方法。


<details>
  <summary>Details</summary>
Motivation: 探讨当前AI研究方法是否足以创建成功、高效且盈利的AI应用。

Method: 对自动光学检测中的误报减少用例进行案例研究，识别相关同行评审工作中的七个弱点并进行实验。

Result: 最佳实践方法在此用例中会失败。

Conclusion: 强调需求感知指标、明确成功标准和分析实验数据集时间动态的必要性，鼓励研究者评估方法。

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [153] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Main category: cs.LG

TL;DR: 本文提出LLMNet，借助大语言模型实现图神经网络（GNN）的自动化设计，在多个数据集和任务上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 有效网络决策依赖图结构数据学习，GNN配置和调优费力，需自动化设计方法。

Method: 开发一组构建图相关知识库的代理，利用检索增强生成（RAG），通过知识引导的进化过程支持GNN模型的自动配置和优化。

Result: LLMNet在三个图学习任务的十二个数据集上表现出色。

Conclusion: LLMNet在GNN模型设计方面有效。

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [154] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 本文提出针对临床场景校准阈值分类器的评估框架，改进交叉熵评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有评分规则不能充分反映临床关键优先级，如校准性、对分布偏移的鲁棒性和对非对称错误成本的敏感性。

Method: 基于适当评分规则理论，特别是Schervish表示，推导交叉熵的调整变体，对临床相关类平衡范围的成本加权性能进行平均。

Result: 得到的评估方法易于应用，对临床部署条件敏感。

Conclusion: 该评估框架能优先选择校准且对现实世界变化鲁棒的模型。

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [155] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Main category: cs.LG

TL;DR: 提出高斯过程动态混合模型（GPDMM）用于人类运动数据单样本学习，与其他模型对比评测。


<details>
  <summary>Details</summary>
Motivation: 解决数据有限且需要模型可解释性的人类运动建模问题，如假肢控制等医疗应用。

Method: 将多个高斯过程动态模型（GPDM）结合在概率专家混合框架中，利用嵌入几何特征编码不同序列。

Result: 对GPDMM进行分类准确性和生成能力评分，展示模型变体，并与LSTM、VAE和transformer进行基准测试。

Conclusion: GPDMM在单样本学习的人类运动数据建模中有实用价值，尤其适用于数据有限和需模型可解释性的场景。

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [156] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 本文提出在DPO中引入细粒度标记级奖励的方法，实验显示该方法比DPO有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在DPO中利用标记级奖励作为指导具有挑战性，希望解决该问题以提升DPO性能。

Method: 将序列级PPO分解为一系列标记级近端策略优化问题，推导出闭式最优标记级策略和对应奖励，结合Bradley - Terry模型建立DPO损失函数框架，并提出实用奖励指导。

Result: 方法比DPO有显著性能提升，在MT - Bench、AlpacaEval 2和Arena - Hard上胜率分别提高达7.5、6.2和4.3个百分点。

Conclusion: 提出的方法有效，能在DPO中利用标记级奖励提升性能。

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [157] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Main category: cs.LG

TL;DR: 引入NAL架构结合ABA与深度学习用于图像分析，实验表明该架构有竞争力


<details>
  <summary>Details</summary>
Motivation: 随着对深度学习依赖增加，其安全、可靠和可解释性受关注，需新架构解决

Method: 引入NAL架构，含神经和符号组件，前者用对象中心学习处理图像，后者用ABA学习构建框架

Result: 在合成数据实验中，NAL架构可与最先进替代方案竞争

Conclusion: NAL架构在图像分析中有一定优势，有应用潜力

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [158] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: 本文指出样本嵌入语义分布不平衡会导致虚假语义关联，提出SCISSOR方法解决快捷学习问题，在多个任务和模型上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究将快捷学习归因于表面特征偏差，本文认为样本嵌入语义分布不平衡会导致虚假语义关联，影响模型鲁棒性，需解决该问题。

Method: 提出基于孪生网络的去偏方法SCISSOR，通过抑制被用作快捷方式的潜在聚类来重新映射语义空间，无需数据增强和重写。

Result: 在4个基准测试的6个模型上评估，相比基线，在不同任务上F1分数有显著提升，对轻量级模型也有较大优势。

Conclusion: 通过解决被忽视的语义偏差问题，重新定义了模型泛化格局，SCISSOR可作为缓解快捷学习、促进更鲁棒、抗偏差AI系统的基础框架。

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [159] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Main category: cs.LG

TL;DR: 提出使用基于分数的先验分布训练基于似然的分布匹配新方法，在多任务表现优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有非参数、对抗性和基于似然的分布匹配方法存在可扩展性、不稳定性、模式崩溃、引入不必要偏差和训练困难等问题。

Method: 引入使用基于分数的先验分布训练基于似然的分布匹配新方法，通过去噪分数匹配训练先验。

Result: 消除固定先验偏差，能更好利用保几何正则化，避免学习显式先验密度模型，稳定性和计算效率更好，多任务实验表现优。

Conclusion: 基于分数的方法是稳定有效的分布匹配方法。

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [160] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Main category: cs.LG

TL;DR: 提出可行性驱动的信赖域贝叶斯优化算法FuRBO，通过在基准测试上与基线对比，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法在处理有昂贵约束、可行区域小且难识别的优化任务时，花费大量预算寻找首个可行解，效率受限。

Method: 提出FuRBO算法，迭代定义信赖域，利用目标和约束代理模型信息选择下一个候选解，采用自适应策略调整信赖域。

Result: 在BBOB - 约束COCO基准套件和其他物理启发基准上进行广泛测试，与最先进的基线对比。

Conclusion: FuRBO能快速聚焦搜索，持续加速可行且高质量解的发现，具有有效性。

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [161] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 现有视觉反事实解释器存在不足，本文提出新的反事实生成机制并结合成SCE算法，通过评估证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉反事实解释器过于关注样本质量或变化最小化，未考虑解释的整体性需求，如保真度、可理解性和充分性。

Method: 探索新的反事实生成机制，将其组合成SCE算法。

Result: 通过对合成数据和真实数据的系统评估，证明了SCE算法的有效性。

Conclusion: 新的反事实生成机制和SCE算法有助于满足解释的整体性需求。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [162] [NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning](https://arxiv.org/abs/2506.14138)
*Ashish Gautam,Prasanna Date,Shruti Kulkarni,Robert Patton,Thomas Potok*

Main category: cs.NE

TL;DR: 介绍了基于FPGA的SNN模拟器NeuroCoreX，支持全连接，有生物启发学习机制，提供UART接口和Python界面，开源以加速研究。


<details>
  <summary>Details</summary>
Motivation: 为了灵活共同设计和测试SNN，推动节能、受生物启发的计算研究。

Method: 设计NeuroCoreX模拟器，支持全连接，采用基于STDP的学习机制，实现LIF神经元模型，提供UART接口和Python界面。

Result: 开发出NeuroCoreX模拟器并开源。

Conclusion: NeuroCoreX可加速节能、受生物启发计算的研发。

Abstract: Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.

</details>


### [163] [Is Selection All You Need in Differential Evolution?](https://arxiv.org/abs/2506.14425)
*Tomofumi Kitamura,Alex Fukunaga*

Main category: cs.NE

TL;DR: 提出无界差分进化（UDE）框架，无需基于适应度丢弃个体，消除代际替换，简化搜索算法。


<details>
  <summary>Details</summary>
Motivation: 现代差分进化（DE）实现存在因固定种群大小导致种群多样性有限的问题，存档机制引入额外设计考量。

Method: 提出UDE框架，将所有生成的候选个体加入种群，不进行代际替换，仅依靠选择机制。

Result: UDE是一种全新的DE方法，消除了存档管理和动态种群大小的复杂性。

Conclusion: UDE提供了一种更直接强大的搜索算法。

Abstract: Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.

</details>


### [164] [A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks](https://arxiv.org/abs/2506.14464)
*Maximilian Baronig,Yeganeh Bahariasl,Ozan Özdenizci,Robert Legenstein*

Main category: cs.NE

TL;DR: 本文提出HYPR算法结合并行效率与近似在线前向学习，应用于特定神经元模型训练效果好，缩小与BPTT性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有RSNN训练方法BPTT无法在线训练且内存消耗大，前向传播梯度学习方法执行慢、性能差，需改进。

Method: 提出HYbrid PRopagation (HYPR)算法，结合并行效率与近似在线前向学习，对RSNN子序列参数更新计算进行并行化。

Result: 该算法实现高吞吐量在线学习，内存需求恒定，应用于具有振荡亚阈值动力学的尖峰神经元网络训练效果好，与BPTT性能差距小。

Conclusion: HYPR算法有效，能解决现有训练方法问题，对特定神经元模型训练优势明显。

Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [165] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Main category: cs.SE

TL;DR: 本文提出开源的基于代理的框架，结合大语言模型和HL7 FHIR数据处理电子病历，用合成数据评估，优于传统方法，支持多FHIR格式，推动个性化数字健康。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康中增强临床决策支持、减轻文档负担和提高患者健康素养的挑战。

Method: 提出基于代理的框架，通过MCP集成大语言模型与HL7 FHIR数据，利用JSON配置访问FHIR资源，用SMART Health IT sandbox的合成EHR数据评估。

Result: 框架可实现电子病历实时总结、解释和个性化沟通，能提供可扩展、可解释和可互操作的AI电子病历应用。

Conclusion: 代理架构支持多种FHIR格式，为推进个性化数字健康解决方案奠定基础。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.

</details>


### [166] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Main category: cs.SE

TL;DR: 本文引入指令和解决方案概率作为额外启发式方法扩展指令子集（IS）方法，可大幅缩小归纳编程（IP）搜索空间，结合IS能有超100个数量级的缩减，且启发式方法对未见代码也有效。


<details>
  <summary>Details</summary>
Motivation: 扩展IS方法以进一步缩小IP搜索空间。

Method: 引入指令概率（基于大代码样本中指令出现频率）和解决方案概率（部分或完整程序中所有组成指令概率之积），用代码样本中不同大小程序单元观察到的最小解决方案概率作为阈值来修剪搜索空间。测试两种指令概率公式。

Result: 两种变体都能进一步大幅缩小IP搜索空间，最多达数十个数量级，结合IS能有超100个数量级的缩减。交叉验证表明启发式方法对未见代码有效。

Conclusion: 新方法能有效缩小IP搜索空间，可在未来进一步研究。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.

</details>


### [167] [Signal-First Architectures: Rethinking Front-End Reactivity](https://arxiv.org/abs/2506.13815)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.SE

TL;DR: 论文针对现代前端框架的响应式管理挑战，提出Signal - First架构，通过对比分析量化其优势。


<details>
  <summary>Details</summary>
Motivation: 解决现代前端框架中复杂可观察链导致的性能下降和不可预测重渲染等响应式管理挑战。

Method: 提出Signal - First架构，采用对比分析，通过Chrome DevTools性能跟踪、内存堆快照和Lighthouse审计等进行受控基准测试。

Result: 通过研究量化了Signal - First架构相对于RxJS服务和NgRx全局存储等模式的优势。

Conclusion: Signal - First架构能确保确定性行为，消除隐式订阅并优化响应式图评估。

Abstract: Modern front-end frameworks face escalating reactivity management challenges, including performance degradation from complex observable chains and unpredictable re-renders. This paper introduces Signal-First Architecture--a novel paradigm where granular, dependency-tracked signals are the atomic unit of reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces reactive flows from explicit signal declarations, with derived values via computed() and side effects scoped to effect(). This model ensures deterministic behavior by eliminating implicit subscriptions and optimizing reactive graph evaluation.
  We present a comparative analysis of three Angular reactivity models: RxJS service-based, NgRx global stores, and pure Signal-First implementations. Through controlled benchmarking, including Chrome DevTools performance tracing, memory heap snapshots, and Lighthouse audits, this study quantifies Signal-First advantages.

</details>


### [168] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Main category: cs.SE

TL;DR: 本文提出基于大语言模型的结构化归纳编程方法，成功解决IPARC挑战中的任务，并揭示了基于大语言模型代码生成的见解及人机协作机制。


<details>
  <summary>Details</summary>
Motivation: IPARC挑战中的600个任务难以通过自动化方案解决，需要新方法应对。

Method: 采用基于大语言模型的结构化归纳编程方法。

Result: 成功解决IPARC所有类别任务，揭示大语言模型代码生成的相关见解。

Conclusion: 研究结果为复杂程序合成中的人机协作提供了有价值的机制。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.

</details>


### [169] [Role, cost, and complexity of software in the real-world: a case for formal methods](https://arxiv.org/abs/2506.13821)
*Giovanni Bernardi,Adrian Francalanza,Marco Peressotti,Mohammad Reza Mousavi*

Main category: cs.SE

TL;DR: 概述软件在现代社会的作用及低质量软件成本，说明研究形式化软件验证和程序分析的合理性


<details>
  <summary>Details</summary>
Motivation: 揭示低质量软件带来的高昂成本，强调研究软件验证和程序分析的必要性

Method: 回顾过去40年主要软件故障的成本，并结合成功的工业经验

Result: 提出研究和应用形式化软件验证及程序分析的观点

Conclusion: 软件故障成本高昂，研究和应用形式化软件验证及程序分析是合理的

Abstract: In this chapter we outline the role that software has in modern society, along with the staggering costs of poor software quality. To lay this bare, we recall the costs of some of the major software failures that happened during the last~$40$ years. We argue that these costs justify researching, studying and applying formal software verification and in particular program analysis. This position is supported by successful industrial experiences.

</details>


### [170] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Main category: cs.SE

TL;DR: 文章提出多库调试基准MLDebugging，评估大语言模型在多库Python代码调试能力，发现当前模型仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代码调试研究多关注简单无库或单库场景，忽略复杂多库现实应用情况。

Method: 引入涵盖126个Python库、七种多库代码问题类型的MLDebugging基准，并使用主流开源和闭源大语言模型进行评估。

Result: 当前大语言模型在多库场景下难以正确进行代码调试。

Conclusion: 该工作有望挖掘大语言模型在多库调试场景潜力，为后续研究提供见解。

Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.

</details>


### [171] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Main category: cs.SE

TL;DR: 提出FrontendBench基准评估前端代码生成能力，含自动评估框架，展示模型性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有前端代码生成基准存在任务简单、测试不严谨等问题，阻碍准确评估模型性能。

Method: 人类与大语言模型共同开发FrontendBench，按功能分类任务，引入交互测试场景；构建自动评估框架在沙盒环境评估。

Result: 自动评估框架与专家评估一致性达90.54%；不同模型在FrontendBench上处理前端任务性能差异大。

Conclusion: FrontendBench是可靠可扩展的基准，为前端代码生成研究提供基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.

</details>


### [172] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文对大语言模型在代码任务中的代码推理技术进行研究，涵盖调查、分类、性能评估、解释技术及指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务取得进展，但其在软件工程实际部署的实用性近期才被研究，本文聚焦代码推理技术以探究完成相关任务的能力和范式。

Method: 进行首次专门的代码推理调查，构建技术分类，评估常见基准性能，探索新基准，研究用代码核心属性解释推理技术。

Result: 完成代码推理调查，构建技术分类，评估性能，展示新基准，探索解释技术。

Conclusion: 指出当前研究存在的差距和未来潜在的研究方向。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.

</details>


### [173] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Main category: cs.SE

TL;DR: 本文分析函数调用错误类型，引入用于工具学习的CRITICTOOL基准，验证其策略有效性并分析大模型工具反思能力。


<details>
  <summary>Details</summary>
Motivation: 随着任务变复杂，工具使用易出错，如何有效处理这些错误是工具学习的关键研究方向。

Method: 先分析函数调用错误类型，基于新的数据集构建进化策略引入CRITICTOOL基准，在该基准上进行实验。

Result: 验证了构建基准策略的泛化性和有效性，对不同大模型的工具反思能力进行了深入分析。

Conclusion: 为大模型工具学习领域提供了新视角，代码开源。

Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [174] [Characterising Bugs in Jupyter Platform](https://arxiv.org/abs/2506.14055)
*Yutian Tang,Hongchen Cao,Yuxi Chen,David Lo*

Main category: cs.SE

TL;DR: 本文调查Jupyter平台387个bug，分类原因与症状，给出14个主要发现并指出检测和修复bug的新方向。


<details>
  <summary>Details</summary>
Motivation: Jupyter广泛使用，理解其平台的bug对确保正确性、安全性和鲁棒性至关重要，且此前未研究其平台bug。

Method: 调查Jupyter平台387个bug，对其进行分类。

Result: 将Jupyter bug分为11种根本原因和11种症状，为开发者确定14个主要发现。

Conclusion: 本研究为构建检测和修复Jupyter平台bug的工具开辟新方向。

Abstract: As a representative literate programming platform, Jupyter is widely adopted by developers, data analysts, and researchers for replication, data sharing, documentation, interactive data visualization, and more. Understanding the bugs in the Jupyter platform is essential for ensuring its correctness, security, and robustness. Previous studies focused on code reuse, restoration, and repair execution environment for Jupyter notebooks. However, the bugs in Jupyter notebooks' hosting platform Jupyter are not investigated. In this paper, we investigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified into 11 root causes and 11 bug symptoms. We identify 14 major findings for developers. More importantly, our study opens new directions in building tools for detecting and fixing bugs in the Jupyter platform.

</details>


### [175] [A Quantum Annealing Approach for Solving Optimal Feature Selection and Next Release Problems](https://arxiv.org/abs/2506.14129)
*Shuchang Wang,Xiaopeng Qiu,Yingxing Xue,Yanfu Li,Wei Yang*

Main category: cs.SE

TL;DR: 本文引入量子退火解决多目标搜索式软件工程问题，提出两种算法，经实验验证有减少执行时间和提高计算效率的优势，凸显量子退火潜力。


<details>
  <summary>Details</summary>
Motivation: 传统启发式和整数线性规划方法在大规模实例中的可扩展性未知，需新方法解决多目标搜索式软件工程问题。

Method: 引入量子退火作为子程序，针对不同问题规模提出两种算法，小问题将多目标优化转化为单目标优化，大问题采用基于最大能量影响的分解策略并结合最速下降法。

Result: 与ε - 约束法相比，执行时间显著减少；与NSGA - II相比，能提供更多非支配解且计算效率更高。

Conclusion: 量子退火在推进搜索式软件工程挑战的可扩展和高效解决方案方面具有潜力。

Abstract: Search-based software engineering (SBSE) addresses critical optimization challenges in software engineering, including the next release problem (NRP) and feature selection problem (FSP). While traditional heuristic approaches and integer linear programming (ILP) methods have demonstrated efficacy for small to medium-scale problems, their scalability to large-scale instances remains unknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling multi-objective SBSE problems, leveraging the computational potential of quantum systems. We propose two QA-based algorithms tailored to different problem scales. For small-scale problems, we reformulate multi-objective optimization (MOO) as single-objective optimization (SOO) using penalty-based mappings for quantum processing. For large-scale problems, we employ a decomposition strategy guided by maximum energy impact (MEI), integrating QA with a steepest descent method to enhance local search efficiency. Applied to NRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and the ILP-based $ε$-constraint method. Experimental results reveal that while our methods produce fewer non-dominated solutions than $ε$-constraint, they achieve significant reductions in execution time. Moreover, compared to NSGA-II, our methods deliver more non-dominated solutions with superior computational efficiency. These findings underscore the potential of QA in advancing scalable and efficient solutions for SBSE challenges.

</details>


### [176] [Mobile Application Review Summarization using Chain of Density Prompting](https://arxiv.org/abs/2506.14192)
*Shristi Shrestha,Anas Mahmoud*

Main category: cs.SE

TL;DR: 利用大语言模型（LLMs）和CoD提示总结移动应用评论，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 应用商店评论数量多导致信息过载，影响用户选择应用，需有效总结评论。

Method: 使用Chain of Density (CoD)提示引导OpenAI GPT - 4生成应用评论摘要，迭代提取源文本关键实体并融合成固定长度摘要。

Result: CoD提示聚焦应用特性后，能更好提取用户评论关键主题，生成适合终端用户的自然语言摘要，在增加语义密度同时保持可读性。

Conclusion: 提供了一种有效总结评论中重要用户反馈的机制，可改善移动应用用户体验。

Abstract: Mobile app users commonly rely on app store ratings and reviews to find apps that suit their needs. However, the sheer volume of reviews available on app stores can lead to information overload, thus impeding users' ability to make informed app selection decisions. To address this challenge, we leverage Large Language Models (LLMs) to summarize mobile app reviews. In particular, we use the Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate abstractive, semantically dense, and easily interpretable summaries of mobile app reviews. The CoD prompt is engineered to iteratively extract salient entities from the source text and fuse them into a fixed-length summary. We evaluate the performance of our approach using a large dataset of mobile app reviews. We further conduct an empirical evaluation with 48 study participants to assess the readability of the generated summaries. Our results demonstrate that adapting the CoD prompt to focus on app features improves its ability to extract key themes from user reviews and generate natural language summaries tailored for end-user consumption. The prompt also manages to maintain the readability of the generated summaries while increasing their semantic density. Our work in this paper aims to improve mobile app users' experience by providing an effective mechanism for summarizing important user feedback in the review stream.

</details>


### [177] [The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity, and Inclusion in Software Engineering](https://arxiv.org/abs/2506.14232)
*Sonja M. Hyrynsalmi,Mary Sanchez-Gordon,Anna Szlavi,Letizia Jaccheri*

Main category: cs.SE

TL;DR: 本文研究头部软件公司近年如何改变DEI策略，通过灰色文献研究分析10家公司，对其分类，发现公司应对反DEI浪潮有不同策略，并引入DEI宇宙地图展示趋势。


<details>
  <summary>Details</summary>
Motivation: 反DEI浪潮使许多公司重新评估DEI策略，而目前学术研究对此关注较少，所以开展研究。

Method: 进行灰色文献研究，分析10家头部软件公司DEI举措的现状。

Result: 根据公司对DEI承诺的转变进行分类，发现公司应对反DEI浪潮有减少、增加、重命名或维持DEI举措等策略。

Conclusion: 通过引入DEI宇宙地图展示软件行业DEI承诺和行动的趋势。

Abstract: Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top priority for leading software companies. However, in a short period, a wave of backlash has led many firms to re-assess their DEI strategies. Responding to this DEI backlash is crucial in academic research, especially because, currently, little scholarly research has been done on it. In this paper, therefore, we have set forth the following research question (RQ): "How have leading software companies changed their DEI strategies in recent years?" Given the novelty of the RQ and, consequently, the lack of scholarly research on it, we are conducting a grey literature study, examining the current state of DEI initiatives in 10 leading software companies. Based on our analysis, we have classified companies into categories based on their shift in commitment to DEI. We can identify that companies are indeed responding to the backlash by rethinking their strategy, either by reducing, increasing, or renaming their DEI initiatives. In contrast, some companies keep on with their DEI strategy, at least so far, despite the challenging political climate. To illustrate these changes, we introduce the DEI Universe Map, a visual representation of software industry trends in DEI commitment and actions.

</details>


### [178] [Designing a Custom Chaos Engineering Framework for Enhanced System Resilience at Softtech](https://arxiv.org/abs/2506.14281)
*Ethem Utku Aktas,Burak Tuzlutas,Burak Yesiltas*

Main category: cs.SE

TL;DR: 本文提出针对金融领域软件开发公司Softtech的定制化混沌工程框架设计方案，考虑金融监管，框架迭代可扩展。


<details>
  <summary>Details</summary>
Motivation: 为在金融领域软件开发公司Softtech引入混沌工程，增强软件弹性，同时考虑金融行业监管。

Method: 阐述引入混沌工程的基础概念和活动，构建迭代可扩展的框架。

Result: 无明确提及具体结果。

Conclusion: 无明确提及具体结论，旨在解决定制框架的相关问题。

Abstract: Chaos Engineering is a discipline which enhances software resilience by introducing faults to observe and improve system behavior intentionally. This paper presents a design proposal for a customized Chaos Engineering framework tailored for Softtech, a leading software development company serving the financial sector. It outlines foundational concepts and activities for introducing Chaos Engineering within Softtech, while considering financial sector regulations. Building on these principles, the framework aims to be iterative and scalable, enabling development teams to progressively improve their practices. The study addresses two primary questions: how Softtech's unique infrastructure, business priorities, and organizational context shape the customization of its Chaos Engineering framework and what key activities and components are necessary for creating an effective framework tailored to Softtech's needs.

</details>


### [179] [Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity Effects](https://arxiv.org/abs/2506.14290)
*Daniele La Prova,Emanuele Gentili,Davide Falessi*

Main category: cs.SE

TL;DR: 本文提出并评估了票务级预测（TLP）方法，以识别实现后会引入错误的票务，分析不同阶段TLP准确性及特征预测能力变化。


<details>
  <summary>Details</summary>
Motivation: 现有错误预测方法支持修复而非预防，本文旨在引入票务级预测方法以在代码编写前进行缺陷预测。

Method: 在票务的三个生命周期阶段（开放、进行中、已关闭）分析TLP，利用72个分属六个不同类别的特征，采用滑动窗口方法平衡特征选择和三个机器学习错误预测分类器。

Result: TLP准确性随票务接近关闭阶段而增加；不同阶段无单一特征类别占主导，开发者相关信号早期最有信息价值，代码和JIT指标接近关闭时占优，温度相关特征全程有补充价值。

Conclusion: 缺陷预测可有效提前，为风险感知的票务分类和开发者分配提供机会。

Abstract: The primary goal of bug prediction is to optimize testing efforts by focusing on software fragments, i.e., classes, methods, commits (JIT), or lines of code, most likely to be buggy. However, these predicted fragments already contain bugs. Thus, the current bug prediction approaches support fixing rather than prevention. The aim of this paper is to introduce and evaluate Ticket-Level Prediction (TLP), an approach to identify tickets that will introduce bugs once implemented. We analyze TLP at three temporal points, each point represents a ticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1) TLP accuracy increases as tickets progress towards the closed stage due to improved feature reliability over time, and (2) the predictive power of features changes across these temporal points. Our TLP approach leverages 72 features belonging to six different families: code, developer, external temperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our TLP evaluation uses a sliding-window approach, balancing feature selection and three machine-learning bug prediction classifiers on about 10,000 tickets of two Apache open-source projects. Our results show that TLP accuracy increases with proximity, confirming the expected trade-off between early prediction and accuracy. Regarding the prediction power of feature families, no single feature family dominates across stages; developer-centric signals are most informative early, whereas code and JIT metrics prevail near closure, and temperature-based features provide complementary value throughout. Our findings complement and extend the literature on bug prediction at the class, method, or commit level by showing that defect prediction can be effectively moved upstream, offering opportunities for risk-aware ticket triaging and developer assignment before any code is written.

</details>


### [180] [Quality Assessment of Python Tests Generated by Large Language Models](https://arxiv.org/abs/2506.14297)
*Victor Alves,Carla Bezerra,Ivan Machado,Larissa Rocha,Tássio Virgínio,Publio Silva*

Main category: cs.SE

TL;DR: 研究评估三个大语言模型生成Python测试代码质量，发现多数测试套件有错误或测试异味，提示上下文影响测试质量，不同模型表现有差异，为改进测试生成提供方向。


<details>
  <summary>Details</summary>
Motivation: 手动生成测试脚本耗时、成本高且易出错，大语言模型在该领域有潜力，需评估其生成测试代码的质量。

Method: 评估GPT - 4o、Amazon Q和LLama 3.3在Text2Code和Code2Code两种提示上下文下生成测试套件的结构可靠性，分析错误和测试异味并关联设计模式。

Result: 多数测试套件有错误或测试异味，断言错误最常见，提示上下文影响测试质量，不同模型在错误率和测试异味检测上表现不同，特定错误与测试用例内聚异味有关。

Conclusion: 有机会改进大语言模型测试生成质量，需开展未来研究探索优化生成场景和更好的提示工程策略。

Abstract: The manual generation of test scripts is a time-intensive, costly, and error-prone process, indicating the value of automated solutions. Large Language Models (LLMs) have shown great promise in this domain, leveraging their extensive knowledge to produce test code more efficiently. This study investigates the quality of Python test code generated by three LLMs: GPT-4o, Amazon Q, and LLama 3.3. We evaluate the structural reliability of test suites generated under two distinct prompt contexts: Text2Code (T2C) and Code2Code (C2C). Our analysis includes the identification of errors and test smells, with a focus on correlating these issues to inadequate design patterns. Our findings reveal that most test suites generated by the LLMs contained at least one error or test smell. Assertion errors were the most common, comprising 64% of all identified errors, while the test smell Lack of Cohesion of Test Cases was the most frequently detected (41%). Prompt context significantly influenced test quality; textual prompts with detailed instructions often yielded tests with fewer errors but a higher incidence of test smells. Among the evaluated LLMs, GPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C), whereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For test smells, Amazon Q had fewer detections in the C2C context (9%), while LLama 3.3 performed best in the T2C context (10%). Additionally, we observed a strong relationship between specific errors, such as assertion or indentation issues, and test case cohesion smells. These findings demonstrate opportunities for improving the quality of test generation by LLMs and highlight the need for future research to explore optimized generation scenarios and better prompt engineering strategies.

</details>


### [181] [Agile and Student-Centred Teaching of Agile/Scrum Concepts](https://arxiv.org/abs/2506.14369)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 介绍软件工程管理课程设计与教学经验，含课程变化、评估挑战，分享经验并分析有效课程结构与在线评估结构。


<details>
  <summary>Details</summary>
Motivation: 使课程教学更以学生为中心、灵活，适应无期末面考情况，应对学生评估挑战。

Method: 总结设计和教学该课程的经验。

Result: 得到课程教学、评估方面的经验教训。

Conclusion: 经验可为本科生和研究生教授敏捷/Scrum概念提供见解，分析了有效课程结构和在线评估结构。

Abstract: In this paper, we discuss our experience in designing and teaching a course on Software Engineering Project Management, where the focus is on Agile/Scrum development and Requirement Engineering activities. The course has undergone fundamental changes since 2020 to make the teaching approach more student-centred and flexible. As many universities abandoned having face-to-face exams at the end of the semester, authentic assessments now play an even more important role than before. This makes assessment of students' work even more challenging, especially if we are dealing with large cohorts of students. The complexity is not only in dealing with diversity in the student cohorts when elaborating the assessment tasks, but also in being able to provide feedback and marks in a timely and fairly. We report our lessons learned, which might provide useful insights for teaching Agile/Scrum concepts to undergraduate and postgraduate students. We also analyse what course structure might be effective to support a blended learning approach, as well as what could be a reasonable structure of online assessments, to keep them both authentic and scalable for large cohorts of students.

</details>


### [182] [Defining the Game Producer: A Mapping of Key Characteristics and Differentiators of the Professional Behind Digital Game Production](https://arxiv.org/abs/2506.14409)
*Rafael C. Lopes,Danilo M. Ribeiro*

Main category: cs.SE

TL;DR: 研究确定数字游戏制作人的特征、技能和能力，为专业培训等提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着数字游戏复杂度增加，明确游戏制作人角色在创意、技术和商业维度的作用，识别并描绘数字游戏制作人的主要特征、技能和能力。

Method: 对11人进行半结构化访谈，用扎根理论分析构建基于专业实践的类别。

Result: 得出游戏制作人必备的个人特征、实践技能和战略能力，沟通、适应性和项目管理是核心要素。

Conclusion: 研究结果为专业培训、招聘策略和游戏开发领导力研究提供基础。

Abstract: Introduction: As digital games grow in complexity, the role of the Game Producer becomes increasingly relevant for aligning creative, technical, and business dimensions. Objective: This study aimed to identify and map the main characteristics, skills, and competencies that define the Digital Game Producer profile. Methodology: A qualitative investigation was conducted with 11 semi-structured interviews, analyzed through Grounded Theory to build categories grounded in professional practice. Results: The study produced a structured set of personal characteristics, practical skills, and strategic competencies considered essential for Game Producers. Communication, adaptability, and project management emerged as central elements across the sample. Conclusion: The resulting model offers a foundation for professional training, recruitment strategies, and future research on leadership roles in game development.

</details>


### [183] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 提出用大语言模型重构Qiskit代码的方法，通过提取迁移场景分类，结合代码输入LLM，解决上下文长度限制问题，结果表明LLM可有效辅助Qiskit代码迁移。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件框架发展，开发者在维护与快速变化的API兼容性方面面临挑战。

Method: 从Qiskit官方文档提取迁移场景分类，将分类和原Python代码输入LLM，识别代码中迁移场景并提出重构方案，以有效方式解决LLM上下文长度限制。

Result: 当有特定领域迁移知识引导时，LLM能有效辅助自动化Qiskit代码迁移。

Conclusion: 提供了Qiskit代码从早期版本迁移到0.46版本的有效提示和分类，以及评估LLM辅助量子代码迁移能力的方法。

Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.

</details>


### [184] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Main category: cs.SE

TL;DR: 欧盟资助的Climaborough项目助力欧洲城市2030年实现碳中和，开发平台监测进度，论文介绍项目采用低代码/无代码策略部署气候仪表盘。


<details>
  <summary>Details</summary>
Motivation: 为响应项目快速部署气候仪表盘的目标。

Method: 采用低代码策略加速仪表盘开发，仪表盘嵌入无代码理念，让不同公民配置和调整。

Result: 无明确提及具体结果。

Conclusion: 无明确提及具体结论。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [185] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 文档是94篇论文总结，含软件需求可追溯性等章节，介绍与AACS 2025和SAIV 2025差异及后两者情况。


<details>
  <summary>Details</summary>
Motivation: 未明确提及，推测是对相关研究进行总结梳理并展示不同论文情况。

Method: 未提及具体方法。

Result: 给出94篇论文总结，说明了与类似标题论文的差异及AACS 2025和SAIV 2025的提交、评审等情况。

Conclusion: 未提及明确结论。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.

</details>


### [186] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Main category: cs.SE

TL;DR: 本文回顾软件测试自动化中AI增强的研究，提出ai4st分类法并识别开放研究问题。


<details>
  <summary>Details</summary>
Motivation: 软件测试自动化设计、开发和维护成本高，AI在多领域的突破为软件测试带来新视角。

Method: 回顾从无自动化到全自动化的AI增强软件测试自动化研究，提出ai4st分类法。

Result: 提出了ai4st分类法用于分类近期研究。

Conclusion: 识别出了相关开放研究问题。

Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.

</details>


### [187] [Issue Retrieval and Verification Enhanced Supplementary Code Comment Generation](https://arxiv.org/abs/2506.14649)
*Yanzhen Zou,Xianlin Zhao,Xinglu Pan,Bing Xie*

Main category: cs.SE

TL;DR: 提出IsComment方法用于代码注释生成，减少幻觉问题，实验表明该方法优于LLMs和现有工作。


<details>
  <summary>Details</summary>
Motivation: 解决基于问题报告进行代码注释生成时，减少生成注释中幻觉问题的挑战。

Method: 通过代码 - 注释 - 问题分析确定五类代码补充信息，检索含此类信息的问题句子生成候选注释，过滤无关或不可验证的候选注释。

Result: 相比LLMs，IsComment提高了人工补充注释的覆盖率；相比现有工作，能生成更丰富有用的补充代码注释。

Conclusion: IsComment方法在代码注释生成方面表现良好，能为编程理解提供更可靠的补充注释。

Abstract: Issue reports have been recognized to contain rich information for retrieval-augmented code comment generation. However, how to minimize hallucinations in the generated comments remains significant challenges. In this paper, we propose IsComment, an issue-based LLM retrieval and verification approach for generating method's design rationale, usage directives, and so on as supplementary code comments. We first identify five main types of code supplementary information that issue reports can provide through code-comment-issue analysis. Next, we retrieve issue sentences containing these types of supplementary information and generate candidate code comments. To reduce hallucinations, we filter out those candidate comments that are irrelevant to the code or unverifiable by the issue report, making the code comment generation results more reliable. Our experiments indicate that compared with LLMs, IsComment increases the coverage of manual supplementary comments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and from 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can generate richer and more useful supplementary code comments for programming understanding, which is quantitatively evaluated through the MESIA metric on both methods with and without manual code comments.

</details>


### [188] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 文章围绕大语言模型技术发展下自动化编码展开，开发USEagent并构建USEbench评估，USEagent较现有通用代理效果更好但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理是否等同于AI软件工程师，构建能处理多能力的统一代理应对软件开发复杂场景。

Method: 开发统一软件工程代理USEagent，构建包含多种任务的统一软件工程基准USEbench进行评估。

Result: 在包含1271个仓库级软件工程任务的USEbench评估中，USEagent比现有通用代理如OpenHands CodeActAgent效果更好。

Conclusion: USEagent虽有效果提升，但在某些编码任务能力上存在差距，为未来AI软件工程师发展提供方向。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [189] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi,Agathe Fernandes Machado,Arthur Charpentier*

Main category: stat.ML

TL;DR: 本文从可解释性角度重新审视合作博弈理论，介绍超越Shapley值的有效分配方案，提供构建特征归因的蓝图。


<details>
  <summary>Details</summary>
Motivation: Shapley值相关方法基于公理的合理性在特征归因方面存在争议，需更广泛且有原则地使用合作博弈理论工具。

Method: 介绍Weber和Harsanyi集合这两类有效分配方案，阐明价值函数和聚合规则的区别，提出构建特征归因的三步蓝图。

Result: 提出了超越固定公理的框架，为XAI社区设计有意义且稳健的归因方法提供基础。

Conclusion: 能够为XAI社区提供一个连贯框架，设计出有意义且能适应方法趋势变化的归因方法。

Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.

</details>


### [190] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

Main category: stat.ML

TL;DR: 多数监督机器学习文献假设训练数据独立同分布，但现实中该假设常不成立。本文考虑非不可约或非周期的迭代随机函数生成的训练数据，建立样本误差一致收敛结果，证明算法可学习性并推导依赖数据分布的学习率界。


<details>
  <summary>Details</summary>
Motivation: 现有文献中独立同分布假设在现实问题中常不成立，且学习率与数据分布无关会导致假设类选择受限和样本复杂度欠佳。

Method: 假设控制函数对第一个参数有收缩性且假设类满足一定正则条件，先建立样本误差一致收敛结果，再证明近似经验风险最小化算法的可学习性。

Result: 得到依赖数据分布的样本误差一致收敛结果和学习率界，以拉德马赫复杂度表示。

Conclusion: 本文得到的学习率界依赖数据分布，能更准确反映数据生成分布的特性。

Abstract: Most existing literature on supervised machine learning assumes that the training dataset is drawn from an i.i.d. sample. However, many real-world problems exhibit temporal dependence and strong correlations between the marginal distributions of the data-generating process, suggesting that the i.i.d. assumption is often unrealistic. In such cases, models naturally include time-series processes with mixing properties, as well as irreducible and aperiodic ergodic Markov chains. Moreover, the learning rates typically obtained in these settings are independent of the data distribution, which can lead to restrictive choices of hypothesis classes and suboptimal sample complexities for the learning algorithm. In this article, we consider the case where the training dataset is generated by an iterated random function (i.e., an iteratively defined time-homogeneous Markov chain) that is not necessarily irreducible or aperiodic. Under the assumption that the governing function is contractive with respect to its first argument and subject to certain regularity conditions on the hypothesis class, we first establish a uniform convergence result for the corresponding sample error. We then demonstrate the learnability of the approximate empirical risk minimization algorithm and derive its learning rate bound. Both rates are data-distribution dependent, expressed in terms of the Rademacher complexities of the underlying hypothesis class, allowing them to more accurately reflect the properties of the data-generating distribution.

</details>


### [191] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte,David Rügamer,Thomas Nagler*

Main category: stat.ML

TL;DR: 研究利用预训练神经网络的潜在特征在ATE估计中调整混杂因素，分析潜在特征学习挑战，指出神经网络可解决相关问题。


<details>
  <summary>Details</summary>
Motivation: 将ATE估计扩展到非表格数据，避免忽略混杂因素导致结果偏差和错误结论。

Method: 研究利用预训练神经网络潜在特征调整混杂因素，以双机器学习为例，分析潜在特征学习和下游参数估计挑战。

Result: 明确潜在特征在ATE估计中实现有效调整和统计推断的条件，指出传统线性模型假设对潜在特征不现实，神经网络能适应学习问题的稀疏性和维度实现快速收敛。

Conclusion: 神经网络在处理非表格数据的ATE估计中，对潜在特征学习和下游参数估计中高维、不可识别性等问题不敏感，可实现快速收敛。

Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.

</details>


### [192] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

Main category: stat.ML

TL;DR: 本文针对人口统计学平等约束下的回归问题，给出可验证回归算法公平极小极大最优性的元定理，还表明可通过后处理方法实现公平极小极大最优回归。


<details>
  <summary>Details</summary>
Motivation: 现有公平极小极大最优回归算法分析与特定数据生成模型紧密耦合，需要更通用的方法。

Method: 提出可应用于各种情况的元定理，采用后处理方法实现公平极小极大最优回归。

Result: 得到能验证对应回归算法公平极小极大最优性的元定理，证明可通过后处理实现公平极小极大最优回归。

Conclusion: 研究者和实践者可专注于改进传统回归技术以实现公平回归。

Abstract: We address the regression problem under the constraint of demographic parity, a commonly used fairness definition. Recent studies have revealed fair minimax optimal regression algorithms, the most accurate algorithms that adhere to the fairness constraint. However, these analyses are tightly coupled with specific data generation models. In this paper, we provide meta-theorems that can be applied to various situations to validate the fair minimax optimality of the corresponding regression algorithms. Furthermore, we demonstrate that fair minimax optimal regression can be achieved through post-processing methods, allowing researchers and practitioners to focus on improving conventional regression techniques, which can then be efficiently adapted for fair regression.

</details>


### [193] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau,Tian-Yi Zhou,Xiangchi Yuan,Jizhou Chen,Wenke Lee,Xiaoming Huo*

Main category: stat.ML

TL;DR: 文章将训练分类器区分正常数据与合成异常值的原则拓展到半监督异常检测，提出结合已知和合成异常值的框架，给出数学公式并在多个基准测试中验证效果。


<details>
  <summary>Details</summary>
Motivation: 在半监督异常检测场景中，训练数据包含有限标记的异常子集，需将区分正常与异常值的原则拓展到该场景。

Method: 提出结合已知和合成异常值的半监督异常检测框架，引入半监督异常检测的数学公式。

Result: 在五个不同基准测试中验证了框架的有效性，性能有一致提升，且合成异常值原则对其他基于分类的异常检测方法也有推广性。

Conclusion: 合成异常值原则在半监督异常检测中有效，具有理论依据和良好的泛化性。

Abstract: Anomaly detection (AD) is a critical task across domains such as cybersecurity and healthcare. In the unsupervised setting, an effective and theoretically-grounded principle is to train classifiers to distinguish normal data from (synthetic) anomalies. We extend this principle to semi-supervised AD, where training data also include a limited labeled subset of anomalies possibly present in test time. We propose a theoretically-grounded and empirically effective framework for semi-supervised AD that combines known and synthetic anomalies during training. To analyze semi-supervised AD, we introduce the first mathematical formulation of semi-supervised AD, which generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i) better anomaly modeling in low-density regions and (ii) optimal convergence guarantees for neural network classifiers -- the first theoretical result for semi-supervised AD. We empirically validate our framework on five diverse benchmarks, observing consistent performance gains. These improvements also extend beyond our theoretical framework to other classification-based AD methods, validating the generalizability of the synthetic anomaly principle in AD.

</details>


### [194] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios,Tin Sum Cheng,Aurelien Lucchi,Haitz Sáez de Ocáriz Borde*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.

</details>


### [195] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

Main category: stat.ML

TL;DR: 本文开发了一类广泛的镜像下降（MD）算法，通过使用Tempesta多参数变形对数作为镜像函数，学习超参数调整算法以适应数据。


<details>
  <summary>Details</summary>
Motivation: 开发在机器学习中起关键作用的广泛的镜像下降（MD）算法。

Method: 构建约束优化问题，利用Bregman散度和Tempesta多参数变形对数作为镜像函数，估计广义指数函数推导MD更新，学习超参数来适应数据。

Result: 生成了一个新的广泛且灵活的MD和无镜像MD更新族。

Conclusion: 应用多参数对数的概念可产生新的MD和无镜像MD更新，能根据数据调整算法。

Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [196] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan,Jose Blanchet,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 提出评估极端数据中处理效应的新框架以捕获罕见事件因果效应，用多元正则变化理论建模极端情况，开发一致估计量并分析性能，用合成和半合成数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断文献主要处理常见事件的处理效应，对于罕见但有重大影响事件（如极端气候事件）的政策干预效果估计，标准因果推断方法不适用。

Method: 引入新框架，采用多元正则变化理论对极端情况建模，开发极端处理效应的一致估计量并进行严格非渐近性能分析。

Result: 开发了极端处理效应的一致估计量，通过合成和半合成数据说明了估计量的性能。

Conclusion: 所提出的新框架可用于评估极端数据中的处理效应，捕获罕见事件的因果效应。

Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [197] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke,Mingyue Xu*

Main category: stat.ML

TL;DR: 本文研究不可知设定下二元分类经验风险最小化（ERM）的通用学习问题，揭示了三种可能的不可知通用率并给出概念类分类的完整刻画，还建立了目标依赖和贝叶斯依赖通用率的完整刻画。


<details>
  <summary>Details</summary>
Motivation: 现有通用学习多数聚焦可实现情况，可实现假设在实际中过于严格，不可知情况研究不足，因此研究不可知设定下二元分类ERM的通用学习问题。

Method: 考虑不可知设定下二元分类ERM的通用学习问题，分析“学习曲线”随样本量增加时超额风险的衰减情况。

Result: 揭示了三种可能的不可知通用率，即$e^{-n}$、$o(n^{-1/2})$或任意慢，并给出概念类分类的完整刻画，还建立了目标依赖和贝叶斯依赖通用率的完整刻画。

Conclusion: 对不可知设定下二元分类ERM的通用学习有了更全面的认识，明确了不同概念类对应的通用率情况。

Abstract: The universal learning framework has been developed to obtain guarantees on the learning rates that hold for any fixed distribution, which can be much faster than the ones uniformly hold over all the distributions. Given that the Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory and ubiquitous in practical machine learning, the recent work of arXiv:2412.02810 studied the universal rates of ERM for binary classification under the realizable setting. However, the assumption of realizability is too restrictive to hold in practice. Indeed, the majority of the literature on universal learning has focused on the realizable case, leaving the non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for binary classification under the agnostic setting, where the ''learning curve" reflects the decay of the excess risk as the sample size increases. We explore the possibilities of agnostic universal rates and reveal a compact trichotomy: there are three possible agnostic universal rates of ERM, being either $e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete characterization of which concept classes fall into each of these categories. Moreover, we also establish complete characterizations for the target-dependent universal rates as well as the Bayes-dependent universal rates.

</details>


### [198] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

Main category: stat.ML

TL;DR: 本文为线性上下文老虎机提出近乎极小极大最优的汤普森采样方法，实证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 线性上下文老虎机中汤普森采样无法达到最优遗憾边界，需改进。

Method: 开发一种新的估计器，对假设样本进行自适应增强和耦合以实现高效参数学习。

Result: 所提估计器不依赖上下文分布假设就能准确预测所有臂的奖励，实证表现稳健，显著优于现有方法。

Conclusion: 提出的近乎极小极大最优的汤普森采样方法有效，性能良好。

Abstract: In linear contextual bandits, the objective is to select actions that maximize cumulative rewards, modeled as a linear function with unknown parameters. Although Thompson Sampling performs well empirically, it does not achieve optimal regret bounds. This paper proposes a nearly minimax optimal Thompson Sampling for linear contextual bandits by developing a novel estimator with the adaptive augmentation and coupling of the hypothetical samples that are designed for efficient parameter learning. The proposed estimator accurately predicts rewards for all arms without relying on assumptions for the context distribution. Empirical results show robust performance and significant improvement over existing methods.

</details>


### [199] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard,Andrea Paudice*

Main category: stat.ML

TL;DR: 分析MoM在数据分布仅存在前p (1<p≤2)阶矩时，同时估计函数类中各函数均值的性能，给出新样本复杂度界并应用于聚类和回归。


<details>
  <summary>Details</summary>
Motivation: 研究MoM在数据分布仅存在前p (1<p≤2)阶矩时，同时估计函数类中各函数均值的性能。

Method: 采用新颖的对称化技术证明新的样本复杂度界。

Result: 得到新的样本复杂度界，并将结果应用于无界输入的k均值聚类和一般损失的线性回归。

Conclusion: 改进了现有工作，为相关领域提供了更好的方法和结果。

Abstract: The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [200] [On Quantum BSDE Solver for High-Dimensional Parabolic PDEs](https://arxiv.org/abs/2506.14612)
*Howard Su,Huan-Hsin Tseng*

Main category: q-fin.MF

TL;DR: 提出用于近似高维抛物型偏微分方程解的量子机器学习框架，用纯变分量子电路（VQC）作求解器，在两个典型PDE上测试，VQC在多数情况表现更好。


<details>
  <summary>Details</summary>
Motivation: 找到解决高维抛物型偏微分方程解的有效方法，改进现有量子 - 经典网络混合方法。

Method: 采用纯变分量子电路作为核心求解器，通过时间离散化和蒙特卡罗模拟进行路径近似，采用基于模型的强化学习。

Result: 在两个典型PDE上，VQC比经典深度神经网络（DNN）求解器方差更低、精度更高，在高度非线性区域和价外期权方面更稳健。

Conclusion: VQC作为高维随机控制问题的求解器具有可扩展性和稳定性。

Abstract: We propose a quantum machine learning framework for approximating solutions to high-dimensional parabolic partial differential equations (PDEs) that can be reformulated as backward stochastic differential equations (BSDEs). In contrast to popular quantum-classical network hybrid approaches, this study employs the pure Variational Quantum Circuit (VQC) as the core solver without trainable classical neural networks. The quantum BSDE solver performs pathwise approximation via temporal discretization and Monte Carlo simulation, framed as model-based reinforcement learning. We benchmark VQCbased and classical deep neural network (DNN) solvers on two canonical PDEs as representatives: the Black-Scholes and nonlinear Hamilton-Jacobi-Bellman (HJB) equations. The VQC achieves lower variance and improved accuracy in most cases, particularly in highly nonlinear regimes and for out-of-themoney options, demonstrating greater robustness than DNNs. These results, obtained via quantum circuit simulation, highlight the potential of VQCs as scalable and stable solvers for highdimensional stochastic control problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [Consensus Power Inequality: A Comparative Study of Blockchain Networks](https://arxiv.org/abs/2506.14393)
*Kamil Tylinski,Abylay Satybaldy,Paolo Tasca*

Main category: cs.CR

TL;DR: 本文评估五大区块链网络共识权力不平等，发现不同网络权力分配差异，指出不平等驱动因素并建立评估框架，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 共识权力分配是区块链去中心化基石，研究旨在评估不同区块链网络共识权力不平等状况。

Method: 收集2022年1月至2024年7月五大区块链网络数据，用基尼系数和泰尔指数等经济指标定量评估权力分配。

Result: Hedera和比特币权力分配更均衡，以太坊和Cardano不平等程度中等，以太坊转PoS后更集中，Algorand权力高度集中。

Conclusion: 建立评估区块链共识权力不平等的方法框架，强调针对性策略对公平分配权力和增强去中心化系统可持续性的重要性。

Abstract: The distribution of consensus power is a cornerstone of decentralisation, influencing the security, resilience, and fairness of blockchain networks while ensuring equitable impact among participants. This study provides a rigorous evaluation of consensus power inequality across five prominent blockchain networks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data collected from January 2022 to July 2024. Leveraging established economic metrics, including the Gini coefficient and Theil index, the research quantitatively assesses how power is distributed among blockchain network participants. A robust dataset, capturing network-specific characteristics such as mining pools, staking patterns, and consensus nodes, forms the foundation of the analysis, enabling meaningful comparisons across diverse architectures. Through an in-depth comparative study, the paper identifies key disparities in consensus power distribution. Hedera and Bitcoin demonstrate more balanced power distribution, aligning closely with the principles of decentralisation. Ethereum and Cardano demonstrate moderate levels of inequality. However, contrary to expectations, Ethereum has become more concentrated following its transition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced centralisation of power. Moreover, the findings highlight the structural and operational drivers of inequality, including economic barriers, governance models, and network effects, offering actionable insights for more equitable network design. This study establishes a methodological framework for evaluating blockchain consensus power inequality, emphasising the importance of targeted strategies to ensure fairer power distribution and enhancing the sustainability of decentralised systems. Future research will build on these findings by integrating additional metrics and examining the influence of emerging consensus mechanisms.

</details>


### [202] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文指出推理大语言模型存在过度推理导致计算成本高的问题，提出新的损失框架攻击模型，实验表明能增加推理长度且攻击有可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有推理大语言模型存在过度推理致计算成本高的问题，且对抗输入可利用此行为大幅增加计算开销，因此开展研究。

Method: 提出由Priority Cross - Entropy Loss、Excessive Reasoning Loss和Delayed Termination Loss组成的损失框架，在特定数据集和模型上进行优化和评估。

Result: 推理长度提升3到9倍，效用性能相当，且制作的对抗输入对其他模型有可迁移性，能引发计算开销。

Conclusion: 所提出的损失框架攻击有效，能增加推理大语言模型的计算开销，且攻击具有可迁移性。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.

</details>


### [203] [LLM-Powered Intent-Based Categorization of Phishing Emails](https://arxiv.org/abs/2506.14337)
*Even Eilertsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 研究大语言模型检测钓鱼邮件的潜力，用自定义数据集实验，结果表明其有检测和分类能力。


<details>
  <summary>Details</summary>
Motivation: 传统检测系统聚焦用户不可见的邮件元数据，难以检测经验丰富用户可凭文本识别的钓鱼邮件，需探索新方法。

Method: 构建包含合法和钓鱼邮件的自定义数据集，让大语言模型根据邮件意图检测并进行意图类型分类。

Result: 现有大语言模型能够检测和分类钓鱼邮件。

Conclusion: 大语言模型在检测和分类钓鱼邮件领域有应用潜力。

Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.

</details>


### [204] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 论文揭示美国选举制表器使用机器学习分类器的安全风险，引入新数据集、测试多种模型，展示白盒攻击无效，用新方法克服问题并在物理世界开展攻击，证明低成功率攻击也可能影响选举结果。


<details>
  <summary>Details</summary>
Motivation: 分析选举制表器中机器学习模型的潜在安全漏洞，提升选举安全性。

Method: 引入四个新的选票数据集；在新数据集上训练和测试多种模型；用新数据集和训练好的模型验证白盒攻击无效，用改进损失函数克服问题；在物理世界用新方法生成对抗样本进行攻击。

Result: 传统白盒攻击在投票领域因梯度掩码无效，梯度掩码源于数值不稳定；在物理世界可实现低成功率但影响选举结果的攻击。

Conclusion: 美国选举制表器使用机器学习分类器存在安全风险，新方法生成的对抗样本攻击在物理世界可能影响选举结果。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [205] [AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering](https://arxiv.org/abs/2506.13989)
*Johan Östman,Edvin Callisen,Anton Chen,Kristiina Ausmees,Emanuel Gårdh,Jovan Zamac,Jolanta Goldsteine,Hugo Wefer,Simon Whelan,Markus Reimegård*

Main category: cs.SI

TL;DR: 现有洗钱合成数据集有局限，提出开源套件AMLGentex生成交易数据并评估检测方法，可反映现实反洗钱场景复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前洗钱活动难被发现，现有合成数据集无法模拟现实洗钱的结构和行为复杂性，需解决这些局限。

Method: 提出开源套件AMLGentex用于生成现实、可配置的交易数据并对检测方法进行基准测试。

Result: 展示了框架可在反映实际反洗钱场景复杂性的条件下严格评估方法。

Conclusion: AMLGentex能在可控环境中对反洗钱系统进行系统评估，捕捉现实关键挑战。

Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.

</details>


### [206] [Density-aware Walks for Coordinated Campaign Detection](https://arxiv.org/abs/2506.13912)
*Atul Anand Gopalakrishnan,Jakir Hossain,Tuğrulcan Elmas,Ahmet Erdem Sarıyüce*

Main category: cs.SI

TL;DR: 论文聚焦检测社交媒体协调活动，利用LEN数据集，提出基于局部网络结构密度的图分类方法，在分类任务上取得更好结果，证明该方法可识别网络不真实行为。


<details>
  <summary>Details</summary>
Motivation: 协调活动利用社交媒体误导用户，区分其与真实公众话语是挑战，需检测协调活动。

Method: 将检测问题建模为图分类任务，引入利用局部网络结构密度的图分类方法，提出随机加权游走（RWW），用Skip - gram模型编码，训练消息传递神经网络（MPNNs）。

Result: 在二元和多类分类中，使用基于密度感知结构嵌入训练的MPNNs比使用简单节点特征的准确率分别提高近12%和5%。

Conclusion: 结合密度感知结构编码与MPNNs为识别社交媒体网络协调不真实行为提供了可靠框架。

Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [207] [Bayesian Hybrid Machine Learning of Gallstone Risk](https://arxiv.org/abs/2506.14561)
*Chitradipa Chakraborty,Nayana Mukherjee*

Main category: stat.AP

TL;DR: 提出混合机器学习框架解决胆结石病风险因素分析问题，平衡预测准确性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 胆结石病是复杂的多因素疾病，传统逻辑回归在高维数据中表现不佳，需更好方法识别风险因素及其相互作用。

Method: 采用Adaptive LASSO选择特征，用BART建模非线性效应和发现交互作用，用微分方程知识表征交互作用，最后在贝叶斯框架下整合进逻辑回归模型。

Result: 所提框架能增强预测效果并产生可操作的见解。

Conclusion: 该框架是医学研究和决策的有价值支持工具。

Abstract: Gallstone disease is a complex, multifactorial condition with significant global health burdens. Identifying underlying risk factors and their interactions is crucial for early diagnosis, targeted prevention, and effective clinical management. Although logistic regression remains a standard tool for assessing associations between predictors and gallstone status, it often underperforms in high-dimensional settings and may fail to capture intricate relationships among variables. To address these limitations, we propose a hybrid machine learning framework that integrates robust variable selection with advanced interaction detection. Specifically, Adaptive LASSO is employed to identify a sparse and interpretable subset of influential features, followed by Bayesian Additive Regression Trees (BART) to model nonlinear effects and uncover key interactions. Selected interactions are further characterized by physiological knowledge through differential equation-informed interaction terms, grounding the model in biologically plausible mechanisms. The insights gained from these steps are then integrated into a final logistic regression model within a Bayesian framework, providing a balance between predictive accuracy and clinical interpretability. This proposed framework not only enhances prediction but also yields actionable insights, offering a valuable support tool for medical research and decision-making.

</details>


### [208] [The use of cross validation in the analysis of designed experiments](https://arxiv.org/abs/2506.14593)
*Maria L. Weese,Byran J. Smucker,David J. Edwards*

Main category: stat.AP

TL;DR: 研究在结构化实验设计中交叉验证（CV）选模型的有效性，发现留一交叉验证（LOOCV）常有用，k折CV表现不均。


<details>
  <summary>Details</summary>
Motivation: 机器学习及CV在实验设计分析中使用增加，需实证研究CV与其他模型选择方法在结构化实验设计中的有效性。

Method: 对响应面设置和筛选两种情况，对比CV与其他模型选择方法（如小样本自助法）。

Result: LOOCV在小的结构化分析中常有用，更通用的k折CV有竞争力但表现不均。

Conclusion: LOOCV在小结构化实验设计分析中具有实用性，k折CV效果有待进一步优化。

Abstract: Cross-validation (CV) is a common method to tune machine learning methods and can be used for model selection in regression as well. Because of the structured nature of small, traditional experimental designs, the literature has warned against using CV in their analysis. The striking increase in the use of machine learning, and thus CV, in the analysis of experimental designs, has led us to empirically study the effectiveness of CV compared to other methods of selecting models in designed experiments, including the little bootstrap. We consider both response surface settings where prediction is of primary interest, as well as screening where factor selection is most important. Overall, we provide evidence that the use of leave-one-out cross-validation (LOOCV) in the analysis of small, structured is often useful. More general $k$-fold CV may also be competitive but its performance is uneven.

</details>


### [209] [Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior](https://arxiv.org/abs/2506.14762)
*Chengyuan Zhang,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: 经典跟车模型有局限性，引入制度切换框架 FHMM - IDM 建模，实验证明其能揭示驾驶结构，改善交通模拟等。


<details>
  <summary>Details</summary>
Motivation: 经典跟车模型如 IDM 因结构单一，无法捕捉人类驾驶多模态特性，导致保真度低和参数难解释，需改进。

Method: 引入制度切换框架，用含 IDM 动态的阶乘隐马尔可夫模型（FHMM - IDM），通过马尔可夫链蒙特卡罗（MCMC）进行贝叶斯推理估计参数等。

Result: 在 HighD 数据集实验中，FHMM - IDM 揭示人类驾驶可解释结构，有效分离内部驾驶行为和外部交通状况，展现动态制度切换模式。

Conclusion: 该框架为不确定下依赖上下文的驾驶行为建模提供了有效且原则性的解决方案，能改善交通模拟保真度、安全分析效能和以人类为中心的 ADAS 开发。

Abstract: Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [210] [Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space](https://arxiv.org/abs/2506.13809)
*Roman V. Belavkin*

Main category: q-bio.PE

TL;DR: 本文受Fisher几何方法启发，分析汉明空间中字符串有益突变和交叉重组概率，推导转移概率表达式，得出优化条件，指出突变和重组的差异及作用。


<details>
  <summary>Details</summary>
Motivation: 受Fisher几何方法启发，研究汉明空间中字符串有益突变和交叉重组概率，为优化突变和重组算子参数提供基础。

Method: 使用几何和组合分析推导最优值周围球体间的转移概率的闭式表达式。

Result: 得出突变和重组半径的最优条件，发现突变有益概率随距最优值距离增加而减小，交叉重组有益和有害概率平衡且特征具有平移不变性。

Conclusion: 交叉重组可补充突变，在接近最优值时提高进化速率。

Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [211] [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](https://arxiv.org/abs/2506.13827)
*Zhuoying Li,Zhu Xu,Yuxin Peng,Yang Liu*

Main category: cs.GR

TL;DR: 现有基于指令的图像编辑缺乏全面评估指标，本文提出BPM指标，经实验验证其比现有指标更符合人类评估。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑评估指标存在高人力成本或不贴合任务的问题，缺乏全面评估指标。

Method: 引入BPM指标，先定位编辑相关区域，通过区域感知判断和语义感知判断两层过程评估编辑质量，还可将相关区域定位集成到编辑方法中。

Result: 在综合指令编辑数据上验证，BPM指标与人类评估的一致性最高。

Conclusion: BPM指标有效，具有广泛适用性，代码已开源。

Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/

</details>


### [212] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: 提出ReFrame方法探索不同缓存策略优化渲染工作负载，在三个实时渲染任务中平均提速1.4倍且质量损失可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 图形渲染应用中利用神经网络时，任务的时间连贯性可复用中间结果避免重复计算，以降低延迟。

Method: 提出ReFrame，探索不同缓存策略，应用于渲染管道中常见的编码器 - 解码器风格网络。

Result: 在三个实时渲染任务中平均实现1.4倍的提速，质量损失可忽略不计。

Conclusion: ReFrame能有效优化渲染工作负载的质量和性能平衡。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [213] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
*Bruno Martins,Piotr Szymański,Piotr Gramacki*

Main category: cs.CL

TL;DR: 论文指出当前深度研究系统缺乏地理时间能力，提出将地理时间推理集成到深度研究管道的愿景。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统缺乏地理时间能力，无法回答涉及地理和/或时间约束的上下文丰富问题。

Method: 提出增强检索和合成过程以处理地理时间约束，借助开放可重复的基础设施和严格评估协议。

Result: 勾勒出构建更先进、具备地理时间感知能力的深度研究系统的路径。

Conclusion: 该愿景对未来人工智能驱动的信息获取有潜在影响。

Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [214] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
*Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman Mahmoud*

Main category: cs.CL

TL;DR: 介绍GG（Guaranteed Guess），结合大语言模型和软件测试构造进行指令集架构（ISA）转译，评估效果好且开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 为快速、灵活且正确地在不同指令集架构间转译低级别程序，提升现有代码可移植性和寿命，尤其是复杂（CISC）和精简（RISC）硬件架构间的转译。

Method: 使用大语言模型生成候选翻译，并将其嵌入软件测试框架以量化翻译的可信度。

Result: 在两个不同数据集上评估，单元测试代码覆盖率超98%，HumanEval程序功能/语义正确性达99%，BringupBench程序达49%；与Rosetta 2框架相比，运行时性能快1.73倍，能源效率高1.47倍，内存使用优2.41倍。

Conclusion: GG在现实世界的CISC到RISC转译任务中有效，且开源资源可作为ISA级代码转译研究的基础。

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [215] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Main category: cs.CL

TL;DR: 文章介绍自动化构建指令数据方法，构建ClimateChat - Corpus数据集微调开源大模型得到ClimateChat，该模型在气候变化问答任务上表现提升，还评估不同因素对模型性能影响。


<details>
  <summary>Details</summary>
Motivation: 当前气候变化研究中高效生成大量高精度指令数据不足，限制气候变化大语言模型发展。

Method: 利用文档事实和背景知识生成指令，通过网络抓取和收集种子指令增强数据多样性，构建ClimateChat - Corpus数据集微调开源大模型。

Result: ClimateChat在气候变化问答任务上性能显著提升，能适应多种气候变化科学发现任务。

Conclusion: 本研究为构建气候变化指令数据和训练特定大语言模型提供参考和实证支持，强调选择合适基础模型用于指令微调的重要性。

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [216] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
*Antara Raaghavi Bhattacharya,Isabel Papadimitriou,Kathryn Davidson,David Alvarez-Melis*

Main category: cs.CL

TL;DR: 研究大语言模型在跨语言数字系统语言 - 数学谜题上的表现，发现其难以从隐含模式灵活推断组合规则。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在涉及跨语言数字系统的语言 - 数学谜题任务中表现不佳的原因。

Method: 进行一系列实验分离语言和数学方面，开展消融研究探究数字构建和组合参数对性能的影响。

Result: 模型只有在问题中数学运算用已知符号明确标记时才能解决问题，且缺乏对隐含数字结构的概念。

Conclusion: 当前推理模型从人类规模数据隐含模式灵活推断组合规则仍是挑战。

Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [217] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
*Abhilekh Borah,Chhavi Sharma,Danush Khanna,Utkarsh Bhatt,Gurpreet Singh,Hasnat Md Abdullah,Raghav Kaushik Ravi,Vinija Jain,Jyoti Patel,Shubham Singh,Vasu Sharma,Arpita Vats,Rahul Raja,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 引入对齐质量指数（AQI）评估大语言模型对齐情况，提出LITMUS数据集，测试证明AQI有效性并公开代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型进入高风险领域需可靠反映人类价值和安全约束，但现有评估有盲点，模型存在对齐问题。

Method: 引入AQI，通过分析潜在空间中安全和不安全激活的分离来评估，结合多种指标；提出LITMUS数据集用于评估。

Result: 在LITMUS上的实证测试表明AQI与外部评判相关，能揭示拒绝指标遗漏的漏洞。

Conclusion: AQI是一种强大、解码不变的安全审计工具，公开实现代码以促进相关研究。

Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [218] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
*Shang-Chi Tsai,Seiya Kawano,Angel Garcia Contreras,Koichiro Yoshino,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文提出一种用于机器人辅助场景的数据增强框架，可提升机器人动作选择能力，达当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 收集包含视觉和语言元素的大规模数据集用于模型训练困难且耗时，为解决此问题开展研究。

Method: 引入专注于机器人辅助场景的数据增强框架，利用大语言模型模拟对话和环境上下文，用稳定扩散模型生成环境图像，用生成的数据优化多模态模型。

Result: 基于真实场景数据集的实验表明，该方法显著提升机器人动作选择能力，达当前最优性能。

Conclusion: 所提出的数据增强框架有效，能让机器人在有限目标数据下更准确响应，提升动作选择能力。

Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [219] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
*David Kogan,Max Schumacher,Sam Nguyen,Masanori Suzuki,Melissa Smith,Chloe Sophia Bellows,Jared Bernstein*

Main category: cs.CL

TL;DR: 提出Ace - CEFR数据集评估英语对话文本难度，模型实验显示其训练的模型效果好并公开数据集


<details>
  <summary>Details</summary>
Motivation: 评估短对话文本语言难度，用于训练和过滤大语言模型的需求未得到满足

Method: 引入Ace - CEFR数据集，在该数据集上对基于Transformer的模型和大语言模型等进行实验

Result: 在Ace - CEFR上训练的模型比人类专家更准确测量文本难度，且延迟适合生产环境

Conclusion: 发布Ace - CEFR数据集用于研发

Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [220] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
*Essential AI,:,Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani*

Main category: cs.CL

TL;DR: 介绍了24万亿token的数据集Essential - Web v1.0，每个文档有分类注释，通过SQL式过滤可获得有竞争力的数据集，且该数据集在HuggingFace上可用。


<details>
  <summary>Details</summary>
Motivation: 解决因缺乏大规模、组织良好的预训练数据集导致的数据管道成本高且难以访问的问题。

Method: 使用微调的0.5b参数模型EAI - Distill - 0.5b为文档生成十二类分类注释，通过SQL式过滤获取数据集。

Result: 在数学、网页代码、STEM和医学领域获得有竞争力的网络策划数据集。

Conclusion: Essential - Web v1.0数据集可有效解决数据问题，且可在HuggingFace上获取。

Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [221] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
*Tao He,Guang Huang,Yu Yang,Tianshi Xu,Sicheng Zhao,Guiguang Ding,Pengyang Wang,Feng Tian*

Main category: cs.CL

TL;DR: 提出S$^4$C框架解决大语言模型推理延迟问题，实验显示其性能超基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自回归特性导致推理延迟，现有投机采样方法忽略文本生成连贯性，效率受限。

Method: 提出S$^4$C框架，利用多头起草快速生成token，用连续验证树高效验证候选并复用特征。

Result: S$^4$C在主流任务中超越基线方法，在Spec - bench基准上加速比达2.26x - 2.60x。

Conclusion: S$^4$C框架提升了效率和并行性，能用更少计算资源生成更多有效token。

Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [222] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
*Chenglong Wang,Yang Gan,Yifu Huo,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Tong Xiao,Chunliang Zhang,Tongran Liu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文探索使用无标签和有标签数据训练奖励模型，提出生成式奖励模型，结果表明该模型泛化性好，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型标准训练为判别式，仅依赖有标签人类偏好数据，本文探索使用无标签和有标签数据训练奖励模型的方法。

Method: 基于大语言模型中的生成式模型，先通过大规模无监督学习训练生成式奖励模型，再进行有监督微调，使用标签平滑优化正则化的成对排序损失。

Result: 得到基础奖励模型，在多个任务中泛化性好，相比多个强基线模型有显著性能提升。

Conclusion: 提出的方法将生成式模型和判别式模型在同一类训练目标下联系起来，生成的基础奖励模型可应用于广泛任务。

Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [223] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
*Tuan Nguyen,Huy-Dat Tran*

Main category: cs.CL

TL;DR: 研究用合成代码切换（CS）数据构建CS - ASR，提出短语级混合方法生成数据并微调大预训练ASR模型，在三种东南亚语言对测试有效果提升。


<details>
  <summary>Details</summary>
Motivation: 代码切换在多语言环境常见，但因语言复杂、转录数据稀缺昂贵，给自动语音识别（ASR）带来挑战，需研究构建CS - ASR的方法。

Method: 提出短语级混合方法生成合成CS数据，用单语和合成短语混合CS数据微调大预训练ASR模型（Whisper、MMS、SeamlessM4T），并建立新的综合基准评估模型性能。

Result: 提出的训练策略提升了单语和CS测试的ASR性能，其中马来 - 英语（BM - EN）提升最高，其次是泰米尔 - 英语（TA - EN）和普通话 - 马来语（ZH - BM）。

Conclusion: 该研究为CS - ASR开发提供了一种经济有效的方法，对研究和行业有益。

Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [224] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
*Jonathan Hayase,Alisa Liu,Noah A. Smith,Sewoong Oh*

Main category: cs.CL

TL;DR: 提出推理时方法将带BPE分词器的自回归语言模型转换为字符或字节级模型，解决分词问题并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法会引入失真、引发提示边界问题，不匹配的分词器阻碍模型组合和互操作性。

Method: 提出推理时方法，将带BPE分词器的自回归语言模型转换为字符或字节级模型。

Result: 高效解决提示边界问题，统一不同分词器模型的词汇表，集成和代理调优的模型在下游评估中表现优于组成部分。

Conclusion: 所提方法有效解决分词相关问题，能提升模型性能。

Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [225] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
*Md Tanzib Hosain,Salman Rahman,Md Kishor Morol,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 介绍无训练多智能体推理框架Xolver，它让大语言模型有经验记忆，在推理时学习相关策略等，性能超越专业推理智能体，在多个数据集取得新最佳结果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型独立处理问题，未积累整合经验知识，而专家问题解决者会利用丰富经验，因此提出Xolver框架。

Method: 引入Xolver，一个无训练的多智能体推理框架，整合多种经验模式，在推理时学习相关策略等。

Result: Xolver在多个数据集上取得新最佳结果，如GSM8K（98.1%）等，且能超越先进模型。

Conclusion: 整体经验学习是迈向具备专家级推理能力的通用智能体的关键一步。

Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [226] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
*Chenghao Li,Liu Liu,Baosheng Yu,Jiayan Qiu,Yibing Zhan*

Main category: cs.CL

TL;DR: 现有大语言模型集成外部工具方法有局限，提出新的令牌学习方法，实验表明该方法在多任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂任务表现不佳，现有集成外部工具方法未考虑工具和单词令牌关系，限制了预训练模型适应性。

Method: 从初始化角度将工具令牌与现有单词嵌入空间对齐，基于工具名称或描述构建先验令牌嵌入，用于初始化和正则化可学习工具令牌嵌入。

Result: 在数值推理、知识问答和具身计划生成等任务上，相比CoT、REACT等基线有明显提升。

Conclusion: 所提方法能通过相关令牌在不同领域有效增强大语言模型使用工具的能力。

Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [227] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
*Di He,Ajay Jaiswal,Songjun Tu,Li Shen,Ganzhao Yuan,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: 提出AlphaDecay方法，根据模块光谱特性自适应分配权重衰减强度，在预训练任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统统一衰减率方法忽略大语言模型结构多样性和模块光谱特性差异，需要改进。

Method: 基于HT - SR理论，分析权重相关矩阵的经验光谱密度，为不同模块分配不同权重衰减强度。

Result: 在60M到1B不同模型大小的预训练任务中，AlphaDecay比传统统一衰减和其他自适应衰减基线有更好的困惑度和泛化能力。

Conclusion: AlphaDecay通过自适应分配权重衰减强度平衡模块光谱特性差异，提高了大语言模型的性能。

Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [228] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
*Zeinab Sadat Taghavi,Ali Modarressi,Yunpu Ma,Hinrich Schütze*

Main category: cs.CL

TL;DR: 提出ImpliRet基准，将推理挑战转移到文档端处理，评估多种检索器和长上下文模型，发现文档端推理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统依赖表面线索，现有基准将推理负担转移到查询端处理，需新基准评估文档端推理能力。

Method: 提出ImpliRet基准，其查询简单但相关性依赖文档隐含事实；评估多种稀疏和密集检索器；测试长上下文模型。

Result: 所有检索器在ImpliRet基准下表现不佳，最佳nDCG@10仅15.07%；GPT - 4.1在短上下文下得分仅35.06%。

Conclusion: 文档端推理仍是一个具有挑战性的问题。

Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [229] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
*Xiang Cheng,Chengyan Pan,Minjun Zhao,Deyang Li,Fangchao Liu,Xinyu Zhang,Xiao Zhang,Yong Liu*

Main category: cs.CL

TL;DR: 研究ICL+CoT框架在数学推理中对近期强大模型的效果，发现传统及增强CoT示例均难提升推理性能，凸显该框架局限性。


<details>
  <summary>Details</summary>
Motivation: 探究在模型能力不断提升的情况下，CoT示例是否仍能使近期更强模型在数学任务中受益。

Method: 对Qwen2.5系列等模型进行系统实验，研究传统CoT示例效果；构建使用先进模型答案的增强CoT示例并测试其有效性。

Result: 传统CoT示例无法提升推理性能，仅能使输出格式符合人类预期；增强CoT示例也无法提升推理性能，模型倾向忽略示例而关注指令。

Conclusion: 当前ICL+CoT框架在数学推理中有局限性，需重新审视ICL范式和示例定义。

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [230] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
*Daniel D'souza,Julia Kreutzer,Adrien Morisot,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [231] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
*Li-Wei Chen,Takuya Higuchi,Zakaria Aldeneh,Ahmed Hussen Abdelaziz,Alexander Rudnicky*

Main category: cs.CL

TL;DR: 提出端到端变分方法增强语义标记，提升语音生成自然度，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义标记的语音建模生成语音自然度低，添加音高特征有局限且需手动设计。

Method: 提出端到端变分方法，自动学习编码连续语音属性增强语义标记。

Result: 生成的语音延续受人类评估者青睐。

Conclusion: 该方法无需手动提取和选择副语言特征，能有效提升语音生成效果。

Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


### [232] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
*David Wan,Eran Hirsch,Elias Stengel-Eskin,Ido Dagan,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出GenerationPrograms框架，分两阶段生成文本及归因，在多任务中提升归因质量，还能作为事后归因方法并支持模块化改进。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在文本生成时归因细粒度不足，且现有归因方法无法解释模型如何利用源文档生成响应，影响可验证性、信任度和可解释性。

Method: 引入GenerationPrograms模块化生成框架，将过程分为创建可执行程序计划和执行操作两步。

Result: 在两个长问答任务和多文档摘要任务中显著提高文档级和句子级归因质量，作为事后归因方法表现优于传统技术，生成的可解释程序支持模块化改进以提升归因质量。

Conclusion: GenerationPrograms框架能有效解决大语言模型归因问题，提高归因质量和可解释性。

Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [233] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
*Chenchen Yuan,Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出合成多个大语言模型道德判断的框架，实验显示可构建共识并提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂道德困境中判断常出现分歧，需解决差异。

Method: 提出框架合成多个大语言模型道德判断，聚合机制融合连续道德可接受性分数，对偏差模型进行嵌入优化。

Result: 在大规模社会道德困境数据集实验中，该方法构建了稳健共识，提升了个体模型准确性。

Conclusion: 数据驱动的多模型道德对齐有价值，有望实现更安全、一致的AI系统。

Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [234] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
*Leah von der Heyde,Anna-Carolina Haensch,Bernd Weiß,Jessika Daikeler*

Main category: cs.CL

TL;DR: 研究不同大语言模型对开放式调查回复编码的效果，对比多种模型和提示方法，发现模型表现差异大，微调模型表现较好，研究为大语言模型在调查研究中的应用提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在开放式调查回复分类研究多聚焦英语非复杂话题或单一模型，需研究其在其他情境的应用。

Method: 以德语调查参与原因数据为例，对比多种大语言模型和提示方法，用专家编码评估模型性能。

Result: 不同大语言模型整体表现差异大，仅微调模型有满意预测性能；提示方法效果取决于所用模型；未微调时不同原因类别分类表现不均导致类别分布不同。

Conclusion: 研究为开放式回复编码方法研究和实践提供启示，指出研究者选用自动化方法时需权衡。

Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [235] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
*Ring Team,Bin Hu,Cai Chen,Deng Zhao,Ding Liu,Dingnan Jin,Feng Zhu,Hao Dai,Hongzhi Luan,Jia Guo,Jiaming Liu,Jiewei Wu,Jun Mei,Jun Zhou,Junbo Zhao,Junwu Xiong,Kaihong Zhang,Kuan Xu,Lei Liang,Liang Jiang,Liangcheng Fu,Longfei Zheng,Qiang Gao,Qing Cui,Quan Wan,Shaomian Zheng,Shuaicheng Li,Tongkai Yang,Wang Ren,Xiaodong Yan,Xiaopei Wan,Xiaoyun Feng,Xin Zhao,Xinxing Yang,Xinyu Kong,Xuemin Yang,Yang Li,Yingting Wu,Yongkang Liu,Zhankai Xu,Zhenduo Zhang,Zhenglei Zhou,Zhenyu Huang,Zhiqiang Zhang,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: 提出基于MoE的大语言模型Ring - lite，通过强化学习优化，在推理任务表现佳且参数激活少，介绍联合训练流程及应对挑战的方法并将开源。


<details>
  <summary>Details</summary>
Motivation: 构建高效且具备强大推理能力的大语言模型，在参数激活少的情况下达到先进小模型推理性能。

Method: 引入集成蒸馏与强化学习的联合训练流程，提出C3PO方法提升训练稳定性和计算吞吐量，基于熵损失选蒸馏检查点，开发两阶段训练范式处理多领域数据集成。

Result: Ring - lite在挑战性基准测试中达到SOTA小模型性能，仅激活可比模型三分之一的参数。

Conclusion: 所提出的方法有效解决MoE RL训练中的挑战，实现了性能与效率的平衡。

Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [236] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
*Mathurin Videau,Badr Youbi Idrissi,Alessandro Leite,Marc Schoenauer,Olivier Teytaud,David Lopez-Paz*

Main category: cs.CL

TL;DR: 提出自回归U - Net，使模型学习嵌入自己的标记，能处理不同粒度任务且在不同层级有表现优势。


<details>
  <summary>Details</summary>
Motivation: 传统分词方法（如BPE）固定了输入文本粒度，限制模型操作数据和预测能力，需要更灵活的方案。

Method: 引入自回归U - Net，读取原始字节，按不同规模组合词，不同深度层关注不同细节和语义模式。

Result: 浅层次结构与强BPE基线相当，深层次结构有良好趋势，可处理字符级任务并跨低资源语言传递知识。

Conclusion: 将分词融入模型内，使系统能处理不同粒度任务和跨语言知识传递。

Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [237] [TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles](https://arxiv.org/abs/2506.13933)
*Tobias Kerbl,David Brecht,Nils Gehrke,Nijinshan Karunainayagam,Niklas Krauss,Florian Pfab,Richard Taupitz,Ines Trautmannsheimer,Xiyan Su,Maria-Magdalena Wolf,Frank Diermeyer*

Main category: cs.RO

TL;DR: 提出开源远程操作软件栈，集成远程驾驶、远程协助功能，与自动驾驶软件交互，进行性能评估并开源代码。


<details>
  <summary>Details</summary>
Motivation: 当前缺少能集成远程驾驶、远程协助并与真实车辆集成用于测试的开源软件。

Method: 开发模块化、开源的远程操作软件栈，具备标准化接口，支持与多种平台集成。

Result: 评估了不同车辆平台在仿真和现实场景中的延迟和性能。

Conclusion: 该软件栈可作为协作开发基础，用于实际测试和用户研究。

Abstract: Teleoperation is a key enabler for future mobility, supporting Automated Vehicles in rare and complex scenarios beyond the capabilities of their automation. Despite ongoing research, no open source software currently combines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance through high-level interaction with automated driving software modules, and integration with a real-world vehicle for practical testing. To address this gap, we present a modular, open source teleoperation software stack that can interact with an automated driving software, e.g., Autoware, enabling Remote Assistance and Remote Driving. The software featuresstandardized interfaces for seamless integration with various real-world and simulation platforms, while allowing for flexible design of the human-machine interface. The system is designed for modularity and ease of extension, serving as a foundation for collaborative development on individual software components as well as realistic testing and user studies. To demonstrate the applicability of our software, we evaluated the latency and performance of different vehicle platforms in simulation and real-world. The source code is available on GitHub

</details>


### [238] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Main category: cs.RO

TL;DR: 分析基于惩罚的模拟器接触计算梯度误差原因，提出DiffMJX改进硬接触梯度质量，引入CFD机制解决接触梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 基于惩罚的模拟器在硬接触模拟时用硬设置有梯度错误，软设置有仿真与现实差距大的问题。

Method: 分析接触计算找梯度误差原因，提出DiffMJX结合自适应积分与MuJoCo XLA；引入CFD机制，用直通技巧仅在反向传播应用。

Result: DiffMJX显著改善硬接触下梯度质量，CFD机制能在物体接触前产生有用接触梯度。

Conclusion: 所提方法可解决基于惩罚的模拟器在接触力优化中的梯度问题，且不修改正向仿真保证物理真实性。

Abstract: Contact forces pose a major challenge for gradient-based optimization of robot dynamics as they introduce jumps in the system's velocities. Penalty-based simulators, such as MuJoCo, simplify gradient computation by softening the contact forces. However, realistically simulating hard contacts requires very stiff contact settings, which leads to incorrect gradients when using automatic differentiation. On the other hand, using non-stiff settings strongly increases the sim-to-real gap. We analyze the contact computation of penalty-based simulators to identify the causes of gradient errors. Then, we propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to notably improve gradient quality in the presence of hard contacts. Finally, we address a key limitation of contact gradients: they vanish when objects do not touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism that enables the simulator to generate informative contact gradients even before objects are in contact. To preserve physical realism, we apply CFD only in the backward pass using a straight-through trick, allowing us to compute useful gradients without modifying the forward simulation.

</details>


### [239] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: 提出AMPLIFY框架利用大规模无动作视频数据，将视觉动力学编码为运动令牌，评估显示其动力学准确且有用，能提升下游策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 机器人动作标签数据稀缺昂贵限制策略泛化，无动作视频数据多但转化为有效策略有挑战。

Method: 引入AMPLIFY框架，分离视觉运动预测和动作推理，分别训练前向和逆向动力学模型。

Result: 动力学准确，MSE最多好3.7倍，像素预测准确率超2.5倍；下游策略学习在低数据场景提升1.2 - 2.2倍，从无动作人类视频学习平均提升1.4倍，能零分布动作数据泛化到LIBERO任务；可提升视频预测质量。

Conclusion: 提出利用异构数据源构建高效、可泛化世界模型的新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [240] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Main category: cs.RO

TL;DR: 研究提出让预训练策略可操控，通过用户交互纠正策略错误，无需微调。


<details>
  <summary>Details</summary>
Motivation: 预训练策略部署出错时，用户纠正行为机制有限，为下游用例收集数据微调效率低。

Method: 提出推理时转向（利用用户交互切换离散技能）和任务与运动模仿（让用户交互编辑连续动作并满足任务约束）两种框架。

Result: 无需额外训练纠正策略预测偏差，最大化预训练模型效用，实现推理时用户目标。

Conclusion: 让预训练策略可操控是解决策略部署错误且无需微调的有效办法。

Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.

</details>


### [241] [Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation](https://arxiv.org/abs/2506.14294)
*Prashant Kumar Rai,Elham Kowsari,Nataliya Strokina,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 提出结合高分辨率成像雷达与惯性测量单元估计自主导航中自速度的方法，在数据集上评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于雷达的自运动估计技术的局限性。

Method: 用神经网络处理复值原始雷达数据估计瞬时线性自速度及其不确定性，用扩展卡尔曼滤波器将其与惯性测量单元数据融合，利用网络预测的不确定性细化惯性传感器的噪声和偏差参数。

Result: 在公开的ColoRadar数据集上评估，与最接近的公开方法相比误差显著降低，且优于瞬时和基于扫描匹配的技术。

Conclusion: 该方法提高了自运动估计的整体鲁棒性和准确性。

Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.

</details>


### [242] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Main category: cs.RO

TL;DR: 提出GAMORA系统用于高危实验室任务远程操作，介绍其构成、功能与效果，是生物医学研究中可扩展的沉浸式解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着生物危害复杂性增加，需在高危实验室环境中减少人类直接暴露同时保证操作精度。

Method: 集成Oculus Quest 2、NVIDIA Jetson Nano和ROS，结合逆运动学，包含Unity 3D环境构建、实时运动规划和硬件在环测试。

Result: 平均位置偏差2.2mm（从4mm改善），移液精度0.2mL内，50次试验重复性1.2mm，集成YOLOv8增强空间感知，降低50%功率输出。

Conclusion: GAMORA能实现高危实验室任务安全、精确和可重复自动化，为生物医学研究提供可扩展的沉浸式解决方案。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


### [243] [SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.14648)
*Hexian Ni,Tao Lu,Haoyuan Hu,Yinghao Cai,Shuo Wang*

Main category: cs.RO

TL;DR: 提出SENIOR方法解决PbRL反馈和样本效率低问题，实验显示其在多任务中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决Preference-based Reinforcement Learning (PbRL) 反馈和样本效率低，阻碍应用的问题。

Method: 提出SENIOR方法，包括Motion-Distinction-based Selection scheme (MDS)选择有明显运动和不同方向的片段对，以及preference-guided exploration method (PGE)鼓励向高偏好低访问状态探索。

Result: SENIOR在六个模拟机器人操作任务和四个现实任务中，在人类反馈效率和策略收敛速度上优于其他五种现有方法。

Conclusion: SENIOR方法中MDS和PGE的协同作用能显著加速奖励和策略学习。

Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

</details>


### [244] [Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models](https://arxiv.org/abs/2506.14727)
*Huihan Liu,Rutav Shah,Shuijing Liu,Jack Pittenger,Mingyo Seo,Yuchen Cui,Yonatan Bisk,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: 介绍了辅助遥操作系统Casper，利用预训练视觉语言模型中的常识知识进行实时意图推断和灵活技能执行，提升了任务表现、降低人类认知负荷并提高用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有辅助遥操作系统方法局限于简单预设场景或特定任务数据分布，无法满足现实世界辅助需求。

Method: 引入Casper系统，包含开放世界感知模块、基于VLM的意图推断机制和技能库。

Result: 通过大量实证评估，包括人类研究和系统消融实验，表明Casper比直接遥操作和其他辅助遥操作基线有更好表现。

Conclusion: Casper能有效提升任务性能，降低人类认知负荷，提高用户满意度。

Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [245] [Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G](https://arxiv.org/abs/2505.22963)
*Zhuoran Duan,Guoshun Nan,Rushan Li,Zijun Wang,Lihua Xiong,Chaoying Yuan,Guorong Liu,Hui Xu,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 文章提出适用于6G网络的ES3A安全架构，介绍架构原则、指南、组成和机制，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 6G安全和弹性受重视，但因异构网络威胁和多样安全需求，需重新设计安全架构。

Method: 提出ES3A架构，讨论其六大原则，从部署角度给出三个指南，架构含三层三域，采用两阶段编排机制，在基于SDR的真实无线电系统上进行原型设计。

Result: 实验证明ES3A有效，通过案例展示架构优越性。

Conclusion: ES3A能解决6G网络安全架构面临的挑战，是有效的6G网络安全架构。

Abstract: The upcoming 6G will fundamentally reshape mobile networks beyond communications, unlocking a multitude of applications that were once considered unimaginable. Meanwhile, security and resilience are especially highlighted in the 6G design principles. However, safeguarding 6G networks will be quite challenging due to various known and unknown threats from highly heterogeneous networks and diversified security requirements of distinct use cases, calling for a comprehensive re-design of security architecture. This motivates us to propose ES3A (Entire Smart Service-based Security Architecture), a novel security architecture for 6G networks. Specifically, we first discuss six high-level principles of our ES3A that include hierarchy, flexibility, scalability, resilience, endogeny, and trust and privacy. With these goals in mind, we then introduce three guidelines from a deployment perspective, envisioning our ES3A that offers service-based security, end-to-end protection, and smart security automation for 6G networks. Our architecture consists of three layers and three domains. It relies on a two-stage orchestration mechanism to tailor smart security strategies for customized protection in high-dynamic 6G networks, thereby addressing the aforementioned challenges. Finally, we prototype the proposed ES3A on a real-world radio system based on Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A. We also provide a case to show the superiority of our architecture.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [246] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Main category: eess.SP

TL;DR: 本文探讨相位截距失真，实验验证其不可感知性，并将其用于机器学习数据增强取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 研究特殊的相位截距失真，验证其不可感知性并探索其在机器学习中的应用。

Method: 进行人体实验验证假设，将相位截距失真作为数据增强方法用于音频机器学习任务。

Result: 人体实验结果支持相位截距失真不可感知的假设，数据增强方法在音频机器学习任务中取得改进结果。

Conclusion: 相位截距失真不可感知，且可用于机器学习数据增强以提升效果。

Abstract: Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [247] [Projecting U.S. coastal storm surge risks and impacts with deep learning](https://arxiv.org/abs/2506.13963)
*Julian R. Rice,Karthik Balaguru,Fadia Ticona Rollano,John Wilson,Brent Daniel,David Judi,Ning Sun,L. Ruby Leung*

Main category: physics.ao-ph

TL;DR: 利用深度学习风暴潮模型评估美国沿海风暴潮风险，发现到本世纪末因热带气旋强度和海平面上升，受威胁人口增加50%。


<details>
  <summary>Details</summary>
Motivation: 风暴潮是热带气旋带来的致命灾害之一，但其发生罕见且物理过程复杂，评估当前和未来风险困难，需新方法解决。

Method: 利用深度学习风暴潮模型，考虑热带气旋行为和海平面变化，对90万个合成热带气旋事件进行模拟。

Result: 历史100年一遇风暴潮与观测和其他模型结果吻合；到本世纪末，热带气旋强度和海平面上升使受威胁人口增加50%。

Conclusion: 佛罗里达州风险显著增加，佐治亚州和南卡罗来纳州存在关键阈值。

Abstract: Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs), yet assessing its current and future risk is difficult due to the phenomenon's rarity and physical complexity. Recent advances in artificial intelligence applications to natural hazard modeling suggest a new avenue for addressing this problem. We utilize a deep learning storm surge model to efficiently estimate coastal surge risk in the United States from 900,000 synthetic TC events, accounting for projected changes in TC behavior and sea levels. The derived historical 100-year surge (the event with a 1% yearly exceedance probability) agrees well with historical observations and other modeling techniques. When coupled with an inundation model, we find that heightened TC intensities and sea levels by the end of the century result in a 50% increase in population at risk. Key findings include markedly heightened risk in Florida, and critical thresholds identified in Georgia and South Carolina.

</details>


### [248] [AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction](https://arxiv.org/abs/2506.14022)
*Jacob B. Landsberg,Elizabeth A. Barnes,Matthew Newman*

Main category: physics.ao-ph

TL;DR: 本文探索用可解释AI改进季节内到季节尺度（S2S）预测，方法在三个任务上表现佳，还助于理解可预测性来源。


<details>
  <summary>Details</summary>
Motivation: S2S预测对多领域重要但具挑战，探索用此前用于更长时间尺度的可解释AI模型类比方法改进S2S预测。

Method: 用人工神经网络学习权重掩码优化类比选择，应用于三个预测任务。

Result: AI辅助类比法在确定性和概率性技能指标上优于传统类比法、气候学和持续性基线，能更好预测极端温度、改进不确定性表征。

Conclusion: 可解释AI改进S2S预测有效，分析权重掩码有助于理解S2S可预测性来源。

Abstract: Subseasonal-to-seasonal forecasting is crucial for public health, disaster preparedness, and agriculture, and yet it remains a particularly challenging timescale to predict. We explore the use of an interpretable AI-informed model analog forecasting approach, previously employed on longer timescales, to improve S2S predictions. Using an artificial neural network, we learn a mask of weights to optimize analog selection and showcase its versatility across three varied prediction tasks: 1) classification of Week 3-4 Southern California summer temperatures; 2) regional regression of Month 1 midwestern U.S. summer temperatures; and 3) classification of Month 1-2 North Atlantic wintertime upper atmospheric winds. The AI-informed analogs outperform traditional analog forecasting approaches, as well as climatology and persistence baselines, for deterministic and probabilistic skill metrics on both climate model and reanalysis data. We find the analog ensembles built using the AI-informed approach also produce better predictions of temperature extremes and improve representation of forecast uncertainty. Finally, by using an interpretable-AI framework, we analyze the learned masks of weights to better understand S2S sources of predictability.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [249] [DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models](https://arxiv.org/abs/2506.13817)
*Saleem A. Al Dajani,Abel Sanchez,John R. Williams*

Main category: q-bio.GN

TL;DR: 提出用带实时网络搜索的代理基础模型自动标注实验数据，达82.5%准确率，可推动虚拟细胞基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI基础模型在处理结构化生物数据有潜力，解决监督学习中结构化组学数据标注瓶颈。

Method: 使用带实时网络搜索的代理基础模型自动标注实验数据。

Result: 标注准确率达82.5%，能推动虚拟细胞基础模型发展以进行下游任务。

Conclusion: 该方法随数据量增长或超人类标注表现，可用于健康监测和诊断。

Abstract: Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [250] [High Socioeconomic Status is Associated with Diverse Consumption across Brands and Price Levels](https://arxiv.org/abs/2506.13840)
*Yuanmo He,Milena Tsvetkova*

Main category: econ.GN

TL;DR: 研究消费实践受经济、社会和文化因素影响，发现高收入与消费多样性显著相关，且不能仅用地理因素解释。


<details>
  <summary>Details</summary>
Motivation: 探究经济约束与消费多样性的关系，验证消费多样性假说。

Method: 分析纽约州数千个普查街区组到数千家商店的数百万次移动跟踪访问数据。

Result: 高收入与跨品牌和价格水平的多样化消费显著相关，在必需品消费、纽约市等情况中关联较弱，该关联在教育程度和得克萨斯州也成立。

Conclusion: 消费多样性与收入等社会经济地位指标的关联不能用简单地理约束解释，更深层的社会和文化因素在起作用。

Abstract: Consumption practices are determined by a combination of economic, social, and cultural forces. We posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestyle-based niche consumption. We provide empirical evidence for this diversity hypothesis by analysing millions of mobile-tracked visits from thousands of Census Block Groups to thousands of stores in New York State. The results show that high income is significantly associated with diverse consumption across brands and price levels. The associations between diversity and income persist but are less prominent for necessity-based consumption and for the densely populated and demographically diverse New York City. The associations replicate for education as an alternative measure of socioeconomic status and for the state of Texas. We further illustrate that the associations cannot be explained by simple geographic constraints, including the neighbourhoods' demographic diversity, the residents' geographic mobility and the stores' local availability, so deeper social and cultural factors must be at play.

</details>


### [251] [Life cycle assessment tools for road design: analysing linearity assumptions](https://arxiv.org/abs/2506.13896)
*Nikolaos Kalyviotis*

Main category: econ.GN

TL;DR: 本文回顾道路运输生命周期评估工具，指出其基于线性假设的局限及未与设计流程集成的问题，通过案例研究建议探索非线性关系用于可持续道路设计。


<details>
  <summary>Details</summary>
Motivation: 道路设计对减排至关重要，需评估现有道路运输生命周期评估工具以助力减排。

Method: 回顾现有运输生命周期评估工具，关注数据来源、排放估算方法、软件及工具局限性；进行案例研究。

Result: 生命周期评估分析存在线性假设错误，现有工具未与设计流程集成；案例研究发现道路面积与排放显著相关，坡度调整可减排，土壤类型影响排放。

Conclusion: 未来研究应探索非线性关系用于可持续道路设计。

Abstract: Road infrastructure significantly impacts how people move and live and the emissions associated with travel behaviour. The design of roads is crucial in mitigating emissions. This paper reviews existing transport life cycle assessment tools that have been developed by various entities and can be used for roads. The review focuses on data sources used in the analysis, methods of estimating carbon dioxide emissions, the underlying software that is used to make the estimates, and any limitations of the tools. A critical issue identified in life cycle assessment analysis is the erroneous assumption that relationships within the assessed systems are linear. The current tools focusing on transport infrastructure assessment were developed based on the linear assumptions and limitations of the life cycle assessment analysis. A significant research gap identified is that existing life cycle assessment tools are not integrated with the design process. The analysis is an add-on process to design and the results of an assessment are not then used iteratively to enhance the design. A case study on aggregate road design found that road area significantly correlates with emissions, slope adjustments reduce emissions, and soil type impacts emissions, suggesting future research should explore non-linear relationships for sustainable road design.

</details>


### [252] [The Anatomy of India's Industrial Interdependencies: 7-Digit Product-Level Supply-Use and Input-Output Tables from ASI Data (2016-2022) with a Case Study of the Mobile Phone Sector](https://arxiv.org/abs/2506.13936)
*Sourish Dutta*

Main category: econ.GN

TL;DR: 本文利用2016 - 2022年印度工业年度调查微观数据构建7位产品级高分辨率供给使用表和对称投入产出表，通过案例展示其分析能力，为印度经济分析提供资源。


<details>
  <summary>Details</summary>
Motivation: 构建适用于印度经济的高分辨率供给使用表和对称投入产出表，用于分析印度经济各部门相互依存关系、产业政策有效性及全球价值链参与情况。

Method: 利用工业年度调查微观数据，构建详细投入产出流，通过NPCMS - NIC一致性协调数据，采用产业技术假设将供给使用表转换为对称投入产出表。

Result: 构建了高分辨率投入产出表框架，以手机制造业为例，发现产出增长、进口替代、出口增加、国内增加值与国外增加值份额变化、就业增长等情况。

Conclusion: 构建的表格是重要的方法进步，为印度经济相关分析提供了宝贵的实证资源。

Abstract: Outlines the construction of high-resolution, 7-digit product-level Supply-Use Tables (SUTs) and symmetric Input-Output Tables (IOTs) for the Indian economy, leveraging microdata from the Annual Survey of Industries (ASI) for the period 2016-2022. We delineate a robust methodology encompassing the generation of detailed input and output flows, with a particular focus on the reconciliation of data from registered and unregistered manufacturing sectors through a meticulously developed NPCMS-NIC concordance. The critical transformation from the often-rectangular SUTs to square, symmetric product-by-product IOTs is explicated using the Industry Technology Assumption, a choice justified by its suitability for handling by-products prevalent in a diverse manufacturing landscape. The analytical prowess of this newly constructed high-resolution IOT framework is then demonstrated through its application to assess key economic impacts, specifically the Domestic Value Added (DVA) generated and the employment supported by production and export activities. A detailed case study of India's rapidly evolving mobile phone manufacturing sector (NPCMS 4722200) for the 2016-2022 period reveals profound structural shifts: significant output growth coupled with notable import substitution, a remarkable surge in exports, and a dynamic evolution in the DVA versus Foreign Value Added (FVA) shares, particularly in export-oriented production. The analysis further uncovers substantial employment growth, albeit with an increasing reliance on contractual labour and a heartening rise in female participation in the workforce. These meticulously constructed tables represent a significant methodological advancement and provide an invaluable empirical resource for nuanced analysis of sectoral interdependencies, the efficacy of industrial policy, and the complex dynamics of India's engagement with global value chains.

</details>


### [253] [The Bones and Shapes of the Phillips Curve](https://arxiv.org/abs/2506.14030)
*Hanyuan Jiang*

Main category: econ.GN

TL;DR: 使用MSA层面面板数据，用2SLS工具变量策略和阈值模型分析美国菲利普斯曲线在疫情期间及之后斜率是否改变。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情重新引发对美国菲利普斯曲线的讨论，要分析曲线斜率在疫情期间及之后是否改变。

Method: 采用Two - Stage Least Squares (2SLS)工具变量策略和转移份额工具，通过阈值模型区分结构斜率稳定性和状态依赖的非线性。

Result: 未提及具体结果

Conclusion: 区分结构权衡变化和非线性激活能为反通胀的失业成本提供关键见解。

Abstract: The COVID-19 pandemic reignited debate on the U.S. Phillips curve. Using MSA-level panel data (2001-2024), we employ a Two-Stage Least Squares (2SLS) instrumental variable strategy with a shift-share instrument to estimate core non-tradable inflation's response to a v/u-based slack measure. We distinguish structural slope stability from state-dependent non-linearities via a threshold model. Our analysis addresses whether the slope of the Phillips Curve changed during and after the Pandemic in the United States by evaluating if recent inflation dynamics reflect an altered structural trade-off ("bones") or the activation of non-linear "shapes" in response to extreme labor market tightness. This distinction offers critical insights into the unemployment cost of disinflation.

</details>


### [254] [Numerical evaluation of deliberative discussions of the UK food system: stimuli, demographics, and opinion reversion](https://arxiv.org/abs/2506.14102)
*John Buckell,Thomas Hancock*

Main category: econ.GN

TL;DR: 本文开发数值方法记录公众对食品系统改革责任的偏好，分析不同利益相关者责任感知及变化，结果可支持定性分析和政策制定。


<details>
  <summary>Details</summary>
Motivation: 现有审议过程定性报告常不清晰精确，可能降低其对政策制定者的价值，因此开发数值方法作为补充。

Method: 以食品对话项目数据为基础，让参与者在工作坊中两次对5类利益相关者改变食品系统的责任进行0 - 10分评分，分析责任感知及变化。

Result: 政府责任感知最高，农民最低；个体责任感知变化最大，食品行业最小；发现存在逆转效应，且在打算投票者中更明显。

Conclusion: 结果可支持定性分析、为食品系统政策制定提供信息，该方法可用于其他审议过程进行统计评估。

Abstract: There is increasing acknowledgement - including from the UK government - of the benefit of employing deliberative processes (deliberative fora, citizens' juries, etc.). Evidence suggests that the qualitative reporting of deliberative fora are often unclear or imprecise. If this is the case, their value to policymakers could be diminished. In this study we develop numerical methods of deliberative processes to document people's preferences, as a complement to qualitative analysis. Data are taken from the Food Conversation, a nationwide public consultation on reformations of the food system comprising 345 members of the general public. Each participant attended 5 workshops, each with differing stimuli covering subtopics of the food system. In each workshop, individuals twice reported responsibility, from 0-10, for changing the food system for 5 stakeholders (governments, the food industry, supermarkets, farmers, individuals). Analyses examined individuals' perceptions of food system change responsibility. Governments were most responsible and farmers least so. We assessed variation by workshop content, and by demographics. Reported responsibility changed most for individuals, and changed least for the food industry. We devise a model to document a reversion effect, where shifts in perceptions on responsibility that occurred during workshops waned over time; this was strongest among those who intended to vote (rather than not to). These results can support qualitative analyses and inform food system policy development. These methods are readily adopted for any such deliberative process, allowing for statistical evaluation of whether they can induce opinion change.

</details>


### [255] [An advanced reliability reserve incentivizes flexibility investments while safeguarding the electricity market](https://arxiv.org/abs/2506.14664)
*Franziska Klaucke,Karsten Neuhoff,Alexander Roth,Wolf-Peter Schill,Leon Stolle*

Main category: econ.GN

TL;DR: 本文分析集中式容量市场和高激活价格的先进可靠性备用对需求侧灵活性技术投资的影响，认为先进可靠性备用更优，政策制定者应考虑其在容量机制决策中的应用。


<details>
  <summary>Details</summary>
Motivation: 为保障电力供应安全，多国采用或讨论引入容量机制，同时可再生能源发展增加对电力系统灵活性需求，因此分析不同容量机制对需求侧灵活性技术投资的影响。

Method: 以2030年德国为例，使用开源容量扩展模型，纳入工业、工艺热和区域供热等需求侧灵活性潜力。

Result: 集中式容量市场有效限制批发电力市场峰值价格，降低需求侧灵活性投资激励；先进可靠性备用促使更高灵活性投资，总体供电成本和供应安全水平相近。

Conclusion: 先进可靠性备用可创造灵活性技术学习环境，引入更快且更灵活，政策制定者应在容量机制决策中考虑这一概念。

Abstract: To ensure security of supply in the power sector, many countries are already using or discussing the introduction of capacity mechanisms. Two main types of such mechanisms include capacity markets and capacity reserves. Simultaneously, the expansion of variable renewable energy sources increases the need for power sector flexibility, for which there are promising yet often under-utilized options on the demand side. In this paper, we analyze how a centralized capacity market and an advanced reliability reserve with a moderately high activation price affect investments in demand-side flexibility technologies. We do so for a German case study of 2030, using an open-source capacity expansion model and incorporating detailed demand-side flexibility potentials across industry, process heat, and district heating. We show that a centralized capacity market effectively caps peak prices in the wholesale electricity market and thus reduces incentives for investments in demand-side flexibility options. The advanced reliability reserve induces substantially higher flexibility investments while leading to similar overall electricity supply costs and ensuring a similar level of security of supply. The advanced reliability reserve could thus create a learning environment for flexibility technologies to support the transition to climate neutral energy systems. Additionally, an advanced reliability reserve could be introduced faster and is more flexible than a centralized capacity market. While concrete design parameters are yet to be specified, we argue that policymakers should consider the reliability reserve concept in upcoming decision on capacity mechanisms in Germany and beyond.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [256] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/abs/2506.13807)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Ujjwal Baid,Hendrik Möller,Josef A. Buchner,Felix Steinbauer,Eva Oswald,Ezequiel de la Rosa,Ivan Ezhov,Constantin von See,Jan Kirschke,Anton Schmick,Sarthak Pati,Akis Linardos,Carla Pitarch,Sanyukta Adap,Jeffrey Rudie,Maria Correia de Verdier,Rachit Saluja,Evan Calabrese,Dominic LaBella,Mariam Aboian,Ahmed W. Moawad,Nazanin Maleki,Udunna Anazodo,Maruf Adewole,Marius George Linguraru,Anahita Fathi Kazerooni,Zhifan Jiang,Gian Marco Conte,Hongwei Li,Juan Eugenio Iglesias,Spyridon Bakas,Benedikt Wiestler,Marie Piraud,Bjoern Menze*

Main category: eess.IV

TL;DR: 为加速BraTS挑战中算法和模型的传播，引入开源Python包BraTS orchestrator，可无缝访问先进算法，方便不同用户使用。


<details>
  <summary>Details</summary>
Motivation: BraTS挑战中开发的算法和模型在科研和临床界应用有限，需加速其传播。

Method: 引入开源Python包BraTS orchestrator，提供直观教程，抽象深度学习复杂性。

Result: 该包可让不同用户轻松部署获奖算法进行推理。

Conclusion: BraTS orchestrator使BraTS社区的专业知识更易获取，惠及更广泛的神经放射和神经肿瘤领域受众。

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [257] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Main category: eess.IV

TL;DR: 提出基于短波红外光谱的双模态AI框架用于无创血糖监测，评估后证明有良好效果并具应用潜力。


<details>
  <summary>Details</summary>
Motivation: 实现可靠的连续无创血糖监测。

Method: 第一模态用多波长SWIR成像系统结合CNN捕捉与葡萄糖吸收相关的空间特征；第二模态用紧凑型光电二极管电压传感器和机器学习回归器处理归一化光信号，在合成血液模型和仿皮肤材料上进行评估。

Result: CNN在650 nm处平均绝对百分比误差为4.82%，在Clarke误差网格中A区覆盖率达100%；光电二极管系统A区准确率达86.4%。

Conclusion: 该框架是兼顾临床准确性、成本效益和可穿戴集成的先进解决方案，为可靠的连续无创血糖监测铺平道路。

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [258] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: 研究对比多模态和基于CNN的方法对乳腺钼靶密度自动分类，发现端到端微调的CNN模型性能更强。


<details>
  <summary>Details</summary>
Motivation: 乳腺钼靶密度分类对癌症风险评估很重要，但主观解读和观察者间差异使其具有挑战性。

Method: 使用BI - RADS系统，在零样本分类、基于文本描述的线性探测、基于数字标签的微调三种学习场景下，评估BioMedCLIP和ConvNeXt。

Result: 零样本分类表现一般，微调的ConvNeXt模型优于BioMedCLIP线性探测，线性探测有潜力但不如全微调。

Conclusion: 尽管多模态学习有前景，但端到端微调的CNN模型在医学影像分类表现更好，未来放射学应用需更详细文本表示和特定领域适配。

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


### [259] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: 本文提出无监督训练方法用于颌骨放射性骨坏死（ONJ）影像扫描异常识别，在模拟和真实患者数据上实现成功分割，能提供快速初始分割方案。


<details>
  <summary>Details</summary>
Motivation: ONJ成像中标记数据稀缺，使有监督训练不可行，因此要开发无监督训练方法自动识别影像扫描中的异常。

Method: 提出两阶段训练流程，第一阶段训练VQ - GAN准确重建正常受试者，第二阶段应用随机立方体掩码和ONJ特定掩码训练新编码器恢复数据。

Result: 所提方法在模拟和真实患者数据上都实现了成功分割。

Conclusion: 该方法提供了快速的初始分割解决方案，减轻手动标记负担，结合手动调整后处理有潜力直接用于3D打印。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [260] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Main category: eess.IV

TL;DR: 本文提出orGAN系统生成高保真、带注释的手术出血图像，解决医学影像深度学习数据问题，评估中取得高准确率，推动AI在手术中应用。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习存在数据多样性有限、伦理问题等障碍，手术中出血检测和定位因缺乏高质量数据集而具挑战性。

Method: 提出基于GAN的orGAN系统，利用小的“模拟器官”数据集，结合Relational Positional Learning的StyleGAN模拟出血事件并标记坐标，用LaMa-based inpainting模块恢复术前画面以实现精确注释。

Result: orGAN与模拟器官图像的平衡数据集在手术环境中检测准确率达90%，帧级准确率达99%。

Conclusion: 尽管开发数据存在器官形态多样性不足和术中伪影问题，但orGAN显著推动了符合伦理、高效且经济的逼真注释出血数据集的创建，支持AI在手术实践中的更广泛应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [261] [Towards an Approach for Evaluating the Impact of AI Standards](https://arxiv.org/abs/2506.13839)
*Julia Lane*

Main category: cs.CY

TL;DR: 文章提出一种分析方法评估AI标准影响，借鉴其他领域评估框架，介绍评估设计背景、框架、产出结果及评估有效性方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏衡量AI标准化活动对创新和信任目标影响的正式或共享方法，需评估AI标准是否达成目标。

Method: 借鉴其他领域成功且经过测试的评估框架、工具和指标，描述评估AI标准的可能方法。

Result: 提出可用于衡量、评估AI标准达成目标程度的分析方法。

Conclusion: 文档旨在引发各利益相关方就该方法评估AI标准有效性、实用性和相对价值的潜力展开讨论。

Abstract: There have been multiple calls for investments in the development of AI standards that both preserve the transformative potential and minimize the risks of AI. The goals of AI standards, particularly with respect to AI data, performance, and governance, are to promote innovation and public trust in systems that use AI. However, there is a lack of a formal or shared method to measure the impact of these standardization activities on the goals of innovation and trust. This concept paper proposes an analytical approach that could inform the evaluation of the impact of AI standards. The proposed approach could be used to measure, assess, and eventually evaluate the extent to which AI standards achieve their stated goals, since most Standards Development Organizationss do not track the impact of their standards once completed. It is intended to stimulate discussions with a wide variety of stakeholders, including academia and the standards community, about the potential for the approach to evaluate the effectiveness, utility, and relative value of AI standards. The document draws on successful and well-tested evaluation frameworks, tools, and metrics that are used for monitoring and assessing the effect of programmatic interventions in other domains to describe a possible approach. It begins by describing the context within which an evaluation would be designed, and then introduces a standard evaluation framework. These sections are followed by a description of what outputs and outcomes might result from the adoption and implementation of AI standards and the process whereby those AI standards are developed . Subsequent sections provide an overview of how the effectiveness of AI standards might be assessed and a conclusion.

</details>


### [262] [hyperFA*IR: A hypergeometric approach to fair rankings with finite candidate pool](https://arxiv.org/abs/2506.14349)
*Mauritz N. Cartier van Dissel,Samuel Martin-Gutierrez,Lisette Espín-Noboa,Ana María Jaramillo,Fariba Karimi*

Main category: cs.CY

TL;DR: 提出hyperFA*IR框架评估和执行有限候选集排名公平性，对比二项式模型有优势，还提出算法和定义了平权政策。


<details>
  <summary>Details</summary>
Motivation: 现有排名算法的群体公平性框架在处理排名随机性和有限候选集方面有局限，需更好方法确保排名公平性。

Method: 提出基于超几何分布的hyperFA*IR框架，对比二项式模型，提出基于蒙特卡罗的算法，通过在抽样过程引入权重定义平权政策。

Result: 理论和实证表明该方法能更准确重现有限总体抽样的统计特性，所提算法可有效检测不公平排名。

Conclusion: hyperFA*IR框架能更好评估和执行有限候选集排名的公平性，且有相应有效算法和可定义平权政策。

Abstract: Ranking algorithms play a pivotal role in decision-making processes across diverse domains, from search engines to job applications. When rankings directly impact individuals, ensuring fairness becomes essential, particularly for groups that are marginalised or misrepresented in the data. Most of the existing group fairness frameworks often rely on ensuring proportional representation of protected groups. However, these approaches face limitations in accounting for the stochastic nature of ranking processes or the finite size of candidate pools. To this end, we present hyperFA*IR, a framework for assessing and enforcing fairness in rankings drawn from a finite set of candidates. It relies on a generative process based on the hypergeometric distribution, which models real-world scenarios by sampling without replacement from fixed group sizes. This approach improves fairness assessment when top-$k$ selections are large relative to the pool or when protected groups are small. We compare our approach to the widely used binomial model, which treats each draw as independent with fixed probability, and demonstrate$-$both analytically and empirically$-$that our method more accurately reproduces the statistical properties of sampling from a finite population. To operationalise this framework, we propose a Monte Carlo-based algorithm that efficiently detects unfair rankings by avoiding computationally expensive parameter tuning. Finally, we adapt our generative approach to define affirmative action policies by introducing weights into the sampling process.

</details>


### [263] [Contemporary AI foundation models increase biological weapons risk](https://arxiv.org/abs/2506.13798)
*Roger Brent,T. Greg McKelvey*

Main category: cs.CY

TL;DR: 现有基础AI模型安全评估低估生物武器开发风险，通过案例分析和框架应用发现高级AI模型能指导恢复活脊髓灰质炎病毒，呼吁改进评估基准。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展引发生物武器开发担忧，而现有基础AI模型安全评估低估此风险。

Method: 通过实际案例挑战隐性知识假设，确定生物武器开发“成功要素”框架，并应用于高级AI模型测试。

Result: 高级AI模型Llama 3.1 405B、ChatGPT - 4o和Claude 3.5 Sonnet能准确指导从商业合成DNA中恢复活脊髓灰质炎病毒。

Conclusion: 应改进评估基准，但有意义的实施窗口可能已关闭。

Abstract: The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.

</details>


### [264] [Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases](https://arxiv.org/abs/2506.13805)
*Bonam Mingole,Aditya Majumdar,Firdaus Ahmed Choudhury,Jennifer L. Kraschnewski,Shyam S. Sundar,Amulya Yadav*

Main category: cs.CY

TL;DR: 本文通过大学竞赛评估大语言模型（LLMs）回答日常健康问题的有效性，发现76%的回答准确，还探讨了RAG版本及进行定性分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视LLMs在回答日常健康问题上的实际效果评估，本文旨在填补此研究空白。

Method: 举办大学竞赛，采用众包方法，让34名参与者用212个健康问题询问4个公开LLMs，由9名医生评估回答，还研究RAG版本并访谈7名医学专家。

Result: 医生认为212个LLMs回答中平均76%准确，还获取了关于RAG版本及定性的相关发现。

Conclusion: 为理解LLMs在现实日常健康交流中的表现提供更实际的依据。

Abstract: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.

</details>


### [265] [Students' Reliance on AI in Higher Education: Identifying Contributing Factors](https://arxiv.org/abs/2506.13845)
*Griffin Pitts,Neha Rani,Weedguet Mildort,Eva-Marie Cook*

Main category: cs.CY

TL;DR: 研究大学生对AI工具依赖模式的影响因素，为促进合理依赖提供见解。


<details>
  <summary>Details</summary>
Motivation: 人工智能工具在教育场景的使用引发学生过度依赖担忧，研究潜在影响因素。

Method: 结合前后测调查与控制实验任务，让学生借助AI助手解决编程问题。

Result: 合理依赖与编程自我效能感、编程素养和认知需求显著相关，与任务后信任和满意度负相关；过度依赖与任务后信任和满意度显著相关；依赖不足与编程素养等负相关。

Conclusion: 研究结果可为促进合理依赖AI工具的干预措施提供依据，对课程和教育技术中AI的整合有启示。

Abstract: The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.

</details>


### [266] [The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI](https://arxiv.org/abs/2506.13818)
*Marcelle Momha*

Main category: cs.CY

TL;DR: 合成数据使用增多，产生信任与问责缺失问题，需调整现有政策法律框架应对。


<details>
  <summary>Details</summary>
Motivation: 探讨合成数据生成对隐私和政策制定的影响，解决AI代理信任与问责问题。

Method: 提出对现有框架进行有针对性修订，将合成数据作为独特监管类别。

Result: 未提及具体研究结果。

Conclusion: 无需创建全新政策或法律制度，对现有框架进行针对性修订是最实际的方法。

Abstract: Synthetic data, which is artificially generated and intelligently mimicking or supplementing the real-world data, is increasingly used. The proliferation of AI agents and the adoption of synthetic data create a synthetic mirror that conceptualizes a representation and potential distortion of reality, thus generating trust and accountability deficits. This paper explores the implications for privacy and policymaking stemming from synthetic data generation, and the urgent need for new policy instruments and legal framework adaptation to ensure appropriate levels of trust and accountability for AI agents relying on synthetic data. Rather than creating entirely new policy or legal regimes, the most practical approach involves targeted amendments to existing frameworks, recognizing synthetic data as a distinct regulatory category with unique characteristics.

</details>


### [267] [Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor](https://arxiv.org/abs/2506.14652)
*Alexandra Olteanu,Su Lin Blodgett,Agathe Balayn,Angelina Wang,Fernando Diaz,Flavio du Pin Calmon,Margaret Mitchell,Michael Ekstrand,Reuben Binns,Solon Barocas*

Main category: cs.CY

TL;DR: 当前AI研究对严谨性理解狭窄，需更广泛概念，并提供交流框架。


<details>
  <summary>Details</summary>
Motivation: 解决负责任AI社区对当前AI研究因严谨性概念狭窄产生的担忧。

Method: 提出应在方法严谨性基础上，增加认知、规范、概念、报告和解释等多方面严谨性。

Result: 提出更广泛的严谨AI研究和实践应包含的方面。

Conclusion: 提供语言和框架，促进AI社区相关利益者的对话。

Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [268] [Accurate and scalable exchange-correlation with deep learning](https://arxiv.org/abs/2506.14665)
*Giulia Luise,Chin-Wei Huang,Thijs Vogels,Derk P. Kooi,Sebastian Ehlert,Stephanie Lanius,Klaas J. H. Giesbertz,Amir Karton,Deniz Gunceler,Megan Stanley,Wessel P. Bruinsma,Lin Huang,Xinran Wei,José Garrido Torres,Abylay Katbashev,Bálint Máté,Sékou-Oumar Kaba,Roberto Sordillo,Yingrong Chen,David B. Williams-Young,Christopher M. Bishop,Jan Hermann,Rianne van den Berg,Paola Gori-Giorgi*

Main category: physics.chem-ph

TL;DR: 提出基于深度学习的XC泛函Skala，能在小分子雾化能上达化学精度，兼顾计算效率，且随训练数据增加性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有XC泛函用手工特征构建，难以兼顾精度和计算效率，无法在化学精度上进行实验预测建模。

Method: 直接从数据学习表示，绕过昂贵手工设计特征，用大量高精度参考数据训练。

Result: Skala在小分子雾化能上达化学精度，兼顾计算效率；加入额外数据后在主族化学中精度与最佳混合泛函相当。

Conclusion: 随着训练数据集扩大，Skala将提升第一性原理模拟预测能力。

Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [269] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Main category: eess.AS

TL;DR: 本文应用半监督学习（SSL）预测视频会议中非流畅或不愉快时刻，SSL表现优于监督学习（SL），展示了高效标注框架。


<details>
  <summary>Details</summary>
Motivation: 视频会议中负面体验时刻研究不足，且训练监督学习模型需昂贵手动数据标注。

Method: 应用半监督学习，利用有标签和无标签片段训练多模态（音频、面部、文本）深度特征。

Result: 模态融合的协同训练SSL的ROC - AUC达到0.9，F1分数为0.6，在相同数量有标签数据下比SL模型性能高4%；仅8%有标签数据的最佳SSL模型达到SL模型全量数据性能的96%。

Conclusion: 提出了一种用于建模视频会议体验的高效标注框架。

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [270] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)
*Ivania Donoso-Guzmán,Kristýna Sirka Kacafírková,Maxwell Szymanski,An Jacobs,Denis Parra,Katrien Verbert*

Main category: cs.HC

TL;DR: 当前可解释人工智能（XAI）方法实用价值在现实中未充分探索验证，研究通过系统综述，给出当前评估实践总结、属性关系洞察及评估框架和指南。


<details>
  <summary>Details</summary>
Motivation: XAI方法实用价值在现实中探索不足，缺乏用户评估设计准则，需要开发表征用户体验的框架和评估策略指南。

Method: 对来自五个数据库的82项医疗保健领域评估AI生成解释的用户研究进行系统综述，用预定义编码方案和迭代归纳代码分析。

Result: 总结当前评估实践，强调以人为主的方法；洞察解释属性间关系；更新框架并给出可操作指南。

Conclusion: 所提出的框架和指南可支持跨学科团队为特定应用场景设计和实施有效的XAI系统评估策略。

Abstract: Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.

</details>


### [271] [StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework](https://arxiv.org/abs/2506.14159)
*Shayan Talaei,Meijin Li,Kanu Grover,James Kent Hippler,Diyi Yang,Amin Saberi*

Main category: cs.HC

TL;DR: 本文介绍了名为StorySage的用户驱动软件系统，用于支持自传写作，实验显示其性能良好，还贡献了新架构和对人机合作的见解。


<details>
  <summary>Details</summary>
Motivation: 解决个人记忆分散难组织成连贯自传，现有对话写作助手难以捕捉个人记忆和撰写完整传记的问题。

Method: 引入StorySage系统，采用由访谈者、会话记录员、规划者、章节撰写者和会话协调员组成的多智能体框架，迭代收集用户记忆、更新自传并规划未来对话。

Result: 实验模拟中，StorySage能处理多会话并捕捉用户记忆；用户研究显示，与基线相比，StorySage有更好的对话流程、叙事完整性和用户满意度。

Conclusion: StorySage为自传写作提供了新架构，揭示了多智能体系统如何增强人机创意合作。

Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.

</details>


### [272] [Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.14196)
*Jiayue Melissa Shi,Keran Wang,Dong Whi Yoo,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究聚焦AD/ADRD患者家庭照顾者心理健康，通过访谈明确问题及发展阶段，提出技术改进方向，为设计干预措施提供基础。


<details>
  <summary>Details</summary>
Motivation: AD/ADRD患者家庭照顾者面临心理健康挑战，现有支持系统忽视其需求变化，需深入了解情况。

Method: 对25位AD/ADRD患者家庭照顾者进行半结构化访谈。

Result: 确定心理健康挑战的关键因果，绘制照顾过程三阶段心理健康演变图谱，了解现有技术改进建议。

Conclusion: 研究结果为设计支持照顾者心理健康的动态、分阶段干预措施提供基础，惠及照顾者和患者。

Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.

</details>


### [273] [Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains](https://arxiv.org/abs/2506.14567)
*Emanuel Moss,Elizabeth Watkins,Christopher Persaud,Passant Karunaratne,Dawn Nafus*

Main category: cs.HC

TL;DR: 本文分析集成电路设计领域工程师使用生成式AI工具的情况，识别准确性的作用及使用中遇到的问题，指出控制交互上下文是最大挑战并给出缓解建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在工程工作流中更普遍，探讨高精度领域工作者如何对错误保持警惕以及使用工具可能遇到的问题。

Method: 分析硬件和软件工程师及其合作者的访谈内容。

Result: 梳理使用工具时遇到的问题并映射到生成式AI系统元素，发现控制工程师与工具交互的上下文是最大挑战。

Conclusion: 建议通过增强交互式控制上下文的能力来缓解这一问题。

Abstract: Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.

</details>


### [274] [StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery](https://arxiv.org/abs/2506.14670)
*Jina Kim,Leeje Jang,Yao-Yi Chiang,Guanyu Wang,Michelle Pasco*

Main category: cs.HC

TL;DR: 提出StreetLens工作流用于邻里环境评估，使研究人员能利用VLM灵活开展研究。


<details>
  <summary>Details</summary>
Motivation: 传统邻里研究方法耗时且依赖专家干预，现有自动化方法缺乏适应性，需改进。

Method: 提出StreetLens工作流，将社会科学专业知识嵌入VLM，基于既定访谈协议提问，检索街景图像并生成语义注释，支持整合调查数据。

Result: 提供Google Colab笔记本，方便研究人员使用公共或自定义街景数据集。

Conclusion: StreetLens推动向灵活、自主的AI系统转变，加速和扩展邻里研究。

Abstract: Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.

</details>


### [275] [Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach](https://arxiv.org/abs/2506.14677)
*Yingchao Li*

Main category: cs.HC

TL;DR: 本文提出以人为中心、实时、用户自适应的语音到手语动画系统，结合Transformer运动生成与可编辑JSON层，实验表明其能提升多方面表现，可用于实时通信和教育。


<details>
  <summary>Details</summary>
Motivation: 克服先前手语技术的关键局限，增强自然度、表现力和用户自主性。

Method: 集成基于Transformer的运动生成与可编辑JSON中间层，利用流式Conformer编码器和自回归Transformer - MDN解码器，引入人在环优化循环。

Result: 对20名聋人手语使用者和5名口译员的实验显示，可编辑界面和参与式反馈显著提升理解、自然度、可用性和信任度，降低认知负荷，每帧推理时间低于20毫秒。

Conclusion: 技术和参与式创新共同推动手语技术实现可访问、可解释和用户自适应的人工智能。

Abstract: This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [276] [Generating uniform linear extensions using few random bits](https://arxiv.org/abs/2506.14725)
*Mark Huber*

Main category: cs.CC

TL;DR: 本文提出一种新方法，在生成偏序的均匀线性扩展方面，平均所需操作和独立同分布公平随机比特数优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 改进现有生成偏序均匀线性扩展方法，减少操作和随机比特使用量。

Method: 提出一种新方法来生成均匀线性扩展。

Result: 新方法平均仅需2.75 n^3 ln(n)次操作和1.83 n^3 ln(n)个独立同分布公平随机比特。

Conclusion: 新方法在生成偏序均匀线性扩展上优于当前最佳方法。

Abstract: A \emph{linear extension} of a partial order \(\preceq\) over items \(A = \{ 1, 2, \ldots, n \}\) is a permutation \(σ\) such that for all \(i < j\) in \(A\), it holds that \(\neg(σ(j) \preceq σ(i))\). Consider the problem of generating uniformly from the set of linear extensions of a partial order. The best method currently known uses \(O(n^3 \ln(n))\) operations and \(O(n^3 \ln(n)^2)\) iid fair random bits to generate such a permutation. This paper presents a method that generates a uniform linear extension using only \(2.75 n^3 \ln(n)\) operations and \( 1.83 n^3 \ln(n) \) iid fair bits on average.

</details>


### [277] [The Complexity of Counting Small Sub-Hypergraphs](https://arxiv.org/abs/2506.14081)
*Marco Bressan,Julian Brinkmann,Holger Dell,Marc Roth,Philip Wellnitz*

Main category: cs.CC

TL;DR: 本文研究超图子图计数问题，对其复杂度进行完整分类，给出固定参数可处理的条件，并与图的情况作对比。


<details>
  <summary>Details</summary>
Motivation: 超图子图计数问题此前几乎被忽略，本文旨在填补这一研究空白。

Method: 假设指数时间假设，引入新的图参数‘分数余独立边覆盖数’，对超图子图计数问题复杂度进行分析。

Result: 证明#Sub($\mathcal{H}$)在$\mathcal{H}$有界分数余独立边覆盖数时固定参数可处理，#IndSub($\mathcal{H}$)在$\mathcal{H}$有界分数边覆盖数时固定参数可处理，且这两种情况不太可能是多项式时间的。

Conclusion: 研究结果涵盖图的已有结果作为特例，显示出与图的情况在复杂度上的分离。

Abstract: Subgraph counting is a fundamental and well-studied problem whose computational complexity is well understood. Quite surprisingly, the hypergraph version of subgraph counting has been almost ignored. In this work, we address this gap by investigating the most basic sub-hypergraph counting problem: given a (small) hypergraph $H$ and a (large) hypergraph $G$, compute the number of sub-hypergraphs of $G$ isomorphic to $H$. Formally, for a family $\mathcal{H}$ of hypergraphs, let #Sub($\mathcal{H}$) be the restriction of the problem to $H \in \mathcal{H}$; the induced variant #IndSub($\mathcal{H}$) is defined analogously. Our main contribution is a complete classification of the complexity of these problems. Assuming the Exponential Time Hypothesis, we prove that #Sub($\mathcal{H}$) is fixed-parameter tractable if and only if $\mathcal{H}$ has bounded fractional co-independent edge-cover number, a novel graph parameter we introduce. Moreover, #IndSub($\mathcal{H}$) is fixed-parameter tractable if and only if $\mathcal{H}$ has bounded fractional edge-cover number. Both results subsume pre-existing results for graphs as special cases. We also show that the fixed-parameter tractable cases of #Sub($\mathcal{H}$) and #IndSub($\mathcal{H}$) are unlikely to be in polynomial time, unless respectively #P = P and Graph Isomorphism $\in$ P. This shows a separation with the special case of graphs, where the fixed-parameter tractable cases are known to actually be in polynomial time.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [278] [Evolutionary chemical learning in dimerization networks](https://arxiv.org/abs/2506.14006)
*Alexei V. Tkachenko,Bortolo Matteo Mognetti,Sergei Maslov*

Main category: cond-mat.stat-mech

TL;DR: 提出基于竞争性二聚化网络（CDNs）的化学学习框架，可体外训练执行复杂学习任务，是模拟物理计算有前景的平台。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需数字硬件和显式参数调整就能实现复杂学习任务的化学学习框架。

Method: 通过定向进化在体外训练CDNs，利用基于DNA组件的突变、选择和扩增训练协议；采用对比增强损失函数指导。

Result: CDNs能稳健区分嘈杂输入模式，分类器输出对比度强，输入输出互信息高；与计算机模拟梯度下降训练性能密切相关。

Conclusion: CDNs是模拟物理计算的有前景平台，能连接合成生物学和机器学习，推动自适应、节能分子计算系统发展。

Abstract: We present a novel framework for chemical learning based on Competitive Dimerization Networks (CDNs) - systems in which multiple molecular species, e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show that these networks can be trained in vitro through directed evolution, enabling the implementation of complex learning tasks such as multiclass classification without digital hardware or explicit parameter tuning. Each molecular species functions analogously to a neuron, with binding affinities acting as tunable synaptic weights. A training protocol involving mutation, selection, and amplification of DNA-based components allows CDNs to robustly discriminate among noisy input patterns. The resulting classifiers exhibit strong output contrast and high mutual information between input and output, especially when guided by a contrast-enhancing loss function. Comparative analysis with in silico gradient descent training reveals closely correlated performance. These results establish CDNs as a promising platform for analog physical computation, bridging synthetic biology and machine learning, and advancing the development of adaptive, energy-efficient molecular computing systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [279] [A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems](https://arxiv.org/abs/2506.13950)
*Dimitrios G. Patsatzis,Nikolaos Kazantzis,Ioannis G. Kevrekidis,Constantinos Siettos*

Main category: math.NA

TL;DR: 提出混合机器学习方案学习离散映射不变流形构建动力系统降阶模型，经测试该方案优于纯多项式和纯神经网络近似。


<details>
  <summary>Details</summary>
Motivation: 构建动力系统降阶模型，探索更高效准确的学习离散映射不变流形的方法。

Method: 结合多项式级数与浅层神经网络，利用两者优势，用三个基准示例评估方案效率。

Result: 该混合方案在数值近似精度上优于纯多项式近似和独立浅层神经网络近似。

Conclusion: 所提混合机器学习方案在构建动力系统降阶模型方面表现出色，有更好的数值近似精度。

Abstract: We propose a hybrid machine learning scheme to learn -- in physics-informed and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete maps for constructing reduced-order models (ROMs) for dynamical systems. The proposed scheme combines polynomial series with shallow neural networks, exploiting the complementary strengths of both approaches. Polynomials enable an efficient and accurate modeling of ROMs with guaranteed local exponential convergence rate around the fixed point, where, under certain assumptions, the IM is demonstrated to be analytic. Neural networks provide approximations to more complex structures beyond the reach of the polynomials' convergence. We evaluate the efficiency of the proposed scheme using three benchmark examples, examining convergence behavior, numerical approximation accuracy, and computational training cost. Additionally, we compare the IM approximations obtained solely with neural networks and with polynomial expansions. We demonstrate that the proposed hybrid scheme outperforms both pure polynomial approximations (power series, Legendre and Chebyshev polynomials) and standalone shallow neural network approximations in terms of numerical approximation accuracy.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [280] [A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations](https://arxiv.org/abs/2506.13835)
*Masakazu Inoue,Motoshige Sato,Kenichi Tomeoka,Nathania Nah,Eri Hatakeyama,Kai Arulkumaran,Ilya Horiguchi,Shuntaro Sasai*

Main category: q-bio.QM

TL;DR: 本文引入可处理异构电极位置EEG/EMG的神经网络，通过多任务训练在大规模数据集上实现无声语音解码，提高了分类准确率，证明开发实用系统的可行性。


<details>
  <summary>Details</summary>
Motivation: 无声语音解码可提升言语障碍者的沟通便利性，但数据收集困难且实验设置多样，难以获得大规模同质数据集。

Method: 引入能处理异构电极位置EEG/EMG的神经网络，在大规模EEG/EMG数据集上进行多任务训练。

Result: 在健康参与者和言语障碍患者中都提高了单词分类准确率，且在跨语言校准性能上有提升。

Conclusion: 准确率的提升表明开发实用的无声语音解码系统是可行的，尤其对于言语障碍患者。

Abstract: Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [281] [Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms](https://arxiv.org/abs/2506.13865)
*Kasidit Srimahajariyapong,Supanut Thanasilp,Thiparat Chotibut*

Main category: quant-ph

TL;DR: 研究基于无序Ising链淬火的模拟变分量子算法（VQA）ansätze，对比热化相和多体局域（MBL）相表现，提出MBL初始化策略。


<details>
  <summary>Details</summary>
Motivation: 数字门方法构建的参数化量子态存在可扩展性问题，如贫瘠高原，需要寻找更好的VQA方法。

Method: 研究由无序Ising链的M次淬火组成的模拟VQA ansätze，调节无序强度使每次淬火处于热化相或MBL相，分析ansätze的表达能力和损失方差的缩放。

Result: 两个相在大M时达到最大表达能力，但热化相比MBL相在小得多的M时就出现贫瘠高原。

Conclusion: 将量子物相与VQA可训练性联系起来，为模拟硬件VQA的扩展提供实用指南。

Abstract: Variational quantum algorithms (VQAs) promise near-term quantum advantage, yet parametrized quantum states commonly built from the digital gate-based approach often suffer from scalability issues such as barren plateaus, where the loss landscape becomes flat. We study an analog VQA ansätze composed of $M$ quenches of a disordered Ising chain, whose dynamics is native to several quantum simulation platforms. By tuning the disorder strength we place each quench in either a thermalized phase or a many-body-localized (MBL) phase and analyse (i) the ansätze's expressivity and (ii) the scaling of loss variance. Numerics shows that both phases reach maximal expressivity at large $M$, but barren plateaus emerge at far smaller $M$ in the thermalized phase than in the MBL phase. Exploiting this gap, we propose an MBL initialisation strategy: initialise the ansätze in the MBL regime at intermediate quench $M$, enabling an initial trainability while retaining sufficient expressivity for subsequent optimization. The results link quantum phases of matter and VQA trainability, and provide practical guidelines for scaling analog-hardware VQAs.

</details>


### [282] [Hamiltonian Formalism for Comparing Quantum and Classical Intelligence](https://arxiv.org/abs/2506.14456)
*Elija Perrier*

Main category: quant-ph

TL;DR: 为对比经典和量子环境下AGI操作，引入哈密顿形式论描述AGI任务并分解其动力学。


<details>
  <summary>Details</summary>
Motivation: 研究在量子基底上实现AGI，需要数学框架对比其在经典和量子环境下的操作。

Method: 引入哈密顿形式论描述经典和量子AGI任务，将AGI动力学分解为核心功能的哈密顿生成元。

Result: 未提及具体结果。

Conclusion: 该形式论有助于发展精确数学语言，描述量子和经典智能体通过环境交互的差异。

Abstract: The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [283] [A Survey of Physics-Informed AI for Complex Urban Systems](https://arxiv.org/abs/2506.13777)
*En Xu,Huandong Wang,Yunke Zhang,Sibo Li,Yinzhou Tang,Zhilun Zhou,Yuming Lin,Yuan Yuan,Xiaochen Fan,Jingtao Ding,Yong Li*

Main category: physics.soc-ph

TL;DR: 本文对城市应用中物理信息AI方法进行全面综述，分类方法并分析其在八个城市领域应用，指出差距与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 城市系统是复杂系统，结合基于物理的建模和人工智能可提升预测准确性、可解释性和决策能力。

Method: 提出分类法将现有方法分为三种范式和七种代表性方法，并系统研究其在八个城市关键领域的应用。

Result: 分析表明这些方法利用物理定律和数据驱动模型应对城市挑战，提高系统可靠性、效率和适应性。

Conclusion: 综合现有方法及其应用，确定关键差距并规划未来研究方向，为下一代智能城市系统建模奠定基础。

Abstract: Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.

</details>


### [284] [Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents](https://arxiv.org/abs/2506.13783)
*Soyeon Choi,Kangwook Lee,Oliver Sng,Joshua M. Ackerman*

Main category: physics.soc-ph

TL;DR: 研究用GABM测试传染病威胁对生成式智能体社交性的影响，发现接触传染病新闻的智能体社交参与度降低，且能区分传染病与非传染病，凸显GABM探索人类社会动态的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究传染病威胁如何影响生成式智能体的社交性。

Method: 使用由大语言模型驱动的生成式智能体建模（GABM）进行实验测试。

Result: 接触传染病新闻的生成式智能体社交参与度显著降低，能区分传染病与非传染病并选择性减少社交。

Conclusion: GABM作为实验工具在大规模探索复杂人类社会动态方面有潜力。

Abstract: How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [285] [Complete Characterization for Adjustment in Summary Causal Graphs of Time Series](https://arxiv.org/abs/2506.14534)
*Clément Yvernes,Emilie Devijver,Eric Gaussier*

Main category: math.ST

TL;DR: 研究时间序列中多个干预下，仅知摘要因果图时干预可识别性问题，给出调整准则充要条件和判断算法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列中仅知摘要因果图时，多个干预下总因果效应能否用无do公式表示并从观测数据估计的可识别性问题。

Method: 提出调整准则的充要条件，并给出伪线性算法。

Result: 所提调整准则在此设置下是完备的。

Conclusion: 能利用所提准则和算法判断查询是否可识别。

Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [286] [Causal Mediation Analysis with Multiple Mediators: A Simulation Approach](https://arxiv.org/abs/2506.14019)
*Jesse Zhou,Geoffrey T. Wodtke*

Main category: stat.ME

TL;DR: 本文介绍了一种通过模拟潜在结果来估计因果中介分析中多种效应量的通用方法，包括参数和非参数实现，并通过实例说明。


<details>
  <summary>Details</summary>
Motivation: 在因果中介分析涉及暴露诱导的混杂因素或多个中介变量时，研究者需估计多种不同效应量，现有方法有局限。

Method: 提出通过一系列分布模型模拟潜在结果的通用方法，包括参数模型实现和利用深度神经网络的非参数模型实现。

Result: 参数实现可处理多种关系和变量类型，但依赖模型正确设定；非参数实现可降低模型误设风险。

Conclusion: 两种方法能有效估计因果中介分析中的多种效应量，通过实例验证了方法的可行性。

Abstract: Analyses of causal mediation often involve exposure-induced confounders or, relatedly, multiple mediators. In such applications, researchers aim to estimate a variety of different quantities, including interventional direct and indirect effects, multivariate natural direct and indirect effects, and/or path-specific effects. This study introduces a general approach to estimating all these quantities by simulating potential outcomes from a series of distribution models for each mediator and the outcome. Building on similar methods developed for analyses with only a single mediator (Imai et al. 2010), we first outline how to implement this approach with parametric models. The parametric implementation can accommodate linear and nonlinear relationships, both continuous and discrete mediators, and many different types of outcomes. However, it depends on correct specification of each model used to simulate the potential outcomes. To address the risk of misspecification, we also introduce an alternative implementation using a novel class of nonparametric models, which leverage deep neural networks to approximate the relevant distributions without relying on strict assumptions about functional form. We illustrate both methods by reanalyzing the effects of media framing on attitudes toward immigration (Brader et al. 2008) and the effects of prenatal care on preterm birth (VanderWeele et al. 2014).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [287] [The Sample Complexity of Distributed Simple Binary Hypothesis Testing under Information Constraints](https://arxiv.org/abs/2506.13686)
*Hadi Kazemi,Ankit Pensia,Varun Jog*

Main category: cs.IT

TL;DR: 本文解决arXiv:2403.16981中关于信息约束下分布式简单二元假设检验样本复杂度的两个开放问题，表明顺序交互无帮助并导出最优紧界。


<details>
  <summary>Details</summary>
Motivation: 解决arXiv:2403.16981中关于分布式简单二元假设检验样本复杂度的两个开放问题，一是交互是否降低样本复杂度，二是收紧通信约束下样本复杂度界限。

Method: 给出满足关键张量性质的简单二元假设检验贝叶斯误差的单次下界；简化无约束简单二元假设检验样本复杂度公式的证明；推广Hellinger - λ散度的反向数据处理不等式。

Result: 证明顺序交互不能帮助降低样本复杂度，导出通信约束下简单二元假设检验的最优紧界。

Conclusion: 成功解决两个开放问题，有三项主要技术贡献。

Abstract: This paper resolves two open problems from a recent paper, arXiv:2403.16981, concerning the sample complexity of distributed simple binary hypothesis testing under information constraints. The first open problem asks whether interaction reduces the sample complexity of distributed simple binary hypothesis testing. In this paper, we show that sequential interaction does not help. The second problem suggests tightening existing sample complexity bounds for communication-constrained simple binary hypothesis testing. We derive optimally tight bounds for this setting and resolve this problem. Our main technical contributions are: (i) a one-shot lower bound on the Bayes error in simple binary hypothesis testing that satisfies a crucial tensorisation property; (ii) a streamlined proof of the formula for the sample complexity of simple binary hypothesis testing without constraints, first established in arXiv:2403.16981; and (iii) a reverse data-processing inequality for Hellinger-$λ$ divergences, generalising the results from arXiv:1812.03031 and arXiv:2206.02765.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [288] [Nonlinear Nonlocal Diffusion Equations for the Analysis of Continuous Coordination and Anti-Coordination Type Games](https://arxiv.org/abs/2506.13929)
*John S. McAlister,Nina H. Fefferman,Tadele A. Mengesha*

Main category: math.DS

TL;DR: 本文将结构化协调型博弈扩展到连续空间和连续策略的设置中，研究其动力学，证明初值问题的存在唯一性，对真实协调博弈有更多结论并给出建模结果。


<details>
  <summary>Details</summary>
Motivation: 以往对协调博弈动力学行为的研究仅在玩家数量可数、关系结构离散的情况下进行，本文旨在通过扩展到连续设置，用偏微分方程和非局部方程的工具研究协调与合作行为。

Method: 提出结构化协调型博弈到连续空间和连续策略设置的严格扩展，利用非线性、非局部扩散方程描述博弈动力学。

Result: 证明了无边界数据时初值问题的存在唯一性，对真实协调博弈证明了最大值原理、弱正则性结果和一些数值结果，给出建模结果并刻画了驻定解。

Conclusion: 得出关于非齐次问题的一个结果。

Abstract: Coordination games with explicit spatial or relational structure are of interest to economists, ecologists, sociologists, and others studying emergent global properties in collective behavior. When assemblies of individuals seek to coordinate action with one another through myopic best response or other replicator dynamics, the resulting dynamical system can exhibit many rich behaviors. However, these behaviors have been studied only in the case where the number of players is countable and the relational structure is described discretely. By giving an extension of a general class of coordination-like games, including true coordination games themselves, into a continuous setting, we can begin to study coordination and cooperative behavior with a new host of tools from PDEs and nonlocal equations. In this study, we propose a rigorously supported extension of structured coordination-type games into a setting with continuous space and continuous strategies and show that, under certain hypotheses, the dynamics of these games are described through a nonlinear, nonlocal diffusion equation. We go on to prove existence and uniqueness for the initial value problem in the case where no boundary data are prescribed. For true coordination games, we go further and prove a maximum principle, weak regularity results, as well as some numerical results toward understanding how solutions to the coordination equation behave. We present several modeling results, characterizing stationary solutions both rigorously and through numerical experiments and conclude with a result towards the inhomogeneous problem.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [289] [Asymptotically Smaller Encodings for Graph Problems and Scheduling](https://arxiv.org/abs/2506.14042)
*Bernardo Subercaseaux*

Main category: cs.LO

TL;DR: 本文展示图问题可更精简编码为CNF，给出独立集新编码并应用于实际问题减小编码规模。


<details>
  <summary>Details</summary>
Motivation: 探索更高效的图问题CNF编码方式，理解“有界变量添加”预处理工具的成功原因。

Method: 利用Erdős等人关于图双团覆盖的结果进行编码。

Result: 将多个图问题编码为CNF时子句数量减少，给出独立集新编码并应用于字符串压缩和调度问题减小编码规模。

Conclusion: 新的编码方式在理论和实际应用上都有优势，可减小编码规模。

Abstract: We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many clauses, as opposed to the $Ω(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $Ω(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.

</details>


### [290] [Varanus: Runtime Verification for CSP](https://arxiv.org/abs/2506.14426)
*Matt Luckcuck,Angelo Ferrando,Fatma Faruq*

Main category: cs.LO

TL;DR: 本文介绍了RV工具Varanus，它能根据CSP规范监测系统，可复用已有规范，在模拟的自主机器人探测器场景中验证其性能并展示违规检测能力，合成监测器和检查事件有较好的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统验证方法不适用于多变未知环境下的自主系统，CSP尚无RV工具，因此需要开发能基于CSP规范进行运行时验证的工具。

Method: 开发Varanus工具，用其对模拟的自主机器人探测器进行监测，并与其他使用不同语言的RV工具进行性能比较。

Result: Varanus能从CSP进程大致以线性时间合成监测器，以大致恒定时间检查每个事件。

Conclusion: Varanus工具可行且有效，能根据CSP规范监测系统，复用已有规范并检测违规。

Abstract: Autonomous systems are often used in changeable and unknown environments, where traditional verification may not be suitable. Runtime Verification (RV) checks events performed by a system against a formal specification of its intended behaviour, making it highly suitable for ensuring that an autonomous system is obeying its specification at runtime. Communicating Sequential Processes (CSP) is a process algebra usually used in static verification, which captures behaviour as a trace of events, making it useful for RV as well. Further, CSP has more recently been used to specify autonomous and robotic systems. Though CSP is supported by two extant model checkers, so far it has no RV tool. This paper presents Varanus, an RV tool that monitors a system against an oracle built from a CSP specification. This approach enables the reuse without modifications of a specification that was built, e.g during the system's design. We describe the tool, apply it to a simulated autonomous robotic rover inspecting a nuclear waste store, empirically comparing its performance to two other RV tools using different languages, and demonstrate how it can detect violations of the specification. Varanus can synthesise a monitor from a CSP process in roughly linear time, with respect to the number of states and transitions in the model; and checks each event in roughly constant time.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [291] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Main category: cs.MA

TL;DR: 研究基于路由器的多智能体系统用于基础设计计算自动化，对比三种方法，用多种模型评估，结果表明该系统性能佳，仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 探索自动化基础设计计算的有效方法。

Method: 评估单智能体处理、多智能体设计 - 检查架构和基于路由器的专家选择三种方法，用DeepSeek R1、ChatGPT 4 Turbo、Grok 3和Gemini 2.5 Pro等模型进行性能评估。

Result: 基于路由器的配置在浅基础和桩设计中表现良好，比Grok 3分别提高8.75和3.13个百分点，优于传统工作流10.0 - 43.75个百分点，Grok 3独立性能佳，双层分类框架有效区分基础类型。

Conclusion: 基于路由器的多智能体系统是基础设计自动化的最优选择，但在土木工程中仍需人工监督，仅作为计算辅助工具。

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [292] [Collaborative Charging Scheduling via Balanced Bounding Box Methods](https://arxiv.org/abs/2506.14461)
*Fangting Zhou,Balázs Kulcsár,Jiaming Wu*

Main category: math.OC

TL;DR: 本文探讨共享充电的城市物流调度问题，提出模型和方法，经案例验证方法可行有效且有扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决电动出行中基础设施建设成本高和充电站利用率低的问题。

Method: 将协作调度问题建模为双目标非线性整数规划模型，引入B3Ms求有效前沿，用合作谈判方法确定最终解。

Result: 数值案例表明B3Ms可显著减少计算时间，维持前沿完整性。

Conclusion: 提出的方法及合作谈判为解决双目标优化问题提供有效框架。

Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [293] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Main category: cs.CV

TL;DR: 提出基于特征的方法，利用Delaunay三角剖分检测和识别场景图像中模板的扭曲实例，效果良好。


<details>
  <summary>Details</summary>
Motivation: 对象检测和识别在计算机视觉领域至关重要，为解决几何模型不适用时的对象检测问题。

Method: 通过图像与模板特征匹配的增量分组，以Delaunay三角剖分作为工具，依据局部特征的几何和光度属性的局部一致性标准进行评估。

Result: 在无畸变场景中表现与基于单应性的RANSAC相当或更好，在畸变显著时描述性能更佳。

Conclusion: 所提方法能在几何模型不适用的情况下实现对象检测和识别。

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [294] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究T2I模型生成图像中的偏见问题，发现生成图像存在显著偏见，强调需更具包容性的数据和开发实践。


<details>
  <summary>Details</summary>
Motivation: T2I模型虽改变视觉内容创作，但可能复制和放大社会偏见，需研究这些问题。

Method: 策划多样提示词，用Stable Diffusion 1.5和Flux - 1模型生成超16000张图像，从谷歌搜图收集8000张对比图，过滤不合理结果后分析。

Result: 生成图像在性别、种族等人类相关因素的呈现上有显著差异，常反映并强化有害刻板印象。

Conclusion: 需更具包容性的数据集和开发实践，以促进生成视觉系统的公平性。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [295] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出GAN - RM高效奖励建模框架，无需手动偏好注释和明确质量维度工程，在多应用中有效。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模方法依赖大量人工标注偏好数据或精心设计但不完整且工程密集的质量维度，存在实现复杂性问题。

Method: 受GAN中对抗训练启发，通过区分少量有代表性的未配对目标样本（偏好代理数据）和模型生成的普通输出训练奖励模型。

Result: 在包括测试时缩放、监督微调、直接偏好优化等多个关键应用中验证了GAN - RM的有效性。

Conclusion: GAN - RM是一种有效的奖励建模框架，能解决当前奖励建模方法的问题。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [296] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/abs/2506.13910)
*Aritra Dutta,Pushpita Boral,G Suseela*

Main category: cs.CV

TL;DR: 利用机器学习构建暴力检测分类框架，用3D CNN等模型，在定制数据集训练，结合树莓派摄像头，提升计算资源效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 全球犯罪率上升，传统监控方法难以及时检测多样暴力行为，有自动暴力检测的需求。

Method: 引入综合框架，用监督学习进行二分类和多分类暴力分类，检测模型用3D CNN，分类模型用可分离卷积3D模型和双向LSTM，在定制数据集训练，结合树莓派摄像头。

Result: 模型在计算资源效率和准确性上有改进。

Conclusion: 所构建的暴力检测分类框架能有效应对暴力检测问题，提升性能。

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [297] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/abs/2506.13925)
*Numair Nadeem,Saeed Anwar,Muhammad Hamza Asad,Abdul Bais*

Main category: cs.CV

TL;DR: 提出HierVL框架用于半监督语义分割，在多数据集提升性能，显示语言引导分割优势。


<details>
  <summary>Details</summary>
Motivation: 半监督语义分割在标签稀缺和领域可变性下有挑战，视觉方法泛化难，视觉 - 语言模型缺空间定位。

Method: 将抽象文本嵌入集成到掩码变压器架构，有层次语义查询生成器、跨模态空间对齐模块、双查询变压器解码器，还有针对性正则化损失。

Result: 在COCO、Pascal VOC、ADE20和Cityscapes数据集上，交并比分别提升4.4%、3.1%、5.9%和1.8%。

Conclusion: 语言引导分割缩小标签效率差距，实现细粒度、实例感知泛化。

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [298] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/abs/2506.14035)
*Chelsi Jain,Yiran Wu,Yifan Zeng,Jiale Liu,S hengyu Dai,Zhenwen Shao,Qingyun Wu,Huazheng Wang*

Main category: cs.CV

TL;DR: 提出用于DocVQA的轻量级框架SimpleDoc，在多数据集上表现优且检索页面少，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决DocVQA任务，改进现有多模态处理方法。

Method: 先通过嵌入相似度检索候选页面，再基于页面摘要过滤和重排；单VLM推理器反复调用双线索检索器迭代获取页面。

Result: 在4个DocVQA数据集上平均比之前基线模型高3.2%，且检索页面少。

Conclusion: SimpleDoc是用于DocVQA的有效轻量级框架。

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [299] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/abs/2506.14096)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文系统综述了大语言模型增强图像分割在智能交通系统中的应用、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与计算机视觉的融合为智能交通系统的场景理解带来新能力，有必要对相关领域进行系统研究。

Method: 对当前方法基于提示机制和核心架构进行分类。

Result: 强调了这些创新可提升自动驾驶、交通监控和基础设施维护等方面的道路场景理解。

Conclusion: 指出实时性能和安全关键可靠性等挑战，认为可解释、以人为中心的AI是该技术在下一代交通系统成功部署的前提。

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [300] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 提出基于逻辑的知识蒸馏框架用于运动目标分割，提升精度并保持实时效率，在数据集上取得好结果。


<details>
  <summary>Details</summary>
Motivation: 现有运动目标分割方法在平衡精度和实时推理方面存在挑战。

Method: 采用基于鸟瞰图投影的模型为学生模型、非投影模型为教师模型；解耦移动和非移动类别并应用定制蒸馏策略；引入动态上采样并优化网络架构。

Result: 在SemanticKITTI - MOS数据集隐藏测试集上IoU达78.8%，在Apollo数据集上有有竞争力的结果，参数数量减少7.69%。

Conclusion: 所提方法能有效提升运动目标分割精度，同时保持实时效率。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [301] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Main category: cs.CV

TL;DR: 提出首个覆盖英格兰大部分地区、高分辨率的农村景观特征地图Farmscapes，助力生态保护规划。


<details>
  <summary>Details</summary>
Motivation: 因缺乏详细的大规模生态地图，阻碍了农业景观的有效管理和全球生物多样性目标的实现。

Method: 使用基于942个手动标注的航拍图像切片的新数据集训练的深度学习分割模型生成地图。

Result: 模型能准确识别关键栖息地，林地F1分数达96%，耕地达95%，树篱F1分数72%。

Conclusion: 发布英格兰范围的地图为生态学家和政策制定者提供了开放获取工具，支持栖息地恢复规划和生物多样性战略监测。

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [302] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/abs/2506.14144)
*Juho Bai,Inwook Shim*

Main category: cs.CV

TL;DR: 提出SceneAware框架，结合场景理解提升行人轨迹预测精度，在ETH/UCY数据集上超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法常忽略环境背景信息，而环境背景显著影响人类运动模式，需提升预测精度。

Method: 利用ViT场景编码器处理静态场景图像的环境背景，用MLLMs生成二进制可通行性掩码；结合基于Transformer的轨迹编码器和基于ViT的场景编码器；集成碰撞惩罚机制；有确定性和随机性两种变体。

Result: 在ETH/UCY基准数据集上表现优于现有方法，较之前模型提升超50%，不同轨迹类别上表现稳定。

Conclusion: 明确使用场景信息很重要，场景感知方法有效可靠，能生成准确且符合物理规律的预测。

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [303] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Main category: cs.CV

TL;DR: 提出VideoMAR图像到视频模型，结合多种方法解决视频生成问题，在VBench - I2V基准上超越SOTA且资源需求少。


<details>
  <summary>Details</summary>
Motivation: 基于掩码的自回归模型在视频生成方面潜力待挖掘，且长序列自回归建模成本高、难度大。

Method: 提出VideoMAR模型，确定视频AR模型原则及下一帧扩散损失，采用时间短到长课程学习、空间渐进分辨率训练，推理时用渐进温度策略，利用3D旋转嵌入。

Result: 在VBench - I2V基准上超越Cosmos I2V，参数只需9.3%，训练数据只需0.5%，GPU资源只需0.2%。

Conclusion: VideoMAR是一种高效的图像到视频模型，在视频生成上有较好表现且资源需求低。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [304] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/abs/2506.14170)
*Shulong Zhang,Mingyuan Yao,Jiayin Zhao,Xiao Liu,Haihua Wang*

Main category: cs.CV

TL;DR: 提出MAINet网络量化鱼类摄食强度，实验表明其性能优于对比模型，消融实验验证改进策略有效性。


<details>
  <summary>Details</summary>
Motivation: 当前研究在模态选择、特征提取融合和联合推理决策上有局限，限制多模态融合模型性能提升，需准确有效评估鱼类摄食强度以降低饲料成本和计算最佳投喂时间。

Method: 提出通用特征提取框架提取图像、音频和水波数据特征；设计ARPM机制进行模态间交互生成增强特征；引入ER规则融合各模态输出结果并决策。

Result: MAINet在准确率、精确率、召回率和F1分数分别达96.76%、96.78%、96.79%和96.79%，性能显著高于对比模型，相比单模态、双模态融合及不同决策融合方法的模型有明显优势。

Conclusion: 提出的改进策略对提高模型鲁棒性和特征利用效率有关键作用，能有效提高鱼类摄食强度量化结果的准确性。

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [305] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出Hierarchical Gaussian Splatting (HRGS)框架解决3DGS高分辨率场景下内存扩展问题，经实验在高分辨率任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS)在高分辨率场景面临内存扩展问题。

Method: 生成全局粗高斯表示，将场景分区，通过高斯分区和训练数据分区进行块级优化；引入Importance-Driven Gaussian Pruning (IDGP)减少计算需求；结合预训练模型的法线先验提升表面重建质量。

Result: 在三个基准测试中，HRGS在高分辨率新视图合成和表面重建任务中取得了最先进的性能。

Conclusion: 该方法能在内存受限情况下实现高质量、高分辨率的3D场景重建。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [306] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 提出新方法精炼形状间对应图，训练图像扩散模型处理功能映射，推理时用点映射引导，效果有竞争力。


<details>
  <summary>Details</summary>
Motivation: 寻找精炼给定形状间对应图的有效方法。

Method: 将对应图视为功能映射和2D图像，在功能映射空间训练图像扩散模型，推理时用点映射引导。

Result: 该方法与现有地图精炼方法有竞争力，引导扩散模型为功能映射处理提供前景。

Conclusion: 引导扩散模型是功能映射处理的有前景途径。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [307] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Main category: cs.CV

TL;DR: 提出MoTE方法训练混合三元专家模型，内存效率高，性能与全精度基线相当，在内存受限设备上表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往大模型多模态混合专家模型在稀疏上循环时用全精度专家，专家数量多导致内存占用高，难以在边缘设备部署。

Method: 从密集检查点训练混合三元专家（MoTE）模型，将预训练FFN作为共享专家，训练参数为{-1, 0, 1}的三元路由专家。

Result: MoTE性能与全精度基线MoE - LLaVA相当，内存占用更低；结合后训练量化方法，在3.4GB专家内存占用下，平均准确率比MoE - LLaVA高4.3%。

Conclusion: MoTE方法可扩展、内存效率高，对内存受限设备有效且有潜力。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [308] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 本文提出用集成梯度增强知识蒸馏的方法用于模型压缩，在CIFAR - 10上验证效果良好，且在不同架构和压缩比下均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 模型压缩对在资源受限设备上部署深度学习模型至关重要，需找到有效压缩方法。

Method: 将集成梯度（IG）图叠加到输入图像作为数据增强策略，训练前预计算IG图，将运行成本转化为一次性预处理步骤。

Result: 在CIFAR - 10上测试准确率达92.6%，压缩因子4.1x，比未蒸馏模型提高1.1个百分点，推理时间从140ms减至13ms，多种实验验证方法有效性和泛化性。

Conclusion: 基于IG的知识蒸馏是一种可行的压缩技术，可在边缘设备上实际部署并保持竞争力的准确率。

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [309] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/abs/2506.14356)
*Xiaoqi Wang,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 提出EVA02 - AT视频语言基础模型解决以自我为中心视频语言理解问题，实验证明其少参高效且性能优。


<details>
  <summary>Details</summary>
Motivation: 现有以自我为中心视频语言理解方法存在预训练成本高、时空编码无效、学习目标不精确等问题。

Method: 通过单阶段预训练将基于图像的CLIP模型转为统一视频编码器；引入时空旋转位置嵌入和联合注意力；提出对称多相似性损失和新训练框架。

Result: 在多个数据集零样本和微调设置实验中，EVA02 - AT少参情况下达到多任务最优，含SMS损失模型在多实例检索基准有显著提升。

Conclusion: EVA02 - AT在以自我为中心视频语言任务中表现优异，代码和模型已公开。

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [310] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 研究基于基础模型（FM）的一次性子集选择，发现其在细粒度数据集上优于传统方法，提出RAM - APL方法并在细粒度数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统信息提取器（IE）依赖数据集，基础模型可能缓解此局限，探究其在子集选择中的表现。

Method: 进行实验对比FM和传统IE在不同数据集的表现，提出RAM - APL方法，利用多个FM的互补优势进行子集选择。

Result: FM在细粒度数据集上始终优于传统IE，在粗粒度含噪声标签数据集优势减弱，RAM - APL在细粒度数据集达SOTA。

Conclusion: 基于实验结果，提出的RAM - APL方法在细粒度图像数据集子集选择中效果良好。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [311] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/abs/2506.14382)
*Ning Zhou,Shanxiong Chen,Mingting Zhou,Haigang Sui,Lieyun Hu,Han Li,Li Hua,Qiming Zhou*

Main category: cs.CV

TL;DR: 本文提出深度提示二维遥感语义分割框架DepthSeg，可缓解光谱混淆和阴影遮挡影响，在LiuZhou数据集实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有遥感语义分割方法忽略目标高程差异，导致复杂场景下土地覆盖误分类。

Method: 提出DepthSeg框架，特征提取阶段引入轻量级适配器微调预训练的视觉变压器编码器；深度提示阶段提出深度提示器建模深度/高度特征；语义预测阶段引入语义分类解码器结合深度提示与高维土地覆盖特征。

Result: 在LiuZhou数据集实验验证了DepthSeg框架在土地覆盖制图任务中的优势，消融研究凸显深度提示的重要性。

Conclusion: DepthSeg框架有效缓解光谱混淆和阴影遮挡影响，提高遥感语义分割精度，深度提示在其中有重要意义。

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [312] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 提出新的可解释机器学习方法通过多任务预测建模估计膝骨关节炎进展风险，在数据集上提升SOTA并加快推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法用于膝骨关节炎风险估计缺乏可解释性，生成未来图像方法复杂不实用，且无法定位解剖学膝关节标志。

Method: 使用多任务预测建模，在类条件潜在空间利用扩散模型生成高质量未来图像，对未来膝骨关节炎严重程度分类并预测解剖学膝关节标志。

Result: 应用于Osteoarthritis Initiative数据集，相比SOTA提升2%，预测膝骨关节炎进展的AUC达0.71，推理时间快约9%。

Conclusion: 新方法能有效估计膝骨关节炎进展风险，兼具可解释性和高效性。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [313] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 提出解耦无分类器引导（DCFG）框架用于反事实图像生成，实验表明其能提高干预保真度等。


<details>
  <summary>Details</summary>
Motivation: 标准无分类器引导（CFG）应用单一全局权重会导致身份保存不佳和属性放大问题，需要改进。

Method: 提出DCFG框架，采用属性拆分嵌入策略解耦语义输入，基于因果图划分属性集并对不同集合应用不同引导。

Result: 在CelebA - HQ、MIMIC - CXR和EMBED上实验显示，DCFG提高干预保真度、减轻意外变化、增强可逆性。

Conclusion: DCFG能实现更可信和可解释的反事实图像生成。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [314] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出基于VLM的因果忠实框架用于反事实视频生成，在LDMs中有效生成因果忠实视频，兼容黑盒视频编辑系统。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像潜在扩散模型用于视频编辑时难以维持视频内容的因果关系，忽略因果关系会产生不真实或误导性结果。

Method: 提出基于VLM的因果忠实框架，通过基于假设因果图优化文本提示来指导生成，不依赖底层视频编辑系统内部机制和微调。

Result: 通过标准视频质量指标和反事实特定标准评估，证明可在LDMs的学习分布内有效生成因果忠实的视频反事实。

Conclusion: 该方法与任何黑盒视频编辑系统兼容，在医疗和数字媒体等领域生成逼真“假设”视频场景有巨大潜力。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [315] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 本文引入新目标和技术训练流映射模型Align Your Flow，在图像生成基准测试中实现少步长生成的最优性能，文本到图像模型也表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散和基于流的模型采样步骤多，一致性模型增加步数性能会下降，需新方法解决这些问题。

Method: 引入两个新的连续时间目标训练流映射，采用新训练技术，结合自动引导和对抗性微调。

Result: 在ImageNet 64x64和512x512上实现少步长生成的最优性能，文本到图像模型优于现有非对抗训练的少步长采样器。

Conclusion: 提出的流映射模型在图像和文本到图像生成的少步长生成任务中表现优异。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [316] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 本文用无配对数据集从逆问题角度处理图像恢复任务，方法假设少、依赖小的无配对数据集，在多项任务表现佳，还在镜头校准概念验证中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需完全了解前向模型或有配对数据，不适用于前向模型未知或配对数据收集成本高的现实场景，因此提出新方法。

Method: 利用条件流匹配对退化观测的分布建模，同时通过框架自然产生的分布匹配损失学习前向模型。

Result: 在去模糊、非均匀点扩散函数校准任务上优于单图像盲方法和无监督方法，在盲超分辨率上达到了最先进水平，在镜头校准概念验证中有效。

Conclusion: 所提方法在图像恢复任务中有效，尤其适用于现实场景，能以最小的数据采集工作完成相关任务。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [317] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/abs/2506.14418)
*Jiayi Chen,Yanbiao Ma,Andi Zhang,Weidong Tang,Wei Dai,Bowei Liu*

Main category: cs.CV

TL;DR: 论文针对图像分类中视觉属性不平衡问题，定义图像属性，构建视觉属性字典，分析属性不平衡对模型的影响，提出基于属性稀有性调整采样概率并结合数据增强的方法，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决图像分类中视觉属性不平衡这一常见但研究不足的问题，提升模型性能和泛化能力。

Method: 定义图像的一级和二级属性，引入基于CLIP的框架构建视觉属性字典；分析单属性和组合属性不平衡；根据组合属性稀有性调整样本采样概率，并结合多种数据增强技术。

Result: 在基准数据集上的大量实验表明，该方法有效缓解了属性不平衡问题。

Conclusion: 强调了对视觉属性分布建模的重要性，为长尾图像分类任务提供了可扩展的解决方案。

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [318] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/abs/2506.14451)
*Aditya Shourya,Michel Dumontier,Chang Sun*

Main category: cs.CV

TL;DR: 本文针对放射视觉问答模型开发各阶段的挑战，微调轻量级视觉语言模型，提出训练管道，模型表现良好，并引入诊断工具。


<details>
  <summary>Details</summary>
Motivation: 现有放射视觉问答模型在数据获取、建模和评估方面存在挑战，需要改进。

Method: 微调3B参数轻量级视觉语言模型，提出从合成问答对生成到多阶段微调的训练管道，在特定放射领域数据集上训练。

Result: 模型参数少、训练数据规模有限的情况下，仍取得有前景的性能。

Conclusion: 小模型经精心调优和特定数据训练可实现良好性能，引入的诊断工具能帮助专家检查模型性能。

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [319] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 本文提出框架平衡扩散模型图像生成质量与计算成本，依提示复杂度分配计算量，在数据集上验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高保真图像计算成本高，需平衡质量与计算成本。

Method: 提出框架，根据提示复杂度将其路由到合适的文本到图像生成函数，学习为复杂提示保留高成本选择，为简单提示用低成本选择。

Result: 在COCO和DiffusionDB上，学习路由到九个已训练模型，平均质量高于单个模型。

Conclusion: 所提方法能有效平衡质量和计算成本，实现最优权衡。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [320] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/abs/2506.14583)
*Krishna Sahukara,Zineddine Bettouche,Andreas Fischer*

Main category: cs.CV

TL;DR: 提出基于LaTeX的自动化流程生成合成数据增强Marmot基准，训练TableNet并取得一定性能，还减少手动标注工作量。


<details>
  <summary>Details</summary>
Motivation: 解决手动提取文档页面中表格缓慢且易出错的问题。

Method: 引入基于LaTeX的自动化流程，合成具有视觉多样表格布局和对齐真实掩码的两列页面数据，增强Marmot基准。

Result: 在合成测试集上，256x256输入分辨率时像素级XOR误差为4.04%，1024x1024时为4.33%；Marmot基准上最佳性能为9.18%（256x256）。

Conclusion: 该方法在训练TableNet上有较好效果，同时通过自动化减少了手动标注工作。

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [321] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2506.14596)
*Ming Xu,Xu Zhang*

Main category: cs.CV

TL;DR: 提出PoseGRAF框架解决单目3D姿态估计问题，实验效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D姿态估计方法依赖关节位置特征，忽略骨骼内在方向和角度关联，在关节遮挡或快速运动变化时表现不佳。

Method: 构建双图卷积结构处理关节和骨骼图，引入交叉注意力模块，设计动态融合模块，以残差方式加入改进的Transformer编码器。

Result: 在Human3.6M和MPI - INF - 3DHP数据集上超越现有方法，在野外视频评估中验证了泛化性。

Conclusion: 提出的PoseGRAF框架有效解决现有单目3D姿态估计方法的问题。

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [322] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/abs/2506.14684)
*Aditya Bhattacharjee,Ivan Meresman Higgs,Mark Sandler,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 本文提出轻量级可扩展编码架构用于自动样本识别，用少量参数达可比性能，还引入两阶段方法提升检索质量并对短查询进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有自动样本识别（ASID）系统难以识别经音乐修改的样本，构建对常见音乐制作变换鲁棒的系统是重要挑战。

Method: 提出在对比学习框架中使用图神经网络的轻量级可扩展编码架构，引入两阶段方法，包括初始粗相似度搜索选候选和交叉注意力分类器优化排序，还针对短查询用新细粒度注释对Sample100数据集进行基准测试。

Result: 模型仅使用当前最先进系统9%的可训练参数，达到44.2%的平均精度均值（mAP）。

Conclusion: 所提方法在自动样本识别任务上有较好效果，在参数使用和检索质量上有优势。

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [323] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/abs/2506.13833)
*Xiaoliang Chen,Le Chang,Xin Yu,Yunhe Huang,Xianling Tu*

Main category: cs.SD

TL;DR: 本文对基于声学物理信息的世界模型领域进行全面综述，涵盖理论基础、方法框架、应用及挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 全面了解基于声学物理信息的世界模型这一新兴领域，推动该领域发展。

Method: 先阐述声学信号编码物理信息的理论基础，再回顾核心方法支柱如PINNs、生成模型和自监督多模态学习框架，最后介绍应用、挑战和未来方向。

Result: 明确了声学世界模型在机器人、自动驾驶、医疗和金融等领域的重要应用，指出了技术和伦理挑战。

Conclusion: 提出了构建具身主动声学智能的研究路径，使AI系统能通过声音构建内部‘直觉物理’引擎。

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [324] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/abs/2506.13970)
*Charles C Onu*

Main category: cs.SD

TL;DR: 论文探讨机器学习用于医学音频信号分析挑战，聚焦婴儿哭声预测病症，在低数据学习、模型压缩、领域适应和数据集发布四方面有贡献，为AI医疗奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统医学靠专家听诊分析医疗声音，自动化分析可标准化处理、在资源匮乏地区筛查及发现细微模式，助力早期诊疗。

Method: 运用神经迁移学习利用成人语音数据库；采用张量分解对循环网络进行端到端模型压缩；提出音频模型领域适应技术并借鉴计算机视觉方法；与全球临床医生合作发布开源婴儿哭声数据集。

Result: 在低数据场景能开发更准确健壮模型；模型压缩达数百倍且适用于资源受限设备；领域适应技术解决数据集偏差、提升泛化能力；发布独特开源数据集。

Conclusion: 为将婴儿哭声作为生命体征识别奠定基础，凸显AI驱动音频监测对未来可及、经济医疗的变革潜力。

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [325] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Main category: cs.SD

TL;DR: 提出音乐预训练数据集Sleeping - DISCO 9M，解决现有数据集不能反映真实音乐的问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于生成式音乐建模任务的开源高质量流行歌曲数据集，现有数据集无法反映真实世界音乐及其特色。

Method: 使用实际流行音乐和世界知名艺术家的作品构建数据集。

Result: 创建了Sleeping - DISCO 9M数据集。

Conclusion: Sleeping - DISCO 9M数据集改变了现有局面，能满足生成式音乐社区需求。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [326] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/abs/2506.14434)
*Bidisha Sharma,Karthik Pandia Durai,Shankar Venkatesan,Jeena J Prakash,Shashi Kumar,Malolan Chetlur,Andreas Stolcke*

Main category: cs.SD

TL;DR: 提出统一框架训练端到端ASR模型，利用动态右上下文，在多个数据集测试，降低词错率，可灵活控制延迟与准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 统一流式和非流式自动语音识别（ASR）模型以降低开发、训练和部署成本。

Method: 提出统一框架，在基于zipformer的ASR模型训练中使用动态右上下文和分块注意力掩码，分析右上下文帧数对准确率和延迟的影响。

Result: 提出策略使词错率相对降低7.9%，增加右上下文帧数可使流式性能接近非流式模型。

Conclusion: 该方法能降低词错率，实现流式与非流式统一，可根据客户需求灵活控制延迟与准确率权衡。

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [327] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/abs/2506.14723)
*Yusong Wu,Tim Cooijmans,Kyle Kastner,Adam Roberts,Ian Simon,Alexander Scarlatos,Chris Donahue,Cassie Tarakajian,Shayegan Omidshafiei,Aaron Courville,Pablo Samuel Castro,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: 提出在线生成模型ReaLchords为用户旋律即兴创作和弦伴奏，经微调后能适配陌生输入并生成合适伴奏，为实时即兴演奏等打开大门。


<details>
  <summary>Details</summary>
Motivation: 当前音乐生成模型无法在线与其他音乐家同时创作，需要开发能在线即兴创作和弦伴奏的模型。

Method: 从最大似然预训练的在线模型开始，用强化学习微调，微调目标结合新奖励模型和发散项。

Result: 通过定量实验和听力测试，模型能很好适应陌生输入并生成合适伴奏。

Conclusion: ReaLchords为实时即兴演奏和多模态同步共创提供了可能。

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [328] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/abs/2506.14750)
*Gaobin Yang,Maokui He,Shutong Niu,Ruoyu Wang,Hang Chen,Jun Du*

Main category: cs.SD

TL;DR: 提出NSD - MS2S和NSD - MS2S - SSMoE系统用于说话人分割，在多数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升说话人分割系统的性能和泛化能力，减少模型偏差。

Method: 提出NSD - MS2S系统，结合记忆感知多说话人嵌入模块与序列到序列架构；引入SS - MoE模块得到NSD - MS2S - SSMoE模型。

Result: 在CHiME - 6、DiPCo、Mixer 6和DIHARD - III等复杂声学数据集实验中，系统的鲁棒性和泛化能力有显著提升。

Conclusion: 所提方法达到了当前最优结果，在具有挑战性的真实场景中有效。

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [329] [Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation](https://arxiv.org/abs/2506.13961)
*Mohamed Serry,Haoyu Li,Ruikun Zhou,Huan Zhang,Jun Liu*

Main category: eess.SY

TL;DR: 提出精确估计离散时间自治非线性系统安全吸引域的框架，用神经网络近似解Zubov方程并通过验证框架获可认证估计，经数值算例验证。


<details>
  <summary>Details</summary>
Motivation: 现有非线性系统吸引域估计方法保守或限于低维系统，考虑状态约束时估计更具挑战。

Method: 推导新Zubov方程，用神经网络近似其解，提出可利用标准验证工具实现的验证框架。

Result: Zubov方程的解唯一且在整个状态空间连续，可从神经网络近似解获得吸引域的可认证估计。

Conclusion: 所提框架能有效估计离散时间自治非线性系统的安全吸引域。

Abstract: Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $α,\!β$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [330] [SETI@home: Data Acquisition and Front-End Processing](https://arxiv.org/abs/2506.14718)
*Eric J. Korpela,David P. Anderson,Jeff Cobb,Matt Lebofsky,Wei Liu,Dan Werthimer*

Main category: astro-ph.IM

TL;DR: 介绍SETI@home项目利用志愿者家用电脑分析射电数据搜索地外智慧信号的前端工作及相关参数。


<details>
  <summary>Details</summary>
Motivation: 寻找地外智慧生命的技术信号，提升搜索的灵敏度和普遍性。

Method: 将时域数据通过互联网分发给超 $10^{5}$ 台志愿者家用电脑分析，采用相干积分技术，搜索多种信号类型，前端产生超过功率和拟合优度阈值的检测结果。

Result: 积累了约 $1.2×10^{10}$ 个超过阈值的检测结果。

Conclusion: 本文描述了SETI@home的前端工作及阿雷西博天文台的主要数据源参数，后端及结果在另一篇论文中描述。

Abstract: SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project, looking for technosignatures in data recorded at multiple observatories from 1998 to 2020. Most radio SETI projects analyze data using dedicated processing hardware. SETI@home uses a different approach: time-domain data is distributed over the Internet to $\gt 10^{5}$ volunteered home computers, which analyze it. The large amount of computing power this affords ($\sim 10^{15}$ floating-point operations per second (FPOP/s)) allows us to increase the sensitivity and generality of our search in three ways. We use coherent integration, a technique in which data is transformed so that the power of drifting signals is confined to a single discrete Fourier transform (DFT) bin. We perform this coherent search over 123 000 Doppler drift rates in the range ($\pm$100 Hz s$^{-1}$). Second, we search for a variety of signal types, such as pulsed signals and arbitrary repeated waveforms. The analysis uses a range of DFT sizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front end of SETI@home produces a set of detections that exceed thresholds in power and goodness of fit. We accumulated $\sim 1.2\times 10^{10}$ such detections. The back end of SETI@home takes these detections, identifies and removes radio frequency interference (RFI), and looks for groups of detections that are consistent with extraterrestrial origin and that persist over long timescales. This paper describes the front end of SETI@home and provides parameters for the primary data source, the Arecibo Observatory; the back end and its results are described in a companion paper.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [331] [Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion](https://arxiv.org/abs/2506.14488)
*Dong Xu,Zhangfan Yang,Ka-chun Wong,Zexuan Zhu,Jiangqiang Li,Junkai Ji*

Main category: q-bio.BM

TL;DR: 介绍了名为READ的方法用于解决基于受体的分子设计中权衡问题，实验显示其在CBGBench中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决基于受体的分子设计中平衡口袋特定几何匹配与严格价态和合成约束的问题。

Method: 引入Retrieval-Enhanced Aligned Diffusion（READ），将分子检索增强生成与SE(3)等变扩散模型结合，用对比预训练编码器在训练时对齐原子级表示，推理时检索口袋匹配支架的图嵌入来引导反向扩散步骤。

Result: READ在CBGBench中取得有竞争力的表现，超越了现有生成模型和天然配体。

Conclusion: 检索和扩散可以共同优化，以实现更快、更可靠的基于结构的药物设计。

Abstract: Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [332] [NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions](https://arxiv.org/abs/2506.14270)
*Gijs Vermariën,Thomas G. Bisbas,Serena Viti,Yue Zhao,Xuefei Tang,Rahul Ravichandran*

Main category: astro-ph.GA

TL;DR: 提出可替代原化学代码的代理模型Latent Augmented Neural Ordinary Differential Equations，在不同数据集训练，能加速并重现数据列密度图，实现快速化学推断。


<details>
  <summary>Details</summary>
Motivation: 高分辨率望远镜时代需小尺度天体化学建模，耦合三维流体动力学和化学的模拟计算成本巨大，需要代理模型替代化学求解器。

Method: 提出Latent Augmented Neural Ordinary Differential Equations代理模型，在三个物理复杂度递增的数据集上训练，最后一个数据集来自用3D - PDR代码对分子云的三维模拟。

Result: 代理模型能加速计算并重现数据集的原始可观测列密度图。

Conclusion: 代理模型可实现化学的快速推断，利于对观测进行更快的统计推断或提高天体物理环境流体动力学模拟的分辨率。

Abstract: Computational astrochemical models are essential for helping us interpret and understand the observations of different astrophysical environments. In the age of high-resolution telescopes such as JWST and ALMA, the substructure of many objects can be resolved, raising the need for astrochemical modeling at these smaller scales, meaning that the simulations of these objects need to include both the physics and chemistry to accurately model the observations. The computational cost of the simulations coupling both the three-dimensional hydrodynamics and chemistry is enormous, creating an opportunity for surrogate models that can effectively substitute the chemical solver. In this work we present surrogate models that can replace the original chemical code, namely Latent Augmented Neural Ordinary Differential Equations. We train these surrogate architectures on three datasets of increasing physical complexity, with the last dataset derived directly from a three-dimensional simulation of a molecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show that these surrogate models can provide speedup and reproduce the original observable column density maps of the dataset. This enables the rapid inference of the chemistry (on the GPU), allowing for the faster statistical inference of observations or increasing the resolution in hydrodynamical simulations of astrophysical environments.

</details>
