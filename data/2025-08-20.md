<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.AR](#cs.AR) [Total: 9]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.MA](#cs.MA) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.CL](#cs.CL) [Total: 20]
- [cs.HC](#cs.HC) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [cs.CV](#cs.CV) [Total: 21]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 介绍Chain - of - Agents(CoA)范式和Agent Foundation Models(AFMs)，通过多智能体蒸馏框架和强化学习提升模型能力，在多基准测试中达新SOTA，研究全开源。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统基于手动提示/工作流工程，计算效率低、能力有限且无法从以数据为中心的学习中受益。

Method: 引入CoA范式，使用多智能体蒸馏框架将多智能体系统蒸馏为链智能体轨迹进行监督微调，再用强化学习进一步提升模型能力。

Result: AFM在网络智能体和代码智能体设置的多个基准测试中建立了新的SOTA性能。

Conclusion: 研究成果全开源，为智能体模型和智能体强化学习的未来研究提供了坚实起点。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [2] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: 提出Cognitive Workspace范式，超越传统RAG，通过三项创新解决大语言模型上下文管理局限，实证效果好，提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文管理方面存在根本局限，传统检索增强生成（RAG）无法捕捉人类记忆管理的动态、任务驱动特性。

Method: 借鉴认知科学基础理论，提出Cognitive Workspace范式，包括主动内存管理、分层认知缓冲区和任务驱动的上下文优化三项核心创新。

Result: Cognitive Workspace平均内存复用率达58.6%，净效率提升17 - 18%，统计分析证明优势显著。

Conclusion: Cognitive Workspace是从信息检索到真正认知增强的根本性转变，提供了综合理论框架。

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [3] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: 提出AlphaEval框架评估自动阿尔法挖掘模型，经实验验证其效果好且开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标存在计算密集、评估不全面、模型闭源影响可复现性等问题，阻碍阿尔法挖掘领域发展。

Method: 提出统一、可并行、无回测的AlphaEval框架，从五个互补维度评估生成的阿尔法质量。

Result: AlphaEval评估一致性与全面回测相当，提供更全面见解和更高效率，能有效识别优质阿尔法。

Conclusion: AlphaEval是有效的评估框架，开源可促进可复现性和社区参与。

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [4] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [5] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: 提出HiFo - Prompt框架用于进化计算中基于大语言模型的自动启发式设计，实证表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计因使用静态算子和缺乏知识积累机制，有效性受限。

Method: 引入HiFo - Prompt框架，采用前瞻性和后见性两种协同提示策略引导大语言模型。

Result: HiFo - Prompt显著优于现有基于大语言模型的自动启发式设计方法，生成更高质量启发式，收敛更快且查询效率更高。

Conclusion: HiFo - Prompt框架有效，通过两种提示策略可解决现有方法的不足。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [6] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 本文提出CESQL模型提升文本到SQL模型能力，在WikiSQL数据集表现出色，为复杂查询研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在实际应用中的基础能力和泛化能力。

Method: 将模型可解释性分析与执行引导策略结合用于SQL查询WHERE子句语义解析，辅以过滤调整、逻辑相关性细化和模型融合设计CESQL模型。

Result: 在WikiSQL数据集上表现优异，显著提高预测结果准确性，减少对表条件列数据依赖和手动标注训练数据影响。

Conclusion: 本次提升基础数据库查询准确性的尝试，能为处理复杂查询和不规则数据场景研究提供新视角。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [7] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 结合pymdp和定制计算图，降低Active Inference部署的计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: Active Inference在资源受限环境中部署时，计算和内存需求带来挑战。

Method: 结合pymdp的灵活性和效率，使用统一、稀疏的计算图以实现硬件高效执行。

Result: 将延迟降低2倍以上，内存减少达35%。

Conclusion: 推动了高效Active Inference智能体在实时和嵌入式应用中的部署。

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [8] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 文章指出搜索式大语言模型代理评估中存在搜索时污染（STC）问题，以HuggingFace为例说明其对基准测试的影响，提出应对措施并公开实验日志。


<details>
  <summary>Details</summary>
Motivation: 识别搜索式大语言模型代理评估中类似数据污染的问题，保证基准测试的完整性和评估的可信度。

Method: 在三个常用能力基准测试中观察，进行阻断实验和消融实验。

Result: 约3%的问题搜索式代理能在HuggingFace上直接找到带真实标签的数据集，阻断HuggingFace后污染子集准确率下降约15%，且公开评估数据集可能不是STC唯一来源。

Conclusion: 提出基准设计和结果报告的最佳实践，公开实验日志以方便审计评估结果。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [9] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出QuickMerge框架用于高效的下一个token预测，在多模态领域评估显示可提升计算-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 生成模型输入规模增大，token级计算成本成瓶颈，现有token选择方法存在静态、特定模态或与自回归生成不兼容等问题。

Method: 基于注意力范数大小动态选择减少的token数量，由基于熵的预算估计器引导；引入轻量级transformer先验以保持自回归兼容性。

Result: 在多模态领域评估显示，QuickMerge能大幅减少token数量，性能匹配甚至超过学习型分词器和固定补丁基线。

Conclusion: QuickMerge结合语义显著性估计、灵活token预算和自回归对齐，能以更少token实现准确生成。

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [10] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 研究国际象棋中人类与AI战略决策的权衡，发现高竞争力AI比精英人类维持战略紧张度更久，人类会限制紧张度和复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究战略决策中即时机会和长期目标间的权衡。

Method: 提出基于网络的棋子交互指标量化棋盘上的战略紧张度，对比人类对弈和AI对弈动态。

Result: 高竞争力AI比精英人类在更长时间维持更高战略紧张度；AI累积紧张度与算法复杂度有关，人类比赛中累积紧张度在特定Elo等级有突变；AI和人类有不同策略。

Conclusion: 人类和AI策略差异对复杂战略环境中AI的使用有影响。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [11] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 本文提出多跳个性化推理任务，构建数据集和评估框架，实验评估不同内存方法，提出HybridMem方法并验证其有效性，还开源项目。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体内存方法在处理复杂多跳推理任务时存在局限，无法满足现实需求。

Method: 定义多跳个性化推理任务，构建数据集和统一评估框架，实现多种显式和隐式内存方法进行实验，探索混合方法并提出HybridMem。

Result: 通过大量实验验证了所提出的HybridMem方法的有效性。

Conclusion: 提出的多跳个性化推理任务及HybridMem方法有助于解决现有内存方法在多跳推理上的局限，推动相关研究。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [12] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: 提出DIVE多智能体工作流程，从科学文献图形元素读取和整理实验数据，在固态储氢材料上验证效果良好，工作流程可广泛迁移。


<details>
  <summary>Details</summary>
Motivation: 科学文献中大量材料数据存在于非结构化图表中，阻碍基于大语言模型的AI代理构建用于自动化材料设计。

Method: 提出Descriptive Interpretation of Visual Expression (DIVE)多智能体工作流程来读取和整理科学文献中图形元素的实验数据。

Result: DIVE相比多模态模型直接提取，显著提高数据提取的准确性和覆盖率；基于整理的数据库建立快速逆向设计工作流程，能在两分钟内识别未报道的储氢成分。

Conclusion: 所提出的AI工作流程和智能体设计可广泛应用于不同材料，为AI驱动的材料发现提供范式。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [13] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: 心血管疾病是全球首要死因，医护人员短缺，现有AI代理临床应用受限，提出CardAIc - Agents多模态框架，实验显示其比主流模型更高效。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病死亡率高且医护人员短缺，现有AI代理临床应用存在多种局限，需改进。

Method: 提出CardAIc - Agents多模态框架，包括CardiacRAG代理生成计划、主代理执行计划，采用逐步更新策略和多学科讨论工具，提供视觉审查面板。

Result: 在三个数据集上的实验表明CardAIc - Agents比主流视觉 - 语言模型、先进代理系统和微调的视觉 - 语言模型更高效。

Conclusion: CardAIc - Agents多模态框架能有效解决现有AI代理在临床应用中的局限，提高效率。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [14] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: 提出多模态框架STONK改进股票走势预测，性能超仅用数值的基线模型，有评估及代码


<details>
  <summary>Details</summary>
Motivation: 解决孤立分析在股票走势预测中的局限性，改进每日股票走势预测

Method: 通过特征拼接和跨模态注意力结合数值和文本嵌入构建统一管道

Result: 回测显示STONK性能优于仅用数值的基线模型

Conclusion: 对融合策略和模型配置的综合评估为可扩展的多模态金融预测提供了基于证据的指导

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [15] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: 提出新颖神经符号规划框架LOOP，在标准IPC基准域表现出色，证明让神经网络和符号推理器全程交互是可靠规划关键。


<details>
  <summary>Details</summary>
Motivation: 现有神经规划方法在复杂领域表现不佳，经典规划器缺乏灵活性和自然语言理解能力，现有神经符号方法未让神经和符号组件共同完善解决方案。

Method: 将规划视为神经和符号组件的迭代对话，集成13个协调的神经特征，生成PDDL规范并根据符号反馈迭代优化，从执行轨迹构建因果知识库。

Result: 在六个标准IPC基准域上，LOOP成功率达85.8%，高于LLM+P（55.0%）、LLM-as - Planner（19.2%）和Tree - of - Thoughts（3.3%）。

Conclusion: 可靠规划关键在于让神经网络和符号推理器在整个过程中相互‘对话’，LOOP为构建可用于关键现实应用的自主系统提供蓝图。

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [16] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: 本文提出SPANER框架，将多模态输入嵌入统一语义空间，实验证明其在少样本检索表现佳，强调对齐嵌入结构对可扩展多模态学习重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态参数高效微调方法忽视多模态嵌入空间结构，导致模态特定表示孤立，限制跨模态泛化。

Method: 引入SPANER框架，采用共享提示机制作为概念锚点，将不同模态输入嵌入统一语义空间，且设计具有扩展性。

Result: 在视觉 - 语言和视听基准测试中，SPANER展现出有竞争力的少样本检索性能，且学习的嵌入空间保持高语义连贯性。

Conclusion: 对于可扩展多模态学习，对齐嵌入结构比仅调整适配器权重更重要。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [17] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出连续学习的表格提取系统TASER处理真实金融表格，表现优于现有模型，强调连续学习重要性并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 现实金融文档表格杂乱、多页且碎片化，需有效提取信息方法。

Method: 构建TASER系统，利用初始模式执行表格检测、分类、提取和推荐，推荐代理审查输出、建议模式修订。

Result: TASER比现有模型性能高10.1%，大批次增加模式推荐利用率和提取量，创建首个真实金融表格数据集。

Conclusion: 代理式、模式引导的提取系统在理解真实金融表格方面有前景。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [18] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: 本文展示了一种领域无关的智能体AI系统可独立完成科学研究流程，进行心理学研究并产出成果，向具身AI迈进但也引发思考。


<details>
  <summary>Details</summary>
Motivation: 现有AI局限于狭窄领域且需大量人工监督，科学文献增长和领域专业化限制研究者跨学科知识整合，因此探索通用AI系统。

Method: 开发领域无关的智能体AI系统，让其独立完成心理学研究的从假设生成到论文撰写的科学工作流程。

Result: AI系统能完成有理论推理和方法严谨性的非平凡研究，但在概念细微差别和理论解释上有局限。

Conclusion: 这是迈向具身AI的一步，可加速科学发现，但也引发对科学理解本质和科研功劳归属的问题。

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [19] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: 提出STPFormer解决时空交通预测难题，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 时空交通预测因复杂模式、动态结构和多样输入格式具有挑战性，Transformer模型有刚性时间编码和弱时空融合问题。

Method: 提出STPFormer，集成TPA、SSA、STGM和Attention Mixer四个模块。

Result: 在五个真实数据集上取得新的SOTA结果，消融和可视化实验证实有效性和泛化性。

Conclusion: STPFormer能通过统一且可解释的表征学习实现时空交通预测的SOTA性能。

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [20] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 本文引入离散最小 - 最大违规（DMMV）问题，提出 GPU 加速启发式算法求解，通过三个用例验证其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决具有最坏情况性能要求的离散值分配优化问题，提供通用解决方案。

Method: 数学定义 DMMV 问题，利用其数学特性开发 GPU 加速启发式算法。

Result: 在语言模型量化中平均提升 14%；离散断层扫描中降低 16% 重建误差，GPU 计算加速 6 倍；FIR 滤波器设计中波纹减少近 50%。

Conclusion: 研究 DMMV 有好处，提出的启发式算法在三个问题上有优势，代码开源促进相关研究。

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [21] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 研究语言模型代理安全问题，开发评估框架发现性能差距，据此开发风险验证器使风险执行显著降低。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在安全关键场景有潜在严重风险，且其风险意识和安全执行能力存在差距。

Method: 开发综合评估框架从三方面评估代理安全，利用评估发现的差距开发风险验证器和抽象器。

Result: 评估发现性能差距，所开发系统使风险动作执行显著降低55.3%。

Conclusion: 单纯提升模型能力或推理计算不能解决安全问题，所开发系统能有效降低风险动作执行。

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [22] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: 本文提出CrafterDojo，一套基础模型和工具，将Crafter环境打造成适合通用具身智能体研究的轻量级测试平台。


<details>
  <summary>Details</summary>
Motivation: 开发通用具身智能体是AI核心挑战，Minecraft不适合快速原型开发，而Crafter因缺乏基础模型应用受限，需解决此问题。

Method: 引入CrafterVPT、CrafterCLIP和CrafterSteve - 1分别用于行为先验、视觉语言基础和指令跟随，还提供生成行为和字幕数据集的工具包、参考智能体实现、基准评估和完整开源代码库。

Result: 未提及明确实验结果

Conclusion: CrafterDojo让Crafter环境成为轻量级、适合原型开发且类似Minecraft的通用具身智能体研究测试平台。

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [23] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: 提出EAG - RL框架提升大语言模型电子病历推理能力，实验显示有显著效果且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在电子病历推理任务表现不佳，现有方法无法提升其内在推理能力且存在泛化局限，需改进。

Method: 提出EAG - RL两阶段训练框架，先用专家引导的蒙特卡罗树搜索构建推理轨迹初始化模型策略，再通过强化学习优化策略。

Result: 在两个真实电子病历数据集实验中，EAG - RL平均提升大语言模型电子病历推理能力14.62%，增强了对特征扰动的鲁棒性和对未见临床领域的泛化能力。

Conclusion: EAG - RL在临床预测任务的实际部署中有实用潜力。

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [24] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 研究探索强化学习在图表到代码生成任务中的应用，提出MSRL方法突破监督微调性能瓶颈，在相关基准测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在处理需要深入理解信息丰富图像和生成结构化输出的任务上研究不足，监督微调在图表到代码生成任务中存在性能瓶颈，需要有效强化学习策略。

Method: 提出多模态结构化强化学习（MSRL）方法，构建含300万图表 - 代码对的训练语料库，利用多粒度结构化奖励系统，结合文本和视觉反馈，在两阶段课程中训练。

Result: MSRL显著突破监督微调性能瓶颈，在ChartMimic和ReachQA基准测试中高级指标分别提高6.2%和9.9%，与先进闭源模型表现相当。

Conclusion: MSRL方法在图表到代码生成任务中有效，能突破监督微调性能限制，取得较好效果。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [25] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 传统GUI元素定位方法有局限，提出V2P方法，在两个基准测试取得良好性能，证明其泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽略空间交互不确定性和视觉语义层次，现有方法存在注意漂移和点击不精确问题，需改进GUI元素精确本地化方法。

Method: 提出V2P方法，引入抑制注意力机制减少背景干扰，采用受菲茨定律启发的方法将GUI交互建模为二维高斯热图区分中心和边缘。

Result: V2P训练的模型在ScreenSpot - v2和ScreenSpot - Pro两个基准测试分别达到92.3%和50.5%的性能，消融实验证实各组件贡献。

Conclusion: V2P方法对精确GUI定位任务具有良好泛化性。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [26] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 提出带软约束的查询回答问题，介绍神经查询重排器（NQR），实验证明其能捕获软约束并保持查询回答性能。


<details>
  <summary>Details</summary>
Motivation: 现有查询回答方法聚焦一阶逻辑形式化查询，而现实中很多查询有模糊或上下文相关的软约束，存在研究空白。

Method: 提出NQR，通过纳入软约束调整查询答案分数，基于偏好和非偏好实体的增量示例交互式细化答案，扩展现有QA基准生成带软约束的数据集。

Result: 实验表明NQR能捕获软约束并保持查询回答性能。

Conclusion: NQR可有效处理带软约束的查询回答问题。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [27] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 针对LIME在数据受限场景下的局限性，提出ITL - LIME框架提升解释保真度和稳定性。


<details>
  <summary>Details</summary>
Motivation: LIME在数据受限场景下因随机扰动和采样存在局部性和不稳定性问题，导致生成不真实样本，难以准确近似原模型决策边界，需改进。

Method: 在LIME框架中引入实例迁移学习，用聚类划分源域，选取与目标实例最相似原型所在簇的源实例，结合目标实例邻域实例，构建基于对比学习的编码器作为加权机制，用加权实例训练代理模型。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [28] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 研究描述家庭行动的知识图谱，指出情景知识图谱特性使许多链接预测算法不适用。


<details>
  <summary>Details</summary>
Motivation: 知识图谱可用于多种用途，描述家庭行动的知识图谱对控制家用机器人和分析视频片段有益，且视频信息不完整，需完善知识图谱。

Method: 未提及

Result: 情景知识图谱有特殊特性，使许多链接预测算法不适合该工作，甚至无法超越简单基线。

Conclusion: 许多标准链接预测算法不适用于情景知识图谱。

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [29] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: 为改善第三方简历质量，提出MHSNet框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 第三方获取的简历常不完整不准确，需进行重复检测，但简历文本语义复杂、结构异构、信息不完整，检测有挑战。

Method: 提出MHSNet，用对比学习微调BGE - M3，MoE生成简历多层次稀疏和密集表示计算语义相似度，用状态感知MoE处理不完整简历。

Result: 实验验证了MHSNet的有效性。

Conclusion: MHSNet可用于改善第三方简历质量，丰富公司人才库。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [30] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 本文全面回顾了神经符号方法提升大语言模型推理能力的进展，介绍推理任务形式化和学习范式，从三方面讨论方法，指出挑战和未来方向并发布相关资源库。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力是挑战，开发强推理能力AI系统是实现AGI的关键里程碑，受学界和业界关注，神经符号方法是有前景的提升途径。

Method: 先对推理任务进行形式化并介绍神经符号学习范式，再从Symbolic->LLM、LLM->Symbolic和LLM+Symbolic三个视角讨论提升推理能力的神经符号方法。

Result: 梳理了神经符号方法提升大语言模型推理能力的研究进展。

Conclusion: 讨论了关键挑战和有前景的未来方向，发布了相关资源库。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [31] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: 提出名为DeepLog的神经符号AI理论与操作框架，介绍其组成、特点并通过实验展示其通用性和效率。


<details>
  <summary>Details</summary>
Motivation: 为神经符号AI提供一个理论和操作框架。

Method: 引入构建块和原语，由DeepLog语言和扩展代数电路两个关键组件构成神经符号抽象机，软件实现并利用GPU实现代数电路的最新见解。

Result: 通过实验对比不同模糊和概率逻辑、逻辑在架构或损失函数中的使用以及基于CPU和基于GPU的实现，展示了DeepLog神经符号机的通用性和效率。

Conclusion: DeepLog是一个通用且高效的神经符号AI框架。

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [32] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: 针对大语言模型代理在协作任务中因果推理不足问题，提出CausalPlan框架，实验证明其能减少无效动作、提升协作表现，凸显因果驱动规划价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在协作任务中依赖表面关联，缺乏因果推理，导致动作无效或不连贯，影响协调和规划性能。

Method: 提出CausalPlan两阶段框架，核心是SCA模型，从代理轨迹学习因果图，为LLM生成的提议分配因果分数，指导动作选择，且无需微调LLM。

Result: 在Overcooked - AI基准的多代理协调任务上对不同大小的四个LLM评估，CausalPlan在AI - AI和人类 - AI场景中均减少无效动作、提升协作表现，优于强化学习基线。

Conclusion: 因果驱动规划对部署高效、可解释和可泛化的多代理LLM系统有重要价值。

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [33] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: 文章提出Expertise - aware Multi - LLM Recruitment and Collaboration (EMRC)框架提升医疗决策系统准确性和可靠性，经评估优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单LLM方法在医疗决策中受参数知识和静态训练语料限制，无法有效整合临床信息，需新方法解决该问题。

Method: 框架分两阶段，一是构建LLM专业知识表动态选择最优LLM作为医疗专家代理；二是所选代理生成带自信分数的响应，通过置信融合和对抗验证提高诊断可靠性。

Result: 在三个公共医疗决策数据集上评估，EMRC优于现有单和多LLM方法，如在MMLU - Pro - Health数据集上准确率达74.45%，比GPT - 4 - 0613高2.69%。

Conclusion: EMRC框架有效，其专业知识感知的代理招募策略和利用各LLM专业能力的代理互补性表现良好。

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [34] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 本文提出一种新的量化公式实例化方法，利用概率上下文无关文法生成新项，平衡量词推理的利用与探索。


<details>
  <summary>Details</summary>
Motivation: 量化公式因不可判定性对SMT求解器构成挑战，现有实例化技术相互补充，需新方法。

Method: 将观察到的实例化视为潜在语言样本，用概率上下文无关文法生成新的相似项，还可反转学习到的项概率。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [35] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文对基于RAG系统的集成方法进行研究，从理论和机制角度分析RAG集成框架，实验表明聚合多个RAG系统具有泛化性和鲁棒性，为多RAG系统集成研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 单一RAG框架难以适应广泛的下游任务，需探索利用多个RAG系统的优势。

Method: 从理论（信息熵角度）和机制（管道和模块层面）对RAG集成框架进行分析，选取四种管道和三种模块解决七个研究问题。

Result: 实验显示，在管道和模块层面，聚合多个RAG系统具有泛化性和鲁棒性。

Conclusion: 研究为多RAG系统集成的类似研究奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [36] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 本文提出改进方法提升PDDL规划中广义计划质量，在17个基准领域实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 以往工作仅生成一种策略且直接用于程序生成，策略错误会导致广义计划错误。

Method: 以伪代码形式生成策略并自动调试，扩展Python调试阶段加入反思步骤，生成多个程序变体并选最优。

Result: 在17个基准领域实验表明，这些扩展显著提升广义计划质量，在12个领域中，最佳Python程序可解决对应实例生成器能生成的所有任务。

Conclusion: 提出的方法可有效提升广义计划质量且不会使其变差。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [37] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: 本文介绍了用于金融应用的时间序列建模工作流自动化框架TS - Agent，经评估其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在金融市场决策中至关重要，但构建高性能、可解释和可审计的模型存在挑战，现有AutoML框架缺乏适应性，而大语言模型提供了工作流自动化新思路。

Method: 引入模块化的代理框架TS - Agent，将流程形式化为模型选择、代码优化和微调三个阶段的迭代决策过程，核心是配备结构化知识库和模型库的规划代理。

Result: 在金融预测和合成数据生成任务上，TS - Agent始终优于最先进的AutoML和代理基线，实现了更高的准确性、鲁棒性和决策可追溯性。

Conclusion: TS - Agent能有效自动化和增强金融应用的时间序列建模工作流，满足金融服务等高风险环境的关键需求。

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [38] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: 研究多梯队供应链中生成式AI代理的战略行为，发现“协作悖论”，提出结合两层策略实现系统弹性的框架。


<details>
  <summary>Details</summary>
Motivation: 探究经济环境中自主AI驱动代理在多梯队供应链合作环境下的涌现战略行为。

Method: 在受控供应链模拟中对由大语言模型驱动的生成式AI代理进行计算实验。

Result: 发现“协作悖论”，即理论上优越的协作AI代理表现比非AI基线更差，原因是代理囤积库存使系统缺货；提出结合高层AI驱动的主动策略设定和低层协作执行协议的框架可实现系统弹性。

Conclusion: 为协作AI代理的涌现行为提供关键见解，为设计稳定有效的AI驱动商业分析系统提供蓝图。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [39] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 本文探讨将预训练大语言模型定制为虚拟助手以辅助PyChrono仿真工具使用，提出框架并取得一定效果，框架具通用性。


<details>
  <summary>Details</summary>
Motivation: 研究能否将预训练大语言模型细化定制为辅助专家有效使用仿真工具的虚拟助手。

Method: 提出一个细化和定制开源及闭源大语言模型的框架来生成PyChrono仿真脚本。

Result: 通过该过程对几类大语言模型细化定制，生成的脚本质量有可量化提升，虽不完美但可作修改起点，还能回答API问题和推荐建模方法。

Conclusion: 框架具有通用性，可降低其他领域仿真工具的使用门槛。

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [40] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 本文用BRKGA算法解决LRS问题，与MMAS和CPLEX对比，表明BRKGA是当前解决该问题的先进技术，但大字母表输入串仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 解决LRS这一NP难的生物信息子序列问题，该问题在基因组重组中有应用。

Method: 采用BRKGA算法，注重个体评估的计算效率；开发并实现MMAS算法；使用整数线性规划求解器CPLEX。

Result: 计算结果表明BRKGA是解决LRS问题的先进技术。

Conclusion: BRKGA目前是解决LRS问题的先进技术，但大字母表输入串场景下有改进空间。

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [41] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: 介绍ComputerRL框架用于桌面智能，采用API - GUI范式，开发分布式RL基础设施和Entropulse策略，在模型上测试取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决机器智能体与以人类为中心的桌面环境不匹配问题，实现复杂数字工作空间操作，并解决端到端RL训练在不同桌面任务上的扩展性和泛化性难题。

Method: 采用API - GUI范式，开发分布式RL基础设施以加速大规模在线RL，提出Entropulse训练策略缓解熵坍缩。

Result: 在OSWorld基准测试中，基于GLM - 4 - 9B - 0414的AutoGLM - OS - 9B达到48.1%的新SOTA准确率。

Conclusion: ComputerRL框架及相关策略有效提升了通用智能体在桌面自动化方面的性能，算法和框架被用于构建AutoGLM。

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [42] [A novel geometric predictive algorithm for assessing Compressive Elastic Modulus in MEX additive processes, based on part nonlinearities and layers stiffness,validated with PETG and PLA materials](https://arxiv.org/abs/2508.13164)
*J. M. Mercado Colmenero,C. Martin Donate*

Main category: cs.CE

TL;DR: 论文提出基于新预测算法的创新方法，能获取MEX制造塑料材料弹性模量及弹性区力学行为，经实验和数值验证，改进现有技术。


<details>
  <summary>Details</summary>
Motivation: 获取MEX制造塑料材料的弹性模量和弹性区力学行为，改进现有技术。

Method: 开发新的预测算法，仅需输入各向同性塑料材料长丝的压缩弹性模量和MEX工艺制造参数，计算每层刚度。

Result: 使用PETG和PLA材料在试样和变拓扑案例研究中对算法进行了实验和数值验证，算法对每种打印模式和制造方向都有效。

Conclusion: 新算法显著改进现有技术，适用于大多数适合MEX 3D打印的塑料聚合物材料，可用于复杂几何形状部件，无需机械分析软件或昂贵实验验证。

Abstract: The paper presents an innovative methodology based on the use of a new
predictive algorithm created by the researchers capable of obtaining the
elastic modulus of a plastic material manufactured with MEX and its mechanical
behaviour in the elastic zone under compressive loads. The predictive algorithm
only needs as input the compressive elastic modulus of the isotropic plastic
material filament and the manufacturing parameters of the MEX process. The
smart developed algorithm calculates the stiffness of each layer considering
the number of holes in the projected area. The innovative predictive algorithm
has been experimentally and numerically validated using PETG Polyethylene
Terephthalate Glycol material and PLA Polylactic Acid on test specimens and on
a case study of variable topology. The predictive algorithm is valid for each
print pattern and manufacturing direction. The new algorithm improves the
existing state of the art significantly since this algorithm extends its
utility to most plastic polymer materials suitable for MEX 3D printing,
provided that the mechanical and elastic properties of the filament are known.
Its versatility extends to complex component geometries subjected to uniaxial
compression loads, eliminating the need for mechanical analysis software or
expensive experimental validations.

</details>


### [43] [From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models](https://arxiv.org/abs/2508.13491)
*Ziyan Kuang,Feiyu Zhu,Maowei Jiang,Yanzhao Lai,Zelin Wang,Zhitong Wang,Meikang Qiu,Jiajia Huang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CE

TL;DR: 现有金融大语言模型基准存在不足，本文提出FinCDM评估框架和CPA - QKA数据集，实验表明FinCDM能揭示模型知识缺口等，推动模型开发且数据公开。


<details>
  <summary>Details</summary>
Motivation: 现有金融大语言模型基准仅依赖分数评估，数据集覆盖概念窄，无法满足实际应用，需改进评估方式。

Method: 引入FinCDM评估框架，可在知识技能层面评估模型；构建CPA - QKA数据集，涵盖真实会计和金融技能，由专家严格标注。

Result: 对30个大语言模型实验显示，FinCDM能揭示隐藏知识缺口、发现传统基准忽视的测试领域、找出模型行为集群。

Conclusion: FinCDM开创金融大语言模型评估新范式，支持可解释、技能感知诊断，推动更可靠和有针对性的模型开发，数据和脚本将公开。

Abstract: Large Language Models (LLMs) have shown promise for financial applications,
yet their suitability for this high-stakes domain remains largely unproven due
to inadequacies in existing benchmarks. Existing benchmarks solely rely on
score-level evaluation, summarizing performance with a single score that
obscures the nuanced understanding of what models truly know and their precise
limitations. They also rely on datasets that cover only a narrow subset of
financial concepts, while overlooking other essentials for real-world
applications. To address these gaps, we introduce FinCDM, the first cognitive
diagnosis evaluation framework tailored for financial LLMs, enabling the
evaluation of LLMs at the knowledge-skill level, identifying what financial
skills and knowledge they have or lack based on their response patterns across
skill-tagged tasks, rather than a single aggregated number. We construct
CPA-QKA, the first cognitively informed financial evaluation dataset derived
from the Certified Public Accountant (CPA) examination, with comprehensive
coverage of real-world accounting and financial skills. It is rigorously
annotated by domain experts, who author, validate, and annotate questions with
high inter-annotator agreement and fine-grained knowledge labels. Our extensive
experiments on 30 proprietary, open-source, and domain-specific LLMs show that
FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax
and regulatory reasoning overlooked by traditional benchmarks, and uncovers
behavioral clusters among models. FinCDM introduces a new paradigm for
financial LLM evaluation by enabling interpretable, skill-aware diagnosis that
supports more trustworthy and targeted model development, and all datasets and
evaluation scripts will be publicly released to support further research.

</details>


### [44] [Data-Driven Discovery of Multi-Dimensional Breakage Population Balance Equations](https://arxiv.org/abs/2508.13763)
*Suet Lin Leong,Firnaaz Ahamed,Yong Kuen Ho*

Main category: cs.CE

TL;DR: 本文提出mPBE - ID算法利用数据驱动稀疏回归技术从数据中直接发现多维破碎种群平衡方程，能处理噪声和有限数据，有望成为通用框架。


<details>
  <summary>Details</summary>
Motivation: 当前逆解技术局限于一维情况且依赖先验知识，难以系统发现多维破碎潜在控制方程。

Method: 开发mPBE - ID算法，采用破碎信息约束稀疏回归、基于动态模式分解构建候选库函数、集成方法处理噪声和有限数据。

Result: mPBE - ID算法能发现不同形式的多维破碎种群平衡方程，即使数据有噪声和有限。

Conclusion: mPBE - ID算法可作为基础框架，用于推广多维种群平衡方程在各种高维颗粒现象中的发现。

Abstract: Multi-dimensional breakage is a ubiquitous phenomenon in natural systems, yet
the systematic discovery of underlying governing equations remains a
long-standing challenge. Current inverse solution techniques are restricted to
one-dimensional cases and typically depend on the availability of a priori
system knowledge, thus limiting their applicability. By leveraging advances in
data-driven sparse regression techniques, we develop the Multi-Dimensional
Breakage Population Balance Equation Identification (mPBE ID) algorithm for
discovering multi-dimensional breakage population balance equations (mPBEs)
directly from data. Our mPBE-ID enables tractable identification of mPBEs by
incorporating several key strategies, namely, a breakage-informed constrained
sparse regression, targeted candidate library functions construction via
insights from Dynamic Mode Decomposition (DMD), and robust handling of
noisy/limited data through ensembling (bagging/bragging). Notably, we
demonstrate how the DMD is indispensable for distilling dominant breakage
dynamics which can then be used to facilitate the systematic inclusion of
candidate library terms. We showcase the ability of the mPBE-ID to discover
different forms of mPBE (including those with discontinuous stoichiometric
kernels) even when tested against noisy and limited data. We anticipate that
the mPBE-ID will serve as a foundational framework for future extensions to
generalize the discovery of multi-dimensional PBEs for various high-dimensional
particulate phenomena.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Scavenger: Better Space-Time Trade-Offs for Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13909)
*Jianshun Zhang,Fang Wang,Sheng Qiu,Yi Wang,Jiaxin Ou,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: 本文分析KV分离LSM树空间放大问题，提出Scavenger，实验表明其能提升写入性能并降低空间放大。


<details>
  <summary>Details</summary>
Motivation: 现有KV分离LSM树存在空间放大问题，且垃圾回收策略优化性能不足，现有方案忽视索引LSM树空间放大影响。

Method: 提出I/O高效的垃圾回收方案减少I/O开销，采用基于补偿大小的空间感知压缩策略降低索引LSM树空间放大。

Result: Scavenger显著提升写入性能，比其他KV分离LSM树实现更低的空间放大。

Conclusion: Scavenger能在性能和空间放大之间取得更好的平衡。

Abstract: Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)
have gained widespread acceptance in storage systems. Nonetheless, a
significant challenge arises in the form of high write amplification due to the
compaction process. While KV-separated LSM-trees successfully tackle this
issue, they also bring about substantial space amplification problems, a
concern that cannot be overlooked in cost-sensitive scenarios. Garbage
collection (GC) holds significant promise for space amplification reduction,
yet existing GC strategies often fall short in optimization performance,
lacking thorough consideration of workload characteristics. Additionally,
current KV-separated LSM-trees also ignore the adverse effect of the space
amplification in the index LSM-tree. In this paper, we systematically analyze
the sources of space amplification of KV-separated LSM-trees and introduce
Scavenger, which achieves a better trade-off between performance and space
amplification. Scavenger initially proposes an I/O-efficient garbage collection
scheme to reduce I/O overhead and incorporates a space-aware compaction
strategy based on compensated size to minimize the space amplification of index
LSM-trees. Extensive experiments show that Scavenger significantly improves
write performance and achieves lower space amplification than other
KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).

</details>


### [46] [Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13935)
*Jianshun Zhang,Fang Wang,Jiaxin Ou,Yi Wang,Ming Zhao,Sheng Qiu,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: 分析KV分离LSM树空间放大问题，提出Scavenger+方案，实验表明其能提升写入性能并减少空间放大。


<details>
  <summary>Details</summary>
Motivation: 现有KV分离LSM树存在空间放大问题，且垃圾回收策略低效，当前方案忽略索引LSM树导致的空间放大。

Method: 提出Scavenger+，包含I/O高效的垃圾回收方案、基于补偿大小的空间感知压缩策略和适应系统负载的动态GC调度器。

Result: 与现有KV分离LSM树相比，Scavenger+显著提高了写入性能并减少了空间放大。

Conclusion: Scavenger+实现了更好的性能 - 空间权衡。

Abstract: Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are
widely used in storage systems but face significant challenges, such as high
write amplification caused by compaction. KV-separated LSM-trees address write
amplification but introduce significant space amplification, a critical concern
in cost-sensitive scenarios. Garbage collection (GC) can reduce space
amplification, but existing strategies are often inefficient and fail to
account for workload characteristics. Moreover, current key-value (KV)
separated LSM-trees overlook the space amplification caused by the index
LSM-tree. In this paper, we systematically analyze the sources of space
amplification in KV-separated LSM-trees and propose Scavenger+, which achieves
a better performance-space trade-off. Scavenger+ introduces (1) an
I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a
space-aware compaction strategy based on compensated size to mitigate
index-induced space amplification, and (3) a dynamic GC scheduler that adapts
to system load to make better use of CPU and storage resources. Extensive
experiments demonstrate that Scavenger+ significantly improves write
performance and reduces space amplification compared to state-of-the-art
KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.

</details>


### [47] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: 本文对数据库、数据仓库、网页和知识图谱日志的使用进行系统调研，分析300多篇文献，揭示日志使用现状并为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 日志有效利用价值高，但各领域相关研究分散，缺乏综合研究。

Method: 分析300多篇出版物，探讨不同类型日志的结构和功能特性、使用标准流程、利用的约束和非功能需求。

Result: 发现端到端方法有限，日志使用流程缺乏标准化，不同类型日志存在共享结构元素。

Conclusion: 本调研为研究人员和从业者提供日志使用的全面概述，指明未来研究有前景的方向，特别是知识图谱日志的利用和普及。

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: 介绍全栈分布式框架MELISO+用于高效内存计算，可减少误差、提升能效和降低延迟，推动可持续高维计算。


<details>
  <summary>Details</summary>
Motivation: 传统架构能耗高，基于RRAM的内存计算有设备非理想性和可扩展性问题，需解决以满足全球计算需求。

Method: 提出两层纠错机制减轻设备非理想性，开发分布式RRAM计算框架实现大规模矩阵计算。

Result: 减少90%以上算术误差，提升能效三到五个数量级，降低延迟100倍。

Conclusion: MELISO+结合算法硬件协同设计与可扩展架构，推动适用于大语言模型等的可持续高维计算。

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [49] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: 分析Comb基准测试套件中模板通信在使用非阻塞、持久和分区通信例程时的性能，以及各优化在不同规模的影响，发现持久和分区MPI通信有显著加速。


<details>
  <summary>Details</summary>
Motivation: 并行应用的迭代模板操作在大规模下性能受通信成本主导，需分析MPI优化对模板通信性能的影响。

Method: 在Comb基准测试套件中使用非阻塞、持久和分区通信例程，分析各优化在不同规模的影响，以及进程数、线程数和消息大小对分区通信例程的影响。

Result: 持久MPI通信比基线MPI通信最多加速37%，分区MPI通信最多加速68%。

Conclusion: 持久和分区MPI通信能有效提高模板通信性能。

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [50] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: 提出OrbitChain协作分析框架实现地球观测实时分析，实验表明比现有框架完成更多工作负载并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测卫星因地-星连接带宽和时长限制，无法满足实时应用需求，如及时灾害响应。

Method: 引入OrbitChain框架，将分析应用分解为微服务，分配计算资源，设计流量路由算法，采用流水线工作流。

Result: 实现硬件在环轨道计算测试平台，系统比现有框架多完成60%分析工作负载，通信开销最多降低72%。

Conclusion: OrbitChain能完成地球观测任务，促进时间敏感应用和星座间协作。

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [51] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: 本文提出针对大型GPU感知全规约操作的优化方法，利用多CPU核心加速，在多台超级计算机上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 深度学习中大型GPU间全规约操作受通信成本瓶颈限制，且并行应用中大量CPU核心闲置。

Method: 将车道感知规约扩展到GPU，并为每个GPU使用多个CPU核心加速操作。

Result: 在NCSA的Delta超级计算机的NVIDIA A100 GPU上，大型MPI全规约操作加速比达2.45倍；在两台先进超级计算机上，分别在NVIDIA和AMD的集体通信库中实现最高1.77倍和1.71倍加速。

Conclusion: 提出的多CPU加速的GPU感知车道全规约方法能有效加速大型全规约操作。

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [52] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: 本文分析DDoS攻击类型、检测与预防技术，为提升网络安全防护提供见解和指南。


<details>
  <summary>Details</summary>
Motivation: 当前DDoS攻击复杂性和频率增加，检测和缓解难度大，需有效应对。

Method: 分析各类DDoS攻击，研究现有检测技术（如包过滤、入侵检测系统、机器学习方法）和预防技术（如防火墙、速率限制等），评估其效果和适用性。

Result: 评估了不同检测和预防技术的效果及对不同攻击类型和环境的适用性。

Conclusion: 提供了DDoS攻击类型、检测和预防技术的全面概述，为组织和个人增强网络安全提供见解和指南。

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [53] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: 介绍LAMMPS将Kokkos库集成到现有C++代码以适应现代异构计算环境，研究不同原子势的性能可移植性，展示在不同GPU和美国超算上的性能结果。


<details>
  <summary>Details</summary>
Motivation: 使LAMMPS适应现代异构计算环境。

Method: 将Kokkos性能可移植性库集成到现有的C++代码中，研究简单成对、多体反应和机器学习力场原子间势的性能可移植性。

Result: 展示了不同供应商和代际GPU的结果，分析性能趋势，在当前美国所有百亿亿次级机器上展示了三种势的强扩展性。

Conclusion: LAMMPS通过集成Kokkos库在现代异构计算环境中表现出良好的性能可移植性和扩展性。

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [54] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: 科学数据量激增带来问题，提出评估问题，介绍LUNDIsim数据集用于评估和处理工作流，给出下载链接。


<details>
  <summary>Details</summary>
Motivation: 科学数据量增长引发可计算性、可解释性和可持续性担忧，尤其在地球科学领域。

Method: 提出五个评估问题，采用无损和有损压缩技术，分享LUNDIsim数据集。

Result: 提供LUNDIsim数据集，有不同分辨率和环境，适合多种工作流。

Conclusion: LUNDIsim数据集可用于数据大小缩减算法评估和地质与油藏工程的高级网格处理工作流。

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [55] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: 开发碳足迹插件扩展Batsim模拟器功能，评估数据中心任务和资源管理策略环境影响。


<details>
  <summary>Details</summary>
Motivation: 全面评估数据中心任务和资源管理策略的环境影响。

Method: 在SimGrid中开发插件，基于模拟平台能耗和机器碳强度因子计算碳排放，集成到Batsim。

Result: 成功开发插件并集成到Batsim，与现有模拟工作流兼容。

Conclusion: 该插件能让研究人员评估调度策略的碳效率。

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [56] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: 提出CaPGNN框架用于单服务器多GPU的全批次GNN训练，通过自适应缓存和资源感知分区算法减少通信开销、平衡负载，实验显示有效降低通信成本并加速训练。


<details>
  <summary>Details</summary>
Motivation: 现有全批次GNN训练在分布式环境中存在高通信开销和负载不平衡问题，限制了可扩展性。

Method: 提出联合自适应缓存算法利用CPU和GPU内存减少顶点特征重复传输；引入资源感知图分区算法根据GPU异构计算和通信能力动态调整子图大小。

Result: 在大规模基准数据集实验中，CaPGNN相比现有方法最多降低96%通信成本，最多加速训练12.7倍。

Conclusion: 自适应缓存和资源感知分区算法有助于在分布式计算环境中实现可扩展、高效且实用的全批次GNN训练。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [57] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: 研究SG2044在HPC性能，与SG2042等架构对比，发现SG2044高核心数优势大，缩小与其他架构性能差距。


<details>
  <summary>Details</summary>
Motivation: RISC - V在HPC未普及，对SG2044进行HPC性能研究。

Method: 对比SG2044与SG2042及其他架构性能。

Result: SG2044高核心数性能优势大，64核时比SG2042高4.91倍，缩小与其他架构性能差距。

Conclusion: SG2044中RVV v1.0支持和内存子系统升级有效，尤其适用于计算密集型工作负载。

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [58] [Tight Bounds for Sparsifying Random CSPs](https://arxiv.org/abs/2508.13345)
*Joshua Brakensiek,Venkatesan Guruswami,Aaron Putterman*

Main category: cs.DS

TL;DR: 本文开启随机CSP稀疏化研究，分析r - 分块和均匀两种模型，发现r - 分块模型有明确阈值现象，均匀模型情况更复杂，还给出判定特定谓词情况的方法。


<details>
  <summary>Details</summary>
Motivation: 研究给定CSP实例最稀疏的重加权问题，开启随机CSP稀疏化的研究。

Method: 考虑r - 分块和均匀两个随机模型，分析不同模型下随机实例的稀疏化情况。

Result: r - 分块模型有明确阈值现象；均匀模型较复杂，部分谓词稀疏性非单调，给出判定特定谓词情况的有效计算程序。

Conclusion: 明确不同随机模型下CSP实例的稀疏化特性及判定方法。

Abstract: The problem of CSP sparsification asks: for a given CSP instance, what is the
sparsest possible reweighting such that for every possible assignment to the
instance, the number of satisfied constraints is preserved up to a factor of $1
\pm \epsilon$? We initiate the study of the sparsification of random CSPs. In
particular, we consider two natural random models: the $r$-partite model and
the uniform model. In the $r$-partite model, CSPs are formed by partitioning
the variables into $r$ parts, with constraints selected by randomly picking one
vertex out of each part. In the uniform model, $r$ distinct vertices are chosen
at random from the pool of variables to form each constraint.
  In the $r$-partite model, we exhibit a sharp threshold phenomenon. For every
predicate $P$, there is an integer $k$ such that a random instance on $n$
vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and
can be sparsified to size $\approx n^k$ if $m \ge n^k$. Here, $k$ corresponds
to the largest copy of the AND which can be found within $P$. Furthermore,
these sparsifiers are simple, as they can be constructed by i.i.d. sampling of
the edges.
  In the uniform model, the situation is a bit more complex. For every
predicate $P$, there is an integer $k$ such that a random instance on $n$
vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and
can sparsified to size $\approx n^k$ if $m \ge n^{k+1}$. However, for some
predicates $P$, if $m \in [n^k, n^{k+1}]$, there may or may not be a nontrivial
sparsifier. In fact, we show that there are predicates where the
sparsifiability of random instances is non-monotone, i.e., as we add more
random constraints, the instances become more sparsifiable. We give a precise
(efficiently computable) procedure for determining which situation a specific
predicate $P$ falls into.

</details>


### [59] [On the 2D Demand Bin Packing Problem: Hardness and Approximation Algorithms](https://arxiv.org/abs/2508.13347)
*Susanne Albers,Waldo Gálvez,Ömer Behic Özdemir*

Main category: cs.DS

TL;DR: 研究二维需求装箱问题2D Demand Bin Packing，证明简单变体难近似，给出近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究经典装箱问题的二维推广2D Demand Bin Packing，将矩形任务分配到最少数量的箱子中。

Method: 基于通用框架，为较大任务计算结构化解决方案，用First - Fit算法变体处理较小任务。

Result: 证明简单变体在优于2倍因子内难近似，给出最佳近似算法，一般情况给出3 - 近似算法。

Conclusion: 提出的算法为2D Demand Bin Packing问题提供了有效的近似解决方案。

Abstract: We study a two-dimensional generalization of the classical Bin Packing
problem, denoted as 2D Demand Bin Packing. In this context, each bin is a
horizontal timeline, and rectangular tasks (representing electric appliances or
computational requirements) must be allocated into the minimum number of bins
so that the sum of the heights of tasks at any point in time is at most a given
constant capacity. We prove that simple variants of the problem are NP-hard to
approximate within a factor better than $2$, namely when tasks have short
height and when they are squares, and provide best-possible approximation
algorithms for them; we also present a simple $3$-approximation for the general
case. All our algorithms are based on a general framework that computes
structured solutions for relatively large tasks, while including relatively
small tasks on top via a generalization of the well-known First-Fit algorithm
for Bin Packing.

</details>


### [60] [Concurrent Double-Ended Priority Queues](https://arxiv.org/abs/2508.13399)
*Panagiota Fatourou,Eric Ruppert,Ioannis Xiradakis*

Main category: cs.DS

TL;DR: 本文提出首个专为双端优先队列（DEPQ）设计的并发实现。


<details>
  <summary>Details</summary>
Motivation: 提供双端优先队列的并发实现。

Method: 描述向支持插入和提取最小操作的并发优先队列添加提取最大操作的通用方法，用两个线性化单消费者优先队列构建线性化双消费者DEPQ，还使用基于锁的组合方案允许多个消费者。

Result: 将技术应用于基于列表的优先队列进行说明。

Conclusion: 成功提出双端优先队列的并发实现，且保留无锁性。

Abstract: This work provides the first concurrent implementation specifically designed
for a double-ended priority queue (DEPQ). We do this by describing a general
way to add an ExtractMax operation to any concurrent priority queue that
already supports Insert and ExtractMin operations. The construction uses two
linearizable single-consumer priority queues to build a linearizable
dual-consumer DEPQ (only one process can perform Extract operations at each
end). This construction preserves lock-freedom. We then describe how to use a
lock-based combining scheme to allow multiple consumers at each end of the
DEPQ. To illustrate the technique, we apply it to a list-based priority queue.

</details>


### [61] [Generating the Spanning Trees of Series-Parallel Graphs up to Graph Automorphism](https://arxiv.org/abs/2508.13480)
*Mithra Karamchedu,Lucas Bang*

Main category: cs.DS

TL;DR: 本文研究图的自同构下生成生成树问题，给出系列并行图的算法，包括有向、半有向和无向情况，并提出相关开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决图在自同构下生成生成树的问题，特别是系列并行图的情况。

Method: 针对不同类型的系列并行图（有向、半有向、无向）分别设计算法，利用图的递归结构，不明确计算自同构群。

Result: 给出了有向系列并行图在输出线性时间内生成不等价生成树的算法，还将有向算法适配到半有向情况，对无向情况给出一些观察和开放问题。

Conclusion: 所提出的算法能生成图的不等价生成树，揭示了图的自同构群递归结构与生成树递归结构的相似性。

Abstract: In this paper, we investigate the problem of generating the spanning trees of
a graph $G$ up to the automorphisms or "symmetries" of $G$. After introducing
and surveying this problem for general input graphs, we present algorithms that
fully solve the case of series-parallel graphs, under two standard definitions.
We first show how to generate the nonequivalent spanning trees of a oriented
series-parallel graph $G$ in output-linear time, where both terminals of $G$
have been individually distinguished (i.e. applying an automorphism that
exchanges the terminals produces a different series-parallel graph).
Subsequently, we show how to adapt these oriented algorithms to the case of
semioriented series-parallel graphs, where we still have a set of two
distinguished terminals but neither has been designated as a source or sink.
Finally, we discuss the case of unoriented series-parallel graphs, where no
terminals have been distinguished and present a few observations and open
questions relating to them. The algorithms we present generate the
nonequivalent spanning trees of $G$ but never explicitly compute the
automorphism group of $G$, revealing how the recursive structure of $G$'s
automorphism group mirrors that of its spanning trees.

</details>


### [62] [Finding subdigraphs in digraphs of bounded directed treewidth](https://arxiv.org/abs/2508.13830)
*Raul Lopes,Ignasi Sau*

Main category: cs.DS

TL;DR: 本文探讨有界有向树宽有向图中可高效找到的子图，除有向路径外仅星型图表现良好，并给出正负结果与后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决除有向路径外，有界有向树宽有向图中能否高效找到其他子图的问题。

Method: 文中未明确提及具体方法。

Result: 除有向路径外，似乎只有星型图在有向树宽方面表现良好，同时给出了一系列正负结果，推广了文献中的若干结果。

Conclusion: 除有向路径外，星型图是有界有向树宽有向图中能高效找到的子图，还指出了进一步研究方向。

Abstract: It is well known that directed treewidth does not enjoy the nice algorithmic
properties of its undirected counterpart. There exist, however, some positive
results that, essentially, present XP algorithms for the problem of finding, in
a given digraph $D$, a subdigraph isomorphic to a digraph $H$ that can be
formed by the union of $k$ directed paths (with some extra properties),
parameterized by $k$ and the directed treewidth of $D$. Our motivation is to
tackle the following question: Are there subdigraphs, other than the directed
paths, that can be found efficiently in digraphs of bounded directed treewidth?
In a nutshell, the main message of this article is that, other than the
directed paths, the only digraphs that seem to behave well with respect to
directed treewidth are the stars. For this, we present a number of positive and
negative results, generalizing several results in the literature, as well as
some directions for further research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [63] [Fair Division Among Couples and Small Groups](https://arxiv.org/abs/2508.13432)
*Paul Gölz,Hannane Yaghoubizade*

Main category: cs.GT

TL;DR: 研究不可分割物品在代理组间的公平分配，针对小团体大小情况，分析EF1和PROP分配的存在性与算法。


<details>
  <summary>Details</summary>
Motivation: 解决不可分割物品在代理组间的公平分配问题，聚焦小团体情况。

Method: 分析不同团体数量下EF1分配存在性，设计算法寻找PROP分配并保证相关特性。

Result: 两个情侣组存在且可高效找到EF1分配；三组及以上不一定存在。当团体大小至多为k时，存在且可高效找到PROPk分配，算法还有额外保证。特殊情况存在任意数量情侣组的PROP1分配。

Conclusion: 明确了不同情况下公平分配的存在性与可高效实现性。

Abstract: We study the fair allocation of indivisible goods across groups of agents,
where each agent fully enjoys all goods allocated to their group. We focus on
groups of two (couples) and other groups of small size. For two couples, an EF1
allocation -- one in which all agents find their group's bundle no worse than
the other group's, up to one good -- always exists and can be found
efficiently. For three or more couples, EF1 allocations need not exist.
  Turning to proportionality, we show that, whenever groups have size at most
$k$, a PROP$k$ allocation exists and can be found efficiently. In fact, our
algorithm additionally guarantees (fractional) Pareto optimality, and PROP1 to
the first agent in each group, PROP2 to the second, etc., for an arbitrary
agent ordering. In special cases, we show that there are PROP1 allocations for
any number of couples.

</details>


### [64] [Reactive Users vs. Recommendation Systems: An Adaptive Policy to Manage Opinion Drifts](https://arxiv.org/abs/2508.13473)
*Atefeh Mollabagher,Parinaz Naghizadeh*

Main category: cs.GT

TL;DR: 研究反应型用户能否通过调整内容消费选择防止推荐系统影响观点，对比两种策略，发现自适应策略能防观点漂移且效用更优，经模拟验证。


<details>
  <summary>Details</summary>
Motivation: 探究有认知的反应型用户能否通过调整内容消费选择来防止推荐系统对自身观点的影响。

Method: 研究用户在被动（点击推荐内容概率固定）和反应（观点大幅漂移后点击概率自适应降低）两种随机策略下的观点动态，解析推导期望观点和用户效用。

Result: 自适应策略可帮助用户防止观点漂移，当用户重视观点保留时，自适应策略的期望效用优于固定策略，理论结果经数值模拟验证。

Conclusion: 研究结果有助于更好理解用户层面策略如何应对推荐系统带来的偏差。

Abstract: Recommendation systems are used in a range of platforms to maximize user
engagement through personalization and the promotion of popular content. It has
been found that such recommendations may shape users' opinions over time. In
this paper, we ask whether reactive users, who are cognizant of the influence
of the content they consume, can prevent such changes by adaptively adjusting
their content consumption choices. To this end, we study users' opinion
dynamics under two types of stochastic policies: a passive policy where the
probability of clicking on recommended content is fixed and a reactive policy
where clicking probability adaptively decreases following large opinion drifts.
We analytically derive the expected opinion and user utility under these
policies. We show that the adaptive policy can help users prevent opinion
drifts and that when a user prioritizes opinion preservation, the expected
utility of the adaptive policy outperforms the fixed policy. We validate our
theoretical findings through numerical simulations. These findings help better
understand how user-level strategies can challenge the biases induced by
recommendation systems.

</details>


### [65] [Optimal Candidate Positioning in Multi-Issue Elections](https://arxiv.org/abs/2508.13841)
*Colin Cleveland,Bart de Keijzer,Maria Polukarov*

Main category: cs.GT

TL;DR: 研究多维空间投票选举中候选人的战略定位，证明计算新候选人最优位置的复杂度，给出特定情况下的算法及近似保证，为竞选策略提供工具。


<details>
  <summary>Details</summary>
Motivation: 研究多维空间投票选举中战略候选人定位问题，以指导竞选策略。

Method: 理论证明计算复杂度，提出超平面枚举算法和径向扫描例程，推导近似保证，将几何方法扩展到计分规则。

Result: 计算新候选人最优位置对单个对手是NP难问题，固定问题数量时可解，给出特定维度算法，得到多候选人情况近似保证，几何方法可扩展到计分规则。

Conclusion: 明确了多维空间选举的算法格局，为竞选策略提供了可实际实施的工具。

Abstract: We study strategic candidate positioning in multidimensional spatial-voting
elections. Voters and candidates are represented as points in $\mathbb{R}^d$,
and each voter supports the candidate that is closest under a distance induced
by an $\ell_p$-norm. We prove that computing an optimal location for a new
candidate is NP-hard already against a single opponent, whereas for a constant
number of issues the problem is tractable: an $O(n^{d+1})$
hyperplane-enumeration algorithm and an $O(n \log n)$ radial-sweep routine for
$d=2$ solve the task exactly. We further derive the first approximation
guarantees for the general multi-candidate case and show how our geometric
approach extends seamlessly to positional-scoring rules such as $k$-approval
and Borda. These results clarify the algorithmic landscape of multidimensional
spatial elections and provide practically implementable tools for campaign
strategy.

</details>


### [66] [Control by Deleting Players from Weighted Voting Games Is NP^PP-Complete for the Penrose-Banzhaf Power Index](https://arxiv.org/abs/2508.13868)
*Joanna Kaczmarek,Jörg Rothe*

Main category: cs.GT

TL;DR: 本文研究加权投票博弈中删除玩家以改变或维持玩家权力的控制问题复杂度，表明部分问题对NP^PP是完全的，优化了已知硬度下界。


<details>
  <summary>Details</summary>
Motivation: 加权投票博弈可用于建模决策场景，此前添加玩家控制的复杂度已解决，但删除玩家控制的复杂度未知，故研究此问题。

Method: 以概率Penrose - Banzhaf指数衡量玩家权力，分析删除玩家控制问题。

Result: 部分删除玩家控制问题对NP^PP是完全的。

Conclusion: 研究结果优化了已知复杂度类的硬度下界，能在实际应用中抵御SAT求解技术。

Abstract: Weighted voting games are a popular class of coalitional games that are
widely used to model real-life situations of decision-making. They can be
applied, for instance, to analyze legislative processes in parliaments or
voting in corporate structures. Various ways of tampering with these games have
been studied, among them merging or splitting players, fiddling with the quota,
and controlling weighted voting games by adding or deleting players. While the
complexity of control by adding players to such games so as to change or
maintain a given player's power has been recently settled, the complexity of
control by deleting players from such games (with the same goals) remained
open. We show that when the players' power is measured by the probabilistic
Penrose-Banzhaf index, some of these problems are complete for NP^PP -- the
class of problems solvable by NP machines equipped with a PP ("probabilistic
polynomial time") oracle. Our results optimally improve the currently known
lower bounds of hardness for much smaller complexity classes, thus providing
protection against SAT-solving techniques in practical applications.

</details>


### [67] [A Mechanism for Mutual Fairness in Cooperative Games with Replicable Resources -- Extended Version](https://arxiv.org/abs/2508.13960)
*Björn Filter,Ralf Möller,Özgür Lütfü Özçep*

Main category: cs.GT

TL;DR: 论文聚焦AI协作系统中公平奖励分配问题，指出经典合作博弈理论不足，提出新机制满足平衡互惠公理保证互公平性。


<details>
  <summary>Details</summary>
Motivation: 设计AI协作系统时需保证安全、符合人类价值观及公平分配奖励，经典合作博弈理论无法满足数据和模型可复制场景需求。

Method: 通过指定公平公理和设计具体机制，提出满足平衡互惠公理的新机制。

Result: 提出一种机制并证明其满足平衡互惠公理，确保每对参与者相互受益均等。

Conclusion: 新机制能解决AI协作系统中参与者互惠利益失衡问题，实现公平奖励分配。

Abstract: The latest developments in AI focus on agentic systems where artificial and
human agents cooperate to realize global goals. An example is collaborative
learning, which aims to train a global model based on data from individual
agents. A major challenge in designing such systems is to guarantee safety and
alignment with human values, particularly a fair distribution of rewards upon
achieving the global goal. Cooperative game theory offers useful abstractions
of cooperating agents via value functions, which assign value to each
coalition, and via reward functions. With these, the idea of fair allocation
can be formalized by specifying fairness axioms and designing concrete
mechanisms. Classical cooperative game theory, exemplified by the Shapley
value, does not fully capture scenarios like collaborative learning, as it
assumes nonreplicable resources, whereas data and models can be replicated.
Infinite replicability requires a generalized notion of fairness, formalized
through new axioms and mechanisms. These must address imbalances in reciprocal
benefits among participants, which can lead to strategic exploitation and
unfair allocations. The main contribution of this paper is a mechanism and a
proof that it fulfills the property of mutual fairness, formalized by the
Balanced Reciprocity Axiom. It ensures that, for every pair of players, each
benefits equally from the participation of the other.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [68] [Research on Conversational Recommender System Considering Consumer Types](https://arxiv.org/abs/2508.13209)
*Yaying Luo,Hui Fang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出CT - CRS框架，整合消费者类型建模到对话推荐，实时推断用户类型调整策略，实验显示其提升推荐成功率并减少交互轮数。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统大多忽视用户异质决策风格和知识水平，限制准确性和效率。

Method: 基于消费者类型理论定义四类用户，利用交互历史和微调大语言模型实时推断用户类型，将用户类型融入状态表示，设计类型自适应策略，采用反向强化学习优化对话策略。

Result: 在LastFM、Amazon - Book和Yelp上实验表明，CT - CRS比基线提升推荐成功率、减少交互轮数，消融实验证实消费者类型建模和反向强化学习对性能提升贡献大。

Conclusion: CT - CRS通过整合心理建模和高级策略优化，为提升对话推荐系统个性化提供可扩展且可解释的解决方案。

Abstract: Conversational Recommender Systems (CRS) provide personalized services
through multi-turn interactions, yet most existing methods overlook users'
heterogeneous decision-making styles and knowledge levels, which constrains
both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer
Type-Enhanced Conversational Recommender System), a framework that integrates
consumer type modeling into dialogue recommendation. Based on consumer type
theory, we define four user categories--dependent, efficient, cautious, and
expert--derived from two dimensions: decision-making style (maximizers vs.
satisficers) and knowledge level (high vs. low). CT-CRS employs interaction
histories and fine-tunes the large language model to automatically infer user
types in real time, avoiding reliance on static questionnaires. We incorporate
user types into state representation and design a type-adaptive policy that
dynamically adjusts recommendation granularity, diversity, and attribute query
complexity. To further optimize the dialogue policy, we adopt Inverse
Reinforcement Learning (IRL), enabling the agent to approximate expert-like
strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book,
and Yelp show that CTCRS improves recommendation success rate and reduces
interaction turns compared to strong baselines. Ablation studies confirm that
both consumer type modeling and IRL contribute significantly to performance
gains. These results demonstrate that CT-CRS offers a scalable and
interpretable solution for enhancing CRS personalization through the
integration of psychological modeling and advanced policy optimization.

</details>


### [69] [FLAIR: Feedback Learning for Adaptive Information Retrieval](https://arxiv.org/abs/2508.13390)
*William Zhang,Yiwen Zhu,Yunlei Lu,Mathieu Demarne,Wenjing Wang,Kai Deng,Nutan Sahoo,Katherine Lin,Miso Cilimdzic,Subru Krishnan*

Main category: cs.IR

TL;DR: 本文介绍轻量级反馈学习框架FLAIR，通过整合领域专家反馈调整检索策略，经评估性能提升显著且已集成到实际系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动副驾驶在复杂技术场景应用，需要专业信息检索解决方案。

Method: FLAIR分离线和在线两阶段，离线阶段从用户反馈和文档合成问题获取指标并分散存储，在线阶段用双轨排名机制结合原始相似度分数和收集指标。

Result: 广泛评估显示，FLAIR在新旧查询上性能显著提升，超越现有方法，已集成到Copilot DECO为数千用户服务。

Conclusion: FLAIR具有可扩展性和有效性，能用于实际操作环境。

Abstract: Recent advances in Large Language Models (LLMs) have driven the adoption of
copilots in complex technical scenarios, underscoring the growing need for
specialized information retrieval solutions. In this paper, we introduce FLAIR,
a lightweight, feedback learning framework that adapts copilot systems'
retrieval strategies by integrating domain-specific expert feedback. FLAIR
operates in two stages: an offline phase obtains indicators from (1) user
feedback and (2) questions synthesized from documentation, storing these
indicators in a decentralized manner. An online phase then employs a two-track
ranking mechanism to combine raw similarity scores with the collected
indicators. This iterative setup refines retrieval performance for any query.
Extensive real-world evaluations of FLAIR demonstrate significant performance
gains on both previously seen and unseen queries, surpassing state-of-the-art
approaches. The system has been successfully integrated into Copilot DECO,
serving thousands of users at Microsoft, demonstrating its scalability and
effectiveness in operational environments.

</details>


### [70] [CASPER: Concept-integrated Sparse Representation for Scientific Retrieval](https://arxiv.org/abs/2508.13394)
*Lam Thanh Do,Linh Van Nguyen,David Fu,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: 提出科学搜索稀疏检索模型CASPER，利用参考挖掘训练数据，在八个基准测试中表现出色，还可用于关键短语生成任务。


<details>
  <summary>Details</summary>
Motivation: 科学文献指数级增长，研究者难以跟上文献更新，需减轻该问题。

Method: 提出CASPER模型，以标记和关键短语为表示单位；利用学术参考文献挖掘训练数据。

Result: CASPER在八个科学检索基准测试中优于强密集和稀疏检索基线；经简单后处理用于关键短语生成任务，性能与CopyRNN相当，生成短语更多样且速度快近四倍。

Conclusion: CASPER模型在科学检索和关键短语生成任务中具有有效性和优势。

Abstract: The exponential growth of scientific literature has made it increasingly
difficult for researchers to keep up with the literature. In an attempt to
alleviate this problem, we propose CASPER, a sparse retrieval model for
scientific search that utilizes tokens and keyphrases as representation units
(i.e. dimensions in the sparse embedding space), enabling it to represent
queries and documents with research concepts and match them at both granular
and conceptual levels. To overcome the lack of suitable training data, we
propose mining training data by leveraging scholarly references (i.e. signals
that capture how research concepts of papers are expressed in different
settings), including titles, citation contexts, author-assigned keyphrases, and
co-citations. CASPER outperforms strong dense and sparse retrieval baselines on
eight scientific retrieval benchmarks. Moreover, we demonstrate that through
simple post-processing, CASPER can be effectively used for the keyphrase
generation tasks, achieving competitive performance with the established
CopyRNN while producing more diverse keyphrases and being nearly four times
faster.

</details>


### [71] [AdaptJobRec: Enhancing Conversational Career Recommendation through an LLM-Powered Agentic System](https://arxiv.org/abs/2508.13423)
*Qixin Wang,Dawei Wang,Kun Chen,Yaowei Hu,Puneet Girdhar,Ruoteng Wang,Aadesh Gupta,Chaitanya Devella,Wenlai Guo,Shangwen Huang,Bachir Aoun,Greg Hayworth,Han Li,Xintao Wu*

Main category: cs.IR

TL;DR: 本文提出首个利用自主智能体集成个性化推荐算法工具的对话式工作推荐系统AdaptJobRec，可平衡处理复杂查询和降低延迟，在沃尔玛场景评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统存在响应延迟问题，需平衡处理复杂查询和降低延迟。

Method: 提出AdaptJobRec系统，采用用户查询复杂度识别机制，根据查询复杂度采用不同处理方式。

Result: 在沃尔玛真实职业推荐场景评估中，AdaptJobRec较基线平均响应延迟最多降低53.3%，显著提高推荐准确率。

Conclusion: AdaptJobRec能有效平衡处理复杂查询和降低延迟，提高推荐效果。

Abstract: In recent years, recommendation systems have evolved from providing a single
list of recommendations to offering a comprehensive suite of topic focused
services. To better accomplish this task, conversational recommendation systems
(CRS) have progressed from basic retrieval augmented LLM generation to agentic
systems with advanced reasoning and self correction capabilities. However,
agentic systems come with notable response latency, a longstanding challenge
for conversational recommendation systems. To balance the trade off between
handling complex queries and minimizing latency, we propose AdaptJobRec, the
first conversational job recommendation system that leverages autonomous agent
to integrate personalized recommendation algorithm tools. The system employs a
user query complexity identification mechanism to minimize response latency.
For straightforward queries, the agent directly selects the appropriate tool
for rapid responses. For complex queries, the agent uses the memory processing
module to filter chat history for relevant content, then passes the results to
the intelligent task decomposition planner, and finally executes the tasks
using personalized recommendation tools. Evaluation on Walmart's real world
career recommendation scenarios demonstrates that AdaptJobRec reduces average
response latency by up to 53.3% compared to competitive baselines, while
significantly improving recommendation accuracy.

</details>


### [72] [LLM-Enhanced Linear Autoencoders for Recommendation](https://arxiv.org/abs/2508.13500)
*Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出L3AE将大语言模型集成到线性自动编码器框架，通过两阶段优化策略整合语义和交互知识，实验表明性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有结合文本信息的线性自动编码器依赖稀疏词共现模式，难以捕捉丰富文本语义。

Method: 提出L3AE，先从大语言模型导出的项目表示构建语义项目关联矩阵，再从协同信号学习项目权重矩阵并将语义项目关联作为正则化，各阶段通过闭式解优化。

Result: 在三个基准数据集上，L3AE始终优于最先进的大语言模型增强模型，Recall@20提升27.6%，NDCG@20提升39.3%。

Conclusion: L3AE能有效整合文本语义和用户 - 项目交互的异构知识，性能表现良好。

Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic
representation of textual item information in recommender systems. However,
existing linear autoencoders (LAEs) that incorporate textual information rely
on sparse word co-occurrence patterns, limiting their ability to capture rich
textual semantics. To address this, we propose L3AE, the first integration of
LLMs into the LAE framework. L3AE effectively integrates the heterogeneous
knowledge of textual semantics and user-item interactions through a two-phase
optimization strategy. (i) L3AE first constructs a semantic item-to-item
correlation matrix from LLM-derived item representations. (ii) It then learns
an item-to-item weight matrix from collaborative signals while distilling
semantic item correlations as regularization. Notably, each phase of L3AE is
optimized through closed-form solutions, ensuring global optimality and
computational efficiency. Extensive experiments demonstrate that L3AE
consistently outperforms state-of-the-art LLM-enhanced models on three
benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.
The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

</details>


### [73] [Heterogeneous Influence Maximization in User Recommendation](https://arxiv.org/abs/2508.13517)
*Hongru Hou,Jiachen Sun,Wenqing Lin,Wendong Bi,Xiangrong Wang,Deqing Yang*

Main category: cs.IR

TL;DR: 提出HeteroIR和HeteroIM模型解决现有用户推荐和信息传播方法的问题，实验效果好且已部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法无法发挥候选者传播能力，IM方法未考虑交互意愿，需改进。

Method: 提出HeteroIR和HeteroIM模型，HeteroIR用两阶段框架估计传播利润，HeteroIM增量选择最有影响力邀请者并基于RR集重排序。

Result: 实验显示HeteroIR和HeteroIM显著优于现有基线，在腾讯在线游戏平台A/B测试中有8.5%和10%的提升。

Conclusion: HeteroIR和HeteroIM能有效解决现有方法的问题，可用于实际系统。

Abstract: User recommendation systems enhance user engagement by encouraging users to
act as inviters to interact with other users (invitees), potentially fostering
information propagation. Conventional recommendation methods typically focus on
modeling interaction willingness. Influence-Maximization (IM) methods focus on
identifying a set of users to maximize the information propagation. However,
existing methods face two significant challenges. First, recommendation methods
fail to unleash the candidates' spread capability. Second, IM methods fail to
account for the willingness to interact. To solve these issues, we propose two
models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to
unleash the dissemination potential of user recommendation systems. HeteroIM
fills the gap between the IM method and the recommendation task, improving
interaction willingness and maximizing spread coverage. The HeteroIR introduces
a two-stage framework to estimate the spread profits. The HeteroIM
incrementally selects the most influential invitee to recommend and rerank
based on the number of reverse reachable (RR) sets containing inviters and
invitees. RR set denotes a set of nodes that can reach a target via
propagation. Extensive experiments show that HeteroIR and HeteroIM
significantly outperform the state-of-the-art baselines with the p-value <
0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online
gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B
test, respectively. Implementation codes are available at
https://github.com/socialalgo/HIM.

</details>


### [74] [ENCODE: Breaking the Trade-Off Between Performance and Efficiency in Long-Term User Behavior Modeling](https://arxiv.org/abs/2508.13567)
*Wenji Zhou,Yuhang Zheng,Yinfu Feng,Yunan Ye,Rong Xiao,Long Chen,Xiaosong Yang,Jun Xiao*

Main category: cs.IR

TL;DR: 提出ENCODE两阶段长序列建模方法，兼顾在线服务效率与精度，实验证明其有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从长期行为序列准确捕捉用户长期兴趣，还破坏了序列建模的两个基本要求，影响在线服务系统性能。

Method: 提出ENCODE方法，包括离线提取阶段（聚类行为序列、设计降维算法）和在线推理阶段（利用提取的用户兴趣预测与目标项的关联），并采用相同相关性度量。

Result: 与SOTA方法的广泛实验和比较证明了ENCODE的有效性和高效性。

Conclusion: ENCODE不仅满足序列建模的两个基本要求，还能在在线服务效率和精度之间取得理想平衡。

Abstract: Long-term user behavior sequences are a goldmine for businesses to explore
users' interests to improve Click-Through Rate. However, it is very challenging
to accurately capture users' long-term interests from their long-term behavior
sequences and give quick responses from the online serving systems. To meet
such requirements, existing methods "inadvertently" destroy two basic
requirements in long-term sequence modeling: R1) make full use of the entire
sequence to keep the information as much as possible; R2) extract information
from the most relevant behaviors to keep high relevance between learned
interests and current target items. The performance of online serving systems
is significantly affected by incomplete and inaccurate user interest
information obtained by existing methods. To this end, we propose an efficient
two-stage long-term sequence modeling approach, named as EfficieNt Clustering
based twO-stage interest moDEling (ENCODE), consisting of offline extraction
stage and online inference stage. It not only meets the aforementioned two
basic requirements but also achieves a desirable balance between online service
efficiency and precision. Specifically, in the offline extraction stage, ENCODE
clusters the entire behavior sequence and extracts accurate interests. To
reduce the overhead of the clustering process, we design a metric
learning-based dimension reduction algorithm that preserves the relative
pairwise distances of behaviors in the new feature space. While in the online
inference stage, ENCODE takes the off-the-shelf user interests to predict the
associations with target items. Besides, to further ensure the relevance
between user interests and target items, we adopt the same relevance metric
throughout the whole pipeline of ENCODE. The extensive experiment and
comparison with SOTA have demonstrated the effectiveness and efficiency of our
proposed ENCODE.

</details>


### [75] [Understanding Distribution Structure on Calibrated Recommendation Systems](https://arxiv.org/abs/2508.13568)
*Diego Correa da Silva,Denis Robson Dantas Boaventura,Mayki dos Santos Oliveira,Eduardo Ferreira da Silva,Joel Machado Pires,Frederico Araújo Durão*

Main category: cs.IR

TL;DR: 传统推荐系统有缺陷，校准推荐系统可解决，研究实现15个模型评估，结果显示离群检测模型效果好。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统生成的推荐列表可能忽略用户偏好中不太突出的类别，影响用户体验，因此需要改进。

Method: 校准推荐系统结合三种分布工作，实现15个模型，在电影领域的三个数据集上评估用户模式。

Result: 离群检测模型能更好地理解分布结构，校准系统生成的推荐列表与传统列表表现相似，用户可同等程度改变偏好组。

Conclusion: 校准推荐系统能有效解决传统推荐系统的问题，离群检测模型表现更佳。

Abstract: Traditional recommender systems aim to generate a recommendation list
comprising the most relevant or similar items to the user's profile. These
approaches can create recommendation lists that omit item genres from the less
prominent areas of a user's profile, thereby undermining the user's experience.
To solve this problem, the calibrated recommendation system provides a
guarantee of including less representative areas in the recommended list. The
calibrated context works with three distributions. The first is from the user's
profile, the second is from the candidate items, and the last is from the
recommendation list. These distributions are G-dimensional, where G is the
total number of genres in the system. This high dimensionality requires a
different evaluation method, considering that traditional recommenders operate
in a one-dimensional data space. In this sense, we implement fifteen models
that help to understand how these distributions are structured. We evaluate the
users' patterns in three datasets from the movie domain. The results indicate
that the models of outlier detection provide a better understanding of the
structures. The calibrated system creates recommendation lists that act
similarly to traditional recommendation lists, allowing users to change their
groups of preferences to the same degree.

</details>


### [76] [MUFFIN: Mixture of User-Adaptive Frequency Filtering for Sequential Recommendation](https://arxiv.org/abs/2508.13670)
*Ilwoong Baek,Mincheol Yoon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出频域模型MUFFIN解决现有频域SR模型的不足，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有频域SR模型存在频带覆盖有限和缺乏个性化频率过滤的问题。

Method: 提出MUFFIN模型，包含全局过滤模块和局部过滤模块，采用用户自适应滤波器。

Result: 在五个基准数据集上，MUFFIN始终优于现有频域SR模型。

Conclusion: MUFFIN能有效捕捉全频谱的用户行为模式。

Abstract: Sequential recommendation (SR) aims to predict users' subsequent interactions
by modeling their sequential behaviors. Recent studies have explored frequency
domain analysis, which effectively models periodic patterns in user sequences.
However, existing frequency-domain SR models still face two major drawbacks:
(i) limited frequency band coverage, often missing critical behavioral patterns
in a specific frequency range, and (ii) lack of personalized frequency
filtering, as they apply an identical filter for all users regardless of their
distinct frequency characteristics. To address these challenges, we propose a
novel frequency-domain model, Mixture of User-adaptive Frequency FIlteriNg
(MUFFIN), operating through two complementary modules. (i) The global filtering
module (GFM) handles the entire frequency spectrum to capture comprehensive
behavioral patterns. (ii) The local filtering module (LFM) selectively
emphasizes important frequency bands without excluding information from other
ranges. (iii) In both modules, the user-adaptive filter (UAF) is adopted to
generate user-specific frequency filters tailored to individual unique
characteristics. Finally, by aggregating both modules, MUFFIN captures diverse
user behavioral patterns across the full frequency spectrum. Extensive
experiments show that MUFFIN consistently outperforms state-of-the-art
frequency-domain SR models over five benchmark datasets. The source code is
available at https://github.com/ilwoong100/MUFFIN.

</details>


### [77] [Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation](https://arxiv.org/abs/2508.13745)
*Shouxing Ma,Yawen Zeng,Shiqing Wu,Guandong Xu*

Main category: cs.IR

TL;DR: 文章指出当前多模态推荐系统存在简单特征对比和同构关系挖掘不足问题，提出REARM框架，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态推荐系统在处理稀疏数据时，简单多模态特征对比效果不佳和同构关系挖掘不完整的问题。

Method: 提出REARM框架，采用元网络和正交约束策略完善多模态对比学习，整合用户兴趣图和物品共现图进行图学习。

Result: 在三个真实数据集上的实验表明REARM优于各种先进基线，可视化显示其在区分模态共享和模态独特特征上有改进。

Conclusion: REARM框架能有效解决现有多模态推荐系统的局限，提升推荐性能。

Abstract: Multi-modal recommender system focuses on utilizing rich modal information (
i.e., images and textual descriptions) of items to improve recommendation
performance. The current methods have achieved remarkable success with the
powerful structure modeling capability of graph neural networks. However, these
methods are often hindered by sparse data in real-world scenarios. Although
contrastive learning and homography ( i.e., homogeneous graphs) are employed to
address the data sparsity challenge, existing methods still suffer two main
limitations: 1) Simple multi-modal feature contrasts fail to produce effective
representations, causing noisy modal-shared features and loss of valuable
information in modal-unique features; 2) The lack of exploration of the
homograph relations between user interests and item co-occurrence results in
incomplete mining of user-item interplay.
  To address the above limitations, we propose a novel framework for
\textbf{R}\textbf{E}fining multi-mod\textbf{A}l cont\textbf{R}astive learning
and ho\textbf{M}ography relations (\textbf{REARM}). Specifically, we complement
multi-modal contrastive learning by employing meta-network and orthogonal
constraint strategies, which filter out noise in modal-shared features and
retain recommendation-relevant information in modal-unique features. To mine
homogeneous relationships effectively, we integrate a newly constructed user
interest graph and an item co-occurrence graph with the existing user
co-occurrence and item semantic graphs for graph learning. The extensive
experiments on three real-world datasets demonstrate the superiority of REARM
to various state-of-the-art baselines. Our visualization further shows an
improvement made by REARM in distinguishing between modal-shared and
modal-unique features. Code is available
\href{https://github.com/MrShouxingMa/REARM}{here}.

</details>


### [78] [UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion](https://arxiv.org/abs/2508.13843)
*Zihan Liang,Yufei Ma,ZhiPeng Qian,Huangyu Dai,Zihan Wang,Ben Chen,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 提出统一多模态电商搜索框架UniECS，含灵活架构、综合训练策略和新基准M - BEER，实验和实际部署表现佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有电商多模态检索系统针对特定任务优化、缺乏统一检索评估基准的问题。

Method: 提出带门控多模态编码器的灵活架构，开发综合训练策略，创建M - BEER基准。

Result: 在四个电商基准上表现超现有方法，在M - BEER上有显著提升，在快手电商平台部署后CTR和Revenue提高。

Conclusion: 该方法在实验和实际场景均有效，代码等将公开。

Abstract: Current e-commerce multimodal retrieval systems face two key limitations:
they optimize for specific tasks with fixed modality pairings, and lack
comprehensive benchmarks for evaluating unified retrieval approaches. To
address these challenges, we introduce UniECS, a unified multimodal e-commerce
search framework that handles all retrieval scenarios across image, text, and
their combinations. Our work makes three key contributions. First, we propose a
flexible architecture with a novel gated multimodal encoder that uses adaptive
fusion mechanisms. This encoder integrates different modality representations
while handling missing modalities. Second, we develop a comprehensive training
strategy to optimize learning. It combines cross-modal alignment loss (CMAL),
cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and
adaptive loss weighting. Third, we create M-BEER, a carefully curated
multimodal benchmark containing 50K product pairs for e-commerce search
evaluation. Extensive experiments demonstrate that UniECS consistently
outperforms existing methods across four e-commerce benchmarks with fine-tuning
or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial
improvements in cross-modal tasks (up to 28\% gain in R@10 for text-to-image
retrieval) while maintaining parameter efficiency (0.2B parameters) compared to
larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy
UniECS in the e-commerce search platform of Kuaishou Inc. across two search
scenarios, achieving notable improvements in Click-Through Rate (+2.74\%) and
Revenue (+8.33\%). The comprehensive evaluation demonstrates the effectiveness
of our approach in both experimental and real-world settings. Corresponding
codes, models and datasets will be made publicly available at
https://github.com/qzp2018/UniECS.

</details>


### [79] [Bites of Tomorrow: Personalized Recommendations for a Healthier and Greener Plate](https://arxiv.org/abs/2508.13870)
*Jiazheng Jing,Yinan Zhang,Chunyan Miao*

Main category: cs.IR

TL;DR: 本文提出GRAPE推荐系统推动用户选择可持续食物，设计两种绿色损失函数，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 极端气候事件提升可持续生活意识，现有传统方法推动可持续行为转变存在要求过高或被动参与的问题。

Method: 提出GRAPE系统推荐符合用户偏好的可持续食物，设计两种适应不同优先级绿色指标的绿色损失函数。

Result: 在真实数据集上的大量实验证明了GRAPE系统的有效性。

Conclusion: 采用推荐系统能有效推动用户做出更可持续的选择。

Abstract: The recent emergence of extreme climate events has significantly raised
awareness about sustainable living. In addition to developing energy-saving
materials and technologies, existing research mainly relies on traditional
methods that encourage behavioral shifts towards sustainability, which can be
overly demanding or only passively engaging. In this work, we propose to employ
recommendation systems to actively nudge users toward more sustainable choices.
We introduce Green Recommender Aligned with Personalized Eating (GRAPE), which
is designed to prioritize and recommend sustainable food options that align
with users' evolving preferences. We also design two innovative Green Loss
functions that cater to green indicators with either uniform or differentiated
priorities, thereby enhancing adaptability across a range of scenarios.
Extensive experiments on a real-world dataset demonstrate the effectiveness of
our GRAPE.

</details>


### [80] [CARE: Contextual Adaptation of Recommenders for LLM-based Conversational Recommendation](https://arxiv.org/abs/2508.13889)
*Chuang Li,Yang Deng,Hengchang Hu,See-Kiong Ng,Min-Yen Kan,Haizhou Li*

Main category: cs.IR

TL;DR: 本文提出CARE框架，将大语言模型与外部推荐系统集成，提升对话式推荐准确性并减少流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的对话式推荐方法存在推荐准确性低、忽视实体协作关系的问题，需要改进。

Method: 引入CARE框架，将外部推荐系统作为领域专家，结合上下文信息进行推荐。

Result: 在ReDial和INSPIRED数据集上，推荐准确性平均提升54%和25%，CARE框架最有效策略是大语言模型基于上下文对外部推荐的候选项目进行选择和重排序。

Conclusion: CARE框架有效解决了现有问题，减少了外部推荐器的流行度偏差。

Abstract: We tackle the challenge of integrating large language models (LLMs) with
external recommender systems to enhance domain expertise in conversational
recommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-
or few-shot methods for generating item recommendations based on user queries,
but this method faces two significant challenges: (1) without domain-specific
adaptation, LLMs frequently recommend items not in the target item space,
resulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue
context for content-based recommendations, neglecting the collaborative
relationships among entities or item sequences. To address these limitations,
we introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE
customizes LLMs for CRS tasks, and synergizes them with external recommendation
systems. CARE (a) integrates external recommender systems as domain experts,
producing recommendations through entity-level insights, and (b) enhances those
recommendations by leveraging contextual information for more accurate and
unbiased final recommendations using LLMs. Our results demonstrate that
incorporating external recommender systems with entity-level information
significantly enhances recommendation accuracy of LLM-based CRS by an average
of 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in
the CARE framework involves LLMs selecting and reranking candidate items that
external recommenders provide based on contextual insights. Our analysis
indicates that the CARE framework effectively addresses the identified
challenges and mitigates the popularity bias in the external recommender.

</details>


### [81] [InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems](https://arxiv.org/abs/2508.13930)
*Matey Krastev,Miklos Hamar,Danilo Toapanta,Jesse Brouwers,Yibin Lei*

Main category: cs.IR

TL;DR: 本文借助InPars工具包改进神经信息检索的合成查询生成管道，引入两项扩展，结果显示可减少过滤并提升检索性能，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 改进神经信息检索（NIR）的合成查询生成管道。

Method: 先评估InPars、InPars - V2和Promptagator管道的可重复性和有效性，引入两项扩展：通过对比偏好优化（CPO）微调查询生成大语言模型；用DSPy框架的动态思维链（CoT）优化提示替换静态提示模板。

Result: 两项扩展减少了激进过滤的需求，同时提高了检索性能。

Conclusion: 所做扩展有效，相关代码、模型和合成数据集公开以支持进一步研究。

Abstract: This work revisits and extends synthetic query generation pipelines for
Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a
reproducible, end-to-end framework for generating training data using large
language models (LLMs). We first assess the reproducibility of the original
InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and
validate their effectiveness using open-source reranker and generator models.
Building on this foundation, we introduce two key extensions to the pipeline:
(1) fine-tuning a query generator LLM via Contrastive Preference Optimization
(CPO) to improve the signal quality in generated queries, and (2) replacing
static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts
using the DSPy framework. Our results show that both extensions reduce the need
for aggressive filtering while improving retrieval performance. All code,
models, and synthetic datasets are publicly released to support further
research at: \href{https://github.com/danilotpnta/IR2-project}{this https URL}.

</details>


### [82] [Democratizing News Recommenders: Modeling Multiple Perspectives for News Candidate Generation with VQ-VAE](https://arxiv.org/abs/2508.13978)
*Hardy,Sebastian Padó,Amelie Wührl,Tanise Ceron*

Main category: cs.IR

TL;DR: 现有新闻推荐系统存在多样性不足问题，提出 A2CG 框架解决，评估显示其能在推荐早期平衡个性化与多样性，生成更多样候选，有望用于新闻推荐。


<details>
  <summary>Details</summary>
Motivation: 现有基于点击的新闻推荐系统限制内容多样性，多样性感知算法有未考虑规范多样性、在系统后期应用多样性的局限，需改进。

Method: 提出 A2CG 框架，在推荐流程早期引入多样性，用可配置机制使多样性与民主目标对齐，用多方面表示新闻，用 VQ - VAE 生成离散多面表示，用解码器模型学习用户偏好，在候选检索时反转查询向量某些方面的符号注入多样性。

Result: 在 MIND 数据集上评估，能在推荐早期灵活平衡个性化与多样性，生成更多新颖、多样和意外的候选，有效考虑加强民主价值的方面。

Conclusion: 该方法是下游民主化新闻推荐系统的有前景的方法。

Abstract: Current News Recommender Systems based on past clicks are designed for
engagement, but come at the cost of limiting diversity in the suggested
content. While diversity-aware algorithms exist, they suffer from two major
limitations. First, they fail to account for normative diversity, which
requires fair access to a broad range of perspectives. Second, they typically
apply diversity late in the system's pipeline, after a lot of content has
already been filtered out. Both limitations confine their effectiveness and
prevent them from promoting true normative diversity in news recommendations.
  We propose Aspect-Aware Candidate Generation (A2CG) to address these
limitations. Our framework introduces diversity into the earliest pipeline
stage and uses a configurable mechanism to align diversity with specific
democratic goals. A2CG represents each news article using multiple aspects of
perspectives (e.g., sentiment, political leaning, frame) and uses a Vector
Quantized Variational Autoencoder (VQ-VAE) to create a discrete, multi-faceted
representation. A decoder-only model then learns user preferences over these
aspect codes. We then inject diversity directly by reversing the sign on some
of the query vector's aspects during the candidate retrieval process, ensuring
a more diverse set of candidates.
  Our method, evaluated on the MIND dataset, enables a flexible trade-off
between personalization and diversity early in the recommendation pipeline. It
also generates more novel, diverse, and serendipitous candidates while
effectively taking into account aspects that strengthen democratic values.
These empirical results make it a promising approach for downstream
democratized news recommendation systems.

</details>


### [83] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出TaoSR1框架直接部署大语言模型解决查询-产品相关性预测任务，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: BERT模型缺乏复杂推理能力，现有大语言模型应用方式存在不足，需解决推理错误积累、幻觉及部署可行性问题。

Method: TaoSR1框架分三步：用CoT监督微调灌输推理；离线采样结合DPO提升生成质量；基于难度动态采样结合GRPO减少幻觉，还有后处理和分区方法实现高效部署。

Result: TaoSR1在离线数据集上显著优于基线模型，在线人工评估有显著提升。

Conclusion: 提出将CoT推理应用于相关性分类的新范式。

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: 本文聚焦图表视觉问答子任务，开发BERT - VQA模型并与基线对比，结果否定核心假设，为解决该问题提供见解。


<details>
  <summary>Details</summary>
Motivation: 视觉问答需模型融合视觉和语言信息，本文旨在解决图表视觉问答这一子任务。

Method: 开发基于VisualBERT的BERT - VQA模型，使用预训练ResNet 101图像编码器，可能添加联合融合，与包含LSTM、CNN和浅层分类器的基线模型对比。

Result: 最终结果否定了VisualBERT中跨模态模块对对齐图表组件和问题短语至关重要这一核心假设。

Conclusion: 工作为图表问答挑战的难度以及不同模型架构解决该问题的适用性提供了有价值的见解。

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [85] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 提出一种社交媒体多模态情感分析新方法，结合CNN图像分析和LLM文本处理，引入上下文注意力机制，实验效果好，对灾害管理有实际意义。


<details>
  <summary>Details</summary>
Motivation: 在自然灾害背景下，理解公众情绪对有效危机管理至关重要，传统方法分别处理文本和图像模态，需新方法。

Method: 无缝集成基于CNN的图像分析和基于LLM的文本处理，利用GPT和提示工程从CrisisMMD数据集提取情感相关特征，在融合过程中引入上下文注意力机制。

Result: 在将社交媒体数据分类为信息性和非信息性类别方面取得显著进展，准确率提高2.43%，F1分数提高5.18%。

Conclusion: 该方法为人工智能驱动的危机管理解决方案提供了有前景的方向，可优化应急干预的准确性。

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [86] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: 本文系统研究物理信息神经网络中训练点分布对求解微分方程的影响，测试多种策略和网络架构，引入正弦训练点，发现训练点分布影响解的精度且与方程特性有关。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络求解微分方程性能依赖多种因素，本文旨在系统研究其核心组件训练点分布。

Method: 测试两个常微分方程和两个偏微分方程，采用五种训练数据生成策略和浅网络架构，引入基于正弦的训练点，并使用不同参数组合进行测试。

Result: 结果显示训练点分布对解的精度有影响，且与微分方程的特性相关。

Conclusion: 训练点分布会影响物理信息神经网络求解微分方程的精度，且与方程特性存在联系。

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [87] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: 本文提出用于学习时间交互网络的DGNPP模型，在三个公开数据集上实验表明其在事件和时间预测任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 以往学习时间交互网络被视为粗粒度多序列预测问题，忽略网络拓扑结构影响，本文旨在解决此局限。

Method: 提出DGNPP模型，包含节点聚合层和自注意力层，将动态和静态嵌入融入事件强度函数，通过最大似然估计优化模型。

Result: 在三个公开数据集上的实验显示，DGNPP在事件预测和时间预测任务中表现高效，显著优于基线模型。

Conclusion: DGNPP能有效缓解先前方法的局限，在相关预测任务中有优越表现。

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [88] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: 本文提出用于自动机器学习（AutoML）系统的元学习方法，可加速优化过程，减少运行时间和搜索空间，且不影响预测性能，还探讨了相关策略权衡。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML搜索和优化策略计算成本高，且搜索空间大易导致过拟合。

Method: 引入元学习方法，利用历史元知识选择搜索空间的有前景区域。

Result: 实验表明，该方法在随机搜索中可减少89%运行时间，缩小搜索空间，应用于Auto - Sklearn时也有竞争力。

Conclusion: 提出的元学习方法能有效解决AutoML现有问题，研究还涵盖元特征选择、元模型可解释性及搜索空间缩减策略权衡等内容。

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [89] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: 本文研究递归神经网络在S - P图聚类方法中的应用，提出基于网络动力学的聚类方法并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着学生数量增加，S - P图难以处理，需要将大图表分类为小图表。

Method: 提出基于网络动力学的简单聚类方法，网络有多个不动点，吸引域对应小S - P图聚类；提出平均警示指数评估聚类性能。

Result: 通过基础实验，证实了该方法的有效性。

Conclusion: 基于网络动力学的聚类方法能有效解决S - P图在学生数量增加时难以处理的问题。

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [90] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: 提出RISE框架解决视觉语言模型复杂图像标注任务问题，在多任务上表现优于SFT和Visual - RFT。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂图像标注任务中存在问题，标准监督微调忽略推理过程，视觉强化微调因缺乏高质量思维链导致推理不一致。

Method: 提出两阶段RISE框架，第一阶段RISE - CoT通过强化学习生成视觉基础且逻辑一致的思维链；第二阶段RISE - R1利用过滤后的思维链子集进行监督微调与强化微调。

Result: 在复杂和简单图像标注任务中，RISE训练的Qwen2 - VL - 2B优于SFT和Visual - RFT，表现稳健且可解释性增强。

Conclusion: RISE为提升视觉语言模型推理能力提供了无需手动标注思维链的自监督解决方案。

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [91] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: 本文提出基于已知先验动态行为识别反馈线性化物理系统的新方法，结合堆叠回归算法和相对度条件发现并反馈线性化物理模型的真实控制方程。


<details>
  <summary>Details</summary>
Motivation: 发现物理系统的控制方程并设计有效反馈控制器具有挑战性，需要深入理解系统行为包括非线性因素，这是研究的动力。

Method: 先使用稀疏回归算法识别系统，再对发现的系统应用李导数到输出函数字典来设计反馈控制器，结合堆叠回归算法和相对度条件。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [92] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 论文引入基于物理模拟的PPDA方法应对传感器HAR中标注数据稀缺问题，实验表明其优于传统STDA，能降低训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 传感器HAR中高质量标注数据稀缺，传统STDA方法生成的数据可能不符合物理实际，影响活动标签含义。

Method: 引入PPDA，利用人体运动数据和物理模拟增加现实变化，在三个公开数据集上与传统STDA对比，分别评估和组合评估。

Result: PPDA平均提高宏观F1分数3.7个百分点（最高13个百分点），用少60%的训练对象就能达到与STDA相当的性能。

Conclusion: PPDA在数据增强中追求物理合理性有优势，物理模拟可生成合成IMU数据，是解决HAR标注稀缺问题的经济可扩展方法。

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [93] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 提出协作匹配系统comatch以实现人机互补，经800人实验验证其效果优于人或算法单独决策，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动算法匹配系统未实现人机互补，使用系统时人的决策不一定优于人或算法单独决策，需解决此问题。

Method: 提出协作匹配系统comatch，它选择最有信心的决策，其余交给人类决策者，并优化决策分配以最大化性能，还进行了800人参与的大规模人类主体研究。

Result: comatch产生的匹配结果优于人类参与者或算法单独产生的结果。

Conclusion: comatch能有效实现人机互补，提升匹配决策效果。

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [94] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: 本文提出分层共形分类（HCC）方法，将类层次结构融入共形预测，通过优化求解预测集，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测（CP）在分类时忽略类标签间语义关系和层次结构，本文旨在改进这一问题。

Method: 将HCC表述为约束优化问题求解预测集，证明较小的候选解集能保证覆盖率和最优性。

Result: 在音频、图像和文本数据的三个新基准上评估显示方法优势，用户研究表明注释者更倾向分层预测集。

Conclusion: HCC能有效将类层次结构融入共形预测，具有良好性能和用户接受度。

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [95] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: 本文研究带约束的Flow Matching样本生成问题，提出两种场景下的方法，通过实验验证效果，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决Flow Matching生成样本时需满足给定约束的问题。

Method: 针对可微距离函数已知和仅能通过成员查询访问约束集两种场景，分别提出带惩罚项的FM目标调整和采用随机化学习平均流的方法，还提出两阶段方法提高计算效率。

Result: 通过合成案例表明提出的方法在满足约束和匹配目标分布上有显著提升，还展示了基于预言机约束训练对抗样本生成器的应用。

Conclusion: 给出了未来研究方向，代码开源。

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [96] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: 本文针对单机调度问题提出数据驱动调度启发式方法，结合机器学习与问题特性，实验显示该方法性能优于现有技术，还对ML模型进行系统探索。


<details>
  <summary>Details</summary>
Motivation: 现有单机调度研究多聚焦精确算法，在部分问题空间表现不佳，数据驱动方法针对特定数据集结构有良好扩展性，因此研究数据驱动的单机调度问题。

Method: 引入结合机器学习与问题特性的新型数据驱动调度启发式方法，确保可行解，并进行系统的ML模型探索。

Result: 该方法在最优性差距、最优解数量和不同数据场景适应性方面显著优于现有技术。

Conclusion: 提出的方法具有灵活性，适用于实际应用，详细的模型选择过程为类似研究提供了参考。

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [97] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: 提出新型MoE训练系统X - MoE，解决现有MoE架构可扩展性问题，在Frontier超算上表现良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有专家专用的MoE架构可扩展性受激活内存开销和通信成本限制，且在非NVIDIA平台性能不佳，未充分利用计算潜力。

Method: 采用高效无填充MoE训练、冗余绕过调度、序列分片MoE块的混合并行等新技术。

Result: 在Frontier超算上，X - MoE能将DeepSeek风格的MoE扩展到5450亿参数，比现有方法在相同硬件预算下可训练的最大模型大10倍，且保持高训练吞吐量。

Conclusion: X - MoE是一个能为下一代MoE架构提供可扩展训练性能的有效系统。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [98] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: 本文探讨放松机器学习语言时全可观测的假设，提出更具挑战性的设置，给出示例并提出学习算法以促进语言习得。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习语言时未考虑的部分可观测性问题，即学习者无法看到所有相关信息。

Method: 先给出问题的示例，在简单场景中展示解决方法，再正式探讨一般场景的挑战，最后提出基于学习的算法进行私有信息解码。

Result: 展示了在简单场景中解决问题的方法，并提出了用于解码私有信息以促进语言习得的学习算法。

Conclusion: 放松全可观测假设，提出的学习算法有助于在部分可观测情况下实现语言习得。

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [99] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: 本文提出新方法改善不平衡回归框架下表格数据学习，结合解纠缠变分自编码器与平滑自举法，通过基准数据集评估其效率。


<details>
  <summary>Details</summary>
Motivation: 不平衡分布学习是预测建模常见挑战，现有方法多针对分类问题，对回归关注有限，需新方法解决不平衡回归问题。

Method: 提出用变分自编码器（VAEs）建模数据分布的潜在表示，结合解纠缠VAE和平滑自举法开发创新数据生成方法。

Result: 文中未明确提及具体结果，仅表明通过基准数据集与竞争对手进行了数值比较来评估方法效率。

Conclusion: 文中未明确给出结论。

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [100] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: 提出用于自闭症诊断的新框架，结合动态图创建和时空注意力机制，在ABIDE数据集子集上表现优于静态图方法，验证了动态连接和时空上下文联合建模的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前功能MRI分类方法存在局限，需要更好的方法来理解复杂神经活动动力学以助力神经科学发展。

Method: 使用基于Transformer的注意力机制动态推断每个时间间隔的脑功能连接，构建时变图并用GCN和Transformer处理。

Result: 在ABIDE数据集子集上，模型准确率达63.2，AUC为60.0，优于静态图方法。

Conclusion: 联合建模动态连接和时空上下文用于fMRI分类是有效的，核心创新在于注意力驱动的动态图创建和GCN - Transformer融合的分层时空特征融合。

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [101] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: 提出Trans - XFed架构用于供应链信用评估，结合联邦学习与可解释AI技术，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决供应链信用评估中隐私、信息孤岛、类别不平衡、Non - IID数据和模型可解释性等关键挑战。

Method: 引入基于性能的客户端选择策略（PBCS）处理类别不平衡和Non - IID问题；采用增强同态加密的FedProx架构为核心模型并加入Transformer编码器；使用集成梯度可解释AI技术。

Result: 在真实供应链数据集上实验表明，与多个基线模型相比，能进行准确的信用评估，同时保持透明度和隐私。

Conclusion: Trans - XFed架构在供应链信用评估中有效，能准确评估且兼顾隐私和透明度。

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [102] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [103] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: 提出线性代数方法证明求解$L^2$函数逼近问题的线性方法的维度下界，并用于给出核方法样本量下界。


<details>
  <summary>Details</summary>
Motivation: 为求解$L^2$函数逼近问题的线性方法提供维度下界证明方法，给出核方法样本量下界。

Method: 采用线性代数方法，应用已有文献中建立柯尔莫哥洛夫$n$ - 宽度下界的论证。

Result: 给出了求解$L^2$函数逼近问题的线性方法的维度下界，以及核方法的样本量下界。

Conclusion: 线性代数方法可用于证明求解$L^2$函数逼近问题的线性方法的维度下界，并能应用于核方法样本量下界的推导。

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [104] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 提出基于时间序列扩散的ODE - Diff框架，结合专家模型指导，在多案例中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有预测复杂动力系统反事实分布的方法在数据稀缺时表现不佳，需要更可靠的方法。

Method: 提出时间序列扩散框架ODE - Diff，结合不完美专家模型的指导，提取高级信号作为生成模型的结构化先验。

Result: 在半合成COVID - 19模拟、合成药物动力学和真实案例研究中，ODE - Diff在点预测和分布准确性上始终优于强基线。

Conclusion: ODE - Diff能桥接机械和数据驱动方法，实现更可靠和可解释的因果推断。

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [105] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 提出基于共形预测的统一框架将采样轨迹转换为有理论覆盖保证的校准预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有生成的未来轨迹通常未校准，需要解决该问题。

Method: 提出基于共形预测的统一框架，引入在线更新和优化步骤。

Result: 可围绕每条轨迹产生不连续预测区间，自然捕捉时间依赖，得到更清晰、更自适应的不确定性估计。

Conclusion: 该框架能有效将采样轨迹转换为校准预测区间。

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [106] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出统一框架J3O，用于在资源约束下优化多任务模型部署和查询路由，实验显示其能高效达到高推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有框架多针对单任务单模型场景，而现实应用需多任务并发执行，需在资源约束下优化多任务模型部署和查询路由。

Method: 将问题建模为混合整数规划，提出J3O交替算法，通过拉格朗日松弛子模优化选择模型部署，通过约束线性规划确定最优卸载，并扩展J3O以处理边缘批处理。

Result: 在多任务基准测试中，J3O始终能达到最优准确率的97%以上，且运行时间不到最优求解器的15%。

Conclusion: J3O能在内存、计算和通信约束下有效提高多任务推理的整体准确率，且具有良好的可扩展性。

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [107] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 本文提出评估临床癫痫发作起始区（SOZ）与通过啁啾事件时频分析确定的统计异常通道空间一致性的定量框架，展示了该方法在不同患者中的表现，表明其对SOZ定位有帮助。


<details>
  <summary>Details</summary>
Motivation: 为临床癫痫发作起始区（SOZ）与统计异常通道的空间一致性评估提供定量框架。

Method: 采用两步法，一是基于啁啾的时频特征用局部离群因子（LOF）分析进行无监督离群检测；二是进行空间相关分析，计算精确共现指标和加权指数相似度。

Result: LOF方法有效检测离群值，加权指标匹配在SOZ定位上优于精确匹配；无癫痫发作和手术成功患者表现更好，失败患者一致性较低。

Conclusion: 基于啁啾的离群检测结合加权空间指标为SOZ定位提供了补充方法，尤其适用于手术成功患者。

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [108] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: 本文引入NovoMolGen基础模型进行从头分子生成，研究语言建模实践对分子生成性能的影响，取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有对标准语言建模实践如何影响分子生成性能了解有限，需系统研究。

Method: 引入基于transformer的NovoMolGen模型，在15亿个分子上预训练，进行大量实证分析。

Result: 发现预训练性能指标与实际下游性能弱相关，揭示分子和通用NLP训练动态差异，NovoMolGen在无约束和目标导向分子生成任务中大幅超越先前模型。

Conclusion: NovoMolGen为推进高效有效的分子建模策略提供了坚实基础。

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [109] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: 本文针对网络环境下的上下文线性老虎机问题，提出NetLinUCB和Net - SGD - UCB算法，可降低学习复杂度，两算法各有优势且在模拟定价环境中有效。


<details>
  <summary>Details</summary>
Motivation: 经典上下文老虎机假设全集中数据或完全孤立学习者，网络环境中信息部分共享的情况研究不足，本文旨在填补此空白。

Method: 开发NetLinUCB和Net - SGD - UCB两种网络感知的上置信界（UCB）算法，将学习分解为全局和局部组件，进行自适应信息共享。

Result: 两种算法通信成本低于全集中设置，将与共享结构相关的学习复杂度从O(N)降至次线性O(√N)；NetLinUCB在低噪声细粒度异质性场景表现好，Net - SGD - UCB在高维高方差上下文环境中更稳健；在模拟定价环境中比基准方法有效。

Conclusion: 提出的两种算法能解决网络环境下上下文线性老虎机问题，降低学习复杂度，且各有适用场景。

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [110] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: 提出MAVIS框架，可在推理时动态控制大语言模型行为，且性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多目标设置下微调大语言模型计算成本高且缺乏灵活性，需更优方法。

Method: 训练一组对应不同目标的小价值模型，推理时用用户指定权重组合产生倾斜函数调整输出分布，用迭代算法训练价值模型。

Result: MAVIS性能优于按目标微调模型并事后组合的基线方法，接近按用户精确偏好微调模型的理想性能。

Conclusion: MAVIS是一种轻量级推理时对齐框架，无需修改基础模型权重就能动态控制大语言模型行为，效果良好。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [111] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: 提出EventTSF框架，融合历史时间序列和文本事件进行非平稳时间序列预测，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有单模态时间序列预测方法缺乏上下文知识，性能不佳，且融合文本事件的多模态预测未充分探索，存在同步、不确定性和对齐问题。

Method: 提出EventTSF自回归生成框架，使用自回归扩散和流匹配捕捉时间 - 事件交互，根据事件语义信号自适应控制流匹配时间步，用多模态U形扩散变压器融合多分辨率的时间和文本模态。

Result: 在8个数据集上实验，EventTSF在多种场景下优于12个基线，预测精度提高10.7%，训练效率提升1.13倍。

Conclusion: EventTSF框架能有效解决多模态非平稳时间序列预测问题，提高预测准确性和训练效率。

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [112] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: 本文提出SVDformer框架用于有向图表示学习，在节点分类任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有有向图神经网络因各向同性聚合和局部滤波机制，难以同时捕捉方向语义和全局结构模式。

Method: 结合SVD和Transformer架构，通过多头自注意力细化奇异值嵌入，将奇异向量作为方向投影基、奇异值作为缩放因子，用Transformer建模边模式间的多尺度交互。

Result: 在六个有向图基准测试的节点分类任务中，SVDformer始终优于最先进的GNN和方向感知基线。

Conclusion: SVDformer为有向图表示学习建立了新范式。

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [113] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: 提出ASAP方法动态调整学习率应对在线标签转移，实验表明其能提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在实际应用中面临在线标签转移问题，有效适应需谨慎选择学习率。

Method: 提出ASAP方法，通过计算当前和先前无标签输出的余弦距离并映射到有界范围来动态调整学习率，仅使用先前softmax输出进行快速轻量级适应。

Result: 在多个数据集和转移场景的实验中，ASAP持续提高了准确性和效率。

Conclusion: ASAP适用于无监督模型适应，具有实用性。

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [114] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出基于MTL的HCAL分类器，结合原型对比学习和自适应任务加权机制，在三个数据集实验中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决分层多标签分类（HMC）在多任务学习（MTL）中保持结构一致性和平衡损失权重的问题。

Method: 提出HCAL分类器，具有语义一致性，采用自适应损失加权机制，设计原型扰动机制，并定义分层违规率（HVR）指标。

Result: 在三个数据集上的实验表明，所提出的分类器具有更高的分类准确率和更低的分层违规率。

Conclusion: 所提出的HCAL分类器能有效解决HMC在MTL中的问题，性能优于基线模型。

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [115] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 研究利用t - SNE对啁啾特征可视化，进行三项分类任务，评估四个分类器，发现Random Forest和k - NN表现优，还生成特征影响敏感度图，展示可解释嵌入和局部特征归因潜力。


<details>
  <summary>Details</summary>
Motivation: 实现对不同结果场景下啁啾特征的可解释可视化，用于临床分层和决策支持。

Method: 使用t - SNE处理啁啾特征数据集，在2D t - SNE嵌入上进行三项分类任务，用分层5折交叉验证评估四个分类器，用SHAP解释生成特征影响敏感度图。

Result: Random Forest和k - NN分类器表现优，最优案例检测准确率达88.8%，生成的特征影响敏感度图揭示数据潜在结构。

Conclusion: 集成框架展示了可解释嵌入和局部特征归因用于临床分层和决策支持的潜力。

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [116] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: 本文提出DyMixOp框架用于近似偏微分方程（PDE）的非线性动力系统，能减少预测误差并保持计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决用神经网络近似PDE控制的非线性动力系统时，将系统转换为合适格式的挑战。

Method: 基于惯性流形理论将无限维非线性PDE动力学转换到有限维潜空间；采用受湍流对流动力学启发的局部 - 全局混合（LGM）变换；使用动态感知架构连接多个LGM层。

Result: 在不同PDE基准测试中取得了最先进的性能，显著降低预测误差，在对流主导场景中最高可降低86.7%，并保持计算效率和可扩展性。

Conclusion: DyMixOp框架能有效应对用神经网络近似PDE控制的非线性动力系统的挑战。

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [117] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 本文提出不确定性管这一可视化方法来表示神经网络推导的粒子路径中的不确定性，并结合多种技术在合成和模拟数据集上展示其应用。


<details>
  <summary>Details</summary>
Motivation: 当前有效量化和可视化神经网络预测粒子轨迹时的不确定性存在挑战，缺乏对不确定性的理解会影响模型在对可靠性要求高的应用中的可靠性。

Method: 设计并实现超椭圆管来捕捉和传达非对称不确定性，结合Deep Ensembles、Monte Carlo Dropout和Stochastic Weight Averaging - Gaussian等不确定性量化技术。

Result: 展示了不确定性管在合成和模拟数据集上的实际应用。

Conclusion: 不确定性管是一种新颖且计算高效的可视化方法，可用于表示神经网络推导的粒子路径中的不确定性。

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [118] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: 探讨算法不透明性的两种理解方式、伦理影响及可解释AI面临的挑战


<details>
  <summary>Details</summary>
Motivation: 解决复杂机器学习算法不透明性这一阻碍人工智能伦理发展的问题

Method: 先剖析两种不透明性及伦理影响，再研究克服技术不透明性的解释方法

Result: 分析表明可解释AI仍面临诸多挑战

Conclusion: 可解释AI发展存在困难，需进一步研究应对不透明性问题

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [119] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: 随着可再生能源发电渗透增加，开发了可扩展开源平台MuFlex用于多建筑灵活性协调控制策略的基准测试，案例显示其能降低总峰值需求并维持室内环境质量。


<details>
  <summary>Details</summary>
Motivation: 现有建筑领域测试平台多针对单建筑，多建筑平台有限且存在不能完全捕捉物理细节、适用性受限等问题，需要开发新平台。

Method: 开发MuFlex平台，实现EnergyPlus建筑模型间同步信息交换，遵循OpenAI Gym接口，使用Soft Actor - Critic算法结合微调超参数进行案例研究。

Result: 聚合四栋建筑灵活性可将总峰值需求降低到指定阈值以下，同时维持室内环境质量。

Conclusion: MuFlex平台能有效用于多建筑灵活性协调控制策略的基准测试和应用。

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [120] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出CALYPSO混合框架用于MRSA传播预测，结合神经网络与机制模型，提高预测性能并支持策略分析。


<details>
  <summary>Details</summary>
Motivation: 现有MRSA预测模型缺乏流行病学可解释性或难校准，需要更好的预测方法。

Method: 提出CALYPSO混合框架，整合神经网络与机制元种群模型，利用患者保险索赔、通勤数据和医疗转移模式学习参数。

Result: CALYPSO比机器学习基线提高全州预测性能超4.5%，能识别高风险区域和资源分配策略。

Conclusion: CALYPSO可实现多空间分辨率准确、可解释的预测，支持感染控制政策和风险分析。

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [121] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: 现有常见基因位点风险预测准确性不足，提出CROC方法用于常见和罕见变异风险预测研究，实验表明该方法更准确。


<details>
  <summary>Details</summary>
Motivation: 现有基于常见基因位点的风险预测准确性不足，需综合考虑常见和罕见变异进行风险预测。

Method: 提出CROC方法，它是FROC方法的扩展，增加处理罕见变异的程序，并使用Genetic Analysis Workshop 17迷你外显子数据集进行评估。

Result: 基于所有SNP构建的预测模型比仅基于常见变异的模型更准确；数据中常见变异减少时，CROC方法比FROC方法更准确；只有罕见变异时，CROC的AUC值更高。

Conclusion: CROC方法在风险预测研究中，尤其在处理罕见变异时具有更好的准确性。

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [122] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: 本文提出一种新的生成概率模型GenHAI来建模患者单次住院期间MRSA测试结果序列，通过与其他模型对比证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 美国疾控中心将MRSA列为严重抗微生物威胁，住院患者感染风险高，需要模型辅助医院管理者降低感染风险。

Method: 提出基于概率编程范式的生成概率模型GenHAI，用于近似回答多种预测、因果和反事实问题。

Result: 使用两个真实世界数据集将GenHAI与判别和生成机器学习模型对比，证明了其有效性。

Conclusion: GenHAI模型可用于从医院管理者角度回答重要问题，降低MRSA感染风险。

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [123] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文将标准自监督对比学习（SSCL）方法推广到广义学习框架（GLF），分析现有方法，提出设计约束部分的见解，针对设计难题提出自适应分布校准（ADC）方法，理论和实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习在下游任务表现优越，但在无标签情况下设计满足类内紧凑性和类间可分性的约束部分具有挑战。

Method: 将标准SSCL方法推广到GLF，分析三种现有SSCL方法；提出设计GLF约束部分的见解；通过迭代捕捉锚点与其他样本动态关系，提出ADC方法。

Result: 理论分析和实验评估都证明了ADC方法的优越性。

Conclusion: ADC方法能有效解决在自监督对比学习中设计约束部分的难题，具有良好的应用前景。

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [124] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: 本文提出在量化离散参数空间进行概率推理，用概率电路学习，经多种模型验证可提升推理效率且不损失精度，推动可扩展、可解释机器学习。


<details>
  <summary>Details</summary>
Motivation: 机器学习界为扩展大模型采用量化或低精度算术，本文旨在探索在量化离散参数空间进行概率推理的方法。

Method: 提出在量化离散参数空间进行概率推理，针对2D密度和量化神经网络，引入用概率电路的易处理学习方法。

Result: 经多种模型验证，在不牺牲精度的情况下提高了推理效率。

Conclusion: 利用离散近似进行概率计算推动了可扩展、可解释的机器学习。

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [125] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: 论文指出因果推断假设常难满足，部分识别是替代方案但应用不足，系统比较多种边界算法，提出方法扩展，经实证研究，给出算法选择决策树和预测模型，发布开源包。


<details>
  <summary>Details</summary>
Motivation: 因果推断假设难满足，部分识别方法分散且缺乏实践指导，需系统比较和实用建议。

Method: 在通用评估框架下实现、扩展和统一多种边界算法，提出熵界方法扩展，进行数千次随机模拟实验。

Result: 评估各方法的边界紧密度、计算效率和对假设违反的鲁棒性，得到算法选择决策树和预测模型。

Conclusion: 发布开源包CausalBoundingEngine，方便用户应用和比较边界算法。

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [126] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: 本文提出FedOL方法，在一轮通信中构建服务器模型，采用知识蒸馏减少通信开销，通过特殊目标函数和策略提高学习可靠性，仿真显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习存在忽视资源异构性、计算需求大、通信开销高等问题，需新方法解决。

Method: 提出FedOL方法，一轮通信构建服务器模型，采用知识蒸馏，引入特殊目标函数迭代优化，结合伪标签生成和知识蒸馏策略。

Result: 仿真结果表明FedOL显著优于现有基线。

Conclusion: FedOL为客户有隐私数据但计算资源有限的移动网络提供了经济有效的解决方案。

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [127] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: 提出T2W扩散变压器框架生成特定任务权重，实验证明其在未见任务上表现佳，推动生成模型在神经网络参数合成的实用性。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络权重生成方法在未见任务泛化和实际应用探索方面存在困难。

Method: 提出T2W框架，将网络参数分层处理为统一块，通过先验注意力机制集成CLIP文本嵌入，采用带权重空间增强的对抗训练。

Result: 在Cifar100、Caltech256和TinyImageNet上实验表明T2W能为未见任务生成高质量权重，优于基于优化的初始化，可实现权重增强和文本引导模型融合等新应用。

Conclusion: 工作将文本语义与权重空间动态联系起来，有开源文本 - 权重对数据集支持，提升了生成模型在神经网络参数合成的实用性。

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [128] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: 本文提出根据随机梯度内在变化自动更新学习率的机制，在多种随机算法中展现高效、鲁棒和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调整机制复杂，需手动调参，有计算开销大、耗时长等问题。

Method: 利用随机二阶算法开发可解释的学习率机制，学习率根据随机梯度范数变化自动增减。

Result: 所提出的学习率机制在不同经典随机算法（SGD、SGDM、SIGNSGD）的机器学习任务中展现出效率、鲁棒性和可扩展性。

Conclusion: 提出的学习率机制简单、自动，无需调参，有良好表现。

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [129] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: 提出FedSheafHN框架解决子图联邦学习中客户端性能差异问题，实验显示其优于现有方法且收敛快、泛化性好。


<details>
  <summary>Details</summary>
Motivation: 子图联邦学习中，因本地子图异质性导致客户端性能差异大，需解决该问题。

Method: 构建基于层协作机制的FedSheafHN框架，利用图级嵌入将客户端子图嵌入协作图，通过层扩散丰富客户端表示，再由服务器优化的超网络生成定制模型。

Result: 在各种图数据集上，FedSheafHN优于现有个性化子图联邦学习方法，模型收敛快，能有效泛化到新客户端。

Conclusion: FedSheafHN是解决子图联邦学习中客户端性能差异问题的有效方法。

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [130] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: 提出GRAFT方法，通过选择数据子集降低训练成本，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在大数据集上训练计算和环境成本高。

Method: 提出GRAFT方法，提取低秩特征表示，使用Fast MaxVol采样器选子集，动态调整子集大小。

Result: GRAFT在准确率和效率上匹配或超过近期选择基线。

Conclusion: GRAFT能在准确率、效率和排放之间取得良好平衡。

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [131] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 提出输入时间缩放新范式，结合大模型元知识优化输入，发现训练测试协同设计现象，低质量数据可能表现更好，实验取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 补充现有大语言模型的缩放方法，探索新的提升性能途径。

Method: 提出输入时间缩放范式，在训练和测试时结合大模型元知识用不同策略优化输入。

Result: 在Qwen2.5 - 32B - Instruct等模型实验中达到SOTA性能，如AIME24和AIME25的高pass@1率。

Conclusion: 输入时间缩放范式有效，低质量数据可能有好表现，数据集规模缩放需谨慎，发现的现象与少即是多现象兼容。

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [132] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 本文将CASH框架扩展以选择和适配现代ML管道，提出PS - PFN方法，实验显示其性能优于其他策略。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型发展，现代ML工作流超越超参数优化，ML管道异质性增加，需要新的AutoML方法。

Method: 提出PS - PFN，将后验采样扩展到最大k臂老虎机问题，利用先验数据拟合网络通过上下文学习估计最大值的后验分布，还扩展方法考虑拉臂成本和为每个臂单独建模奖励分布。

Result: 在一个新的和两个现有的标准基准任务上的实验表明，PS - PFN的性能优于其他老虎机和AutoML策略。

Conclusion: PS - PFN能有效探索和适配现代ML管道，具有良好性能，代码和数据已开源。

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [133] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 提出基于自注意力的通信模块用于多智能体强化学习，可微分，参数数量固定，在SMAC基准测试取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习通信协议复杂且不可微分，需要更好的通信机制。

Method: 引入基于自注意力的通信模块，可与动作值函数分解方法集成，以奖励驱动方式学习生成消息。

Result: 在SMAC基准测试中证明方法有效，在多个地图上达到SOTA性能。

Conclusion: 所提出的基于自注意力的通信模块在多智能体强化学习中有效且性能优越。

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [134] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [135] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: 提出DREAMS方法结合t - SNE和PCA优势，在多尺度上保留数据结构，在7个真实数据集上表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法不能同时很好地表示数据的局部和全局结构。

Method: 通过简单的正则化项将t - SNE的局部结构保留能力与PCA的全局结构保留能力相结合，生成一系列嵌入。

Result: 在7个真实数据集上进行基准测试，定性和定量地展示了其在多尺度上保留结构的能力优于以往方法。

Conclusion: DREAMS方法能有效平衡局部和全局结构的保留，在多尺度结构保留方面表现出色。

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [136] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: 研究随机多臂老虎机中夏普比率最大化的序贯决策问题，提出SRTS算法，理论分析其后悔上界和下界，证明其最优性，实验显示优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 在随机多臂老虎机设置下，解决夏普比率最大化的序贯决策问题，夏普比率优化需平衡收益与风险。

Method: 聚焦于高斯奖励下未知参数的汤普森采样算法，提出针对夏普比率的后悔分解，分析SRTS算法的后悔上界和下界。

Result: 汤普森采样随时间实现对数后悔，分布相关因子体现区分臂的难度，实验表明算法显著优于现有算法。

Conclusion: SRTS算法在夏普比率最大化的序贯决策问题上表现良好，具有理论最优性。

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [137] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: 本文指出RLVR在深度和广度两方面潜力未充分挖掘，提出DARS解决深度问题，通过扩大训练数据广度提升性能，还提出DARS - B，证实二者是释放RLVR推理能力的关键。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在深度和广度两方面未充分探索的问题，释放其推理能力。

Method: 剖析GRPO算法，提出Difficulty Adaptive Rollout Sampling (DARS) 解决深度问题；扩大批量大小，用全批量更新替代小批量迭代解决广度问题；提出DARS - B结合二者。

Result: DARS在无额外推理成本下提升Pass@K；扩大广度显著提升Pass@1；DARS - B同时提升Pass@K和Pass@1。

Conclusion: 广度和深度自适应探索是RLVR中正交维度，是释放推理能力的关键。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [138] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: 本文提出Periodic - Nested Group Attention（PENGUIN）机制用于长时序列预测，实验显示其优于MLP和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列预测的有效性存疑，需更好方法进行长时序列预测。

Method: 提出PENGUIN机制，引入周期性嵌套相对注意力偏差，设计分组注意力机制处理多周期问题。

Result: 在不同基准测试中，PENGUIN始终优于MLP和Transformer模型。

Conclusion: PENGUIN是一种简单有效的长时序列预测机制。

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [139] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出自适应机制ISP动态确定每轮最优客户端数量，提升联邦学习通信效率，验证显示能节省达30%通信成本且不损模型质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型扩展带来挑战，联邦学习虽有潜力但通信效率是瓶颈，现有方法未充分探索训练轮中客户端数量选择问题。

Method: 引入自适应机制ISP动态确定每轮最优客户端数量。

Result: 在多种场景下验证ISP有效性，通信节省达30%且不损失最终质量，突显客户端数量选择是联邦学习独立任务。

Conclusion: ISP能有效提升联邦学习通信效率，不影响模型精度，客户端数量选择应作为独立任务。

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [140] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: 本文实现分布式网络内强化学习框架用于可编程网络自适应路径选择，并在测试床评估。


<details>
  <summary>Details</summary>
Motivation: 在可编程网络中实现自适应路径选择。

Method: 结合随机学习自动机和带内网络遥测收集的实时遥测数据。

Result: 基于SLA的机制能收敛到有效路径选择，并能以线速适应网络条件变化。

Conclusion: 所提出的系统可实现基于本地数据驱动的转发决策，动态适应拥塞条件。

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [141] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: 本文提出首个评估AI训练数据集可信度的正式框架，基于主观逻辑支持信任命题并量化不确定性，以偏差属性为例实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作未用主观逻辑评估数据集整体层面的可信度属性，需评估AI训练数据集可信度。

Method: 构建基于主观逻辑的正式框架，支持信任命题并量化不确定性，以偏差属性为例开展实验。

Result: 方法能捕捉类别不平衡，在集中和联邦环境下可解释且稳健。

Conclusion: 所提框架能有效评估AI训练数据集的可信度。

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [142] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: 文章对一次性剪枝和迭代剪枝进行系统比较，发现不同剪枝率下各有优势，还提出混合剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 以往对迭代剪枝的广泛采用缺乏严格测试，需系统比较一次性剪枝和迭代剪枝方法。

Method: 对两种剪枝方法进行严格定义，在结构化和非结构化设置下进行基准测试，应用不同剪枝标准和模式。

Result: 一次性剪枝在低剪枝率下更有效，迭代剪枝在高剪枝率下表现更好。

Conclusion: 提出基于耐心的剪枝和混合方法，能在特定场景中超越传统方法，为从业者选择剪枝策略提供见解。

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [143] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: 文章提出轻量级联邦无学习算法FedUP，通过修剪受攻击模型的特定连接减轻恶意客户端影响，实验证明其有效、鲁棒且高效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受攻击，联邦无学习在处理恶意且可能勾结的客户端时面临挑战，需要有效解决方案。

Method: FedUP算法依靠客户端在无学习前最后一轮训练的权重，识别并抑制特定连接，选择并归零良性和恶意客户端最新更新中差异最大的高幅值权重。

Result: 在强对抗威胁模型下评估，实验表明FedUP在多种场景中减少恶意影响，降低恶意数据准确率，同时保持良性数据性能。

Conclusion: FedUP能有效实现无学习，比现有技术更快且节省存储。

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [144] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: 文章通过专家调查重新评估生物识别模式，发现各模式属性评级有变化，评估具有一致性，且专家评估与数据集不确定性有较强一致性，同时指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生物识别评估框架无法适应技术发展和新漏洞，需可靠框架评估生物识别模式对特定应用的适用性。

Method: 开展涉及24位生物识别专家的调查。

Result: 各模式属性评级有显著变化，评估具有一致性，专家评估与数据集不确定性大多一致。

Conclusion: 强调将实证证据与专家见解相结合的重要性，识别出的专家分歧可指导未来研究解决关键问题。

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [145] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: 本文介绍了Fisher - Orthogonal Projection (FOP)技术，可恢复二阶方法在大批次规模下的有效性，实现可扩展训练，提升泛化能力和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有优化器在大批次规模下表现不佳，一阶方法难以逃离局部极小值，二阶方法如KFAC在大批次规模下需高阻尼，会降低性能。

Method: 引入FOP技术，利用两个子批次的梯度构建方差感知的更新方向，在Fisher度量下用与平均梯度正交的梯度差分量增强平均梯度。

Result: 恢复了二阶方法在大批次规模下的有效性。

Conclusion: FOP技术可实现可扩展训练，有更好的泛化能力和更快的收敛速度。

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [146] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: 本文重新审视DQL的局限性，提出OFQL框架，可在训练和推理时高效进行单步动作生成，实验表明OFQL优于DQL及其他基线模型，且大幅减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: DQL在训练和推理时依赖多步去噪进行动作生成，限制了其实用性，直接应用单步去噪会导致性能大幅下降。

Method: 提出One - Step Flow Q - Learning (OFQL)框架，将DQL在样本高效的Flow Matching (FM)框架内重新表述，学习平均速度场以实现直接准确的动作生成。

Result: 在D4RL基准测试上的大量实验表明，OFQL优于DQL和其他基于扩散的基线模型，且与DQL相比大幅减少了训练和推理时间。

Conclusion: OFQL消除了DQL中多步采样和递归梯度更新的需求，实现了更快、更稳健的训练和推理。

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [147] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: 提出端到端预测框架，在边缘设备上实现节能推理，对比Transformer和LSTM模型，指出应根据部署优先级选模型


<details>
  <summary>Details</summary>
Motivation: 气候变化使极端天气挑战老化合流污水系统，AI预测方法依赖云计算，通信中断时可靠性受限

Method: 集成轻量级Transformer和LSTM模型，通过整数量化压缩，用自动化硬件感知部署管道搜索最优配置

Result: 8位Transformer模型精度高但能耗高，8位LSTM模型能耗低但精度差、训练时间长

Conclusion: 该工作实现本地节能预测，有助于构建更具弹性的合流污水系统

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [148] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: 论文引入分类策略建模多模态行为，在DeepMind控制套件环境评估，表现优于高斯策略。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习策略常以高斯分布参数化，导致学习行为单峰，无法适应多模态实际决策问题。

Method: 引入分类策略，用中间分类分布建模多模态行为模式，探索两种采样方案确保可微离散潜在结构和高效梯度优化。

Result: 在DeepMind控制套件环境评估，学习策略收敛更快，优于标准高斯策略。

Conclusion: 分类分布是连续控制中结构化探索和多模态行为表示的有力工具。

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [149] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: 研究53种自动特征工程（AutoFE）方法，发现其难用、缺文档、无活跃社区且无时间和内存约束设置，呼吁开发易用、设计良好的AutoFE方法。


<details>
  <summary>Details</summary>
Motivation: 现有AutoFE方法的可用性未被研究，而特征工程自动化很重要，需了解其可用性情况。

Method: 对53种AutoFE方法进行调查。

Result: 这些方法普遍难用、缺乏文档、无活跃社区，且无设置时间和内存约束的功能。

Conclusion: 未来需要开展关于可用、设计良好的AutoFE方法的研究。

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [150] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 提出随机最短路径（SSP）问题的三种算法，证明收敛性并展示优越性能。


<details>
  <summary>Details</summary>
Motivation: SSP问题是强化学习中重要问题类，其他成本准则可在此框架下表述。

Method: 在表格设置下提出两种算法，在函数近似设置下提出一种算法。

Result: 所有算法有渐近几乎必然收敛性，表格算法性能优于其他知名收敛RL算法，函数近似算法性能可靠。

Conclusion: 提出的算法在解决SSP问题上有良好表现。

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [151] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: 本文建立线性标量化与多任务优化方法联系，提出AutoScale框架选权重，高效且表现优。


<details>
  <summary>Details</summary>
Motivation: 不清楚特定权重为何产生最优性能及如何不依赖穷举搜索确定权重。

Method: 建立线性标量化与MTO方法联系，基于关键MTO指标趋势，提出两阶段AutoScale框架指导权重选择。

Result: AutoScale在多种数据集包括新大规模基准上始终高效且性能优越。

Conclusion: AutoScale能利用MTO指标指导线性标量化权重选择，无需昂贵权重搜索。

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [152] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: 介绍KMR框架用于深度学习模型效率技术的表示和推理，展示其优势并为未来研究奠基。


<details>
  <summary>Details</summary>
Motivation: 为深度学习模型效率技术提供统一的表示和推理形式，推动该领域研究。

Method: 将多种模型效率技术抽象为可控旋钮、确定性规则和可测量指标，提出Budgeted - KMR算法进行迭代预算优化。

Result: 展示知名效率方法可实例化为KMR三元组，给出算法模板，凸显方法间关系，便于混合流水线。

Conclusion: KMR为统一和推进模型效率研究提供概念和实用工具。

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [153] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: 将量化神经网络视为噪声通道链，提出通过优化问题识别量化瓶颈的方法，在极端设置下有竞争力且保持效率。


<details>
  <summary>Details</summary>
Motivation: 随着位宽缩小，量化神经网络每层的舍入操作会降低容量，需解决量化瓶颈问题。

Method: 将微调视为平滑的约束优化问题，采用可完全微分的带可学习参数的STE，通过外点惩罚强制执行目标位宽，用蒸馏进行温和的度量平滑。

Result: 该方法在极端W1A1设置下能达到有竞争力的准确率，且保留了STE的效率。

Conclusion: 所提出的方法简单有效，可应对量化神经网络的量化瓶颈问题。

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [154] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: 本文提出基于Transformer的ASDFormer模型，结合MoE捕获自闭症相关神经特征，应用于ABIDE数据集达到先进诊断准确率，有生物标志物发现潜力。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）与大脑连接中断有关，捕捉大脑功能社区内和社区间连接模式、识别与典型发育不同的交互对改善ASD诊断和生物标志物发现很重要。

Method: 引入基于Transformer架构的ASDFormer，结合MoE，集成多个专家分支和注意力机制，自适应强调与自闭症相关的脑区和连接模式。

Result: 应用于ABIDE数据集，ASDFormer达到了先进的诊断准确率，揭示了与ASD相关的功能连接中断的深刻见解。

Conclusion: ASDFormer具有作为生物标志物发现工具的潜力。

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [155] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [156] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 本文提出高效知识图谱（KG）无学习算法，用泰勒展开和近似方法减少计算开销，实验表明该方法在无学习效率和质量上优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 因法规要求去除训练数据及其对模型的影响，全重训练成本高，且KG无学习因KG结构和实体语义关系有挑战，大规模KG计算开销大。

Method: 定义KG无学习的影响函数，用泰勒展开估计数据移除导致的参数变化，用Fisher矩阵和零阶优化近似逆Hessian向量积。

Result: 所提方法在无学习效率和质量上显著优于其他先进的图无学习基线。

Conclusion: 提出的KG无学习算法有效，代码已开源。

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [157] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: 提出贝叶斯学习原子间势（BLIPs）解决机器学习原子间势（MLIPs）的不足，在模拟化学任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: MLIPs在分布外数据或数据稀缺时预测不准确，且无不确定性估计，需改进。

Method: 提出BLIPs，一种基于自适应变分Dropout的可扩展、与架构无关的变分贝叶斯框架，用于训练或微调MLIPs。

Result: 在模拟计算化学任务中，相比标准MLIPs，预测准确性提高，有可信的不确定性估计，微调预训练MLIPs有性能提升和校准的不确定性。

Conclusion: BLIPs能解决MLIPs的不足，在特定场景表现良好。

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [158] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: 提出新框架及LEOPARD算法从人类数据学习奖励函数，在有限反馈下表现优于基线，结合多种反馈有益。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务中难指定奖励函数，现有结合偏好反馈和专家演示的方法存在不足。

Method: 开发奖励 - 理性观测偏序新框架，引入LEOPARD算法从多种数据学习奖励函数。

Result: 有限反馈下LEOPARD显著优于现有基线，结合多种反馈通常有益。

Conclusion: 新框架和LEOPARD算法灵活可扩展，结合不同类型反馈对学习奖励函数有帮助。

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [159] [Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks](https://arxiv.org/abs/2508.13673)
*Yuzhe Liu,Xin Deng,Qiang Yu*

Main category: cs.NE

TL;DR: 本文提出用于SNN训练的多协同可塑性机制框架，在数据集上验证其提升性能和鲁棒性，为开发强大SNN提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有SNN训练方法依赖单一突触可塑性，限制了适应性和表征能力，需要更好的学习机制。

Method: 提出生物启发的训练框架，结合多种协同可塑性机制，使不同学习算法协同调节信息积累。

Result: 在静态图像和动态神经形态数据集上评估，框架比传统学习机制模型显著提升性能和鲁棒性。

Conclusion: 此工作为多策略脑启发学习引导的强大SNN开发提供通用和可扩展基础。

Abstract: Spiking Neural Networks (SNNs) are promising brain-inspired models known for
low power consumption and superior potential for temporal processing, but
identifying suitable learning mechanisms remains a challenge. Despite the
presence of multiple coexisting learning strategies in the brain, current SNN
training methods typically rely on a single form of synaptic plasticity, which
limits their adaptability and representational capability. In this paper, we
propose a biologically inspired training framework that incorporates multiple
synergistic plasticity mechanisms for more effective SNN training. Our method
enables diverse learning algorithms to cooperatively modulate the accumulation
of information, while allowing each mechanism to preserve its own relatively
independent update dynamics. We evaluated our approach on both static image and
dynamic neuromorphic datasets to demonstrate that our framework significantly
improves performance and robustness compared to conventional learning mechanism
models. This work provides a general and extensible foundation for developing
more powerful SNNs guided by multi-strategy brain-inspired learning.

</details>


### [160] [Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems](https://arxiv.org/abs/2508.13783)
*Eike-Manuel Edelmann,Alexander von Bank,Laurent Schmalen*

Main category: cs.NE

TL;DR: 提出基于强化学习的算法优化SNN神经编码参数，应用于IM/DD系统有良好效果


<details>
  <summary>Details</summary>
Motivation: SNN的神经编码参数通常是启发式设置，需要优化

Method: 提出基于强化学习的算法来优化神经编码参数

Result: 应用于IM/DD系统的SNN均衡器和解映射器，提高性能，降低计算负载和网络规模

Conclusion: 基于强化学习的算法能有效优化SNN的神经编码参数

Abstract: Neural encoding parameters for spiking neural networks (SNNs) are typically
set heuristically. We propose a reinforcement learning-based algorithm to
optimize them. Applied to an SNN-based equalizer and demapper in an IM/DD
system, the method improves performance while reducing computational load and
network size.

</details>


### [161] [Zobrist Hash-based Duplicate Detection in Symbolic Regression](https://arxiv.org/abs/2508.13859)
*Bogdan Burlacu*

Main category: cs.NE

TL;DR: 本文分析遗传编程（GP）中进化搜索效率问题，引入基于Zobrist哈希的缓存机制，在实际回归问题中实现高达34%加速且不影响搜索质量。


<details>
  <summary>Details</summary>
Motivation: 发现GP算法在进化搜索中多次重新访问和评估搜索空间中的点，导致计算资源浪费，因此要解决该问题。

Method: 引入基于Zobrist哈希的缓存机制，并使用开源框架Operon实现该缓存方法。

Result: 在一系列实际回归问题中，观察到该方法实现了高达34%的加速，且对搜索质量无不利影响。

Conclusion: 哈希方法是提高运行时性能的直接方式，还能基于缓存信息调整搜索策略。

Abstract: Symbolic regression encompasses a family of search algorithms that aim to
discover the best fitting function for a set of data without requiring an a
priori specification of the model structure. The most successful and commonly
used technique for symbolic regression is Genetic Programming (GP), an
evolutionary search method that evolves a population of mathematical
expressions through the mechanism of natural selection. In this work we analyze
the efficiency of the evolutionary search in GP and show that many points in
the search space are re-visited and re-evaluated multiple times by the
algorithm, leading to wasted computational effort. We address this issue by
introducing a caching mechanism based on the Zobrist hash, a type of hashing
frequently used in abstract board games for the efficient construction and
subsequent update of transposition tables. We implement our caching approach
using the open-source framework Operon and demonstrate its performance on a
selection of real-world regression problems, where we observe up to 34\%
speedups without any detrimental effects on search quality. The hashing
approach represents a straightforward way to improve runtime performance while
also offering some interesting possibilities for adjusting search strategy
based on cached information.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [162] [Multi-Metric Algorithmic Complexity: Beyond Asymptotic Analysis](https://arxiv.org/abs/2508.13249)
*Sergii Kavun*

Main category: cs.PF

TL;DR: 提出加权操作复杂度模型，可跨多维度计算效率得分，开源实现能分析代码并提供建议，验证显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统算法分析将基本操作视为同等成本，掩盖了现代处理器不同计算类型在时间、能耗和成本上的显著差异。

Method: 提出加权操作复杂度模型，为不同指令类型分配多维度成本值，根据用户优先级计算效率得分，可通过自动代码分析或与性能测量工具集成应用。

Result: 开源实现可分析代码、估计多维成本并提供效率建议，验证显示与测量数据强相关（ρ>0.9），在多目标场景中优于基线。

Conclusion: 该方法补充了现有理论模型，能进行考虑性能、可持续性和经济因素的实用架构感知算法比较。

Abstract: Traditional algorithm analysis treats all basic operations as equally costly,
which hides significant differences in time, energy consumption, and cost
between different types of computations on modern processors. We propose a
weighted-operation complexity model that assigns realistic cost values to
different instruction types across multiple dimensions: computational effort,
energy usage, carbon footprint, and monetary cost. The model computes overall
efficiency scores based on user-defined priorities and can be applied through
automated code analysis or integrated with performance measurement tools. This
approach complements existing theoretical models by enabling practical,
architecture-aware algorithm comparisons that account for performance,
sustainability, and economic factors. We demonstrate an open-source
implementation that analyzes code, estimates multi-dimensional costs, and
provides efficiency recommendations across various algorithms. We address two
research questions: (RQ1) Can a multi-metric model predict time/energy with
high accuracy across architectures? (RQ2) How does it compare to baselines like
Big-O, ICE, and EVM gas? Validation shows strong correlations (\r{ho}>0.9) with
measured data, outperforming baselines in multi-objective scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [163] [A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases](https://arxiv.org/abs/2508.13396)
*Dinesh Eswararaj,Ajay Babu Nellipudi,Vandana Kollati*

Main category: cs.SE

TL;DR: 本文对Delta Parquet、Iceberg和Hudi三种数据湖仓格式，使用汽车遥测时间序列数据进行对比分析，给出各格式特点及选型建议。


<details>
  <summary>Details</summary>
Motivation: 汽车行业产生大量数据，高效数据工程对处理延迟、可扩展性和一致性挑战至关重要，需对比分析现代数据湖仓格式。

Method: 使用真实世界的汽车遥测时间序列数据，从建模策略、分区、CDC支持等多方面对Delta Parquet、Iceberg和Hudi进行评估。

Result: Delta Parquet有强ML就绪性和治理能力；Iceberg在批处理分析和云原生工作负载表现好；Hudi适合实时摄入和增量处理，各格式在查询效率等方面有取舍。

Conclusion: 研究为选择或组合格式支持汽车应用提供见解，为扩展数据管道和集成机器学习模型提供实用指导。

Abstract: The automotive industry generates vast amounts of data from sensors,
telemetry, diagnostics, and real-time operations. Efficient data engineering is
critical to handle challenges of latency, scalability, and consistency. Modern
data lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer
features such as ACID transactions, schema enforcement, and real-time
ingestion, combining the strengths of data lakes and warehouses to support
complex use cases. This study presents a comparative analysis of Delta Parquet,
Iceberg, and Hudi using real-world time-series automotive telemetry data with
fields such as vehicle ID, timestamp, location, and event metrics. The
evaluation considers modeling strategies, partitioning, CDC support, query
performance, scalability, data consistency, and ecosystem maturity. Key
findings show Delta Parquet provides strong ML readiness and governance,
Iceberg delivers high performance for batch analytics and cloud-native
workloads, while Hudi is optimized for real-time ingestion and incremental
processing. Each format exhibits tradeoffs in query efficiency, time-travel,
and update semantics. The study offers insights for selecting or combining
formats to support fleet management, predictive maintenance, and route
optimization. Using structured datasets and realistic queries, the results
provide practical guidance for scaling data pipelines and integrating machine
learning models in automotive applications.

</details>


### [164] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: 研究代码格式化元素对大语言模型性能和效率的影响，发现去除格式可减省输入token，提示和微调可减省输出代码长度，还开发了格式处理工具。


<details>
  <summary>Details</summary>
Motivation: 代码格式化元素对大语言模型无益处，还会增加计算成本和响应时间，需研究其作用以降低成本。

Method: 在四种编程语言（Java、Python、C++、C#）的填空式代码补全任务上对十个大语言模型（包括商业和开源模型）进行大规模实验，分析去除格式元素后的token数量和性能。

Result: 大语言模型在有格式和无格式代码上性能相当，平均减少24.5%输入token，输出token减少可忽略；提示和微调大语言模型可使输出代码长度最多减少36.1%且不影响正确性。

Conclusion: 去除代码格式是提高大语言模型效率的实用优化策略，开发的双向代码转换工具可集成到现有推理工作流，兼顾人类可读性和模型效率。

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [165] [COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models](https://arxiv.org/abs/2508.13757)
*James Meaden,Michał Jarosz,Piotr Jodłowski,Grigori Melnik*

Main category: cs.SE

TL;DR: 提出COMPASS评估框架，评估代码生成的正确性、效率和质量，评估模型发现高正确性不意味着高效和可维护，强调多维度评估重要性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要关注功能正确性，忽略算法效率和代码质量，需全面评估框架。

Method: 引入COMPASS框架，含50个竞赛编程问题及大量提交数据，用行业标准工具评估效率和质量。

Result: 评估三个推理增强模型，发现高正确性得分模型不一定产生高效算法和可维护代码。

Conclusion: 强调多维度评估重要性，COMPASS为未来AI系统研究提供方向。

Abstract: Current code generation benchmarks focus primarily on functional correctness
while overlooking two critical aspects of real-world programming: algorithmic
efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional
Programming ASSessment), a comprehensive evaluation framework that assesses
code generation across three dimensions: correctness, efficiency, and quality.
COMPASS consists of 50 competitive programming problems from real Codility
competitions, providing authentic human baselines from 393,150 submissions.
Unlike existing benchmarks that treat algorithmically inefficient solutions
identically to optimal ones provided they pass test cases, COMPASS
systematically evaluates runtime efficiency and code quality using
industry-standard analysis tools. Our evaluation of three leading
reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and
OpenAI O4-Mini-High, reveals that models achieving high correctness scores do
not necessarily produce efficient algorithms or maintainable code. These
findings highlight the importance of evaluating more than just correctness to
truly understand the real-world capabilities of code generation models. COMPASS
serves as a guiding framework, charting a path for future research toward AI
systems that are robust, reliable, and ready for production use.

</details>


### [166] [Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API](https://arxiv.org/abs/2508.13774)
*Peer Trilcke,Ingo Börner,Henny Sluyter-Gäthje,Daniil Skorinkin,Frank Fischer,Carsten Milling*

Main category: cs.SE

TL;DR: 本文实现并评估了用于DraCor的模型上下文协议（MCP）服务器，让大语言模型能与DraCor API自主交互，实验凸显了文档字符串工程的重要性。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型能自主与DraCor API交互，研究其在计算文学研究中的应用及数字人文基础设施的发展。

Method: 采用定性方法，对大语言模型的工具选择和应用进行实验，系统观察提示以评估工具正确性、调用效率和使用可靠性。

Result: 实验表明了自主人工智能在计算文学研究中的潜力，以及可靠数字人文基础设施的必要发展需求。

Conclusion: 强调了文档字符串工程对于优化大语言模型与工具交互的重要性。

Abstract: This paper reports on the implementation and evaluation of a Model Context
Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to
autonomously interact with the DraCor API. We conducted experiments focusing on
tool selection and application by the LLM, employing a qualitative approach
that includes systematic observation of prompts to understand how LLMs behave
when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency",
and "Tool-Use Reliability". Our findings highlight the importance of "Docstring
Engineering", defined as reflexively crafting tool documentation to optimize
LLM-tool interaction. Our experiments demonstrate both the promise of agentic
AI for research in Computational Literary Studies and the essential
infrastructure development needs for reliable Digital Humanities
infrastructures.

</details>


### [167] [Structural and Connectivity Patterns in the Maven Central Software Dependency Network](https://arxiv.org/abs/2508.13819)
*Daniel Ogenrwot,John Businge,Shaikh Arifuzzaman*

Main category: cs.SE

TL;DR: 本文运用网络科学技术研究Maven Central生态系统，揭示其拓扑结构特点及枢纽库风险。


<details>
  <summary>Details</summary>
Motivation: 理解大型软件生态系统的结构特征和连接模式，以增强软件复用、提高生态系统弹性和降低安全风险。

Method: 利用Goblin框架，基于度中心性提取前5000个高度连接的工件作为种子节点，进行广度优先搜索扩展，构建图并计算图论指标。

Result: Maven Central呈现高度互联、无标度和小世界拓扑，存在少量支持多数项目的基础设施枢纽，主要是核心生态系统基础设施。

Conclusion: 枢纽库虽利于软件复用和集成，但会带来系统性风险，关键节点故障或漏洞会产生广泛影响。

Abstract: Understanding the structural characteristics and connectivity patterns of
large-scale software ecosystems is critical for enhancing software reuse,
improving ecosystem resilience, and mitigating security risks. In this paper,
we investigate the Maven Central ecosystem, one of the largest repositories of
Java libraries, by applying network science techniques to its dependency graph.
Leveraging the Goblin framework, we extracted a sample consisting of the top
5,000 highly connected artifacts based on their degree centrality and then
performed breadth-first search (BFS) expansion from each selected artifact as a
seed node, traversing the graph outward to capture all libraries and releases
reachable those seed nodes. This sampling strategy captured the immediate
structural context surrounding these libraries resulted in a curated graph
comprising of 1.3 million nodes and 20.9 million edges. We conducted a
comprehensive analysis of this graph, computing degree distributions,
betweenness centrality, PageRank centrality, and connected components
graph-theoretic metrics. Our results reveal that Maven Central exhibits a
highly interconnected, scale-free, and small-world topology, characterized by a
small number of infrastructural hubs that support the majority of projects.
Further analysis using PageRank and betweenness centrality shows that these
hubs predominantly consist of core ecosystem infrastructure, including testing
frameworks and general-purpose utility libraries. While these hubs facilitate
efficient software reuse and integration, they also pose systemic risks;
failures or vulnerabilities affecting these critical nodes can have widespread
and cascading impacts throughout the ecosystem.

</details>


### [168] [Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems](https://arxiv.org/abs/2508.13863)
*Shuai Zhao,Jieyu Jiang,Shenlin Cai,Yaowei Liang,Chen Jie,Yinjie Fang,Wei Zhang,Guoquan Zhang,Yaoyao Gu,Xiang Xiao,Wei Qin,Xiangzhen Ouyang,Wanli Chang*

Main category: cs.SE

TL;DR: 提出新的多核架构下跨核缓存争用分析方法，减少跨核缓存干扰和WCET估计，且不过多增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有多核架构WCET估计方法在考虑跨核缓存争用时，未考虑实际缓存状态和访问次数，导致高估缓存未命中数。

Method: 基于任务中程序区域顺序识别受远程访问影响的内存引用，构建细粒度争用分析，通过动态规划计算任务的整体跨核缓存干扰。

Result: 与现有方法相比，提出的分析方法平均减少跨核缓存干扰52.31%，WCET估计减少8.94%，且未显著增加计算开销。

Conclusion: 新的分析方法能有效减少跨核缓存干扰和WCET估计，同时不过多增加计算成本。

Abstract: WCET (Worst-Case Execution Time) estimation on multicore architecture is
particularly challenging mainly due to the complex accesses over cache shared
by multiple cores. Existing analysis identifies possible contentions between
parallel tasks by leveraging the partial order of the tasks or their program
regions. Unfortunately, they overestimate the number of cache misses caused by
a remote block access without considering the actual cache state and the number
of accesses. This paper reports a new analysis for inter-core cache contention.
Based on the order of program regions in a task, we first identify memory
references that could be affected if a remote access occurs in a region.
Afterwards, a fine-grained contention analysis is constructed that computes the
number of cache misses based on the access quantity of local and remote blocks.
We demonstrate that the overall inter-core cache interference of a task can be
obtained via dynamic programming. Experiments show that compared to existing
methods, the proposed analysis reduces inter-core cache interference and WCET
estimations by 52.31% and 8.94% on average, without significantly increasing
computation overhead.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [169] [AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market](https://arxiv.org/abs/2508.13429)
*Paulo André Lima de Castro*

Main category: q-fin.CP

TL;DR: 研究提出受价值投资启发的AI策略，控制偏见后优于巴西市场基准和常用技术指标，还探讨了未来发展挑战。


<details>
  <summary>Details</summary>
Motivation: 现有自主交易策略在回测和实盘表现差异大，金融AI模型易有偏差导致回测表现虚高。

Method: 提出受价值投资启发的AI策略，通过控制偏见进行一系列计算模拟。

Result: 策略优于巴西主要市场基准，比RSI和MFI表现好，结果有统计学意义。

Conclusion: 策略有效，还讨论了未来构建全面AI价值投资框架的挑战和新兴技术。

Abstract: Autonomous trading strategies have been a subject of research within the
field of artificial intelligence (AI) for aconsiderable period. Various AI
techniques have been explored to develop autonomous agents capable of trading
financial assets. These approaches encompass traditional methods such as neural
networks, fuzzy logic, and reinforcement learning, as well as more recent
advancements, including deep neural networks and deep reinforcement learning.
Many developers report success in creating strategies that exhibit strong
performance during simulations using historical price data, a process commonly
referred to as backtesting. However, when these strategies are deployed in real
markets, their performance often deteriorates, particularly in terms of
risk-adjusted returns. In this study, we propose an AI-based strategy inspired
by a classical investment paradigm: Value Investing. Financial AI models are
highly susceptible to lookahead bias and other forms of bias that can
significantly inflate performance in backtesting compared to live trading
conditions. To address this issue, we conducted a series of computational
simulations while controlling for these biases, thereby reducing the risk of
overfitting. Our results indicate that the proposed approach outperforms major
Brazilian market benchmarks. Moreover, the strategy, named AlphaX, demonstrated
superior performance relative to widely used technical indicators such as the
Relative Strength Index (RSI) and Money Flow Index (MFI), with statistically
significant results. Finally, we discuss several open challenges and highlight
emerging technologies in qualitative analysis that may contribute to the
development of a comprehensive AI-based Value Investing framework in the future

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [170] [Preference Models assume Proportional Hazards of Utilities](https://arxiv.org/abs/2508.13189)
*Chirag Nagpal*

Main category: stat.ML

TL;DR: 本文将Plackett - Luce模型与Cox比例风险模型建立联系并探讨其影响。


<details>
  <summary>Details</summary>
Motivation: 现代AI对齐工具基于Plackett - Luce模型的统计假设，希望通过将其与Cox比例风险模型建立联系来深入了解相关影响。

Method: 将Plackett - Luce模型与Cox比例风险模型建立连接。

Result: 未提及。

Conclusion: 未提及。

Abstract: Approaches for estimating preferences from human annotated data typically
involves inducing a distribution over a ranked list of choices such as the
Plackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling
and Direct Preference Optimization are based on the statistical assumptions
posed by the Plackett-Luce model. In this paper, I will connect the
Plackett-Luce model to another classical and well known statistical model, the
Cox Proportional Hazards model and attempt to shed some light on the
implications of the connection therein.

</details>


### [171] [Structural Foundations for Leading Digit Laws: Beyond Probabilistic Mixtures](https://arxiv.org/abs/2508.13237)
*Vladimir Berman*

Main category: stat.ML

TL;DR: 本文提出研究数值数据首位有效数字分布的确定性框架，用移位不变泛函方程解释分布，分析不同数据集，评估概率模型，建立统一数学基础。


<details>
  <summary>Details</summary>
Motivation: 不依赖传统概率或混合解释，从数据生成过程的算术、算法和结构特性研究首位数字频率。

Method: 围绕移位不变泛函方程，其通解由显式仿射加周期公式给出。

Result: 解释了经验和数学数据集中数字分布的多样性，分析了有限和无限数据集、确定性序列，发现块结构和分形特征。

Conclusion: 为数字现象建立统一数学基础，提供建模和分析数字模式的通用工具集。

Abstract: This article presents a modern deterministic framework for the study of
leading significant digit distributions in numerical data. Rather than relying
on traditional probabilistic or mixture-based explanations, we demonstrate that
the observed frequencies of leading digits are determined by the underlying
arithmetic, algorithmic, and structural properties of the data-generating
process. Our approach centers on a shift-invariant functional equation, whose
general solution is given by explicit affine-plus-periodic formulas. This
structural formulation explains the diversity of digit distributions
encountered in both empirical and mathematical datasets, including cases with
pronounced deviations from logarithmic or scale-invariant profiles.
  We systematically analyze digit distributions in finite and infinite
datasets, address deterministic sequences such as prime numbers and recurrence
relations, and highlight the emergence of block-structured and fractal
features. The article provides critical examination of probabilistic models,
explicit examples and counterexamples, and discusses limitations and open
problems for further research. Overall, this work establishes a unified
mathematical foundation for digital phenomena and offers a versatile toolset
for modeling and analyzing digit patterns in applied and theoretical contexts.

</details>


### [172] [Flow Matching-Based Generative Modeling for Efficient and Scalable Data Assimilation](https://arxiv.org/abs/2508.13313)
*Taos Transue,Bohan Chen,So Takao,Bao Wang*

Main category: stat.ML

TL;DR: 本文提出基于流匹配的集成流滤波器（EnFF）用于数据同化，相比现有方法采样更快、设计更灵活，理论上涵盖经典滤波方法，实验显示有更好的成本 - 精度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成建模的数据同化方法因采样慢导致计算负担大，需新方法加速采样和灵活设计概率路径。

Method: 引入基于流匹配的集成流滤波器（EnFF），集成蒙特卡罗估计器和局部引导来同化观测，且无需训练。

Result: 实验在高维滤波基准上显示，EnFF有更好的成本 - 精度权衡，能利用更大的集合。

Conclusion: 流匹配作为可扩展工具，在高维应用滤波中很有前景，可使用大集合。

Abstract: Data assimilation (DA) is the problem of sequentially estimating the state of
a dynamical system from noisy observations. Recent advances in generative
modeling have inspired new approaches to DA in high-dimensional nonlinear
settings, especially the ensemble score filter (EnSF). However, these come at a
significant computational burden due to slow sampling. In this paper, we
introduce a new filtering framework based on flow matching (FM) -- called the
ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible
design of probability paths. EnFF -- a training-free DA approach -- integrates
MC estimators for the marginal FM vector field (VF) and a localized guidance to
assimilate observations. EnFF has faster sampling and more flexibility in VF
design compared to existing generative modeling for DA. Theoretically, we show
that EnFF encompasses classical filtering methods such as the bootstrap
particle filter and the ensemble Kalman filter as special cases. Experiments on
high-dimensional filtering benchmarks demonstrate improved cost-accuracy
tradeoffs and the ability to leverage larger ensembles than prior methods. Our
results highlight the promise of FM as a scalable tool for filtering in
high-dimensional applications that enable the use of large ensembles.

</details>


### [173] [Smooth Flow Matching](https://arxiv.org/abs/2508.13831)
*Jianbin Tan,Anru R. Zhang*

Main category: stat.ML

TL;DR: 本文提出Smooth Flow Matching (SFM)框架用于功能数据生成建模，解决现有分析挑战，经模拟研究和实际应用验证其优势。


<details>
  <summary>Details</summary>
Motivation: 功能数据统计分析面临隐私约束、采样稀疏不规则等挑战，需要有效方法进行生成建模。

Method: 提出SFM框架，基于流匹配思想构建半参数copula流生成无限维功能数据。

Result: 模拟研究表明SFM在合成数据质量和计算效率上有优势，应用于MIMIC - IV EHR数据库能生成高质量替代数据。

Conclusion: SFM是灵活实用的解决方案，能提高EHR数据在临床应用中的效用。

Abstract: Functional data, i.e., smooth random functions observed over a continuous
domain, are increasingly available in areas such as biomedical research, health
informatics, and epidemiology. However, effective statistical analysis for
functional data is often hindered by challenges such as privacy constraints,
sparse and irregular sampling, infinite dimensionality, and non-Gaussian
structures. To address these challenges, we introduce a novel framework named
Smooth Flow Matching (SFM), tailored for generative modeling of functional data
to enable statistical analysis without exposing sensitive real data. Built upon
flow-matching ideas, SFM constructs a semiparametric copula flow to generate
infinite-dimensional functional data, free from Gaussianity or low-rank
assumptions. It is computationally efficient, handles irregular observations,
and guarantees the smoothness of the generated functions, offering a practical
and flexible solution in scenarios where existing deep generative methods are
not applicable. Through extensive simulation studies, we demonstrate the
advantages of SFM in terms of both synthetic data quality and computational
efficiency. We then apply SFM to generate clinical trajectory data from the
MIMIC-IV patient electronic health records (EHR) longitudinal database. Our
analysis showcases the ability of SFM to produce high-quality surrogate data
for downstream statistical tasks, highlighting its potential to boost the
utility of EHR data for clinical applications.

</details>


### [174] [Online Conformal Selection with Accept-to-Reject Changes](https://arxiv.org/abs/2508.13838)
*Kangdao Liu,Huajun Xi,Chi-Man Vong,Hongxin Wei*

Main category: stat.ML

TL;DR: 提出在线接受 - 拒绝更改的保形选择程序OCS - ARC，可不可逆地选择候选对象，有理论保证并在实验中展现优势


<details>
  <summary>Details</summary>
Motivation: 传统保形选择应用于在线场景有局限，无法适应不可逆选择决策，如药物发现

Method: 提出OCS - ARC方法，将在线Benjamini - Hochberg程序纳入候选选择过程

Result: 理论上保证在i.i.d.和可交换数据假设下控制错误发现率，实验显示在各时间步提升选择能力并有效控制FDR

Conclusion: OCS - ARC能解决传统保形选择在在线场景局限，有理论保障且实验效果好

Abstract: Selecting a subset of promising candidates from a large pool is crucial
across various scientific and real-world applications. Conformal selection
offers a distribution-free and model-agnostic framework for candidate selection
with uncertainty quantification. While effective in offline settings, its
application to online scenarios, where data arrives sequentially, poses
challenges. Notably, conformal selection permits the deselection of previously
selected candidates, which is incompatible with applications requiring
irreversible selection decisions. This limitation is particularly evident in
resource-intensive sequential processes, such as drug discovery, where
advancing a compound to subsequent stages renders reversal impractical. To
address this issue, we extend conformal selection to an online Accept-to-Reject
Changes (ARC) procedure: non-selected data points can be reconsidered for
selection later, and once a candidate is selected, the decision is
irreversible. Specifically, we propose a novel conformal selection method,
Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC),
which incorporates online Benjamini-Hochberg procedure into the candidate
selection process. We provide theoretical guarantees that OCS-ARC controls the
false discovery rate (FDR) at or below the nominal level at any timestep under
both i.i.d. and exchangeable data assumptions. Additionally, we theoretically
show that our approach naturally extends to multivariate response settings.
Extensive experiments on synthetic and real-world datasets demonstrate that
OCS-ARC significantly improves selection power over the baseline while
maintaining valid FDR control across all examined timesteps.

</details>


### [175] [Generalisation and benign over-fitting for linear regression onto random functional covariates](https://arxiv.org/abs/2508.13895)
*Andrew Jones,Nick Whiteley*

Main category: stat.ML

TL;DR: 研究脊回归和无脊最小二乘回归在随机函数协变量下的理论预测性能，给出预测超额风险的概率界和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 突破独立同分布数据的标准假设，研究协变量向量为可交换但非独立情况下的回归预测性能。

Method: 在维度独立、四阶矩等正则条件下，利用Barzilai和Shamir的近期结果，得到预测超额风险的概率界。

Result: 在p相对于n适当快速增长的情况下导出收敛速率，说明模型成分在确定收敛行为中的相互作用和协变量噪声在良性过拟合中的作用。

Conclusion: 为随机函数协变量设置下的回归预测性能提供了理论分析，揭示了模型成分和噪声的影响。

Abstract: We study theoretical predictive performance of ridge and ridge-less
least-squares regression when covariate vectors arise from evaluating $p$
random, means-square continuous functions over a latent metric space at $n$
random and unobserved locations, subject to additive noise. This leads us away
from the standard assumption of i.i.d. data to a setting in which the $n$
covariate vectors are exchangeable but not independent in general. Under an
assumption of independence across dimensions, $4$-th order moment, and other
regularity conditions, we obtain probabilistic bounds on a notion of predictive
excess risk adapted to our random functional covariate setting, making use of
recent results of Barzilai and Shamir. We derive convergence rates in regimes
where $p$ grows suitably fast relative to $n$, illustrating interplay between
ingredients of the model in determining convergence behaviour and the role of
additive covariate noise in benign-overfitting.

</details>


### [176] [A PC Algorithm for Max-Linear Bayesian Networks](https://arxiv.org/abs/2508.13967)
*Carlos Améndola,Benjamin Hollering,Francesco Nowell*

Main category: stat.ML

TL;DR: 研究MLBNs的基于约束的因果发现算法，证明PC算法在特定条件下的一致性，引入新算法PCstar。


<details>
  <summary>Details</summary>
Motivation: MLBNs通常不满足d - 分离忠实性，经典因果发现算法无法准确恢复其真实图结构。

Method: 研究给定条件独立性测试神谕下的基于约束的发现算法，基于* - 分离准则分析PC算法的一致性，引入假设对C* - 分离忠实的PCstar算法。

Result: 若神谕由真实图中的* - 分离准则给出，PC算法保持一致性；PCstar算法能定向仅靠d - 或* - 分离无法定向的额外边。

Conclusion: 为MLBNs的因果发现算法展开研究，提供了有效的算法和相关结论。

Abstract: Max-linear Bayesian networks (MLBNs) are a relatively recent class of
structural equation models which arise when the random variables involved have
heavy-tailed distributions. Unlike most directed graphical models, MLBNs are
typically not faithful to d-separation and thus classical causal discovery
algorithms such as the PC algorithm or greedy equivalence search can not be
used to accurately recover the true graph structure. In this paper, we begin
the study of constraint-based discovery algorithms for MLBNs given an oracle
for testing conditional independence in the true, unknown graph. We show that
if the oracle is given by the $\ast$-separation criteria in the true graph,
then the PC algorithm remains consistent despite the presence of additional CI
statements implied by $\ast$-separation. We also introduce a new causal
discovery algorithm named "PCstar" which assumes faithfulness to
$C^\ast$-separation and is able to orient additional edges which cannot be
oriented with only d- or $\ast$-separation.

</details>


### [177] [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](https://arxiv.org/abs/2508.13990)
*Daniel Klötzl,Ozan Tastekin,David Hägele,Marina Evers,Daniel Weiskopf*

Main category: stat.ML

TL;DR: 提出用高斯混合模型（GMMs）对多维分布建模，将分布投影到低维空间，投影效果优于UAPCA映射，还支持用户定义权重，并进行了评估。


<details>
  <summary>Details</summary>
Motivation: 多维数据常带有非正态分布的不确定性，现有方法不能很好描述，需更好的投影方法。

Method: 用高斯混合模型（GMMs）对多维分布建模，从通用公式推导投影，支持用户定义不同分布间的权重。

Result: 低维投影比UAPCA映射更能展现分布细节，更真实地表示分布。

Conclusion: 所提方法在多维数据低维投影上有优势，可通过对比实验评估。

Abstract: Multidimensional data is often associated with uncertainties that are not
well-described by normal distributions. In this work, we describe how such
distributions can be projected to a low-dimensional space using
uncertainty-aware principal component analysis (UAPCA). We propose to model
multidimensional distributions using Gaussian mixture models (GMMs) and derive
the projection from a general formulation that allows projecting arbitrary
probability density functions. The low-dimensional projections of the densities
exhibit more details about the distributions and represent them more faithfully
compared to UAPCA mappings. Further, we support including user-defined weights
between the different distributions, which allows for varying the importance of
the multidimensional distributions. We evaluate our approach by comparing the
distributions in low-dimensional space obtained by our method and UAPCA to
those obtained by sample-based projections.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [178] [Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray](https://arxiv.org/abs/2508.13236)
*Hyeonjin Choi,Jinse Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: 肺癌早诊难，医生对医疗AI信任不足源于诊断不确定性，本研究提出不确定感知学习策略，模型表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期准确诊断困难，医生对医疗AI信任不足，根源在于其诊断的不确定性。

Method: 提出不确定感知学习策略，学习医生背景知识和胸部X光病变信息，使用Ajou大学医院的2517张无病变图像和656张结节图像。

Result: 所提模型达到92%（IoU 0.2 / FPPI 2），灵敏度比基线模型提高10%，不确定性熵降低0.2。

Conclusion: 不确定感知学习策略可解决医疗AI知识不足问题，提升诊断性能。

Abstract: Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

</details>


### [179] [Sex-Specific Vascular Score: A Novel Perfusion Biomarker from Supervoxel Analysis of 3D pCASL MRI](https://arxiv.org/abs/2508.13173)
*Sneha Noble,Neelam Sinha,Vaanathi Sundareshan,Thomas Gregor Issac*

Main category: eess.IV

TL;DR: 提出利用3D pCASL MRI计算特定性别血管分数的框架，实现高准确率性别分类，评估区域CBF变化和年龄效应，助力神经退行性疾病早期检测。


<details>
  <summary>Details</summary>
Motivation: 量化脑血管健康和潜在疾病易感性，增进对正常脑灌注和衰老的理解，促进神经退行性疾病早期检测和个性化干预。

Method: 使用超体素聚类将大脑划分为同质灌注区域，提取186名认知健康参与者的平均脑血流量值训练自定义卷积神经网络。

Result: 在性别分类中达到95%的准确率，凸显大脑中稳健的特定性别灌注模式，系统评估了区域CBF变化和年龄效应。

Conclusion: 所提出的血管风险评分框架有助于理解脑灌注和衰老，可促进神经退行性疾病的早期检测和个性化干预。

Abstract: We propose a novel framework that leverages 3D pseudo-continuous arterial
spin labeling (3D pCASL) MRI to compute sex-specific vascular scores that
quantify cerebrovascular health and potential disease susceptibility. The brain
is parcellated into spatially contiguous regions of homogeneous perfusion using
supervoxel clustering, capturing both microvascular and macrovascular
contributions. Mean cerebral blood flow (CBF) values are extracted from 186
cognitively healthy participants and used to train a custom convolutional
neural network, achieving 95 percent accuracy in sex classification. This
highlights robust, sex-specific perfusion patterns across the brain.
Additionally, regional CBF variations and age-related effects are
systematically evaluated within male and female cohorts. The proposed vascular
risk-scoring framework enhances understanding of normative brain perfusion and
aging, and may facilitate early detection and personalized interventions for
neurodegenerative diseases such as Alzheimer's.

</details>


### [180] [Automated Cervical Cancer Detection through Visual Inspection with Acetic Acid in Resource-Poor Settings with Lightweight Deep Learning Models Deployed on an Android Device](https://arxiv.org/abs/2508.13253)
*Leander Melroy Maben,Keerthana Prasad,Shyamala Guruvare,Vidya Kudva,P C Siddalingaswamy*

Main category: eess.IV

TL;DR: 本文提出轻量级深度学习算法用于宫颈癌筛查，部署在安卓设备，结果显示有较高准确率，是有前景的低资源筛查方法。


<details>
  <summary>Details</summary>
Motivation: 宫颈癌在中低收入国家致死率高，VIA筛查需专业人员且有主观性，希望用AI自动化消除主观性并实现任务转移，加速低资源地区筛查项目。

Method: 提出包含EfficientDet - Lite3作为感兴趣区域检测器和基于MobileNet - V2的分类模型的轻量级深度学习算法，部署在安卓设备。

Result: 分类模型在测试数据集上准确率92.31%，灵敏度98.24%，特异性88.37%。

Conclusion: 该算法是有前景的自动化低资源筛查方法。

Abstract: Cervical cancer is among the most commonly occurring cancer among women and
claims a huge number of lives in low and middle-income countries despite being
relatively easy to treat. Several studies have shown that public screening
programs can bring down cervical cancer incidence and mortality rates
significantly. While several screening tests are available, visual inspection
with acetic acid (VIA) presents itself as the most viable option for
low-resource settings due to the affordability and simplicity of performing the
test. VIA requires a trained medical professional to interpret the test and is
subjective in nature. Automating VIA using AI eliminates subjectivity and would
allow shifting of the task to less trained health workers. Task shifting with
AI would help further expedite screening programs in low-resource settings. In
our work, we propose a lightweight deep learning algorithm that includes
EfficientDet-Lite3 as the Region of Interest (ROI) detector and a MobileNet- V2
based model for classification. These models would be deployed on an
android-based device that can operate remotely and provide almost instant
results without the requirement of highly-trained medical professionals, labs,
sophisticated infrastructure, or internet connectivity. The classification
model gives an accuracy of 92.31%, a sensitivity of 98.24%, and a specificity
of 88.37% on the test dataset and presents itself as a promising automated
low-resource screening approach.

</details>


### [181] [Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms](https://arxiv.org/abs/2508.13710)
*Nizheen A. Ali,Ramadhan J. Mstafa*

Main category: eess.IV

TL;DR: 本文提出一种新的视频隐写方法，利用遗传算法识别感兴趣区域，加密数据后嵌入视频，经测试有高嵌入容量和效率。


<details>
  <summary>Details</summary>
Motivation: 互联网普及使数据安全隐私需求提升，需研究能高效隐写且不改变视频质量的方法。

Method: 用遗传算法识别视频感兴趣区域，用AES加密秘密数据，将加密数据嵌入最多10%的视频中，用PSNR和编解码时间评估性能。

Result: 该方法嵌入容量和效率高，PSNR在64 - 75 dB之间，编解码速度快。

Conclusion: 该方法适用于实时应用，能在保证视频质量的同时有效嵌入数据。

Abstract: With the widespread use of the internet, there is an increasing need to
ensure the security and privacy of transmitted data. This has led to an
intensified focus on the study of video steganography, which is a technique
that hides data within a video cover to avoid detection. The effectiveness of
any steganography method depends on its ability to embed data without altering
the original video quality while maintaining high efficiency. This paper
proposes a new method to video steganography, which involves utilizing a
Genetic Algorithm (GA) for identifying the Region of Interest (ROI) in the
cover video. The ROI is the area in the video that is the most suitable for
data embedding. The secret data is encrypted using the Advanced Encryption
Standard (AES), which is a widely accepted encryption standard, before being
embedded into the cover video, utilizing up to 10% of the cover video. This
process ensures the security and confidentiality of the embedded data. The
performance metrics for assessing the proposed method are the Peak Signal to
Noise Ratio (PSNR) and the encoding and decoding time. The results show that
the proposed method has a high embedding capacity and efficiency, with a PSNR
ranging between 64 and 75 dBs, which indicates that the embedded data is almost
indistinguishable from the original video. Additionally, the method can encode
and decode data quickly, making it efficient for real time applications.

</details>


### [182] [Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images](https://arxiv.org/abs/2508.13776)
*Sebastian Ibarra,Javier del Riego,Alessandro Catanese,Julian Cuba,Julian Cardona,Nataly Leon,Jonathan Infante,Karim Lekadir,Oliver Diaz,Richard Osuala*

Main category: eess.IV

TL;DR: 提出预对比条件去噪扩散概率模型合成DCE - MRI，对比22种生成模型变体，结果显示减影图像模型表现优，肿瘤感知损失和分割掩码有改善，读者研究证实合成图像高真实性。


<details>
  <summary>Details</summary>
Motivation: DCE - MRI依赖对比剂存在安全隐患、禁忌、成本高和流程复杂等问题，需要寻找替代方法。

Method: 提出预对比条件去噪扩散概率模型，引入肿瘤感知损失函数和明确的肿瘤分割掩码条件，对比22种生成模型变体。

Result: 减影图像模型在五项评估指标上始终优于后对比模型；肿瘤感知损失和分割掩码输入改善评估指标；读者研究证实合成图像高真实性。

Conclusion: 生成对比增强技术具有临床应用潜力，并公开代码。

Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis
and treatment. However, its reliance on contrast agents introduces safety
concerns, contraindications, increased cost, and workflow complexity. To this
end, we present pre-contrast conditioned denoising diffusion probabilistic
models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of
22 generative model variants in both single-breast and full breast settings.
Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions
and explicit tumor segmentation mask conditioning. Using a public multicenter
dataset and comparing to respective pre-contrast baselines, we observe that
subtraction image-based models consistently outperform post-contrast-based
models across five complementary evaluation metrics. Apart from assessing the
entire image, we also separately evaluate the region of interest, where both
tumor-aware losses and segmentation mask inputs improve evaluation metrics. The
latter notably enhance qualitative results capturing contrast uptake, albeit
assuming access to tumor localization inputs that are not guaranteed to be
available in screening settings. A reader study involving 2 radiologists and 4
MRI technologists confirms the high realism of the synthetic images, indicating
an emerging clinical potential of generative contrast-enhancement. We share our
codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.

</details>


### [183] [A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler](https://arxiv.org/abs/2508.13875)
*Wenxuan Zhang,Shuai Li,Xinyi Wang,Yu Sun,Hongyu Kang,Pui Yuk Chryste Wan,Yong-Ping Zheng,Sai-Kit Lam*

Main category: eess.IV

TL;DR: 本文提出AI驱动的实时CoW自动分割系统AAW - YOLO，用TCCD数据训练评估，性能良好，可减少对操作者经验依赖，未来将探索双边建模和大规模验证。


<details>
  <summary>Details</summary>
Motivation: 现有TCCD评估CoW依赖操作者专业知识，限制其广泛应用，且尚无利用TCCD进行AI驱动脑血管分割的研究。

Method: 提出Attention - Augmented Wavelet YOLO (AAW - YOLO) 网络，收集738个标注帧和3419个标记动脉实例的TCCD数据进行模型训练和评估。

Result: AAW - YOLO在分割同侧和对侧CoW血管时表现良好，平均Dice分数0.901，IoU 0.823，精度0.882，召回率0.926，mAP 0.953，每帧推理速度14.199 ms。

Conclusion: 该系统可减少TCCD脑血管筛查对操作者经验的依赖，可用于日常临床工作和资源有限环境，未来将探索双边建模和大规模验证。

Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the
brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is
important for identifying individuals at risk and guiding appropriate clinical
management. Among existing imaging methods, Transcranial Color-coded Doppler
(TCCD) offers unique advantages due to its radiation-free nature,
affordability, and accessibility. However, reliable TCCD assessments depend
heavily on operator expertise for identifying anatomical landmarks and
performing accurate angle correction, which limits its widespread adoption. To
address this challenge, we propose an AI-powered, real-time CoW
auto-segmentation system capable of efficiently capturing cerebral arteries. No
prior studies have explored AI-driven cerebrovascular segmentation using TCCD.
In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)
network tailored for TCCD data, designed to provide real-time guidance for
brain vessel segmentation in the CoW. We prospectively collected TCCD data
comprising 738 annotated frames and 3,419 labeled artery instances to establish
a high-quality dataset for model training and evaluation. The proposed AAW-YOLO
demonstrated strong performance in segmenting both ipsilateral and
contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of
0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame
inference speed of 14.199 ms. This system offers a practical solution to reduce
reliance on operator experience in TCCD-based cerebrovascular screening, with
potential applications in routine clinical workflows and resource-constrained
settings. Future research will explore bilateral modeling and larger-scale
validation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [184] [Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures](https://arxiv.org/abs/2508.13159)
*Ruibai Tang,Wenlai Zhao*

Main category: cs.AR

TL;DR: 论文针对RC长链结构提出三种缩减方法，可提升电路仿真性能，误差小。


<details>
  <summary>Details</summary>
Motivation: 晶体管级仿真计算成本高，需有效缩减方法。

Method: 提出三种针对不同时间常数规模的RC长链结构的缩减方法。

Result: 模拟含多种功能模块的基准电路时，平均性能提升8.8%（最高22%），相对误差仅0.7%。

Conclusion: 所提方法能在较小误差下有效提升电路仿真性能。

Abstract: Transistor-level simulation plays a vital role in validating the physical
correctness of integrated circuits. However, such simulations are
computationally expensive. This paper proposes three novel reduction methods
specifically tailored to RC long-chain structures with different scales of time
constant. Such structures account for an average of 6.34\% (up to 12\%) of the
total nodes in the benchmark circuits. Experimental results demonstrate that
our methods yields an average performance improvement of 8.8\% (up to 22\%) on
simulating benchmark circuits which include a variety of functional modules
such as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,
with only 0.7\% relative error.

</details>


### [185] [Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System](https://arxiv.org/abs/2508.13231)
*Yunhua Fang,Rui Xie,Asad Ul Haq,Linsen Ma,Kaoutar El Maghraoui,Naigang Wang,Meng Wang,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 研究大语言模型推理中动态KV缓存跨异构内存系统的放置问题，得出理论上限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受内存带宽限制，KV缓存访问频繁，现有注意力稀疏技术无法根本解决问题，而现代AI硬件的异构内存系统提供了解决契机。

Method: 对动态KV缓存放置问题进行数学建模，推导理论上限。

Result: 得出了动态KV缓存放置问题在容量约束下实现聚合带宽利用率的理论上限。

Conclusion: 这是首次对大语言模型推理中异构内存系统的动态KV缓存调度进行正式研究，揭示了运行时优化的巨大空间。

Abstract: Large Language Model (LLM) inference is increasingly constrained by memory
bandwidth, with frequent access to the key-value (KV) cache dominating data
movement. While attention sparsity reduces some memory traffic, the relevance
of past tokens varies over time, requiring the full KV cache to remain
accessible and sustaining pressure on both bandwidth and capacity. With
advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now
integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making
heterogeneous memory systems a practical solution. This work investigates
dynamic KV cache placement across such systems to maximize aggregated bandwidth
utilization under capacity constraints. Rather than proposing a specific
scheduling policy, we formulate the placement problem mathematically and derive
a theoretical upper bound, revealing substantial headroom for runtime
optimization. To our knowledge, this is the first formal treatment of dynamic
KV cache scheduling in heterogeneous memory systems for LLM inference.

</details>


### [186] [Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures](https://arxiv.org/abs/2508.13163)
*Yashasvi Makin,Rahul Maliakkal*

Main category: cs.AR

TL;DR: 大规模深度学习和AI模型训练能耗大，本文探索面向GPU架构的环保性能优化方法，强调软硬件协同设计提升能效，指出研究差距与方向，并结合真实案例分析。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习和AI模型训练能耗大，带来可持续性问题，需提高计算效率、降低环境影响。

Method: 探索面向NVIDIA、AMD等GPU架构的软硬件协同设计技术，评估张量和矩阵核心、内存优化方法等，讨论软件层面优化。

Result: 软硬件协同设计可显著提高训练效率，降低AI环境影响，通过真实案例展示方法在实际中的应用。

Conclusion: 软硬件协同设计能在不影响性能的前提下，大幅提高训练效率，降低AI环境影响。

Abstract: In particular, large-scale deep learning and artificial intelligence model
training uses a lot of computational power and energy, so it poses serious
sustainability issues. The fast rise in model complexity has resulted in
exponential increases in energy consumption, increasing the demand for
techniques maximizing computational efficiency and lowering environmental
impact. This work explores environmentally driven performance optimization
methods especially intended for advanced GPU architectures from NVIDIA, AMD,
and other emerging GPU architectures. Our main focus is on investigating
hardware-software co-design techniques meant to significantly increase
memory-level and kernel-level operations, so improving performance-per-watt
measures. Our thorough research encompasses evaluations of specialized tensor
and matrix cores, advanced memory optimization methods, and creative
integration approaches that taken together result in notable energy efficiency
increases. We also discuss important software-level optimizations that augment
hardware capability including mixed-precision arithmetic, advanced energy-aware
scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we
methodically point out important research gaps and suggest future directions
necessary to create really sustainable artificial intelligence systems. This
paper emphasizes how major increases in training efficiency can be obtained by
co-design of hardware and software, so lowering the environmental impact of
artificial intelligence without compromising performance. To back up our
analysis, we use real-world case studies from top companies like Meta, Google,
Amazon, and others that show how these sustainable AI training methods are used
in the real world.

</details>


### [187] [EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code](https://arxiv.org/abs/2508.13156)
*Ping Guo,Yiting Wang,Wanghao Ye,Yexiao He,Ziyao Wang,Xiaopeng Dai,Ang Li,Qingfu Zhang*

Main category: cs.AR

TL;DR: 本文提出EvoVerilog框架结合大语言模型推理能力与进化算法自动生成和优化Verilog代码，实验显示其性能达最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有Verilog代码自动化生成方法依赖人工干预和微调，可扩展性受限，迭代搜索技术探索设计方案能力不足。

Method: 引入EvoVerilog框架，结合大语言模型推理能力与进化算法，采用多目标、基于种群的搜索策略。

Result: EvoVerilog在VerilogEval - Machine和VerilogEval - Human基准测试中pass@10分数分别达89.1和80.2，能探索多样设计并优化资源利用。

Conclusion: EvoVerilog框架有效解决现有方法的局限性，实现了Verilog代码自动化生成的高性能和多样化设计探索。

Abstract: Large Language Models (LLMs) have demonstrated great potential in automating
the generation of Verilog hardware description language code for hardware
design. This automation is critical to reducing human effort in the complex and
error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and
fine-tuning using curated datasets, limiting their scalability in automated
design workflows.
  Although recent iterative search techniques have emerged, they often fail to
explore diverse design solutions and may underperform simpler approaches such
as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that
combines the reasoning capabilities of LLMs with evolutionary algorithms to
automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to
explore a wide range of design possibilities without requiring human
intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art
performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine
and VerilogEval-Human benchmarks, respectively. Furthermore, the framework
showcases its ability to explore diverse designs by simultaneously generating a
variety of functional Verilog code while optimizing resource utilization.

</details>


### [188] [Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists](https://arxiv.org/abs/2508.13157)
*Haohang Xu,Chengjie Liu,Qihang Wang,Wenhao Huang,Yongjian Xu,Weiyu Chen,Anlan Peng,Zhijun Li,Bo Li,Lei Qi,Jun Yang,Yuan Du,Li Du*

Main category: cs.AR

TL;DR: 本文构建开源新数据集，提出 Image2Net 框架将电路图转换为网表，评估显示性能优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的模拟集成电路设计依赖文本描述，但现有电路多为图像形式，且之前转换框架有局限性，有效转换复杂电路图为网表仍是挑战。

Method: 构建并开源含丰富风格电路图及简单和复杂模拟集成电路均衡分布的新数据集；提出 Image2Net 混合框架进行转换；引入网表编辑距离（NED）评估转换结果。

Result: 基于基准测试，Image2Net 成功率达 80.77%，比先前工作高 34.62%-45.19%；平均 NED 为 0.116，比现有技术低 62.1%-69.6%。

Conclusion: 提出的数据集和 Image2Net 框架在将电路图转换为网表方面效果显著，性能优于先前工作。

Abstract: Large Language Model (LLM) exhibits great potential in designing of analog
integrated circuits (IC) because of its excellence in abstraction and
generalization for knowledge. However, further development of LLM-based analog
ICs heavily relies on textual description of analog ICs, while existing analog
ICs are mostly illustrated in image-based circuit diagrams rather than
text-based netlists. Converting circuit diagrams to netlists help LLMs to
enrich the knowledge of analog IC. Nevertheless, previously proposed conversion
frameworks face challenges in further application because of limited support of
image styles and circuit elements. Up to now, it still remains a challenging
task to effectively convert complex circuit diagrams into netlists. To this
end, this paper constructs and opensources a new dataset with rich styles of
circuit diagrams as well as balanced distribution of simple and complex analog
ICs. And a hybrid framework, named Image2Net, is proposed for practical
conversion from circuit diagrams to netlists. The netlist edit distance (NED)
is also introduced to precisely assess the difference between the converted
netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\%
successful rate, which is 34.62\%-45.19\% higher than previous works.
Specifically, the proposed work shows 0.116 averaged NED, which is
62.1\%-69.6\% lower than state-of-the-arts.

</details>


### [189] [Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner](https://arxiv.org/abs/2508.13161)
*Zhexuan Xu,Kexin Zhou,Jie Wang,Zijie Geng,Siyuan Xu,Shixiong Kai,Mingxuan Yuan,Feng Wu*

Main category: cs.AR

TL;DR: 提出Piano框架同时优化模块布局和引脚分配，实验显示在多项指标上有显著提升且无空白区域。


<details>
  <summary>Details</summary>
Motivation: 传统布局规划器在规划阶段常忽略现代约束下的引脚分配，而引脚分配对后续阶段性能影响大。

Method: 构建基于模块几何关系和网络连接的图，迭代搜索最短路径确定引脚分配；采用空白区域移除策略，使用三个局部优化器。

Result: 在常用基准电路实验中，Piano使HPWL平均降低6.81%，馈通线长减少13.39%，馈通模块数量减少16.36%，未放置引脚数量下降21.21%，且无空白区域。

Conclusion: Piano框架在多约束下能有效同时优化模块布局和引脚分配，提升布局质量。

Abstract: Floorplanning is a critical step in VLSI physical design, increasingly
complicated by modern constraints such as fixed-outline requirements,
whitespace removal, and the presence of pre-placed modules. In addition, the
assignment of pins on module boundaries significantly impacts the performance
of subsequent stages, including detailed placement and routing. However,
traditional floorplanners often overlook pin assignment with modern constraints
during the floorplanning stage. In this work, we introduce Piano, a
floorplanning framework that simultaneously optimizes module placement and pin
assignment under multiple constraints. Specifically, we construct a graph based
on the geometric relationships among modules and their netlist connections,
then iteratively search for shortest paths to determine pin assignments. This
graph-based method also enables accurate evaluation of feedthrough and unplaced
pins, thereby guiding overall layout quality. To further improve the design, we
adopt a whitespace removal strategy and employ three local optimizers to
enhance layout metrics under multi-constraint scenarios. Experimental results
on widely used benchmark circuits demonstrate that Piano achieves an average
6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%
reduction in the number of feedthrough modules, and a 21.21% drop in unplaced
pins, while maintaining zero whitespace.

</details>


### [190] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 提出结合大语言模型战略推理与gm/Id方法物理精度的“协同推理”框架用于模拟IC设计，经验证可高效实现设计目标，效率远超高级工程师。


<details>
  <summary>Details</summary>
Motivation: 传统模拟IC设计依赖经验、模拟效率低，直接用大语言模型缺乏工程原理支撑。

Method: 提出“协同推理”框架，让大语言模型结合gm/Id查找表进行设计。

Result: 在两级运放设计中，框架使Gemini模型5次迭代满足TT角规格并扩展到PVT角，消融研究证明gm/Id数据关键，效率比高级工程师提高一个数量级。

Conclusion: 结合大语言模型推理与科学电路设计方法可实现真正的模拟设计自动化。

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


### [191] [ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models](https://arxiv.org/abs/2508.13257)
*Wenhao Lv,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.AR

TL;DR: 论文提出ViTAD方法用于RTL阶段时序违规自动化修复，构建数据集评估，成功率达73.68%，较仅用LLM的基线提升19.30%。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI电路设计中RTL阶段时序优化关键，但传统时序优化依赖人工，需自动化方法。

Method: 先解析Verilog代码和时序报告构建STDG，基于其分析违规路径，用LLM推断根因，从知识库检索知识生成修复方案。

Result: 构建含54个违规案例的数据集，方法修复时序违规成功率73.68%，较仅用LLM的基线提升19.30%。

Conclusion: ViTAD方法能有效分析时序违规根因并生成修复策略，提高修复成功率。

Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the
Register-Transfer Level (RTL) stage presents a critical opportunity for timing
optimization. Addressing timing violations at this early stage is essential, as
modern systems demand higher speeds, where even minor timing violations can
lead to functional failures or system crashes. However, traditional timing
optimization heavily relies on manual expertise, requiring engineers to
iteratively analyze timing reports and debug. To automate this process, this
paper proposes ViTAD, a method that efficiently analyzes the root causes of
timing violations and dynamically generates targeted repair strategies.
Specifically, we first parse Verilog code and timing reports to construct a
Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation
path analysis and use large language models (LLMs) to infer the root causes of
violations. Finally, by analyzing the causes of violations, we selectively
retrieve relevant debugging knowledge from a domain-specific knowledge base to
generate customized repair solutions. To evaluate the effectiveness of our
method, we construct a timing violation dataset based on real-world open-source
projects. This dataset contains 54 cases of violations. Experimental results
show that our method achieves a 73.68% success rate in repairing timing
violations, while the baseline using only LLM is 54.38%. Our method improves
the success rate by 19.30%.

</details>


### [192] [FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design](https://arxiv.org/abs/2508.13162)
*Mahmoud Nazzal,Khoa Nguyen,Deepak Vungarala,Ramtin Zand,Shaahin Angizi,Hai Phan,Abdallah Khreishah*

Main category: cs.AR

TL;DR: 文章引入FedChip的联邦微调方法用于自动化硬件设计生成，创建APTPU - Gen数据集，提出新评估指标Chip@k，实验表明该方法能提升设计质量并保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 当前自动化工具LLMs在AI硬件设计中因数据隐私和缺乏特定领域训练，潜力受限，需要解决这些问题。

Method: 引入FedChip的联邦微调方法，让多方在保护数据隐私的情况下协作改进共享LLM；创建APTPU - Gen数据集；提出新的设计评估指标Chip@k。

Result: 实验结果显示，FedChip比高端LLMs在设计质量上提升超77%，并能维护数据隐私。

Conclusion: FedChip方法在提升自动化硬件设计质量的同时能有效保护数据隐私，具有应用价值。

Abstract: AI hardware design is advancing rapidly, driven by the promise of design
automation to make chip development faster, more efficient, and more accessible
to a wide range of users. Amongst automation tools, Large Language Models
(LLMs) offer a promising solution by automating and streamlining parts of the
design process. However, their potential is hindered by data privacy concerns
and the lack of domain-specific training. To address this, we introduce
FedChip, a Federated fine-tuning approach that enables multiple Chip design
parties to collaboratively enhance a shared LLM dedicated for automated
hardware design generation while protecting proprietary data. FedChip enables
parties to train the model on proprietary local data and improve the shared
LLM's performance. To exemplify FedChip's deployment, we create and release
APTPU-Gen, a dataset of 30k design variations spanning various performance
metric values such as power, performance, and area (PPA). To encourage the LLM
to generate designs that achieve a balance across multiple quality metrics, we
propose a new design evaluation metric, Chip@k, which statistically evaluates
the quality of generated designs against predefined acceptance criteria.
Experimental results show that FedChip improves design quality by more than 77%
over high-end LLMs while maintaining data privacy

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [193] [Benchmarking LLM-based Agents for Single-cell Omics Analysis](https://arxiv.org/abs/2508.13201)
*Yang Liu,Lu Zhou,Ruikun He,Rongbo Shen,Yixue Li*

Main category: q-bio.GN

TL;DR: 引入新基准评估系统评估单细胞组学分析中AI智能体能力，揭示Grok - 3 - beta表现最佳等结果并指出开发挑战。


<details>
  <summary>Details</summary>
Motivation: 传统单细胞组学分析流程有局限，AI智能体有优势但缺乏综合基准，阻碍其进展。

Method: 引入新基准评估系统，含统一平台、多维指标和50个多样真实任务。

Result: Grok - 3 - beta表现最佳；多智能体框架协作和执行效率更高；高质量代码生成对任务成功关键，自我反思影响最大。

Conclusion: 指出代码生成、长上下文处理和上下文感知知识检索仍有挑战，为计算生物学开发强大AI智能体提供实证基础和最佳实践。

Abstract: The surge in multimodal single-cell omics data exposes limitations in
traditional, manually defined analysis workflows. AI agents offer a paradigm
shift, enabling adaptive planning, executable code generation, traceable
decisions, and real-time knowledge fusion. However, the lack of a comprehensive
benchmark critically hinders progress. We introduce a novel benchmarking
evaluation system to rigorously assess agent capabilities in single-cell omics
analysis. This system comprises: a unified platform compatible with diverse
agent frameworks and LLMs; multidimensional metrics assessing cognitive program
synthesis, collaboration, execution efficiency, bioinformatics knowledge
integration, and task completion quality; and 50 diverse real-world single-cell
omics analysis tasks spanning multi-omics, species, and sequencing
technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art
performance among tested agent frameworks. Multi-agent frameworks significantly
enhance collaboration and execution efficiency over single-agent approaches
through specialized role division. Attribution analyses of agent capabilities
identify that high-quality code generation is crucial for task success, and
self-reflection has the most significant overall impact, followed by
retrieval-augmented generation (RAG) and planning. This work highlights
persistent challenges in code generation, long-context handling, and
context-aware knowledge retrieval, providing a critical empirical foundation
and best practices for developing robust AI agents in computational biology.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [194] [Towards safe control parameter tuning in distributed multi-agent systems](https://arxiv.org/abs/2508.13608)
*Abdullah Tokmak,Thomas B. Schön,Dominik Baumann*

Main category: eess.SY

TL;DR: 针对分布式多智能体安全关键问题，提出用安全贝叶斯优化结合自定义时空核解决优化问题，并在模拟中成功应用。


<details>
  <summary>Details</summary>
Motivation: 现有工作在分布式多智能体安全优化问题上存在研究集中式场景、不考虑安全或样本效率低的问题，需要解决未知非凸奖励和约束下的样本效率问题。

Method: 使用带高斯过程回归的安全贝叶斯优化解决优化问题，考虑智能体间近邻通信，将全局优化问题转化为局部时变优化问题，提出自定义时空核整合先验知识。

Result: 算法在模拟中成功部署。

Conclusion: 提出的方法能有效解决分布式多智能体安全优化问题。

Abstract: Many safety-critical real-world problems, such as autonomous driving and
collaborative robots, are of a distributed multi-agent nature. To optimize the
performance of these systems while ensuring safety, we can cast them as
distributed optimization problems, where each agent aims to optimize their
parameters to maximize a coupled reward function subject to coupled
constraints. Prior work either studies a centralized setting, does not consider
safety, or struggles with sample efficiency. Since we require sample efficiency
and work with unknown and nonconvex rewards and constraints, we solve this
optimization problem using safe Bayesian optimization with Gaussian process
regression. Moreover, we consider nearest-neighbor communication between the
agents. To capture the behavior of non-neighboring agents, we reformulate the
static global optimization problem as a time-varying local optimization problem
for each agent, essentially introducing time as a latent variable. To this end,
we propose a custom spatio-temporal kernel to integrate prior knowledge. We
show the successful deployment of our algorithm in simulations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [195] [When Does Selfishness Align with Team Goals? A Structural Analysis of Equilibrium and Optimality](https://arxiv.org/abs/2508.13450)
*Gehui Xu,Thomas Parisini,Andreas A. Malikopoulos*

Main category: math.OC

TL;DR: 本文研究团队最优解与纳什均衡关系，评估自利决策对团队表现的影响，开发参数化模型，给出一致性条件、上界，提出算法并验证理论。


<details>
  <summary>Details</summary>
Motivation: 评估自利决策对团队表现的影响，研究团队最优解与纳什均衡的不一致性。

Method: 开发参数化模型，探索两者一致性与偏差，提出基于超梯度的算法解决双层优化问题。

Result: 给出任意纳什均衡为团队最优的充要条件，建立差异上界，算法收敛到临界点。

Conclusion: 通过大量模拟研究验证了理论结果。

Abstract: This paper investigates the relationship between the team-optimal solution
and the Nash equilibrium (NE) to assess the impact of self-interested decisions
on team performance. In classical team decision problems, team members
typically act cooperatively towards a common objective to achieve a
team-optimal solution. However, in practice, members may behave selfishly by
prioritizing their goals, resulting in an NE under a non-cooperative game. To
study this misalignment, we develop a parameterized model for team and game
problems, where game parameters represent each individual's deviation from the
team objective. The study begins by exploring the consistency and deviation
between the NE and the team-optimal solution under fixed game parameters. We
provide a necessary and sufficient condition for any NE to be a team optimum,
along with establishing an upper bound to measure their difference when the
consistency condition fails. The exploration then focuses on aligning NE
strategies towards the team-optimal solution through the adjustment of game
parameters, resulting in a non-convex and non-smooth bi-level optimization
problem. We propose a hypergradient-based algorithm for this problem, and
establish its convergence to the critical points. Finally, we validate our
theoretical findings through extensive simulation studies.

</details>


### [196] [Online Stochastic Packing with General Correlations](https://arxiv.org/abs/2508.13458)
*Sabri Cetin,Yilun Chen,David A. Goldberg*

Main category: math.OC

TL;DR: 研究具有一般相关性结构的在线随机打包问题，给出近似最优策略，推导相关问题结果并采用新方式实现随机梯度法。


<details>
  <summary>Details</summary>
Motivation: 现代应用中复杂数据集和模型促使人们研究具有更一般相关性结构的在线随机打包问题，而过往研究存在诸多局限。

Method: 通过对相关大规模确定性等价线性规划采用新颖的即时/递归方式实现随机梯度法，还使用了舍入方法。

Result: 对于所有ε，具有一般相关性的在线随机打包线性规划问题有近似最优策略，其每决策运行时间有特定规模，还为网络收益管理、在线二分匹配和有界度图中的独立集问题推导了类似结果。

Conclusion: 提出的方法能有效解决具有一般相关性结构的在线随机打包等相关问题，且策略运行时间有较好表现。

Abstract: There has been a growing interest in studying online stochastic packing under
more general correlation structures, motivated by the complex data sets and
models driving modern applications. Several past works either assume
correlations are weak or have a particular structure, have a complexity scaling
with the number of Markovian "states of the world" (which may be exponentially
large e.g. in the case of full history dependence), scale poorly with the
horizon $T$, or make additional continuity assumptions. Surprisingly, we show
that for all $\epsilon$, the online stochastic packing linear programming
problem with general correlations (suitably normalized and with sparse columns)
has an approximately optimal policy (with optimality gap $\epsilon T$) whose
per-decision runtime scales as the time to simulate a single sample path of the
underlying stochastic process (assuming access to a Monte Carlo simulator),
multiplied by a constant independent of the horizon or number of Markovian
states. We derive analogous results for network revenue management, and online
bipartite matching and independent set in bounded-degree graphs, by rounding.
Our algorithms implement stochastic gradient methods in a novel
on-the-fly/recursive manner for the associated massive deterministic-equivalent
linear program on the corresponding probability space.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [197] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: 本文对四种公开的漏洞评分系统进行大规模、与结果相关的实证比较，发现评分系统对相同漏洞排名有显著差异，强调需更透明一致的评估。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞评分系统目标、方法和输出不同，导致优先级决策不一致，需对评分系统进行比较评估。

Method: 使用来自微软四个月补丁周二披露的600个真实漏洞数据集，研究评分间关系、评估对漏洞管理任务的支持、漏洞分类及捕获实际利用风险的能力。

Result: 不同评分系统对相同漏洞排名存在显著差异。

Conclusion: 需要更透明、一致的可利用性、风险和严重性评估。

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [198] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: 本文对超200篇关于联邦学习攻击与防御机制的论文进行全面综述，分析现有方法优缺点、权衡因素及非IID数据影响，指出研究挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽增强数据隐私，但仍面临安全与隐私威胁，需全面了解攻击与防御机制。

Method: 对超200篇论文进行综述，将防御机制分为安全增强和隐私保护技术。

Result: 分析了现有方法的优缺点，强调了隐私、安全和模型性能间的权衡，讨论了非IID数据分布对防御效果的影响。

Conclusion: 确定了开放研究挑战和未来方向，旨在指导开发健壮且保护隐私的联邦学习系统。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [199] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: 研究发现大语言模型在隐藏提示注入攻击下易受影响，凸显其作为评判工具的稳健性风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为评判工具潜力大，但在提示注入攻击下的稳健性存疑，需测试其是否易被误导。

Method: 在PDF文件中对基本算术的选择题或判断题注入隐藏提示，评估大语言模型。

Result: 大语言模型在这些简单场景下也易受隐藏提示注入攻击。

Conclusion: 大语言模型作为评判工具存在严重的稳健性风险。

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [200] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: 论文提出MCP安全的系统分类法，引入MCPSecBench评估MCP安全，实验显示多数攻击能攻破至少一个平台，MCPSecBench可标准化评估。


<details>
  <summary>Details</summary>
Motivation: MCP在提升基于大语言模型的智能体能力的同时引入新安全风险和扩大攻击面，需要系统评估。

Method: 提出MCP安全的系统分类法，识别4个主要攻击面的17种攻击类型；引入MCPSecBench，集成提示数据集、MCP服务器、客户端和攻击脚本，对三个主要MCP提供商进行评估。

Result: 超85%的攻击能成功攻破至少一个平台，核心漏洞普遍影响Claude、OpenAI和Cursor，基于提示和以工具为中心的攻击在不同主机和模型上表现差异大。

Conclusion: MCPSecBench可标准化评估MCP安全，能对所有MCP层进行严格测试。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [201] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: 本文提出利用大语言模型从黑客行为中提取对损失厌恶认知偏差的量化见解，以增强网络防御策略。


<details>
  <summary>Details</summary>
Motivation: 传统网络防御策略难以动态解读进行中的攻击，IARPA的ReSCIND项目旨在推断、防御和利用攻击者的认知特征，因此需要新方法。

Method: 从招募黑客攻击受控演示网络的实验中收集数据，用大语言模型处理黑客生成的笔记，将行动与预设的持久机制关联，并与操作触发因素相关联。

Result: 大语言模型能有效剖析和解读细微的行为模式。

Conclusion: 通过基于实时行为的分析，为增强网络防御策略提供了变革性方法。

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [202] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究揭示大语言模型存在新的‘非自愿越狱’漏洞，用单一通用提示可突破多数主流模型防护栏，望推动重新评估防护栏鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 发现大语言模型存在与现有越狱攻击不同的新漏洞，该漏洞可能危及整个防护栏结构，需引起关注。

Method: 使用单一通用提示，让大语言模型生成通常会被拒绝的问题及对应详细回答。

Result: 该简单提示策略能持续突破包括Claude Opus 4.1、Grok 4等多数主流大语言模型的防护栏。

Conclusion: 希望此问题能促使研究人员和从业者重新评估大语言模型防护栏的鲁棒性，为未来更强的安全对齐做出贡献。

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [203] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: 现有ITS解决方案依赖特定数据库应对威胁有局限，此前提出的HAL 9000有改进但仍有不足，本文引入自定义抓取器扩展其情报库，提升应对新兴威胁能力。


<details>
  <summary>Details</summary>
Motivation: 现有ITS解决方案依赖NVD和ExploitDB需手动分析新漏洞，响应威胁能力受限；HAL 9000依赖这两个数据库也有局限，需扩展信息源。

Method: 引入自定义抓取器，不断挖掘安全公告、研究论坛等多种威胁源信息，并将其集成到HAL 9000架构中。

Result: 评估表明，将抓取器获取的情报与HAL 9000的风险管理框架集成，显著提高了其应对新兴威胁的能力。

Conclusion: 抓取器集成到架构中能提供新威胁的额外信息，对HAL 9000的管理有积极影响。

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [204] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: 本文提出基于低频多通道PPG信号的智能手表连续认证系统，评估显示性能良好且能降低功耗，确定25Hz为实用下限，多样训练可提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于PPG的生物特征认证方法多依赖高频信号和复杂模型，有能量和计算开销问题，难以用于功耗受限的现实系统。

Method: 使用带注意力机制的Bi - LSTM从4通道PPG短窗口（4s）中提取身份特征。

Result: 在公共数据集和自有数据集上平均测试准确率88.11%，宏F1分数0.88，FAR 0.48%，FRR 11.77%，EER 2.76%；25Hz系统比512Hz降低53%传感器功耗，比128Hz降低19%。

Conclusion: 25Hz为实用下限，多样训练可提升模型在不同生理状态下的鲁棒性。

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [205] [The Multi-Stage Assignment Problem: A Fairness Perspective](https://arxiv.org/abs/2508.13856)
*Vibulan J,Swapnil Dhamal,Shweta Jain*

Main category: cs.MA

TL;DR: 本文探讨多阶段图公平分配问题，指出最小成本分配可能不公平，提出C - Balance和DC - Balance算法，分析其性能并实验验证速度优势。


<details>
  <summary>Details</summary>
Motivation: 解决多阶段图中节点不相交路径分配给多个代理时，最小成本分配可能导致的不公平问题。

Method: 提出C - Balance算法解决两个代理的情况，再提出DC - Balance算法扩展到n个代理，迭代调用C - Balance。

Result: C - Balance算法保证两个代理的嫉妒值有界，成本公平性比有界；DC - Balance算法收敛且嫉妒值接近2M，得出其成本公平性界；实验表明算法比ILP快几个数量级。

Conclusion: 所提算法能有效解决多阶段图公平分配问题，在公平性和计算速度上有优势。

Abstract: This paper explores the problem of fair assignment on Multi-Stage graphs. A
multi-stage graph consists of nodes partitioned into $K$ disjoint sets (stages)
structured as a sequence of weighted bipartite graphs formed across adjacent
stages. The goal is to assign node-disjoint paths to $n$ agents starting from
the first stage and ending in the last stage. We show that an efficient
assignment that minimizes the overall sum of costs of all the agents' paths may
be highly unfair and lead to significant cost disparities (envy) among the
agents. We further show that finding an envy-minimizing assignment on a
multi-stage graph is NP-hard. We propose the C-Balance algorithm, which
guarantees envy that is bounded by $2M$ in the case of two agents, where $M$ is
the maximum edge weight. We demonstrate the algorithm's tightness by presenting
an instance where the envy is $2M$. We further show that the cost of fairness
($CoF$), defined as the ratio of the cost of the assignment given by the fair
algorithm to that of the minimum cost assignment, is bounded by $2$ for
C-Balance. We then extend this approach to $n$ agents by proposing the
DC-Balance algorithm that makes iterative calls to C-Balance. We show the
convergence of DC-Balance, resulting in envy that is arbitrarily close to $2M$.
We derive $CoF$ bounds for DC-Balance and provide insights about its dependency
on the instance-specific parameters and the desired degree of envy. We
experimentally show that our algorithm runs several orders of magnitude faster
than a suitably formulated ILP.

</details>


### [206] [Goal-Directedness is in the Eye of the Beholder](https://arxiv.org/abs/2508.13247)
*Nina Rajcic,Anders Søgaard*

Main category: cs.MA

TL;DR: 探讨预测复杂主体行为中目标导向行为的探测方法，指出目标导向性无法客观衡量，并给出新建模方向。


<details>
  <summary>Details</summary>
Motivation: 研究预测复杂主体行为时目标归因问题，分析两种探测目标导向行为方法的可行性。

Method: 剖析行为和机制两种探测目标导向行为方法背后的假设，识别形式化目标时出现的技术和概念问题。

Result: 得出目标导向性无法客观衡量的结论。

Conclusion: 提出将目标导向性建模为动态多主体系统涌现属性的新方向。

Abstract: Our ability to predict the behavior of complex agents turns on the
attribution of goals. Probing for goal-directed behavior comes in two flavors:
Behavioral and mechanistic. The former proposes that goal-directedness can be
estimated through behavioral observation, whereas the latter attempts to probe
for goals in internal model states. We work through the assumptions behind both
approaches, identifying technical and conceptual problems that arise from
formalizing goals in agent systems. We arrive at the perhaps surprising
position that goal-directedness cannot be measured objectively. We outline new
directions for modeling goal-directedness as an emergent property of dynamic,
multi-agent systems.

</details>


### [207] [BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web](https://arxiv.org/abs/2508.13787)
*Zihan Guo,Yuanjian Zhou,Chenyi Wang,Linlin You,Minjie Bian,Weinan Zhang*

Main category: cs.MA

TL;DR: 本文介绍了区块链赋能的可信代理网络BetaWeb，以解决当前代理生态系统的问题，并提出五阶段进化路线图，认为区块链与LaMAS深度融合可构建数字生态。


<details>
  <summary>Details</summary>
Motivation: 当前代理生态系统碎片化、封闭，现有范式有局限，Agentic Web实施面临核心挑战，需要建立互联可扩展的范式。

Method: 引入区块链赋能的BetaWeb，进行系统框架研究、提出五阶段进化路线图、对比分析现有产品并多视角讨论挑战。

Result: BetaWeb为LaMAS提供可信可扩展基础设施，有望推动Web范式从Web3向Web3.5发展。

Conclusion: 区块链与LaMAS深度融合可奠定有弹性、可信和可持续激励的数字生态基础。

Abstract: The rapid development of large language models (LLMs) has significantly
propelled the development of artificial intelligence (AI) agents, which are
increasingly evolving into diverse autonomous entities, advancing the LLM-based
multi-agent systems (LaMAS). However, current agentic ecosystems remain
fragmented and closed. Establishing an interconnected and scalable paradigm for
Agentic AI has become a critical prerequisite. Although Agentic Web proposes an
open architecture to break the ecosystem barriers, its implementation still
faces core challenges such as privacy protection, data management, and value
measurement. Existing centralized or semi-centralized paradigms suffer from
inherent limitations, making them inadequate for supporting large-scale,
heterogeneous, and cross-domain autonomous interactions. To address these
challenges, this paper introduces the blockchain-enabled trustworthy Agentic
Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not
only offers a trustworthy and scalable infrastructure for LaMAS but also has
the potential to advance the Web paradigm from Web3 (centered on data
ownership) towards Web3.5, which emphasizes ownership of agent capabilities and
the monetization of intelligence. Beyond a systematic examination of the
BetaWeb framework, this paper presents a five-stage evolutionary roadmap,
outlining the path of LaMAS from passive execution to advanced collaboration
and autonomous governance. We also conduct a comparative analysis of existing
products and discuss key challenges of BetaWeb from multiple perspectives.
Ultimately, we argue that deep integration between blockchain and LaMAS can lay
the foundation for a resilient, trustworthy, and sustainably incentivized
digital ecosystem. A summary of the enabling technologies for each stage is
available at https://github.com/MatZaharia/BetaWeb.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [208] [Utilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment](https://arxiv.org/abs/2508.13207)
*S. Z. Pirasteh,Ali A. Kiaei,Mahnaz Bush,Sabra Moghadam,Raha Aghaei,Behnaz Sadeghigol*

Main category: q-bio.QM

TL;DR: 文章提出RAIN方法结合AI和网络荟萃分析为胃癌确定最佳药物组合，确定奥沙利铂、氟尿嘧啶和曲妥珠单抗联用有效。


<details>
  <summary>Details</summary>
Motivation: 胃癌死亡率高且常晚期诊断，有效药物组合对解决疾病异质性、提高疗效、减少耐药和改善患者预后至关重要。

Method: RAIN方法集成Graph SAGE提出药物组合，用p值加权边连接药物、基因和蛋白质的图模型，通过NLP和系统文献综述验证，再进行网络荟萃分析评估疗效，用Python实现。

Result: 确定奥沙利铂、氟尿嘧啶和曲妥珠单抗联用有效，有61项研究支持，氟尿嘧啶单药p值0.0229，与曲妥珠单抗联用改善至0.0099，三联组合为0.0069，疗效更优。

Conclusion: RAIN方法结合AI和网络荟萃分析能有效确定胃癌最佳药物组合，为改善治疗效果和指导卫生政策提供了有前景的策略。

Abstract: Background: Gastric neoplasm, primarily adenocarcinoma, is an aggressive
cancer with high mortality, often diagnosed late, leading to complications like
metastasis. Effective drug combinations are vital to address disease
heterogeneity, enhance efficacy, reduce resistance, and improve patient
outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug
combinations, using a graph model with p-value-weighted edges connecting drugs,
genes, and proteins. NLP and systematic literature review (PubMed, Scopus,
etc.) validated proposed drugs, followed by network meta-analysis to assess
efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and
trastuzumab were identified as effective, supported by 61 studies. Fluorouracil
alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069
for the triple combination, indicating superior efficacy. Conclusion: The RAIN
method, combining AI and network meta-analysis, effectively identifies optimal
drug combinations for gastric neoplasm, offering a promising strategy to
enhance treatment outcomes and guide health policy.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [209] [Diffusion-Driven High-Dimensional Variable Selection](https://arxiv.org/abs/2508.13890)
*Minjie Wang,Xiaotong Shen,Wei Pan*

Main category: stat.ME

TL;DR: 提出利用扩散模型生成合成数据的重采样聚合框架用于高维高相关数据变量选择，理论证明其一致性，模拟显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高维高相关数据变量选择常产生不稳定不可靠模型，需更好方法。

Method: 从拟合原始数据的扩散模型抽取多个伪数据集，用现成选择器选择变量并存储结果，聚合得到稳定预测子集及稳定性分数。

Result: 理论证明方法在温和假设下选择一致，模拟显示在强相关预测变量时优于lasso等基线方法，有更高真阳性率和更低假发现率。

Conclusion: 耦合基于扩散的数据增强和原则性聚合，推进变量选择方法，拓宽复杂科学应用中可解释、统计严谨分析的工具包。

Abstract: Variable selection for high-dimensional, highly correlated data has long been
a challenging problem, often yielding unstable and unreliable models. We
propose a resample-aggregate framework that exploits diffusion models' ability
to generate high-fidelity synthetic data. Specifically, we draw multiple
pseudo-data sets from a diffusion model fitted to the original data, apply any
off-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusion
indicators and coefficients. Aggregating across replicas produces a stable
subset of predictors with calibrated stability scores for variable selection.
Theoretically, we show that the proposed method is selection consistent under
mild assumptions. Because the generative model imports knowledge from large
pre-trained weights, the procedure naturally benefits from transfer learning,
boosting power when the observed sample is small or noisy. We also extend the
framework of aggregating synthetic data to other model selection problems,
including graphical model selection, and statistical inference that supports
valid confidence intervals and hypothesis tests. Extensive simulations show
consistent gains over the lasso, stability selection, and knockoff baselines,
especially when predictors are strongly correlated, achieving higher
true-positive rates and lower false-discovery proportions. By coupling
diffusion-based data augmentation with principled aggregation, our method
advances variable selection methodology and broadens the toolkit for
interpretable, statistically rigorous analysis in complex scientific
applications.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [210] [Modeling GRNs with a Probabilistic Categorical Framework](https://arxiv.org/abs/2508.13208)
*Yiyang Jia,Zheng Wei,Zheng Yang,Guohong Peng*

Main category: q-bio.MN

TL;DR: 论文介绍了Probabilistic Categorical GRN (PC - GRN)框架，它结合三种核心方法，有端到端生成贝叶斯推理引擎，能为基因调控网络提供严格、可解释且考虑不确定性的表示。


<details>
  <summary>Details</summary>
Motivation: 现有基因调控网络建模范式难以有效捕捉复杂调控逻辑和管理网络结构与动力学参数的双重不确定性。

Method: 结合范畴论、贝叶斯类型Petri网，利用GFlowNet学习采样网络拓扑的策略，HyperNetwork进行摊销推理预测参数分布，构建端到端生成贝叶斯推理引擎。

Result: 构建了PC - GRN框架，实现从数据中直接学习BTPN模型的完整后验分布。

Conclusion: PC - GRN框架为基因调控网络提供了数学上严格、生物学上可解释且考虑不确定性的表示，推动了预测建模和系统级分析。

Abstract: Understanding the complex and stochastic nature of Gene Regulatory Networks
(GRNs) remains a central challenge in systems biology. Existing modeling
paradigms often struggle to effectively capture the intricate, multi-factor
regulatory logic and to rigorously manage the dual uncertainties of network
structure and kinetic parameters. In response, this work introduces the
Probabilistic Categorical GRN(PC-GRN) framework. It is a novel theoretical
approach founded on the synergistic integration of three core methodologies.
Firstly, category theory provides a formal language for the modularity and
composition of regulatory pathways. Secondly, Bayesian Typed Petri Nets (BTPNs)
serve as an interpretable,mechanistic substrate for modeling stochastic
cellular processes, with kinetic parameters themselves represented as
probability distributions. The central innovation of PC-GRN is its end-to-end
generative Bayesian inference engine, which learns a full posterior
distribution over BTPN models (P (G, {\Theta}|D)) directly from data. This is
achieved by the novel interplay of a GFlowNet, which learns a policy to sample
network topologies, and a HyperNetwork, which performs amortized inference to
predict their corresponding parameter distributions. The resulting framework
provides a mathematically rigorous, biologically interpretable, and
uncertainty-aware representation of GRNs, advancing predictive modeling and
systems-level analysis.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [211] [End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments](https://arxiv.org/abs/2508.13576)
*Meng-Ping Lin,Enoch Hsin-Ho Huang,Shao-Yi Chien,Yu Tsao*

Main category: eess.AS

TL;DR: 本文介绍了一种新型降噪人工耳蜗系统AVSE - ECS，实验表明其在噪声环境中优于先前策略。


<details>
  <summary>Details</summary>
Motivation: 当前人工耳蜗系统在嘈杂或混响环境中语音理解仍有挑战，深度学习为提升其声音编码能力带来机会。

Method: 引入以视听语音增强（AVSE）模型作为基于深度学习的ElectrodeNet - CS（ECS）声音编码策略的预处理模块的AVSE - ECS系统，并采用联合训练方法构建端到端人工耳蜗系统。

Result: 所提方法在噪声条件下优于先前的ECS策略，客观语音清晰度得分有所提高。

Conclusion: 本研究的方法和发现证明了将AVSE模块集成到端到端人工耳蜗系统中的可行性和潜力。

Abstract: The cochlear implant (CI) is a remarkable biomedical device that successfully
enables individuals with severe-to-profound hearing loss to perceive sound by
converting speech into electrical stimulation signals. Despite advancements in
the performance of recent CI systems, speech comprehension in noisy or
reverberant conditions remains a challenge. Recent and ongoing developments in
deep learning reveal promising opportunities for enhancing CI sound coding
capabilities, not only through replicating traditional signal processing
methods with neural networks, but also through integrating visual cues as
auxiliary data for multimodal speech processing. Therefore, this paper
introduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an
audio-visual speech enhancement (AVSE) model as a pre-processing module for the
deep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically,
a joint training approach is applied to model AVSE-ECS, an end-to-end CI
system. Experimental results indicate that the proposed method outperforms the
previous ECS strategy in noisy conditions, with improved objective speech
intelligibility scores. The methods and findings in this study demonstrate the
feasibility and potential of using deep learning to integrate the AVSE module
into an end-to-end CI system

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [212] [Unified Modelling of Infrastructure Asset Performance Deterioration -- a bounded gamma process approach](https://arxiv.org/abs/2508.13359)
*Wang Chen,Arnold X. -X. Yuan*

Main category: stat.AP

TL;DR: 本文提出新的有界变换伽马过程（BTGP）模型，通过与其他模型对比及实证研究，证实其对基础设施系统的灵活性和重要性。


<details>
  <summary>Details</summary>
Motivation: 现有有界变换伽马过程模型缺乏刻画不同劣化模式的灵活性，需新模型以满足基础设施资产管理系统对灵活劣化模型的需求。

Method: 提出基于传统回归建模的新BTGP模型，从劣化建模和资产管理决策角度与有界非平稳伽马过程（BNGP）模型进行定性和定量比较，并利用真实桥梁状况数据进行实证研究。

Result: 研究结果证实了所提出的BTGP模型对基础设施系统的灵活性和重要性。

Conclusion: 所提出的BTGP模型具有灵活性，对基础设施系统有重要意义。

Abstract: Infrastructure asset management systems require a flexible deterioration
model that can handle various degradation patterns in a unified way. Owing to
its appealing monotonic sample paths, independent increments and mathematical
tractability, gamma process has been widely employed as an infrastructure
performance deterioration model. This model was recently enhanced by
introducing an upper bound to satisfy a practical modelling need that many
infrastructure performance deterioration processes are constrained by physical
or managerial limits. Several bounded transformed gamma process (BTGP)
alternatives had been proposed; however, they lacked due flexibility to
characterize different deterioration patterns. This paper proposed a new BTGP
model that is deeply grounded upon the traditional regression modelling
tradition in infrastructure asset management systems. Qualitative and
quantitative comparisons were carried out between the proposed BTGP and a
bounded nonstationary gamma process (BNGP) model from both deterioration
modelling and asset management decision-making perspectives. An empirical study
using the real-world historical bridge condition data was performed to examine
the flexibility of the BTGP against the BNGP and six other BTGP alternatives.
The results confirmed the flexibility and significance of the proposed BTGP
model for infrastructure systems.

</details>


### [213] [Monotonic Path-Specific Effects: Application to Estimating Educational Returns](https://arxiv.org/abs/2508.13366)
*Aleksei Opacic*

Main category: stat.AP

TL;DR: 本文提出因果中介框架研究教育对收入等结果的影响，分解教育平均处理效应，分析高中完成情况回报发现高中文凭收益主要来自直接劳动力市场回报。


<details>
  <summary>Details</summary>
Motivation: 传统教育效果研究采用的教育衡量方式与个人教育转变的顺序过程不一致，需新方法研究教育效果。

Method: 提出因果中介框架，将教育的平均处理效应分解为直接和间接效应。

Result: 分析NLSY97队列中高中完成情况回报，发现高中文凭收益主要源于直接劳动力市场回报，通过后续升学的中介作用小。

Conclusion: 所提出的分解方法有特殊性质，便于识别因果路径，高中文凭收益主要是直接回报。

Abstract: Conventional research on educational effects typically either employs a
"years of schooling" measure of education, or dichotomizes attainment as a
point-in-time treatment. Yet, such a conceptualization of education is
misaligned with the sequential process by which individuals make educational
transitions. In this paper, I propose a causal mediation framework for the
study of educational effects on outcomes such as earnings. The framework
considers the effect of a given educational transition as operating indirectly,
via progression through subsequent transitions, as well as directly, net of
these transitions. I demonstrate that the average treatment effect (ATE) of
education can be additively decomposed into mutually exclusive components that
capture these direct and indirect effects. The decomposition has several
special properties which distinguish it from conventional mediation
decompositions of the ATE, properties which facilitate less restrictive
identification assumptions as well as identification of all causal paths in the
decomposition. An analysis of the returns to high school completion in the
NLSY97 cohort suggests that the payoff to a high school degree stems
overwhelmingly from its direct labor market returns. Mediation via college
attendance, completion and graduate school attendance is small because of
individuals' low counterfactual progression rates through these subsequent
transitions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [214] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 研究大语言模型上下文学习能力，以物理动力学预测任务为代理，发现上下文越长预测性能越好，通过稀疏自编码器分析揭示模型编码了有意义物理概念，拓宽对上下文学习理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文学习能力提升但机制不明，物理任务是研究该挑战的理想测试床，旨在探究其上下文学习机制，尤其是物理推理能力。

Method: 以物理系统动力学预测任务为代理评估模型上下文学习物理能力，用稀疏自编码器分析模型残差流激活。

Result: 上下文越长动力学预测性能越好，稀疏自编码器捕获的特征与关键物理变量相关。

Conclusion: 研究为拓宽对大语言模型上下文学习机制的理解提供了新案例研究。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [215] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: 提出MM - BrowseComp基准评估智能体多模态检索和推理能力，评估显示当前模型多模态能力欠佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注文本信息，忽略多模态内容，需新基准评估智能体多模态能力。

Method: 引入包含224个精心设计问题的MM - BrowseComp基准，为每个问题提供验证清单。

Result: 对最先进模型评估显示，像OpenAI o3带工具的模型准确率仅29.02%。

Conclusion: 当前模型多模态能力不理想，缺乏原生多模态推理能力。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [216] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 本文提出实时设备端流式语音翻译方法，平衡质量与延迟，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决实时设备端流式语音翻译中集成自动语音识别（ASR）和机器翻译（MT）的挑战，实现实时流式翻译。

Method: 提出同时翻译方法平衡质量与延迟，利用ASR系统生成的语言线索管理上下文，采用超时和强制终结等高效束搜索剪枝技术。

Result: 应用于设备端双语对话语音翻译，在延迟和质量上优于基线。

Conclusion: 该技术缩小了与非流式翻译系统的质量差距，为更准确高效的实时语音翻译铺平道路。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [217] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 提出将LLaMA模型上下文知识融入Whisper以增强ASR的方法，在长音频转录评估中效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决ASR系统在长音频转录中难以保持句法和语义准确性的问题。

Method: 采用两种策略，一是用最优传输进行标记级蒸馏以对齐维度和序列长度，二是最小化Whisper和LLaMA句子嵌入的表示损失。

Result: 在Spoken Wikipedia数据集评估中，WER、NER、大小写和标点成功率显著提升。

Conclusion: 强调将语言上下文融入转录的价值，为长语音的上下文感知ASR奠定基础。

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [218] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: 介绍140亿参数的开放权重语言模型Datarus - R1 - 14B，阐述训练方法，展示其在研究生级别问题和基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 开发能作为虚拟数据分析师和研究生级别问题解决者的语言模型。

Method: 以完整分析轨迹数据训练，结合轨迹合成数据生成器、双奖励框架、内存优化的GRPO，采用余弦课程策略，有双推理接口。

Result: 在研究生级别问题上避免循环和冗余，在基准测试中超越同规模模型，接近更大推理模型，准确率更高且输出代币更少。

Conclusion: Datarus - R1 - 14B在复杂问题解决和推理能力上表现出色。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [219] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 论文提出在母语者自由词语联想规范上进行高效微调，以解决大语言模型文化偏差问题，实验效果良好，表明少量文化关联数据可实现价值对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨文化交流中存在行为偏差，且缺乏有效的文化建模和对齐方法。

Method: 在母语者自由词语联想规范上进行参数高效微调，利用英语和普通话关联数据，通过监督微调（SFT）和基于PPO的偏好优化对Llama - 3.1 - 8B和Qwen - 2.5 - 7B进行调整。

Result: SFT提高了英语和普通话的关联精度，提升了具体性，达到人类水平的情感和唤醒度；微调模型在相关问题上的答案分布向目标文化转移；7 - 8B模型表现可与70B基线模型相媲美。

Conclusion: 基于人类认知的方法在改善AI模型文化对齐方面有前景，且需要未来进一步研究。

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [220] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: 提出ProMed框架使医疗大语言模型从被动转为主动范式，在医学问答中先问有临床价值的问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在交互式医疗问诊中多为被动范式，有诊断错误风险，需转为主动范式。

Method: 提出ProMed框架，核心是SIG奖励，将其融入两阶段训练流程：SIG引导的模型初始化和SIG增强的策略优化。

Result: 在两个新的部分信息医疗基准上，ProMed平均比现有方法高6.29%，比被动范式有54.45%的提升，且能很好泛化到域外案例。

Conclusion: ProMed能有效让医疗大语言模型从被动转为主动范式，提升交互式医疗问诊表现。

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [221] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: 本文使用沙特方言指令数据集对ALLaM - 7B - Instruct - preview进行LoRA微调以生成沙特方言，对比两种训练方式，评估显示Dialect - Token模型效果最佳，且两种微调模型表现优于通用指令模型，最后发布代码和数据单。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语大语言模型对沙特方言支持有限，难以捕捉方言变体，需改进沙特方言生成能力。

Method: 使用私有沙特方言指令数据集对ALLaM - 7B - Instruct - preview进行LoRA微调，研究Dialect - Token和No - Token两种训练变体，用外部方言分类器结合文本保真度和多样性指标评估。

Result: Dialect - Token模型控制效果最佳，沙特方言率提升，MSA泄漏减少，保真度提高，两种LoRA变体在方言控制和保真度上优于通用指令模型，且避免元数据标签回声问题。

Conclusion: 通过LoRA微调可有效提升大语言模型对沙特方言的生成能力，虽未发布数据集和模型权重，但发布代码和数据单支持独立验证。

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [222] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 提出二维框架评估压缩模型与原大模型的信任等价性，实验表明即使准确率相近，压缩模型与大模型也并非信任等价，部署压缩模型需谨慎评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注压缩对准确率的影响，但性能相当不保证信任等价，因此需评估压缩模型与原大模型的信任等价性。

Method: 提出二维框架，包括用LIME和SHAP测试衡量可解释性对齐，用ECE、MCE、Brier Score和可靠性图评估校准相似度。以BERT - base及其压缩变体进行实验，聚焦两个文本分类任务。

Result: 即使模型间准确率几乎相同，可解释性对齐度低，校准相似度存在显著差异。

Conclusion: 压缩模型与大模型并非信任等价，部署压缩模型不能仅考虑性能，需谨慎评估。

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [223] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: 本文提出ViExam基准评估视觉语言模型（VLMs）在越南语教育评估中的表现，发现多数VLMs不如人类考生，跨语言提示不能提升性能，人在环协作可部分提升。


<details>
  <summary>Details</summary>
Motivation: 探究主要在英语数据上训练的VLMs能否处理现实世界跨语言多模态推理，评估其在低资源语言越南语多模态教育内容中的表现。

Method: 提出包含2548个多模态问题的ViExam基准进行评估。

Result: 最先进的VLMs平均准确率57.74%，开源模型27.70%，多数VLMs不如人类考生，跨语言提示使SOTA VLMs准确率降1个百分点，人在环协作可提升5个百分点。

Conclusion: 当前VLMs在越南语多模态教育评估中表现有限，跨语言提示效果不佳，人在环协作有一定提升作用。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [224] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: 研究大语言模型不同解码策略对医疗任务输出质量的影响，发现确定性策略通常优于随机策略，解码方法和模型选择需谨慎。


<details>
  <summary>Details</summary>
Motivation: 大语言模型解码策略对输出质量影响大，但在医疗领域其影响研究不足，需探究其在医疗任务中的作用。

Method: 在五项开放式医疗任务中，用不同大小的医学专业和通用大语言模型评估11种解码策略，还比较多种评估指标。

Result: 确定性策略通常优于随机策略，慢解码方法质量更好，大模型得分高但推理时间长，医学模型在部分任务表现好但无整体优势且对解码更敏感，评估指标相关性因任务而异。

Conclusion: 医疗应用中需谨慎选择解码方法，其影响有时超模型选择。

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [225] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: 研究利用说话者分配方法研究Speech - LLMs性别偏见，评估Bark模型，发现其无系统偏见但有性别意识和倾向。


<details>
  <summary>Details</summary>
Motivation: 探究Speech - LLMs在性别偏见方面是否与文本大语言模型有相似性。

Method: 提出利用说话者分配作为分析工具，构建职业和性别色彩词汇两个数据集评估Bark模型默认说话者分配。

Result: Bark模型无系统偏见，但有性别意识和一些性别倾向。

Conclusion: 通过研究方法对Bark模型性别偏见情况得出结论，为Speech - LLMs性别偏见研究提供参考。

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [226] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文探讨词嵌入知识解释方法，质疑预测准确性与特征可解释性的关联，指出结果受算法上限影响而非语义信息。


<details>
  <summary>Details</summary>
Motivation: 提高AI系统可解释性，研究词嵌入中隐含知识的解释方法。

Method: 分析常见的将词嵌入映射到语义特征规范的方法，并测试其对随机信息的预测能力。

Result: 这些方法能成功预测随机信息，结果主要由算法上限决定，而非词嵌入中的语义表示。

Conclusion: 仅基于预测性能比较数据集不能可靠表明哪个数据集被词嵌入更好地捕捉，映射主要反映向量空间的几何相似性，而非语义属性的真正出现。

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [227] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 评估28个大语言模型处理20种可废止推理模式的能力，发现模型表现差异大，不同提示方式效果不同，凸显当前大语言模型在默认推理中的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型处理涉及通用概括的可废止推理模式的能力，通用概括在多个领域有重要研究价值。

Method: 对28个大语言模型进行涉及20种可废止推理模式的评估。

Result: 不同模型和提示方式表现差异大，少样本提示有一定提升，思维链提示常导致性能下降，多数模型存在区分推理类型和理解通用概括的问题。

Conclusion: 当前大语言模型在默认推理方面有潜力但也存在局限。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [228] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: 研究发现微调大语言模型执行代理任务会导致意外的未对齐问题，提出 PING 方法提升安全性且不牺牲效果，该方法在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调为代理系统时安全问题常被忽视，微调可能使对齐的模型意外未对齐，导致执行有害任务风险增加。

Method: 提出 Prefix INjection Guard (PING) 方法，通过在代理回复前添加自动生成的自然语言前缀，采用迭代方式交替生成候选前缀和选择优化任务性能与拒绝行为的前缀。

Result: 实验表明 PING 显著提升微调大语言模型代理的安全性，不牺牲效果，在网络导航和代码生成任务的多样基准测试中优于现有提示方法，线性探针分析显示前缀令牌对行为修改至关重要。

Conclusion: PING 是一种简单有效的方法，能在不牺牲性能的前提下提升微调大语言模型代理的安全性。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [229] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 提出基于提示的一次性策略控制大语言模型文本长度，评估效果良好，展示仅通过提示工程实现精确长度控制的可能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以可靠地遵循明确的文本长度指令，控制文本长度具有挑战性。

Method: 提出基于提示的一次性策略，在提示中添加倒计时标记和明确计数规则，让模型“边写边数”。

Result: 在多个设置中评估，如MT - Bench - LI上，使用倒计时提示使GPT - 4.1严格长度合规率从不足30%提升到超95%，且答案质量不变。

Conclusion: 仅通过提示工程可实现精确长度控制，为基于训练或解码的方法提供轻量级替代方案。

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [230] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 评估自然语言生成（NLG）对AI实际应用至关重要，但现有自动评估指标（AEM）无完美方案，本文分析现有指标问题，建议按需选指标及加强验证方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能作为明确解决方案的单一NLG评估指标，且研究选用指标时未充分考虑影响，需全面考察现有指标。

Method: 对现有指标的方法、优缺点、验证方法及与人类判断的相关性进行全面检查。

Result: 发现指标存在仅捕捉文本质量特定方面、有效性因任务和数据集而异、验证实践不规范、与人类判断相关性不一致等问题，最新的LLM - as - a - Judge和检索增强生成（RAG）评估也存在这些问题。

Conclusion: 挑战了寻找“完美指标”的追求，建议按需选择指标，利用互补评估，新指标应注重增强验证方法。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [231] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 研究探索BIM与NLP集成，从法国建筑技术规范文档提取需求，对比多种模型，CamemBERT和Fr_core_news_lg在NER表现优，随机森林在RE有效。


<details>
  <summary>Details</summary>
Motivation: 在建筑行业中，实现从非结构化的法国建筑技术规范文档中自动提取需求。

Method: 采用命名实体识别（NER）和关系提取（RE）技术，利用预训练的CamemBERT和Fr_core_news_lg模型，开发多种基准方法，用自定义特征向量实现四种监督模型，用手工标注数据集对比效果。

Result: CamemBERT和Fr_core_news_lg在NER中F1分数超90%，随机森林在RE中F1分数超80%。

Conclusion: 研究成果未来将以知识图谱呈现，以增强自动验证系统。

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [232] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出 LongMab - PO 框架解决合成数据多样性低和事实不一致问题，提升大模型长上下文能力，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有微调大语言模型提升长上下文能力的方法因合成数据低多样性和事实不一致受限。

Method: 采用多臂老虎机（MAB）滚动策略从长上下文中选信息块，生成高质量多样响应，构建偏好数据对，用直接偏好优化（DPO）训练模型。

Result: LongMab - PO 显著提升偏好数据对的多样性和质量，在长上下文推理基准测试中达最优性能。

Conclusion: LongMab - PO 框架有效解决了现有方法的问题，能提升大模型长上下文能力。

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [233] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: 现有对话系统难提供准确话题引导，本文提出AGQ框架，结合CEIRT模型与大语言模型，提升信息检索效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统因无法识别用户概念混淆，难以提供准确话题引导。

Method: 引入Ask - Good - Question (AGQ)框架，采用改进的Concept - Enhanced Item Response Theory (CEIRT)模型，结合大语言模型根据启发文本直接生成引导问题。

Result: 与其他基线方法对比，该方法显著提升了用户的信息检索体验。

Conclusion: 所提出的方法能有效解决当前对话系统的问题，提高信息检索效率。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [234] [Large Language Models as Visualization Agents for Immersive Binary Reverse Engineering](https://arxiv.org/abs/2508.13413)
*Dennis Brown,Samuel Mulder*

Main category: cs.HC

TL;DR: 本文将大语言模型集成到用于二进制逆向工程的VR平台，评估显示LLM能生成有意义3D调用图但质量差异大，提出LLM作可视化代理的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 利用沉浸式VR降低二进制逆向工程的认知复杂性，拓展之前的VR平台功能。

Method: 将集成大语言模型的代理扩展到之前的VR平台，描述系统架构并进行评估。

Result: 试点研究表明LLM能为小程序生成符合设计原则的有意义3D调用图，但输出质量差异大。

Conclusion: 提出关于LLM作为可视化代理在无明确训练下构建反映认知设计原则的3D表示的开放性问题。

Abstract: Immersive virtual reality (VR) offers affordances that may reduce cognitive
complexity in binary reverse engineering (RE), enabling embodied and external
cognition to augment the RE process through enhancing memory, hypothesis
testing, and visual organization. In prior work, we applied a cognitive systems
engineering approach to identify an initial set of affordances and implemented
a VR environment to support RE through spatial persistence and interactivity.
In this work, we extend that platform with an integrated large language model
(LLM) agent capable of querying binary analysis tools, answering technical
questions, and dynamically generating immersive 3D visualizations in alignment
with analyst tasks. We describe the system architecture and our evaluation
process and results. Our pilot study shows that while LLMs can generate
meaningful 3D call graphs (for small programs) that align with design
principles, output quality varies widely. This work raises open questions about
the potential for LLMs to function as visualization agents, constructing 3D
representations that reflect cognitive design principles without explicit
training.

</details>


### [235] [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
*Yuge Zhang,Nan Chen,Jiahang Xu,Yuqing Yang*

Main category: cs.HC

TL;DR: 现有大语言模型提示实践有不足，本文提出POML语言解决问题并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型提示实践在结构、数据集成、格式敏感性和工具方面存在挑战，现有方法缺乏综合解决方案。

Method: 引入POML，采用基于组件的标记、专用标签、CSS样式系统等，还提供模板和开发工具包。

Result: 通过两个案例研究验证其在复杂应用集成和准确性表现上的作用，以及用户研究评估其实践效果。

Conclusion: POML能有效解决大语言模型提示实践的现有问题。

Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current
practices face challenges in structure, data integration, format sensitivity,
and tooling. Existing methods lack comprehensive solutions for organizing
complex prompts involving diverse data types (documents, tables, images) or
managing presentation variations systematically. To address these gaps, we
introduce POML (Prompt Orchestration Markup Language). POML employs
component-based markup for logical structure (roles, tasks, examples),
specialized tags for seamless data integration, and a CSS-like styling system
to decouple content from presentation, reducing formatting sensitivity. It
includes templating for dynamic prompts and a comprehensive developer toolkit
(IDE support, SDKs) to improve version control and collaboration. We validate
POML through two case studies demonstrating its impact on complex application
integration (PomLink) and accuracy performance (TableQA), as well as a user
study assessing its effectiveness in real-world development scenarios.

</details>


### [236] [Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?](https://arxiv.org/abs/2508.13962)
*Ruiwei Xiao,Xinying Hou,Ying-Jui Tseng,Hsuan Nieu,Guanze Liao,John Stamper,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: 为使学生能负责任地与AI互动，设计基于大语言模型的模块教提示素养，经两次课堂部署评估，结果良好，有推广潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入日常生活，需培养下一代负责任地与AI系统互动的能力，K - 12教育工作者有教学生伦理且有效使用AI进行学习的需求。

Method: 设计基于大语言模型的模块，包含与智能LLM代理直接交互的情景练习活动，在11个中学教室进行两次课堂部署，并评估自动评分器能力、学生提示表现和信心变化及学习与评估材料质量。

Result: AI自动评分器能较好地对学生提示进行评分；教学材料帮助学生提高提示技能，改变对用AI学习的看法；研究2中判断题和开放性问题比选择题更能有效衡量提示素养。

Conclusion: 结果有推广潜力，需更多研究评估学习效果和评估设计。

Abstract: As Artificial Intelligence (AI) becomes increasingly integrated into daily
life, there is a growing need to equip the next generation with the ability to
apply, interact with, evaluate, and collaborate with AI systems responsibly.
Prior research highlights the urgent demand from K-12 educators to teach
students the ethical and effective use of AI for learning. To address this
need, we designed an Large-Language Model (LLM)-based module to teach prompting
literacy. This includes scenario-based deliberate practice activities with
direct interaction with intelligent LLM agents, aiming to foster secondary
school students' responsible engagement with AI chatbots. We conducted two
iterations of classroom deployment in 11 authentic secondary education
classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'
prompting performance and confidence changes towards using AI for learning; and
3) the quality of learning and assessment materials. Results indicated that the
AI-based auto-grader could grade student-written prompts with satisfactory
quality. In addition, the instructional materials supported students in
improving their prompting skills through practice and led to positive shifts in
their perceptions of using AI for learning. Furthermore, data from Study 1
informed assessment revisions in Study 2. Analyses of item difficulty and
discrimination in Study 2 showed that True/False and open-ended questions could
measure prompting literacy more effectively than multiple-choice questions for
our target learners. These promising outcomes highlight the potential for
broader deployment and highlight the need for broader studies to assess
learning effectiveness and assessment design.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [237] [The Rise of Generative AI for Metal-Organic Framework Design and Synthesis](https://arxiv.org/abs/2508.13197)
*Chenru Duan,Aditya Nandy,Shyam Chand Pal,Xin Yang,Wenhao Gao,Yuanqi Du,Hendrik Kraß,Yeonghun Kang,Varinia Bernales,Zuyang Ye,Tristan Pyle,Ray Yang,Zeqi Gu,Philippe Schwaller,Shengqian Ma,Shijing Sun,Alán Aspuru-Guzik,Seyed Mohamad Moosavi,Robert Wexler,Zhiling Zheng*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍生成式人工智能推动金属有机框架（MOF）设计和发现的转变及成果，指出尚存挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能发展，变革传统MOF设计和发现方式，提高高性能MOF材料搜索效率。

Method: 采用深度学习模型，如变分自编码器、扩散模型和大语言模型代理，结合高通量计算筛选和自动化实验形成闭环发现管道。

Result: 形成新的网状化学范式，AI算法更有效地指导高性能MOF材料搜索。

Conclusion: 虽有进展，但仍存在合成可行性、数据集多样性和领域知识融合等挑战。

Abstract: Advances in generative artificial intelligence are transforming how
metal-organic frameworks (MOFs) are designed and discovered. This Perspective
introduces the shift from laborious enumeration of MOF candidates to generative
approaches that can autonomously propose and synthesize in the laboratory new
porous reticular structures on demand. We outline the progress of employing
deep learning models, such as variational autoencoders, diffusion models, and
large language model-based agents, that are fueled by the growing amount of
available data from the MOF community and suggest novel crystalline materials
designs. These generative tools can be combined with high-throughput
computational screening and even automated experiments to form accelerated,
closed-loop discovery pipelines. The result is a new paradigm for reticular
chemistry in which AI algorithms more efficiently direct the search for
high-performance MOF materials for clean air and energy applications. Finally,
we highlight remaining challenges such as synthetic feasibility, dataset
diversity, and the need for further integration of domain knowledge.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [238] [Preliminary suggestions for rigorous GPAI model evaluations](https://arxiv.org/abs/2508.00875)
*Patricia Paskov,Michael J. Byun,Kevin Wei,Toby Webster*

Main category: cs.CY

TL;DR: 本文初步汇编通用人工智能（GPAI）评估实践，含不同类型评估建议，按评估生命周期四阶段组织，借鉴多领域实践，面向多类受众。


<details>
  <summary>Details</summary>
Motivation: 推动通用人工智能评估的内部效度、外部效度和可重复性，为新兴且不断发展的GPAI评估科学领域的讨论做贡献。

Method: 借鉴机器学习、统计学、心理学、经济学、生物学等领域的既定实践。

Result: 形成了涵盖评估生命周期四个阶段的通用人工智能评估实践建议，包括人类提升研究和基准评估等方面的建议。

Conclusion: 这些建议有助于推动通用人工智能评估领域的发展，为相关模型提供者、评估者、政策制定者和学术研究人员提供参考。

Abstract: This document presents a preliminary compilation of general-purpose AI (GPAI)
evaluation practices that may promote internal validity, external validity and
reproducibility. It includes suggestions for human uplift studies and benchmark
evaluations, as well as cross-cutting suggestions that may apply to many
different evaluation types. Suggestions are organised across four stages in the
evaluation life cycle: design, implementation, execution and documentation.
Drawing from established practices in machine learning, statistics, psychology,
economics, biology and other fields recognised to have important lessons for AI
evaluation, these suggestions seek to contribute to the conversation on the
nascent and evolving field of the science of GPAI evaluations. The intended
audience of this document includes providers of GPAI models presenting systemic
risk (GPAISR), for whom the EU AI Act lays out specific evaluation
requirements; third-party evaluators; policymakers assessing the rigour of
evaluations; and academic researchers developing or conducting GPAI
evaluations.

</details>


### [239] [Toward an African Agenda for AI Safety](https://arxiv.org/abs/2508.13179)
*Samuel T. Segun,Rachel Adams,Ana Florido,Scott Timcke,Jonathan Shock,Leah Junck,Fola Adeleke,Nicolas Grossman,Ayantola Alayande,Jerry John Kponyo,Matthew Smith,Dickson Marfo Fosu,Prince Dawson Tetteh,Juliet Arthur,Stephanie Kasaon,Odilile Ayodele,Laetitia Badolo,Paul Plantinga,Michael Gastrow,Sumaya Nur Adan,Joanna Wiaterek,Cecil Abungu,Kojo Apeagyei,Luise Eder,Tegawende Bissyande*

Main category: cs.CY

TL;DR: 本文描绘非洲AI风险状况，指出其面临特定AI安全风险，且非洲观点未融入全球AI安全讨论，提出五点行动计划。


<details>
  <summary>Details</summary>
Motivation: 解决非洲在全球AI安全治理中缺乏影响力，以及非洲面临的AI安全风险问题。

Method: 提出以保护人权、建立研究所、提升公众素养、开发预警系统和举办论坛为核心的五点行动计划。

Result: 无明确提及具体实施结果。

Conclusion: 需要采取五点行动计划来应对非洲面临的AI安全风险，提升非洲在全球AI安全治理中的影响力。

Abstract: This paper maps Africa's distinctive AI risk profile, from deepfake fuelled
electoral interference and data colonial dependency to compute scarcity, labour
disruption and disproportionate exposure to climate driven environmental costs.
While major benefits are promised to accrue, the availability, development and
adoption of AI also mean that African people and countries face particular AI
safety risks, from large scale labour market disruptions to the nefarious use
of AI to manipulate public opinion. To date, African perspectives have not been
meaningfully integrated into global debates and processes regarding AI safety,
leaving African stakeholders with limited influence over the emerging global AI
safety governance agenda. While there are Computer Incident Response Teams on
the continent, none hosts a dedicated AI Safety Institute or office. We propose
a five-point action plan centred on (i) a policy approach that foregrounds the
protection of the human rights of those most vulnerable to experiencing the
harmful socio-economic effects of AI; (ii) the establishment of an African AI
Safety Institute; (iii) promote public AI literacy and awareness; (iv)
development of early warning system with inclusive benchmark suites for 25+
African languages; and (v) an annual AU-level AI Safety & Security Forum.

</details>


### [240] [Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection](https://arxiv.org/abs/2508.13187)
*Jonathan A. Karr Jr.,Benjamin F. Herbst,Ting Hua,Matthew Hauenstein,Georgina Curto,Nitesh V. Chawla*

Main category: cs.CY

TL;DR: 研究运用NLP和LLM识别数字空间中对无家可归者的社会偏见，构建新数据集，评估LLM作为分类器效果，发现LLM表现良好，旨在提高对偏见的认识并为政策提供指标。


<details>
  <summary>Details</summary>
Motivation: 社会污名化是缓解无家可归问题的重大障碍，在线和市政厅话语能反映和影响公众舆论，研究旨在通过作用于公众舆论缓解无家可归问题。

Method: 构建来自Reddit、X等的多模态数据集，运用零样本和少样本分类技术，评估本地LLM和闭源API模型作为分类器。

Result: 本地LLM零样本分类有显著不一致性，但少样本学习分类得分接近闭源LLM，且LLM在所有类别平均表现上优于BERT。

Conclusion: 提高对无家可归者偏见的认识，开发新指标为政策提供信息，提升生成式AI技术的公平性和道德应用。

Abstract: Homelessness is a persistent social challenge, impacting millions worldwide.
Over 770,000 people experienced homelessness in the U.S. in 2024. Social
stigmatization is a significant barrier to alleviation, shifting public
perception, and influencing policymaking. Given that online and city council
discourse reflect and influence part of public opinion, it provides valuable
insights to identify and track social biases. This research contributes to
alleviating homelessness by acting on public opinion. It introduces novel
methods, building on natural language processing (NLP) and large language
models (LLMs), to identify and measure PEH social bias expressed in digital
spaces. We present a new, manually-annotated multi-modal dataset compiled from
Reddit, X (formerly Twitter), news articles, and city council meeting minutes
across 10 U.S. cities. This unique dataset provides evidence of the typologies
of homelessness bias described in the literature. In order to scale up and
automate the detection of homelessness bias online, we evaluate LLMs as
classifiers. We applied both zero-shot and few-shot classification techniques
to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B
Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1,
Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are
significant inconsistencies in local LLM zero-shot classification, the
in-context learning classification scores of local LLMs approach the
classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT
when averaging across all categories. This work aims to raise awareness about
the pervasive bias against PEH, develop new indicators to inform policy, and
ultimately enhance the fairness and ethical application of Generative AI
technologies.

</details>


### [241] [The Course Difficulty Analysis Cookbook](https://arxiv.org/abs/2508.13218)
*Frederik Baucks,Robin Schmucker,Laurenz Wiskott*

Main category: cs.CY

TL;DR: 本文对基于 GPA 和潜在特质模型评估课程难度的现有方法进行全面综述和比较，提供模型选择等教程及应用示例，还提供开源软件包和人工数据集。


<details>
  <summary>Details</summary>
Motivation: 课程难度测量对课程分析至关重要，但现有测量需综合考虑多因素，缺乏全面评估方法，故进行综述比较。

Method: 对现有基于 GPA 和潜在特质模型评估课程难度的方法进行综述和比较，并提供模型选择、假设检验和实际应用的教程。

Result: 完成现有方法的全面综述与比较，提供实用教程，推出开源软件包和人工数据集。

Conclusion: 研究有助于促进高质量、公平和公正的学习体验，支持后续研究和应用。

Abstract: Curriculum analytics (CA) studies curriculum structure and student data to
ensure the quality of educational programs. An essential aspect is studying
course properties, which involves assigning each course a representative
difficulty value. This is critical for several aspects of CA, such as quality
control (e.g., monitoring variations over time), course comparisons (e.g.,
articulation), and course recommendation (e.g., advising). Measuring course
difficulty requires careful consideration of multiple factors: First, when
difficulty measures are sensitive to the performance level of enrolled
students, it can bias interpretations by overlooking student diversity. By
assessing difficulty independently of enrolled students' performances, we can
reduce the risk of bias and enable fair, representative assessments of
difficulty. Second, from a measurement theoretic perspective, the measurement
must be reliable and valid to provide a robust basis for subsequent analyses.
Third, difficulty measures should account for covariates, such as the
characteristics of individual students within a diverse populations (e.g.,
transfer status). In recent years, various notions of difficulty have been
proposed. This paper provides the first comprehensive review and comparison of
existing approaches for assessing course difficulty based on grade point
averages and latent trait modeling. It further offers a hands-on tutorial on
model selection, assumption checking, and practical CA applications. These
applications include monitoring course difficulty over time and detecting
courses with disparate outcomes between distinct groups of students (e.g.,
dropouts vs. graduates), ultimately aiming to promote high-quality, fair, and
equitable learning experiences. To support further research and application, we
provide an open-source software package and artificial datasets, facilitating
reproducibility and adoption.

</details>


### [242] [Consumer Autonomy or Illusion? Rethinking Consumer Agency in the Age of Algorithms](https://arxiv.org/abs/2508.13440)
*Pegah Nokhiz,Aravinda Kanchana Ruwanpathirana*

Main category: cs.CY

TL;DR: 数字时代消费者代理受多种因素制约，研究其影响及应对策略，指出消费者代理需积极培养，干预和教育可增强代理。


<details>
  <summary>Details</summary>
Motivation: 数字时代消费者代理受系统障碍和算法操纵制约，研究这些因素对消费代理的影响。

Method: 研究有约束的贴现消费形式模型，构建消费者面临不同情况的分析场景。

Result: 即使理性效用最大化的消费者在代理受限情况下也可能早期财务崩溃，代理受限与金融不稳定风险相关。

Conclusion: 消费者代理需积极培养，系统干预和消费者教育可增强代理。

Abstract: Consumer agency in the digital age is increasingly constrained by systemic
barriers and algorithmic manipulation, raising concerns about the authenticity
of consumption choices. Nowadays, financial decisions are shaped by external
pressures like obligatory consumption, algorithmic persuasion, and unstable
work schedules that erode financial autonomy. Obligatory consumption (like
hidden fees) is intensified by digital ecosystems. Algorithmic tactics like
personalized recommendations lead to impulsive purchases. Unstable work
schedules also undermine financial planning. Thus, it is important to study how
these factors impact consumption agency. To do so, we examine formal models
grounded in discounted consumption with constraints that bound agency. We
construct analytical scenarios in which consumers face obligatory payments,
algorithm-influenced impulsive expenses, or unpredictable income due to
temporal instability. Using this framework, we demonstrate that even rational,
utility-maximizing agents can experience early financial ruin when agency is
limited across structural, behavioral, or temporal dimensions and how
diminished autonomy impacts long-term financial well-being. Our central
argument is that consumer agency must be treated as a value (not a given)
requiring active cultivation, especially in digital ecosystems. The connection
between our formal modeling and this argument allows us to indicate that
limitations on agency (whether structural, behavioral, or temporal) can be
rigorously linked to measurable risks like financial instability. This
connection is also a basis for normative claims about consumption as a value,
by anchoring them in a formally grounded analysis of consumer behavior. As
solutions, we study systemic interventions and consumer education to support
value deliberation and informed choices. We formally demonstrate how these
measures strengthen agency.

</details>


### [243] [The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats](https://arxiv.org/abs/2508.13700)
*Markov Grey,Charbel-Raphaël Segerie*

Main category: cs.CY

TL;DR: 本文梳理AI风险全谱，分为三类，指出风险放大器，连接现有风险与未来结果，助读者了解AI风险全貌。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，了解其相关风险愈发重要，旨在让读者了解AI风险全貌。

Method: 将AI风险分为滥用风险、失调风险和系统风险三类，并指出风险放大器，连接现有风险与未来结果。

Result: 梳理出AI风险的全谱，明确各类风险及风险放大器，展现现有趋势可能升级为灾难性后果。

Conclusion: 美好未来需协调应对AI风险挑战。

Abstract: As AI systems become more capable, integrated, and widespread, understanding
the associated risks becomes increasingly important. This paper maps the full
spectrum of AI risks, from current harms affecting individual users to
existential threats that could endanger humanity's survival. We organize these
risks into three main causal categories. Misuse risks, which occur when people
deliberately use AI for harmful purposes - creating bioweapons, launching
cyberattacks, adversarial AI attacks or deploying lethal autonomous weapons.
Misalignment risks happen when AI systems pursue outcomes that conflict with
human values, irrespective of developer intentions. This includes risks arising
through specification gaming (reward hacking), scheming and power-seeking
tendencies in pursuit of long-term strategic goals. Systemic risks, which arise
when AI integrates into complex social systems in ways that gradually undermine
human agency - concentrating power, accelerating political and economic
disempowerment, creating overdependence that leads to human enfeeblement, or
irreversibly locking in current values curtailing future moral progress. Beyond
these core categories, we identify risk amplifiers - competitive pressures,
accidents, corporate indifference, and coordination failures - that make all
risks more likely and severe. Throughout, we connect today's existing risks and
empirically observable AI behaviors to plausible future outcomes, demonstrating
how existing trends could escalate to catastrophic outcomes. Our goal is to
help readers understand the complete landscape of AI risks. Good futures are
possible, but they don't happen by default. Navigating these challenges will
require unprecedented coordination, but an extraordinary future awaits if we
do.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [244] [DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer](https://arxiv.org/abs/2508.13786)
*Yisu Liu,Chenxing Li,Wanqian Zhang,Wenfu Wang,Meng Yu,Ruibo Fu,Zheng Lin,Weiping Wang,Dong Yu*

Main category: cs.SD

TL;DR: 提出DegDiT框架用于开放词汇可控音频生成，经实验验证其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 现有可控文本到音频生成方法在准确时间定位、开放词汇可扩展性和实际效率间存在权衡问题。

Method: 提出DegDiT框架，将描述中的事件编码为动态图，用图变换器生成上下文事件嵌入；引入质量平衡数据选择管道；提出共识偏好优化。

Result: 在多个数据集上的实验表明，DegDiT在多种客观和主观评估指标上达到了最优性能。

Conclusion: DegDiT能有效解决现有方法的问题，实现高质量的开放词汇可控音频生成。

Abstract: Controllable text-to-audio generation aims to synthesize audio from textual
descriptions while satisfying user-specified constraints, including event
types, temporal sequences, and onset and offset timestamps. This enables
precise control over both the content and temporal structure of the generated
audio. Despite recent progress, existing methods still face inherent trade-offs
among accurate temporal localization, open-vocabulary scalability, and
practical efficiency. To address these challenges, we propose DegDiT, a novel
dynamic event graph-guided diffusion transformer framework for open-vocabulary
controllable audio generation. DegDiT encodes the events in the description as
structured dynamic graphs. The nodes in each graph are designed to represent
three aspects: semantic features, temporal attributes, and inter-event
connections. A graph transformer is employed to integrate these nodes and
produce contextualized event embeddings that serve as guidance for the
diffusion model. To ensure high-quality and diverse training data, we introduce
a quality-balanced data selection pipeline that combines hierarchical event
annotation with multi-criteria quality scoring, resulting in a curated dataset
with semantic diversity. Furthermore, we present consensus preference
optimization, facilitating audio generation through consensus among multiple
reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime
datasets demonstrate that DegDiT achieves state-of-the-art performances across
a variety of objective and subjective evaluation metrics.

</details>


### [245] [Evaluating Identity Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2508.14012)
*Seungmin Seo,Oleg Aulov,Afzal Godil,Kevin Mangold*

Main category: cs.SD

TL;DR: 提出基准衡量说话人去识别系统身份泄露情况，结果显示现有系统存在身份信息泄露，有隐私风险。


<details>
  <summary>Details</summary>
Motivation: 说话人去识别旨在隐藏说话人身份同时保留语音可懂度，需量化残留身份泄露情况。

Method: 引入基准，用等错误率、累积匹配特征命中率、通过典型相关分析和普罗克拉斯提斯分析测量的嵌入空间相似度三个互补错误率来量化残留身份泄露。

Result: 所有现有说话人去识别系统都泄露身份信息，表现最好的系统仅略好于随机猜测，表现最差的系统基于CMC在前50个候选中有45%的命中率。

Conclusion: 当前说话人去识别技术存在持续的隐私风险。

Abstract: Speaker de-identification aims to conceal a speaker's identity while
preserving intelligibility of the underlying speech. We introduce a benchmark
that quantifies residual identity leakage with three complementary error rates:
equal error rate, cumulative match characteristic hit rate, and embedding-space
similarity measured via canonical correlation analysis and Procrustes analysis.
Evaluation results reveal that all state-of-the-art speaker de-identification
systems leak identity information. The highest performing system in our
evaluation performs only slightly better than random guessing, while the lowest
performing system achieves a 45% hit rate within the top 50 candidates based on
CMC. These findings highlight persistent privacy risks in current speaker
de-identification technologies.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [246] [PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism](https://arxiv.org/abs/2508.13228)
*Yuyan Ye,Hang Xu,Yanghang Huang,Jiali Huang,Qian Weng*

Main category: cs.GR

TL;DR: 提出PreSem - Surf方法，基于NeRF框架，结合多信息快速重建高质量场景表面，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在短时间内从RGB - D序列中重建高质量场景表面。

Method: 整合RGB、深度和语义信息；引入SG - MLP采样结构与PR - MLP进行体素预渲染；采用渐进式语义建模。

Result: 在七个合成场景的六项评估指标中，C - L1、F - score和IoU表现最佳，其他指标结果有竞争力。

Conclusion: PreSem - Surf方法有效且具有实际应用价值。

Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural
Radiance Field (NeRF) framework, capable of reconstructing high-quality scene
surfaces from RGB-D sequences in a short time. The method integrates RGB,
depth, and semantic information to improve reconstruction performance.
Specifically, a novel SG-MLP sampling structure combined with PR-MLP
(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,
allowing the model to capture scene-related information earlier and better
distinguish noise from local details. Furthermore, progressive semantic
modeling is adopted to extract semantic information at increasing levels of
precision, reducing training time while enhancing scene understanding.
Experiments on seven synthetic scenes with six evaluation metrics show that
PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while
maintaining competitive results in NC, Accuracy, and Completeness,
demonstrating its effectiveness and practical applicability.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [247] [PennyLane-Lightning MPI: A massively scalable quantum circuit simulator based on distributed computing in CPU clusters](https://arxiv.org/abs/2508.13615)
*Ji-Hoon Kang,Hoon Ryu*

Main category: quant-ph

TL;DR: 本文提出PennyLane - Lightning MPI，用于可扩展量子电路模拟，有性能和扩展性优势，还能集成到PennyLane框架。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟因量子态空间指数增长面临计算挑战，需实现可扩展模拟。

Method: 采用基于MPI的PennyLane - Lightning MPI扩展，使用索引依赖、门特定的并行化策略。

Result: 在单门和设计好的量子电路基准测试中性能优于基于幺正矩阵运算的通用方法，支持41量子比特和数十万并行进程的模拟。

Conclusion: 该工作通过在标准多核CPU集群实现高性能量子模拟，扩展了PennyLane生态系统，为韩国正在开发的量子计算云服务框架提供后端资源。

Abstract: Quantum circuit simulations play a critical role in bridging the gap between
theoretical quantum algorithms and their practical realization on physical
quantum hardware, yet they face computational challenges due to the exponential
growth of quantum state spaces with increasing qubit size. This work presents
PennyLane-Lightning MPI, an MPI-based extension of the PennyLane-Lightning
suite, developed to enable scalable quantum circuit simulations through
parallelization of quantum state vectors and gate operations across
distributed-memory systems. The core of this implementation is an
index-dependent, gate-specific parallelization strategy, which fully exploits
the characteristic of individual gates as well as the locality of computation
associated with qubit indices in partitioned state vectors. Benchmarking tests
with single gates and well-designed quantum circuits show that the present
method offers advantages in performance over general methods based on unitary
matrix operations and exhibits excellent scalability, supporting simulations of
up to 41-qubit with hundreds of thousands of parallel processes. Being equipped
with a Python plug-in for seamless integration to the PennyLane framework, this
work contributes to extending the PennyLane ecosystem by enabling
high-performance quantum simulations in standard multi-core CPU clusters with
no library-specific requirements, providing a back-end resource for the
cloud-based service framework of quantum computing that is under development in
the Republic of Korea.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [248] [Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment](https://arxiv.org/abs/2508.13559)
*Sukheon Kang,Youngkwon Kim,Jinkyu Yang,Seunghwa Ryu*

Main category: cond-mat.soft

TL;DR: 提出无数据物理信息神经网络框架用于锥形Kresling折纸正逆设计，实验验证其鲁棒性，为折纸超材料编程复杂机械能景观提供途径。


<details>
  <summary>Details</summary>
Motivation: 折纸启发结构设计因复杂非线性力学、多稳态和精确控制展开力需求而具挑战性。

Method: 提出物理信息神经网络（PINN）框架，将机械平衡方程嵌入学习过程。

Result: 模型高精度预测完整能量景观，逆设计可指定目标稳态高度和分离能垒，扩展到分层组件实现顺序展开，有限元模拟和实验验证设计的展开序列和能垒比。

Conclusion: 该方法为折纸启发超材料编程复杂机械能景观提供通用无数据途径，在多领域有广泛应用潜力。

Abstract: Origami-inspired structures provide unprecedented opportunities for creating
lightweight, deployable systems with programmable mechanical responses.
However, their design remains challenging due to complex nonlinear mechanics,
multistability, and the need for precise control of deployment forces. Here, we
present a physics-informed neural network (PINN) framework for both forward
prediction and inverse design of conical Kresling origami (CKO) without
requiring pre-collected training data. By embedding mechanical equilibrium
equations directly into the learning process, the model predicts complete
energy landscapes with high accuracy while minimizing non-physical artifacts.
The inverse design routine specifies both target stable-state heights and
separating energy barriers, enabling freeform programming of the entire energy
curve. This capability is extended to hierarchical CKO assemblies, where
sequential layer-by-layer deployment is achieved through programmed barrier
magnitudes. Finite element simulations and experiments on physical prototypes
validate the designed deployment sequences and barrier ratios, confirming the
robustness of the approach. This work establishes a versatile, data-free route
for programming complex mechanical energy landscapes in origami-inspired
metamaterials, offering broad potential for deployable aerospace systems,
morphing structures, and soft robotic actuators.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [249] [A Category Theory Framework for Macroeconomic Modeling: The Case of Argentina's Bimonetary Economy](https://arxiv.org/abs/2508.13233)
*Luciano Pollicino*

Main category: econ.GN

TL;DR: 本文提出基于范畴论的框架构建更灵活的双货币经济模型，应用于实证数据，发现汇率失衡并提出新指标，与机器学习等工具协同良好。


<details>
  <summary>Details</summary>
Motivation: 传统宏观经济模型无法捕捉阿根廷双货币经济动态，需要构建更灵活结构化的模型。

Method: 基于范畴论，运用对象、态射、学习/遗忘函子、极限和余极限等概念，将模型应用于2018 - 2023年实证数据。

Result: 发现均衡汇率和观测汇率存在显著结构失衡，提出新的衡量贬值风险的综合指标。

Conclusion: 该框架与现代计算工具协同性强，为复杂经济体的政策分析和预测提供更稳健方法。

Abstract: Traditional macroeconomic models, based on static algebraic systems, fail to
capture the dynamics of a bimonetary economy like Argentina's. This paper
proposes a framework based on category theory to develop a more flexible and
structured model that represents the evolving relationships between key
variables such as inflation expectations, interest rates, and currency demand.
Using concepts like objects, morphisms, learning/forgetful functors, limits,
and colimits, the model is applied to empirical data from 2018-2023. The
findings reveal a significant structural misalignment between the equilibrium
and observed exchange rates and propose a new aggregate indicator to measure
devaluation risk. The framework demonstrates a strong synergy with modern
computational tools like machine learning, offering a more robust approach to
policy analysis and forecasting in complex economies.

</details>


### [250] [Interpreting the Interpreter: Can We Model post-ECB Conferences Volatility with LLM Agents?](https://arxiv.org/abs/2508.13635)
*Umberto Collodel*

Main category: econ.GN

TL;DR: 本文用大语言模型模拟金融市场对欧洲央行新闻发布会的反应，评估不同提示策略，结果显示该模拟能捕捉不确定性，可为央行提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 开发新方法模拟金融市场对欧洲央行新闻发布会的反应，为央行提供工具以预测市场反应、完善沟通策略和增强金融稳定。

Method: 创建30个有不同特征的合成交易员进行基于代理的行为模拟，评估朴素、少样本和“LLM-as-a-Judge”三种提示策略。

Result: 即使朴素方法也能使合成分歧与实际市场结果有强相关性，“LLM-as-a-Judge”框架在首次迭代时提高了准确性。

Conclusion: 大语言模型驱动的模拟能捕捉传统方法之外的解释性不确定性，可为央行提供实用工具。

Abstract: This paper develops a novel method to simulate financial market reactions to
European Central Bank (ECB) press conferences using a Large Language Model
(LLM). We create a behavioral, agent-based simulation of 30 synthetic traders,
each with distinct risk preferences, cognitive biases, and interpretive styles.
These agents forecast Euro interest rate swap levels at 3-month, 2-year, and
10-year maturities, with the variation across forecasts serving as a measure of
market uncertainty or disagreement. We evaluate three prompting strategies,
naive, few-shot (enriched with historical data), and an advanced iterative
'LLM-as-a-Judge' framework, to assess the effect of prompt design on predictive
performance. Even the naive approach generates a strong correlation (roughly
0.5) between synthetic disagreement and actual market outcomes, particularly
for longer-term maturities. The LLM-as-a-Judge framework further improves
accuracy at the first iteration. These results demonstrate that LLM-driven
simulations can capture interpretive uncertainty beyond traditional measures,
providing central banks with a practical tool to anticipate market reactions,
refine communication strategies, and enhance financial stability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [251] [Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation](https://arxiv.org/abs/2508.13812)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Hyeongboo Baek,Brent ByungHoon Kang*

Main category: cs.CV

TL;DR: 提出TCA框架减少SNN对抗攻击延迟，实验显示其相比SOTA方法显著降低延迟且保持攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的SNN对抗攻击因多时间步处理存在攻击延迟大问题，不适用于实时应用，且未利用SNN特性。

Method: 提出TCA框架，包含TLBP和A - MPR组件。TLBP利用每时间步评估提前停止；A - MPR预计算并复用初始时间步膜电位。

Result: 在VGG - 11和ResNet - 17及相关数据集上，白盒和黑盒设置下TCA分别最多降低56.6%和57.1%攻击延迟，且保持可比攻击成功率。

Conclusion: TCA框架能有效减少SNN对抗攻击的延迟，适用于实时应用。

Abstract: State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural
networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face
a critical limitation: substantial attack latency from multi-timestep
processing, rendering them infeasible for practical real-time applications.
This inefficiency stems from their design as direct extensions of ANN
paradigms, which fail to exploit key SNN properties. In this paper, we propose
the timestep-compressed attack (TCA), a novel framework that significantly
reduces attack latency. TCA introduces two components founded on key insights
into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our
finding that global temporal information in backpropagation to generate
perturbations is not critical for an attack's success, enabling per-timestep
evaluation for early stopping. Second, adversarial membrane potential reuse
(A-MPR) is motivated by the observation that initial timesteps are
inefficiently spent accumulating membrane potential, a warm-up phase that can
be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the
CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the
required attack latency by up to 56.6% and 57.1% compared to SOTA methods in
white-box and black-box settings, respectively, while maintaining a comparable
attack success rate.

</details>


### [252] [MIRAGE: Towards AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2508.13223)
*Cheng Xia,Manxi Lin,Jiexiang Tan,Xiaoxiong Du,Yang Qiu,Junjun Zheng,Xiangheng Kong,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 本文针对野外场景下AI生成图像检测问题，引入Mirage基准并提出Mirage - R1模型，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在野外场景泛化能力不足，威胁信息安全和公众信任，需解决野外场景检测问题。

Method: 引入Mirage基准，由人工验证的互联网AIGI语料和多专家生成器合成数据集构成；提出Mirage - R1模型，分监督微调冷启动和强化学习两阶段训练，采用推理时自适应思维策略。

Result: 模型在Mirage和公共基准上分别领先现有检测器5%和10%。

Conclusion: 所提方法有效，基准和代码将公开。

Abstract: The spreading of AI-generated images (AIGI), driven by advances in generative
AI, poses a significant threat to information security and public trust.
Existing AIGI detectors, while effective against images in clean laboratory
settings, fail to generalize to in-the-wild scenarios. These real-world images
are noisy, varying from ``obviously fake" images to realistic ones derived from
multiple generative models and further edited for quality control. We address
in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging
benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is
constructed from two sources: (1) a large corpus of Internet-sourced AIGI
verified by human experts, and (2) a synthesized dataset created through the
collaboration between multiple expert generators, closely simulating the
realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a
vision-language model with heuristic-to-analytic reasoning, a reflective
reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a
supervised-fine-tuning cold start, followed by a reinforcement learning stage.
By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is
able to provide either a quick judgment or a more robust and accurate
conclusion, effectively balancing inference speed and performance. Extensive
experiments show that our model leads state-of-the-art detectors by 5% and 10%
on Mirage and the public benchmark, respectively. The benchmark and code will
be made publicly available.

</details>


### [253] [GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis](https://arxiv.org/abs/2508.13300)
*Sirshapan Mitra,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: 提出GaitCrafter框架合成步态序列，提升识别性能并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决步态识别中缺乏大规模标注数据集、收集多样步态样本难且需保护隐私的问题。

Method: 提出基于扩散的GaitCrafter框架，从零开始在步态轮廓数据上训练视频扩散模型。

Result: 生成的合成样本能提升步态识别性能，可生成新身份。

Conclusion: 利用扩散模型实现高质量、可控且保护隐私的步态数据生成向前迈进重要一步。

Abstract: Gait recognition is a valuable biometric task that enables the identification
of individuals from a distance based on their walking patterns. However, it
remains limited by the lack of large-scale labeled datasets and the difficulty
of collecting diverse gait samples for each individual while preserving
privacy. To address these challenges, we propose GaitCrafter, a diffusion-based
framework for synthesizing realistic gait sequences in the silhouette domain.
Unlike prior works that rely on simulated environments or alternative
generative models, GaitCrafter trains a video diffusion model from scratch,
exclusively on gait silhouette data. Our approach enables the generation of
temporally consistent and identity-preserving gait sequences. Moreover, the
generation process is controllable-allowing conditioning on various covariates
such as clothing, carried objects, and view angle. We show that incorporating
synthetic samples generated by GaitCrafter into the gait recognition pipeline
leads to improved performance, especially under challenging conditions.
Additionally, we introduce a mechanism to generate novel identities-synthetic
individuals not present in the original dataset-by interpolating identity
embeddings. These novel identities exhibit unique, consistent gait patterns and
are useful for training models while maintaining privacy of real subjects.
Overall, our work takes an important step toward leveraging diffusion models
for high-quality, controllable, and privacy-aware gait data generation.

</details>


### [254] [Mitigating Easy Option Bias in Multiple-Choice Question Answering](https://arxiv.org/abs/2508.13428)
*Hao Zhang,Chen Li,Basura Fernando*

Main category: cs.CV

TL;DR: 研究发现多选择视觉问答基准存在Easy - Options Bias (EOB) 问题，提出GroundAttack工具修复，评估了视觉语言模型问答能力。


<details>
  <summary>Details</summary>
Motivation: 解决多选择视觉问答基准中存在的EOB问题，实现对视觉语言模型问答能力更真实的评估。

Method: 通过实验找出EOB问题根源，引入GroundAttack工具自动生成与正确答案视觉上同样合理的硬负选项，创建无EOB的标注。

Result: 在无EOB标注上，当前视觉语言模型在(V + O)设置下接近随机准确率，在(V + Q + O)设置下降至不饱和准确率。

Conclusion: 使用GroundAttack工具创建的无EOB标注能更真实地评估视觉语言模型的问答能力，代码和新标注即将发布。

Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some
multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,
RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias
allows vision-language models (VLMs) to select the correct answer using only
the vision (V) and options (O) as inputs, without the need for the question
(Q). Through grounding experiments, we attribute the bias to an imbalance in
visual relevance: the correct answer typically aligns more closely with the
visual contents than the negative options in feature space, creating a shortcut
for VLMs to infer the answer via simply vision-option similarity matching. To
fix this, we introduce GroundAttack, a toolkit that automatically generates
hard negative options as visually plausible as the correct answer. We apply it
to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these
EOB-free annotations, current VLMs approach to random accuracies under (V+O)
settings, and drop to non-saturated accuracies under (V+Q+O) settings,
providing a more realistic evaluation of VLMs' QA ability. Codes and new
annotations will be released soon.

</details>


### [255] [CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification](https://arxiv.org/abs/2508.13280)
*Zeynep Ozdemir,Hacer Yalim Keles,Omer Ozgur Tanriover*

Main category: cs.CV

TL;DR: 提出CLoE课程学习框架用于溃疡性结肠炎内镜图像疾病严重程度评估，在数据集上表现优于基线模型，凸显难度感知训练策略潜力。


<details>
  <summary>Details</summary>
Motivation: 现有Mayo内镜子评分（MES）分类因观察者间差异导致的标签噪声和评分的序数性质而具有挑战性，标准模型常忽略这些问题。

Method: 提出CLoE课程学习框架，利用基于波士顿肠道准备量表（BBPS）标签训练的轻量级模型估计图像质量作为注释置信度代理对样本排序，并结合ResizeMix增强提高鲁棒性。

Result: 在LIMUC和HyperKvasir数据集上，使用CNN和Transformer，CLoE始终优于强监督和自监督基线模型，如ConvNeXt - Tiny在LIMUC上达到82.5%准确率和0.894的QWK，且计算成本低。

Conclusion: 难度感知训练策略在标签不确定情况下改善序数分类有潜力。

Abstract: Estimating disease severity from endoscopic images is essential in assessing
ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to
grade inflammation. However, MES classification remains challenging due to
label noise from inter-observer variability and the ordinal nature of the
score, which standard models often ignore. We propose CLoE, a curriculum
learning framework that accounts for both label reliability and ordinal
structure. Image quality, estimated via a lightweight model trained on Boston
Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation
confidence to order samples from easy (clean) to hard (noisy). This curriculum
is further combined with ResizeMix augmentation to improve robustness.
Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and
Transformers, show that CLoE consistently improves performance over strong
supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches
82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These
results highlight the potential of difficulty-aware training strategies for
improving ordinal classification under label uncertainty. Code will be released
at https://github.com/zeynepozdemir/CLoE.

</details>


### [256] [DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples](https://arxiv.org/abs/2508.13309)
*Abdullah Al Nomaan Nafi,Habibur Rahaman,Zafaryab Haider,Tanzim Mahfuz,Fnu Suya,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CV

TL;DR: 本文提出DAASH框架，结合Lp约束攻击方法生成感知对齐的对抗样本，在多个数据集上表现出色，泛化性好。


<details>
  <summary>Details</summary>
Motivation: 现有Lp范数约束的对抗样本与人类感知对齐不佳，且不清楚Lp约束攻击的见解能否提升感知效果。

Method: 提出DAASH框架，分多阶段聚合多个基于Lp的攻击方法生成的候选对抗样本，用新的元损失函数引导过程。

Result: DAASH在多个数据集上显著优于现有感知攻击方法，攻击成功率更高，视觉质量更好，且泛化性好。

Conclusion: DAASH是评估模型鲁棒性的实用且强大的基线，无需为新防御设计手工自适应攻击。

Abstract: Numerous techniques have been proposed for generating adversarial examples in
white-box settings under strict Lp-norm constraints. However, such norm-bounded
examples often fail to align well with human perception, and only recently have
a few methods begun specifically exploring perceptually aligned adversarial
examples. Moreover, it remains unclear whether insights from Lp-constrained
attacks can be effectively leveraged to improve perceptual efficacy. In this
paper, we introduce DAASH, a fully differentiable meta-attack framework that
generates effective and perceptually aligned adversarial examples by
strategically composing existing Lp-based attack methods. DAASH operates in a
multi-stage fashion: at each stage, it aggregates candidate adversarial
examples from multiple base attacks using learned, adaptive weights and
propagates the result to the next stage. A novel meta-loss function guides this
process by jointly minimizing misclassification loss and perceptual distortion,
enabling the framework to dynamically modulate the contribution of each base
attack throughout the stages. We evaluate DAASH on adversarially trained models
across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on
Lp-constrained based methods, DAASH significantly outperforms state-of-the-art
perceptual attacks such as AdvAD -- achieving higher attack success rates
(e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM,
LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively).
Furthermore, DAASH generalizes well to unseen defenses, making it a practical
and strong baseline for evaluating robustness without requiring handcrafted
adaptive attacks for each new defense.

</details>


### [257] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 提出结构化提示和知识蒸馏框架，生成交通场景注释和风险评估，得到轻量级模型VISTA，性能强且便于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统方法在现实复杂动态环境中处理公路场景理解和交通风险推理时，存在可扩展性和泛化性问题。

Method: 引入结构化提示和知识蒸馏框架，用结构化思维链策略协调两个大视觉语言模型，生成伪注释微调小模型。

Result: 得到3B规模的VISTA模型，在既定字幕指标上表现良好。

Conclusion: 有效的知识蒸馏和结构化多智能体监督能使轻量级视觉语言模型具备复杂推理能力，VISTA便于边缘设备实时风险监测。

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [258] [STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models](https://arxiv.org/abs/2508.13470)
*Tinh-Anh Nguyen-Nhu,Triet Dao Hoang Minh,Dat To-Thanh,Phuc Le-Gia,Tuan Vo-Lan,Tien-Huy Nguyen*

Main category: cs.CV

TL;DR: 本文提出STER - VLM框架解决当前视觉语言模型在交通分析中计算资源需求大、细粒度时空理解不足的问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型用于交通分析时计算资源需求大且难以进行细粒度时空理解。

Method: 提出STER - VLM框架，包括caption分解、时间帧选择与最佳视角过滤、参考驱动理解和精心设计的视觉/文本提示技术。

Result: 在WTS和BDD数据集上语义丰富度和交通场景解释能力显著提升，在AI City Challenge 2025 Track 2测试中获55.655分。

Conclusion: STER - VLM框架能推动资源高效且准确的交通分析在现实世界应用。

Abstract: Vision-language models (VLMs) have emerged as powerful tools for enabling
automated traffic analysis; however, current approaches often demand
substantial computational resources and struggle with fine-grained
spatio-temporal understanding. This paper introduces STER-VLM, a
computationally efficient framework that enhances VLM performance through (1)
caption decomposition to tackle spatial and temporal information separately,
(2) temporal frame selection with best-view filtering for sufficient temporal
information, and (3) reference-driven understanding for capturing fine-grained
motion and dynamic context and (4) curated visual/textual prompt techniques.
Experimental results on the WTS \cite{kong2024wts} and BDD \cite{BDD} datasets
demonstrate substantial gains in semantic richness and traffic scene
interpretation. Our framework is validated through a decent test score of
55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in
advancing resource-efficient and accurate traffic analysis for real-world
applications.

</details>


### [259] [CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving](https://arxiv.org/abs/2508.13485)
*Fuyang Liu,Jilin Mei,Fangyuan Mao,Chen Min,Yan Xing,Yu Hu*

Main category: cs.CV

TL;DR: 提出 CORENet 跨模态去噪框架，利用 LiDAR 监督处理 4D 雷达点云噪声问题，在 Dual - Radar 数据集验证效果好。


<details>
  <summary>Details</summary>
Motivation: 4D 雷达点云稀疏且有噪声，影响有效感知，需解决该局限。

Method: 提出 CORENet 跨模态去噪框架，训练时用 LiDAR 数据监督，推理时仅用雷达数据，可集成到基于体素的检测框架。

Result: 在 Dual - Radar 数据集评估，证明框架能增强检测鲁棒性，实验显示 CORENet 性能优于现有主流方法。

Conclusion: CORENet 是有效的 4D 雷达目标检测跨模态去噪框架，能提升检测性能。

Abstract: 4D radar-based object detection has garnered great attention for its
robustness in adverse weather conditions and capacity to deliver rich spatial
information across diverse driving scenarios. Nevertheless, the sparse and
noisy nature of 4D radar point clouds poses substantial challenges for
effective perception. To address the limitation, we present CORENet, a novel
cross-modal denoising framework that leverages LiDAR supervision to identify
noise patterns and extract discriminative features from raw 4D radar data.
Designed as a plug-and-play architecture, our solution enables seamless
integration into voxel-based detection frameworks without modifying existing
pipelines. Notably, the proposed method only utilizes LiDAR data for
cross-modal supervision during training while maintaining full radar-only
operation during inference. Extensive evaluation on the challenging Dual-Radar
dataset, which is characterized by elevated noise level, demonstrates the
effectiveness of our framework in enhancing detection robustness. Comprehensive
experiments validate that CORENet achieves superior performance compared to
existing mainstream approaches.

</details>


### [260] [Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs](https://arxiv.org/abs/2508.13461)
*Ivan Reyes-Amezcua,Francisco Lopez-Tiro,Clement Larose,Andres Mendez-Vazquez,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 本文对比ViTs和CNN模型对肾结石内镜图像分类性能，ViT-base模型表现更优，证明其是传统CNN可扩展替代方案。


<details>
  <summary>Details</summary>
Motivation: CNN在捕获长距离依赖上能力有限，影响肾结石内镜图像分类性能，需更好方法。

Method: 对ViTs和CNN模型在两个离体数据集上进行对比分析。

Result: ViT-base模型在多成像条件下始终优于ResNet50基线，各指标表现更好。

Conclusion: ViT架构分类性能更优，是传统CNN可扩展替代方案。

Abstract: Kidney stone classification from endoscopic images is critical for
personalized treatment and recurrence prevention. While convolutional neural
networks (CNNs) have shown promise in this task, their limited ability to
capture long-range dependencies can hinder performance under variable imaging
conditions. This study presents a comparative analysis between Vision
Transformers (ViTs) and CNN-based models, evaluating their performance on two
ex vivo datasets comprising CCD camera and flexible ureteroscope images. The
ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50
baseline across multiple imaging conditions. For instance, in the most visually
complex subset (Section patches from endoscopic images), the ViT model achieved
95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.
In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy
versus 78.4% with CNN. These improvements extend across precision and recall as
well. The results demonstrate that ViT-based architectures provide superior
classification performance and offer a scalable alternative to conventional
CNNs for kidney stone image analysis.

</details>


### [261] [Multi-view Clustering via Bi-level Decoupling and Consistency Learning](https://arxiv.org/abs/2508.13499)
*Shihao Dong,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: 提出Bi - level Decoupling and Consistency Learning框架(BDCL)用于多视图聚类，实验表明优于SOTA方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类中面向聚类的表示学习常被忽视，需探索有效表示提升特征的簇间可区分性和簇内紧凑性。

Method: 提出BDCL框架，包含多视图实例学习模块、特征和簇的双层解耦模块、一致性学习模块。

Result: 在五个基准数据集上实验，提出方法优于SOTA方法。

Conclusion: 所提出的BDCL框架在多视图聚类中有效，能提升聚类性能。

Abstract: Multi-view clustering has shown to be an effective method for analyzing
underlying patterns in multi-view data. The performance of clustering can be
improved by learning the consistency and complementarity between multi-view
features, however, cluster-oriented representation learning is often
overlooked. In this paper, we propose a novel Bi-level Decoupling and
Consistency Learning framework (BDCL) to further explore the effective
representation for multi-view data to enhance inter-cluster discriminability
and intra-cluster compactness of features in multi-view clustering. Our
framework comprises three modules: 1) The multi-view instance learning module
aligns the consistent information while preserving the private features between
views through reconstruction autoencoder and contrastive learning. 2) The
bi-level decoupling of features and clusters enhances the discriminability of
feature space and cluster space. 3) The consistency learning module treats the
different views of the sample and their neighbors as positive pairs, learns the
consistency of their clustering assignments, and further compresses the
intra-cluster space. Experimental results on five benchmark datasets
demonstrate the superiority of the proposed method compared with the SOTA
methods. Our code is published on https://github.com/LouisDong95/BDCL.

</details>


### [262] [Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency](https://arxiv.org/abs/2508.13518)
*Yanbiao Ma,Wei Dai,Bowei Liu,Jiayi Chen,Wenke Huang,Guancheng Wan,Zhiwu Lu,Junchi Yan*

Main category: cs.CV

TL;DR: 文章指出深度学习中样本与真实分布存在差距问题，利用基础模型特征分布几何形状的可迁移性，在联邦学习和长尾识别中应用分布校准框架，实验证明有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中观察到的训练样本与潜在真实分布之间的差距问题。

Method: 利用现成（视觉）基础模型进行特征提取，将几何知识引导的分布校准框架应用于联邦学习和长尾识别场景。在联邦学习中获取全局几何形状知识生成新样本；在长尾学习中用富样本类别几何知识恢复稀有类别真实分布。

Result: 综合实验表明，提出的几何知识引导的分布校准有效克服了数据异质性和样本不平衡导致的信息不足问题，提升了各基准测试的性能。

Conclusion: 几何知识引导的分布校准框架能有效处理数据问题，提升模型性能。

Abstract: Despite the fast progress of deep learning, one standing challenge is the gap
of the observed training samples and the underlying true distribution. There
are multiple reasons for the causing of this gap e.g. sampling bias, noise etc.
In the era of foundation models, we show that when leveraging the off-the-shelf
(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the
geometric shapes of the resulting feature distributions exhibit remarkable
transferability across domains and datasets. To verify its practical
usefulness, we embody our geometric knowledge-guided distribution calibration
framework in two popular and challenging settings: federated learning and
long-tailed recognition. In the federated setting, we devise a technique of
acquiring the global geometric shape under privacy constraints, then leverage
this knowledge to generate new samples for clients, in the aim of bridging the
gap between local and global observations. In long-tailed learning, it utilizes
the geometric knowledge transferred from sample-rich categories to recover the
true distribution for sample-scarce tail classes. Comprehensive experiments
show that our proposed geometric knowledge-guided distribution calibration
effectively overcomes information deficits caused by data heterogeneity and
sample imbalance, with boosted performance across benchmarks.

</details>


### [263] [Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models](https://arxiv.org/abs/2508.13524)
*Vamsi Krishna Mulukutla,Sai Supriya Pavarala,Srinivasa Raju Rudraraju,Sridevi Bonthu*

Main category: cs.CV

TL;DR: 本文对开源视觉语言模型和传统深度学习模型在FER - 2013数据集上进行情感识别对比，引入新评估管道，发现传统模型表现更好，还进行成本分析并给出基准。


<details>
  <summary>Details</summary>
Motivation: 面部情感识别在人机交互和心理健康诊断等应用中至关重要，需对开源视觉语言模型和传统深度学习模型进行实证比较。

Method: 在FER - 2013数据集上对比开源视觉语言模型（如Phi - 3.5 Vision和CLIP）与传统深度学习模型（VGG19、ResNet - 50和EfficientNet - B0），引入结合GFPGAN图像恢复的新评估管道，用精度、召回率、F1分数和准确率评估性能，进行详细计算成本分析。

Result: 传统模型（如EfficientNet - B0和ResNet - 50）显著优于视觉语言模型（如CLIP和Phi - 3.5 Vision）。

Conclusion: 强调视觉语言模型需适应嘈杂环境，为情感识别未来研究提供可复现基准。

Abstract: Facial Emotion Recognition (FER) is crucial for applications such as
human-computer interaction and mental health diagnostics. This study presents
the first empirical comparison of open-source Vision-Language Models (VLMs),
including Phi-3.5 Vision and CLIP, against traditional deep learning models
VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,
which contains 35,887 low-resolution grayscale images across seven emotion
classes. To address the mismatch between VLM training assumptions and the noisy
nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based
image restoration with FER evaluation. Results show that traditional models,
particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly
outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting
the limitations of VLMs in low-quality visual tasks. In addition to performance
evaluation using precision, recall, F1-score, and accuracy, we provide a
detailed computational cost analysis covering preprocessing, training,
inference, and evaluation phases, offering practical insights for deployment.
This work underscores the need for adapting VLMs to noisy environments and
provides a reproducible benchmark for future research in emotion recognition.

</details>


### [264] [The 9th AI City Challenge](https://arxiv.org/abs/2508.13564)
*Zheng Tang,Shuo Wang,David C. Anastasiu,Ming-Ching Chang,Anuj Sharma,Quan Kong,Norimasa Kobori,Munkhjargal Gochoo,Ganzorig Batnasan,Munkh-Erdene Otgonbold,Fady Alnajjar,Jun-Wei Hsieh,Tomasz Kornuta,Xiaolong Li,Yilin Zhao,Han Zhang,Subhashree Radhakrishnan,Arihant Jain,Ratnesh Kumar,Vidya N. Murali,Yuxing Wang,Sameer Satish Pusegaonkar,Yizhou Wang,Sujit Biswas,Xunlei Wu,Zhedong Zheng,Pranamesh Chakraborty,Rama Chellappa*

Main category: cs.CV

TL;DR: 2025年第九届AI City Challenge有四个赛道，参与人数增加，数据集下载多，各赛道有不同任务，评估框架保障公平，部分团队获佳绩。


<details>
  <summary>Details</summary>
Motivation: 推动计算机视觉和AI在交通、工业自动化和公共安全等领域的实际应用。

Method: 各赛道采用不同技术和标注方式，如Track 1用详细校准和3D边界框标注，Track 2用3D注视标签，评估框架设提交限制和部分保留测试集。

Result: 数据集下载超30000次，部分团队取得顶级成果，在多任务中设定新基准。

Conclusion: AI City Challenge有效推动了计算机视觉和AI在相关领域的发展和应用。

Abstract: The ninth AI City Challenge continues to advance real-world applications of
computer vision and AI in transportation, industrial automation, and public
safety. The 2025 edition featured four tracks and saw a 17% increase in
participation, with 245 teams from 15 countries registered on the evaluation
server. Public release of challenge datasets led to over 30,000 downloads to
date. Track 1 focused on multi-class 3D multi-camera tracking, involving
people, humanoids, autonomous mobile robots, and forklifts, using detailed
calibration and 3D bounding box annotations. Track 2 tackled video question
answering in traffic safety, with multi-camera incident understanding enriched
by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic
warehouse environments, requiring AI systems to interpret RGB-D inputs and
answer spatial questions that combine perception, geometry, and language. Both
Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4
emphasized efficient road object detection from fisheye cameras, supporting
lightweight, real-time deployment on edge devices. The evaluation framework
enforced submission limits and used a partially held-out test set to ensure
fair benchmarking. Final rankings were revealed after the competition
concluded, fostering reproducibility and mitigating overfitting. Several teams
achieved top-tier results, setting new benchmarks in multiple tasks.

</details>


### [265] [EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors](https://arxiv.org/abs/2508.13537)
*Shikun Zhang,Cunjian Chen,Yiqun Wang,Qiuhong Ke,Yong Li*

Main category: cs.CV

TL;DR: 提出表达式和变形感知的3DGS框架EAvatar用于头部重建，实验显示结果准确、视觉连贯且可控性与细节保真度更高。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的方法在捕捉细粒度面部表情和保持局部纹理连续性方面存在挑战，尤其是在高可变形区域。

Method: 提出基于3DGS的EAvatar框架，引入稀疏表达式控制机制，利用预训练生成模型的3D先验。

Result: 方法能产生更准确、视觉连贯的头部重建结果，提高了表情可控性和细节保真度。

Conclusion: 提出的方法在高保真头部重建任务中表现良好，解决了现有方法的局限性。

Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR,
gaming, and multimedia content creation. Recent advances in 3D Gaussian
Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry
with real-time rendering capability and are now widely used in high-fidelity
head avatar reconstruction tasks. However, existing 3DGS-based methods still
face significant challenges in capturing fine-grained facial expressions and
preserving local texture continuity, especially in highly deformable regions.
To mitigate these limitations, we propose a novel 3DGS-based framework termed
EAvatar for head reconstruction that is both expression-aware and
deformation-aware. Our method introduces a sparse expression control mechanism,
where a small number of key Gaussians are used to influence the deformation of
their neighboring Gaussians, enabling accurate modeling of local deformations
and fine-scale texture transitions. Furthermore, we leverage high-quality 3D
priors from pretrained generative models to provide a more reliable facial
geometry, offering structural guidance that improves convergence stability and
shape accuracy during training. Experimental results demonstrate that our
method produces more accurate and visually coherent head reconstructions with
improved expression controllability and detail fidelity.

</details>


### [266] [FLAIR: Frequency- and Locality-Aware Implicit Neural Representations](https://arxiv.org/abs/2508.13544)
*Sukhun Ko,Dahyeon Kye,Kyle Min,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 提出FLAIR解决现有INRs缺乏频率选择性等问题，在多任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有INRs缺乏频率选择性、空间定位和稀疏表示，有频谱偏差，难以捕捉高频细节。

Method: 提出FLAIR，包含用于频率选择和空间定位的RC - GAUSS激活函数，以及利用离散小波变换的Wavelet - Energy - Guided Encoding。

Result: 在2D图像表示和恢复、3D重建中始终优于现有INRs。

Conclusion: 所提出的FLAIR能有效解决现有INRs存在的问题。

Abstract: Implicit Neural Representations (INRs) leverage neural networks to map
coordinates to corresponding signals, enabling continuous and compact
representations. This paradigm has driven significant advances in various
vision tasks. However, existing INRs lack frequency selectivity, spatial
localization, and sparse representations, leading to an over-reliance on
redundant signal components. Consequently, they exhibit spectral bias, tending
to learn low-frequency components early while struggling to capture fine
high-frequency details. To address these issues, we propose FLAIR (Frequency-
and Locality-Aware Implicit Neural Representations), which incorporates two key
innovations. The first is RC-GAUSS, a novel activation designed for explicit
frequency selection and spatial localization under the constraints of the
time-frequency uncertainty principle (TFUP). The second is
Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet
transform (DWT) to compute energy scores and explicitly guide frequency
information to the network. Our method consistently outperforms existing INRs
in 2D image representation and restoration, as well as 3D reconstruction.

</details>


### [267] [Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery Using Spatially-Aware Visual Clustering](https://arxiv.org/abs/2508.13814)
*Diaa Addeen Abuhani,Marco Seccaroni,Martina Mazzarello,Imran Zualkernan,Fabio Duarte,Carlo Ratti*

Main category: cs.CV

TL;DR: 提出无监督聚类框架，结合街景图像视觉嵌入和空间种植模式估计城市树木生物多样性，应用于北美八城市效果良好，可用于无详细清单城市的生物多样性测绘和监测。


<details>
  <summary>Details</summary>
Motivation: 城市树木生物多样性重要，但多数城市缺乏树冠详细信息，实地调查成本高、耗时，有监督AI方法需标注数据且难以跨区域泛化。

Method: 引入无监督聚类框架，整合街景图像视觉嵌入和空间种植模式来估计生物多样性，无需标注数据。

Result: 应用于北美八个城市，能高保真恢复属级多样性模式，香农和辛普森指数与真实值的Wasserstein距离低，保留空间自相关性。

Conclusion: 该可扩展、细粒度方法可用于缺乏详细清单城市的生物多样性测绘，为低成本持续监测提供途径，支持公平获取绿化和城市生态系统的适应性管理。

Abstract: Urban tree biodiversity is critical for climate resilience, ecological
stability, and livability in cities, yet most municipalities lack detailed
knowledge of their canopies. Field-based inventories provide reliable estimates
of Shannon and Simpson diversity but are costly and time-consuming, while
supervised AI methods require labeled data that often fail to generalize across
regions. We introduce an unsupervised clustering framework that integrates
visual embeddings from street-level imagery with spatial planting patterns to
estimate biodiversity without labels. Applied to eight North American cities,
the method recovers genus-level diversity patterns with high fidelity,
achieving low Wasserstein distances to ground truth for Shannon and Simpson
indices and preserving spatial autocorrelation. This scalable, fine-grained
approach enables biodiversity mapping in cities lacking detailed inventories
and offers a pathway for continuous, low-cost monitoring to support equitable
access to greenery and adaptive management of urban ecosystems.

</details>


### [268] [Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks](https://arxiv.org/abs/2508.13744)
*Yeji Park,Minyoung Lee,Sanghyuk Chun,Junsuk Choe*

Main category: cs.CV

TL;DR: LVLMs perform poorly on multi - image tasks due to cross - image information leakage. FOCUS, a training - free decoding strategy, is proposed to mitigate this issue and improves performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: LVLMs' performance degrades on multi - image tasks because of cross - image information leakage, so a solution is needed.

Method: FOCUS masks all but one image with random noise, repeats this for all target images, aggregates logits, and contrastively refines them with a noise - only reference input.

Result: FOCUS consistently improves performance across four multi - image benchmarks and diverse LVLM families.

Conclusion: FOCUS is a general and practical solution for enhancing multi - image reasoning without extra training or architectural changes.

Abstract: Large Vision-Language Models (LVLMs) demonstrate strong performance on
single-image tasks. However, we observe that their performance degrades
significantly when handling multi-image inputs. This occurs because visual cues
from different images become entangled in the model's output. We refer to this
phenomenon as cross-image information leakage. To address this issue, we
propose FOCUS, a training-free and architecture-agnostic decoding strategy that
mitigates cross-image information leakage during inference. FOCUS sequentially
masks all but one image with random noise, guiding the model to focus on the
single clean image. We repeat this process across all target images to obtain
logits under partially masked contexts. These logits are aggregated and then
contrastively refined using a noise-only reference input, which suppresses the
leakage and yields more accurate outputs. FOCUS consistently improves
performance across four multi-image benchmarks and diverse LVLM families. This
demonstrates that FOCUS offers a general and practical solution for enhancing
multi-image reasoning without additional training or architectural
modifications.

</details>


### [269] [A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports](https://arxiv.org/abs/2508.13796)
*Enobong Adahada,Isabel Sassoon,Kate Hone,Yongmin Li*

Main category: cs.CV

TL;DR: 介绍Med - CTX框架用于可解释的乳腺癌超声分割，结合临床报告提升性能和可解释性，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 构建可解释的乳腺癌超声分割框架，提升计算机辅助诊断的置信度和透明度。

Method: 采用结合ViT和Swin变压器的双分支视觉编码器及不确定性感知融合进行病变描绘；用BioClinicalBERT编码临床语言，通过跨模态注意力与视觉特征结合。

Result: 在BUS - BRA数据集上，Dice分数达99%，IoU为95%，优于U - Net、ViT和Swin；消融实验显示临床文本对分割准确性和解释质量至关重要；实现良好的多模态对齐和置信度校准。

Conclusion: Med - CTX为可靠的多模态医学架构树立了新标杆。

Abstract: We introduce Med-CTX, a fully transformer based multimodal framework for
explainable breast cancer ultrasound segmentation. We integrate clinical
radiology reports to boost both performance and interpretability. Med-CTX
achieves exact lesion delineation by using a dual-branch visual encoder that
combines ViT and Swin transformers, as well as uncertainty aware fusion.
Clinical language structured with BI-RADS semantics is encoded by
BioClinicalBERT and combined with visual features utilising cross-modal
attention, allowing the model to provide clinically grounded, model generated
explanations. Our methodology generates segmentation masks, uncertainty maps,
and diagnostic rationales all at once, increasing confidence and transparency
in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice
score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and
Swin. Clinical text plays a key role in segmentation accuracy and explanation
quality, as evidenced by ablation studies that show a -5.4% decline in Dice
score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP
score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new
bar for trustworthy, multimodal medical architecture.

</details>


### [270] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: 研究多模态大语言模型（MLLMs）识别图像旋转角度的能力，引入RotBench基准测试，发现多数模型存在识别缺陷，与人类感知有差距。


<details>
  <summary>Details</summary>
Motivation: 探究MLLMs准确识别不同旋转角度（0°、90°、180°、270°）输入图像的能力。

Method: 引入包含350张图像的RotBench基准测试，测试多种MLLMs，尝试提供辅助信息、链式思维提示、多方向展示图像、投票机制和微调等方法。

Result: 多数模型能识别正立（0°）图像，部分能识别倒立（180°）图像，无法可靠区分90°和270°；多方向展示对推理模型有一定提升，投票机制改善弱模型性能；微调能提升180°图像识别，但不能改善90°和270°区分。

Conclusion: MLLMs在识别图像旋转的空间推理能力与人类感知存在显著差距。

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


### [271] [GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation](https://arxiv.org/abs/2508.14036)
*Ken Deng,Yunhan Yang,Jingxiang Sun,Xihui Liu,Yebin Liu,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: 提出DetailGen3D方法增强3D形状细节，实验证明有效且训练高效


<details>
  <summary>Details</summary>
Motivation: 现代3D生成方法输出的形状缺乏几何细节，受计算约束

Method: 通过潜在空间中依赖数据的流直接建模粗到细的转换，引入令牌匹配策略，设计匹配合成粗形状特征的训练数据

Result: 能有效增强不同3D生成和重建方法产生的形状

Conclusion: DetailGen3D实现高保真几何细节合成，且训练高效

Abstract: Modern 3D generation methods can rapidly create shapes from sparse or single
views, but their outputs often lack geometric detail due to computational
constraints. We present DetailGen3D, a generative approach specifically
designed to enhance these generated 3D shapes. Our key insight is to model the
coarse-to-fine transformation directly through data-dependent flows in latent
space, avoiding the computational overhead of large-scale 3D generative models.
We introduce a token matching strategy that ensures accurate spatial
correspondence during refinement, enabling local detail synthesis while
preserving global structure. By carefully designing our training data to match
the characteristics of synthesized coarse shapes, our method can effectively
enhance shapes produced by various 3D generation and reconstruction approaches,
from single-view to sparse multi-view inputs. Extensive experiments demonstrate
that DetailGen3D achieves high-fidelity geometric detail synthesis while
maintaining efficiency in training.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [272] [Reactive Semantics for User Interface Description Languages](https://arxiv.org/abs/2508.13610)
*Basile Pesin,Celia Picard,Cyril Allignol*

Main category: cs.PL

TL;DR: 本文为核心反应式UIDL Smalite提出指称语义模型，有望用于生成形式验证的UIDL编译器。


<details>
  <summary>Details</summary>
Motivation: 尽管UIDL广泛用于实现安全关键的GUI，但在其形式化和验证方面投入的工作较少。

Method: 为核心反应式UIDL Smalite提出指称语义模型。

Result: 提出了Smalite的指称语义模型。

Conclusion: 此初步工作可作为生成形式验证的UIDL编译器的垫脚石。

Abstract: User Interface Description Languages (UIDLs) are high-level languages that
facilitate the development of Human-Machine Interfaces, such as Graphical User
Interface (GUI) applications. They usually provide first-class primitives to
specify how the program reacts to an external event (user input, network
message), and how data flows through the program. Although these
domain-specific languages are now widely used to implement safety-critical
GUIs, little work has been invested in their formalization and verification.
  In this paper, we propose a denotational semantic model for a core reactive
UIDL, Smalite, which we argue is expressive enough to encode constructs from
more realistic languages. This preliminary work may be used as a stepping stone
to produce a formally verified compiler for UIDLs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [273] [Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios](https://arxiv.org/abs/2508.13182)
*Prateek Ranka,Fred Morstatter,Andrea Belz,Alexandra Graddy-Reed*

Main category: cs.DL

TL;DR: 提出用人工直觉方法，利用大语言模型生成元数据对科学摘要分类，通过实验证明方法可行性。


<details>
  <summary>Details</summary>
Motivation: 科学摘要分类对战略活动有用，但自动分类有挑战，现有元数据方法有局限，而专家分类轻松，因此想复制专家方法。

Method: 应用人工直觉方法，用大语言模型生成元数据，利用美国国家科学基金会公开摘要创建标签集，在中国国家自然科学基金摘要集上测试。

Result: 通过实验验证了该方法用于研究组合管理、技术侦察等战略活动的可行性。

Conclusion: 此方法在科学摘要分类及相关战略活动中具有可行性。

Abstract: Classification of scientific abstracts is useful for strategic activities but
challenging to automate because the sparse text provides few contextual clues.
Metadata associated with the scientific publication can be used to improve
performance but still often requires a semi-supervised setting. Moreover, such
schemes may generate labels that lack distinction -- namely, they overlap and
thus do not uniquely define the abstract. In contrast, experts label and sort
these texts with ease. Here we describe an application of a process we call
artificial intuition to replicate the expert's approach, using a Large Language
Model (LLM) to generate metadata. We use publicly available abstracts from the
United States National Science Foundation to create a set of labels, and then
we test this on a set of abstracts from the Chinese National Natural Science
Foundation to examine funding trends. We demonstrate the feasibility of this
method for research portfolio management, technology scouting, and other
strategic activities.

</details>


### [274] [The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold](https://arxiv.org/abs/2508.13234)
*Naixuan Zhao,Chunli Wei,Xinyan Zhang,Jiang Li*

Main category: cs.DL

TL;DR: 本文以AlphaFold对结构生物学家的影响为例，研究AI对跨学科合作模式的影响，发现AI促进跨学科合作效果有限。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域加速发展，但其对跨学科合作的机制和影响程度尚不明确，因此开展研究。

Method: 分析1247篇与AlphaFold相关的论文和7700名作者，运用文献计量分析和因果推断，对比AlphaFold采用者和非采用者的跨学科合作情况。

Result: AlphaFold仅使结构生物学 - 计算机科学的合作增加了0.48%，对其他学科无显著影响。

Conclusion: 仅靠人工智能在弥合学科鸿沟或促进有意义的跨学科合作方面效果有限。

Abstract: The acceleration of artificial intelligence (AI) in science is recognized and
many scholars have begun to explore its role in interdisciplinary
collaboration. However, the mechanisms and extent of this impact are still
unclear. This study, using AlphaFold's impact on structural biologists,
examines how AI technologies influence interdisciplinary collaborative
patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from
Scopus, we employ bibliometric analysis and causal inference to compare
interdisciplinary collaboration between AlphaFold adopters and non-adopters.
Contrary to the widespread belief that AI facilitates interdisciplinary
collaboration, our findings show that AlphaFold increased structural
biology-computer science collaborations by just 0.48%, with no measurable
effect on other disciplines. Specifically, AI creates interdisciplinary
collaboration demands with specific disciplines due to its technical
characteristics, but this demand is weakened by technological democratization
and other factors. These findings demonstrate that artificial intelligence (AI)
alone has limited efficacy in bridging disciplinary divides or fostering
meaningful interdisciplinary collaboration.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [275] [Machine Learning H-theorem](https://arxiv.org/abs/2508.14003)
*Ruben Lier*

Main category: cond-mat.stat-mech

TL;DR: 为理解H定理及其与时间箭头的关系，研究随机定向和定位的硬磁盘平衡，用基于DeepSets架构的模型捕捉H - 泛函不可逆性。


<details>
  <summary>Details</summary>
Motivation: H定理是热力学第二定律微观基础且有争议，为更好理解H定理及其与时间箭头的关系。

Method: 研究随机定向和定位、有周期性边界条件的硬磁盘的平衡，使用基于DeepSets架构的模型，该模型保证粒子标签的排列不变性。

Result: 未提及

Conclusion: 未提及

Abstract: H-theorem provides a microscopic foundation of the Second Law of
Thermodynamics and is therefore essential to establishing statistical physics,
but at the same time, H-theorem has been subject to controversy that in part
persists till this day. To better understand H-theorem and its relation to the
arrow of time, we study the equilibration of randomly oriented and positioned
hard disks with periodic boundary conditions. Using a model based on the
DeepSets architecture, which imposes permutation invariance of the particle
labels, we train a model to capture the irreversibility of the H-functional.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [276] [Analog computation with transcriptional networks](https://arxiv.org/abs/2508.14017)
*David Doty,Mina Latifi,David Soloveichick*

Main category: cs.CC

TL;DR: 本文证明合成转录网络控制转录因子产生而非降解在模拟计算上数学完备，还给出编译器将多项式ODE系统转化为转录网络。


<details>
  <summary>Details</summary>
Motivation: 模拟计算在生物系统中至关重要，转录网络模拟计算通常需控制转录和降解率，探究不控制降解是否可行。

Method: 理论证明控制转录因子产生在模拟计算上数学完备，并通过多个例子验证，还提供Python包编译器。

Result: 控制转录因子产生在模拟计算上与同时控制产生和降解的系统能力相当，可实现多种动态。

Conclusion: 为工程化新模拟动态提供无降解控制复杂性的系统方法，增进对自然转录电路能力的理解。

Abstract: Transcriptional networks represent one of the most extensively studied types
of systems in synthetic biology. Although the completeness of transcriptional
networks for digital logic is well-established, *analog* computation plays a
crucial role in biological systems and offers significant potential for
synthetic biology applications. While transcriptional circuits typically rely
on cooperativity and highly non-linear behavior of transcription factors to
regulate *production* of proteins, they are often modeled with simple linear
*degradation* terms. In contrast, general analog dynamics require both
non-linear positive as well as negative terms, seemingly necessitating control
over not just transcriptional (i.e., production) regulation but also the
degradation rates of transcription factors.
  Surprisingly, we prove that controlling transcription factor production
(i.e., transcription rate) without explicitly controlling degradation is
mathematically complete for analog computation, achieving equivalent
capabilities to systems where both production and degradation are programmable.
We demonstrate our approach on several examples including oscillatory and
chaotic dynamics, analog sorting, memory, PID controller, and analog extremum
seeking. Our result provides a systematic methodology for engineering novel
analog dynamics using synthetic transcriptional networks without the added
complexity of degradation control and informs our understanding of the
capabilities of natural transcriptional circuits.
  We provide a compiler, in the form of a Python package that can take any
system of polynomial ODEs and convert it to an equivalent transcriptional
network implementing the system *exactly*, under appropriate conditions.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [277] [Trust and Reputation in Data Sharing: A Survey](https://arxiv.org/abs/2508.14028)
*Wenbo Wu,George Konstantinidis*

Main category: cs.SI

TL;DR: 本文从数据共享视角审视信任与声誉管理系统（TRMSs），开发新分类法，分析其适用性，指出挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数据共享需信任，但现有TRMSs缺乏针对数据共享独特性的方法，因此要从数据共享视角研究TRMSs。

Method: 从数据共享视角研究TRMSs，开发系统设计、信任评估框架和评估指标的新分类法，系统分析现有TRMSs适用性。

Result: 开发了新分类法，分析了现有TRMSs在数据共享中的适用性。

Conclusion: 识别了TRMSs在大规模数据共享生态系统中的开放挑战，提出增强其可解释性、全面性和准确性的未来研究方向。

Abstract: Data sharing is the fuel of the galloping artificial intelligence economy,
providing diverse datasets for training robust models. Trust between data
providers and data consumers is widely considered one of the most important
factors for enabling data sharing initiatives. Concerns about data sensitivity,
privacy breaches, and misuse contribute to reluctance in sharing data across
various domains. In recent years, there has been a rise in technological and
algorithmic solutions to measure, capture and manage trust, trustworthiness,
and reputation in what we collectively refer to as Trust and Reputation
Management Systems (TRMSs). Such approaches have been developed and applied to
different domains of computer science, such as autonomous vehicles, or IoT
networks, but there have not been dedicated approaches to data sharing and its
unique characteristics. In this survey, we examine TRMSs from a data-sharing
perspective, analyzing how they assess the trustworthiness of both data and
entities across different environments. We develop novel taxonomies for system
designs, trust evaluation framework, and evaluation metrics for both data and
entity, and we systematically analyze the applicability of existing TRMSs in
data sharing. Finally, we identify open challenges and propose future research
directions to enhance the explainability, comprehensiveness, and accuracy of
TRMSs in large-scale data-sharing ecosystems.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [278] [Perspective: An outlook on fluorescence tracking](https://arxiv.org/abs/2508.13668)
*Lance W. Q. Xu,Steve Pressé*

Main category: physics.bio-ph

TL;DR: 本文追溯单分子追踪的发展，比较各方法优缺点，探讨新兴追踪技术的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 了解单分子追踪技术发展以推动下一代单分子研究。

Method: 追溯单分子追踪演变，比较不同荧光显微镜方法的优缺点。

Result: 对比了从传统宽场离线追踪到实时共聚焦追踪等方法的优劣。

Conclusion: 新兴物理启发追踪技术有潜力提升时空分辨率和计算数据效率，但也面临挑战。

Abstract: Tracking single fluorescent molecules has offered resolution into dynamic
molecular processes at the single-molecule level. This perspective traces the
evolution of single-molecule tracking, highlighting key developments across
various methodological branches within fluorescence microscopy. We compare the
strengths and limitations of each approach, ranging from conventional widefield
offline tracking to real-time confocal tracking. In the final section, we
explore emerging efforts to advance physics-inspired tracking techniques, a
possibility for parallelization and artificial intelligence, and discuss
challenges and opportunities they present toward achieving higher
spatiotemporal resolution and greater computational and data efficiency in
next-generation single-molecule studies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [279] [Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters](https://arxiv.org/abs/2508.13303)
*Yingfan Zhou,Philip Sanderink,Sigurd Jager Lemming,Cheng Fang*

Main category: cs.RO

TL;DR: 本文提出用Diff - MSM识别肌肉骨骼模型参数，经对比模拟，该方法优于现有基线方法，且在多领域有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 高保真个性化人体肌肉骨骼模型对人机交互系统模拟和安全验证很重要，但识别特定主体的模型参数具有挑战性，尤其是难以直接测量体内生物力学变量。

Method: 使用可微肌肉骨骼模型（Diff - MSM）结合端到端自动微分技术，从可测量的肌肉激活开始，经关节扭矩到可观察的运动，无需测量内部关节扭矩来同时识别肌肉和骨骼参数。

Result: 经大量对比模拟，该方法显著优于现有基线方法，对肌肉参数的估计准确性高，估计值的平均百分比误差低至0.05%。

Conclusion: 除人体肌肉骨骼建模和模拟外，Diff - MSM的新参数识别技术在肌肉健康监测、康复和运动科学等领域有应用潜力。

Abstract: High-fidelity personalized human musculoskeletal models are crucial for
simulating realistic behavior of physically coupled human-robot interactive
systems and verifying their safety-critical applications in simulations before
actual deployment, such as human-robot co-transportation and rehabilitation
through robotic exoskeletons. Identifying subject-specific Hill-type muscle
model parameters and bone dynamic parameters is essential for a personalized
musculoskeletal model, but very challenging due to the difficulty of measuring
the internal biomechanical variables in vivo directly, especially the joint
torques. In this paper, we propose using Differentiable MusculoSkeletal Model
(Diff-MSM) to simultaneously identify its muscle and bone parameters with an
end-to-end automatic differentiation technique differentiating from the
measurable muscle activation, through the joint torque, to the resulting
observable motion without the need to measure the internal joint torques.
Through extensive comparative simulations, the results manifested that our
proposed method significantly outperformed the state-of-the-art baseline
methods, especially in terms of accurate estimation of the muscle parameters
(i.e., initial guess sampled from a normal distribution with the mean being the
ground truth and the standard deviation being 10% of the ground truth could end
up with an average of the percentage errors of the estimated values as low as
0.05%). In addition to human musculoskeletal modeling and simulation, the new
parameter identification technique with the Diff-MSM has great potential to
enable new applications in muscle health monitoring, rehabilitation, and sports
science.

</details>


### [280] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: 构建了可实时视频流传输和语音交互的移动监控机器人，用树莓派等部件和开源软硬件，室内测试效果良好并讨论扩展。


<details>
  <summary>Details</summary>
Motivation: 构建可让用户通过手机或浏览器监控和操控的移动监控机器人。

Method: 使用两个树莓派4单元，FFmpeg传输视频，YOLOv3检测物体，Python库实现语音交互，Kinect RGB - D传感器提供视觉输入。

Result: 室内测试中，机器人能在CPU上以交互帧率检测常见物体，可靠识别命令并自动转化为行动。

Conclusion: 设计依赖现成硬件和开源软件，易复制，还讨论了系统的局限和实际扩展方向。

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


### [281] [MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence](https://arxiv.org/abs/2508.13534)
*Chao Tang,Anxing Xiao,Yuhong Deng,Tianrun Hu,Wenlong Dong,Hanbo Zhang,David Hsu,Hong Zhang*

Main category: cs.RO

TL;DR: 提出MimicFunc框架，可让机器人从单个人类视频学习工具操作技能并泛化到新工具，还能用于训练视觉运动策略，无需大量遥操作数据。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在模仿人类工具操作技能的泛化能力上不足，难以建立功能级对应关系。

Method: 提出MimicFunc框架，通过基于关键点抽象构建以功能为中心的局部坐标系（功能帧）来建立功能对应关系。

Result: 实验表明MimicFunc能使机器人从单个RGB - D人类视频中泛化技能，操作新工具完成功能等效任务，且生成的滚动数据可用于训练视觉运动策略。

Conclusion: MimicFunc框架有效解决了机器人模仿工具操作技能的泛化问题，且可避免大量遥操作数据收集。

Abstract: Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.

</details>


### [282] [Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998)
*Yifu Yuan,Haiqin Cui,Yaoting Huang,Yibin Chen,Fei Ni,Zibin Dong,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: 本文提出以‘指向’作为统一的中间表征，设计Embodied - R1模型，构建Embodied - Points - 200K数据集，用两阶段强化微调训练，在多个基准测试中表现优异，证明该方法可有效缩小机器人感知 - 行动差距。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI中因数据稀缺和具身异质性导致的‘所见与所做差距’问题。

Method: 提出‘指向’作为统一中间表征，定义四种核心具身指向能力；设计Embodied - R1模型；构建Embodied - Points - 200K数据集；采用两阶段强化微调（RFT）课程和专门的多任务奖励设计进行训练。

Result: Embodied - R1在11个具身空间和指向基准测试中达到了最先进的性能；在SIMPLEREnv中成功率达56.2%，在8个现实世界XArm任务中成功率达87.5%，比强基线提高62%；对各种视觉干扰表现出高鲁棒性。

Conclusion: 以指向为中心的表征与RFT训练范式结合，为缩小机器人感知 - 行动差距提供了有效且可推广的途径。

Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer "pointing" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.

</details>


### [283] [Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer](https://arxiv.org/abs/2508.13877)
*Rathnam Vidushika Rasanji,Jin Wei-Kocsis,Jiansong Zhang,Dongming Gan,Ragu Athinarayanan,Paul Asunda*

Main category: cs.RO

TL;DR: 提出Symbolically - Guided Decision Transformer (SGDT)框架用于多机器人协作，评估其在多种场景性能，是首个探索基于DT技术用于多机器人操作的工作。


<details>
  <summary>Details</summary>
Motivation: 强化学习在实际部署中有局限性，Decision Transformers在多机器人操作应用未充分探索，需新框架解决多机器人协作问题。

Method: 提出SGDT框架，神经符号规划器生成高级任务导向计划，目标条件决策变压器进行低级顺序决策。

Result: 在一系列任务场景包括零样本和少样本场景评估了SGDT性能。

Conclusion: SGDT框架实现了复杂多机器人协作任务中结构化、可解释和可泛化的决策。

Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.

</details>


### [284] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: 论文调查HRI领域‘社会背景’定义和用法，提出概念模型并应用，最后讨论开放研究问题。


<details>
  <summary>Details</summary>
Motivation: HRI领域研究者对‘社会背景’使用方式多样，导致沟通障碍，难以关联相关工作。

Method: 调查HRI文献中‘社会背景’的定义和用法，提出概念模型并应用于现有工作。

Result: 提出描述人机交互社会背景的概念模型，探讨一系列社会背景属性。

Conclusion: 讨论了理解和建模人机交互社会背景的开放研究问题。

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>
