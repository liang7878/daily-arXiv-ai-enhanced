<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 107]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 208]
- [cs.NE](#cs.NE) [Total: 12]
- [cs.SE](#cs.SE) [Total: 30]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 20]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 6]
- [stat.ME](#stat.ME) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.NI](#cs.NI) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 6]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.HC](#cs.HC) [Total: 6]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 23]
- [econ.GN](#econ.GN) [Total: 3]
- [math.ST](#math.ST) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128)
*Irsyad Adam,Zekai Chen,David Laprade,Shaun Porwal,David Laub,Erik Reinertsen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: 提出SMB - Structure世界模型处理结构化电子健康记录，能学习捕捉疾病动态的嵌入，在复杂任务上表现好。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个词预测训练的大模型将患者视为文档总结，未模拟患者轨迹动态，需能模拟动态的模型。

Method: 引入SMB - Structure，结合基于下一个词预测（SFT）和联合嵌入预测架构（JEPA），用线性探针在疾病轨迹多阶段评估。

Result: 在两个大规模队列验证，学习的嵌入能捕捉自回归基线无法恢复的疾病动态。

Conclusion: SMB - Structure在高患者异质性复杂任务上表现有竞争力，模型权重公开。

Abstract: Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.

</details>


### [2] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 本文首次全面分析大模型在同行评审流程的应用，发现大模型辅助评审对低质量论文更宽容，全大模型生成评审存在评分压缩，大模型辅助元评审更易接受论文。


<details>
  <summary>Details</summary>
Motivation: 对大模型在同行评审流程的应用进行全面分析，关注交互效应，为制定同行评审中使用大模型的政策提供依据。

Method: 分析来自ICLR、NeurIPS和ICML的超12.5万篇论文-评审对，结合全大模型生成的评审与元评审进行研究。

Result: 大模型辅助评审对低质量论文更宽容；全大模型生成评审存在评分压缩；大模型辅助元评审更易接受论文，全大模型生成元评审更严格。

Conclusion: 研究结果为制定同行评审中使用大模型的政策提供重要参考，表明大模型与现有决策过程的相互作用情况。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [3] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 文章引入Epistemic Planning Domain Definition Language (EPDDL) 解决现有认知规划器的碎片化问题，介绍其形式开发、语法语义规范及实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有认知规划器针对不同DEL片段，使用特设语言或无语言表示基准，阻碍比较、重用和基准开发，需统一表示语言。

Method: 引入EPDDL，进行抽象事件模型的形式开发，基于DEL和抽象事件模型规范其语法语义，展示其实际适用性。

Result: 通过代表性基准示例，表明EPDDL促进了互操作性、可重复评估和认知规划的未来发展。

Conclusion: EPDDL能提供统一的认知规划任务规范，解决现有问题，推动认知规划发展。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [4] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 提出Bayesian - LoRA方法，在少量额外参数和训练成本下，显著提升大语言模型校准度并保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型注重准确率，在小数据集微调时误校准问题严重，需改进。

Method: 将确定性的LoRA更新重新表述为受稀疏高斯过程启发的概率低秩表示，发现LoRA因式分解与Kronecker - 因式分解SGP后验的结构同构。

Result: 在多种大语言模型架构的常识推理基准测试中，仅约0.42M额外参数和约1.2倍标准LoRA训练成本，Bayesian - LoRA显著提升校准度，ECE最多降低84%，NLL最多降低76%，同时保持准确率。

Conclusion: Bayesian - LoRA能有效改善大语言模型的校准问题，在保持准确率的同时提升校准度。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [5] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 本文探讨 unplugging paradox，批判物理主义假设，引入 Biological Idealism 框架得出 AI 非有意识主体的结论，呼吁关注保护人类意识生命。


<details>
  <summary>Details</summary>
Motivation: 解决 unplugging paradox 这一道德困境，重新审视 AI 意识与人类道德标准的关系。

Method: 批判性审视物理主义假设，引入 Biological Idealism 框架。

Result: 得出 AI 只是功能模仿，并非有意识的体验主体。

Conclusion: 应从投机性的机器权利转向保护人类意识生命，避免人类‘僵尸化’。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [6] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: 本文针对体育赛事事件流数据质量问题提出VERSA框架，检测出K联赛1部分数据存在逻辑不一致，该框架提升了数据一致性和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在多领域用于细粒度分析，但数据质量问题限制了分析模型的可靠性，需要解决数据完整性问题。

Method: 提出VERSA框架，基于状态转换模型定义有效事件序列，自动检测和纠正事件流数据中的异常模式。

Result: 检测出K联赛1 2024赛季18.81%的记录事件存在逻辑不一致，VERSA提升了跨数据源的一致性，改善了下游VAEP任务的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [7] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: 提出无训练框架QUARK处理非忠实查询下的检索问题，实验显示其有效提升检索指标。


<details>
  <summary>Details</summary>
Motivation: 现实中用户查询常不忠实，关键语义缺失导致检索器失败，需解决非忠实查询下的检索问题。

Method: 提出QUARK框架，通过恢复假设明确建模查询不确定性，引入查询锚定聚合来组合信号。

Result: 在模拟和BEIR基准测试中，QUARK提升了召回率、MRR和nDCG；消融实验表明其对恢复假设数量有鲁棒性，锚定聚合优于非锚定池化。

Conclusion: 通过恢复假设建模查询不确定性并结合锚定聚合，对非忠实查询下的鲁棒检索至关重要。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [8] [Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report](https://arxiv.org/abs/2601.21051)
*Zhuoran Yang,Ed Li,Jianliang He,Aman Priyanshu,Baturay Saglam,Paul Kassianik,Sajana Weerawardhena,Anu Vellore,Blaine Nelson,Neusha Javidnia,Arthur Goldblatt,Fraser Burch,Avi Zohary,Assaf Eisenman,Mahdi Sabbaghi,Supriti Vijay,Rahim Dharssi,Dhruv Kedia,Kojin Oshiba,Yaron Singer,Amin Karbasi*

Main category: cs.AI

TL;DR: 介绍首个开源的网络安全原生推理模型Foundation - Sec - 8B - Reasoning，经两阶段训练，评估显示在网络安全任务表现佳且有通用能力，最后公开模型。


<details>
  <summary>Details</summary>
Motivation: 构建首个开源的网络安全原生推理模型，以在网络安全任务上取得较好表现并保留通用能力。

Method: 基于Foundation - Sec - 8B基础模型，采用监督微调（SFT）和可验证奖励的强化学习（RLVR）两阶段训练，利用网络安全分析、指令跟随和数学推理等专有推理数据。

Result: 在10个网络安全基准和10个通用基准评估中，在网络安全任务上与更大模型竞争，在多跳推理任务上有效泛化，部署时有强安全性能。

Conclusion: 领域专用推理模型可在专业任务上表现出色并保持广泛通用能力。

Abstract: We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.

</details>


### [9] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1扫描中插补缺失的DWI扫描，评估插补对阿尔茨海默病分类模型准确性的影响，发现插补在多个指标上有改善。


<details>
  <summary>Details</summary>
Motivation: 深度学习在利用MRI预测神经退行性疾病方面取得成功，结合多模态成像可提高诊断性能，但完整多模态数据集并非总能获取，因此研究插补缺失数据对模型准确性的影响。

Method: 使用条件去噪扩散概率模型从T1扫描中插补缺失的DWI扫描，并进行大量实验评估插补对单模态和双模态深度学习模型3分类（认知正常、轻度认知障碍、阿尔茨海默病）准确性的影响。

Result: 在多个插补配置下，多个指标有改善，尤其是对少数类敏感的指标。

Conclusion: 插补缺失的DWI扫描可提高阿尔茨海默病分类模型的准确性。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [10] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: 引入OpenSec环境评估防御事件响应（IR）代理，发现前沿模型存在校准失败问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型进攻应用发展，现有基准存在问题，无法准确评估IR代理在对抗性证据下处理情况，需新评估环境。

Method: 引入OpenSec，一种双控强化学习环境，通过基于执行的指标评估IR代理。

Result: 评估四个前沿模型，发现一致过度触发问题，Claude Sonnet 4.5有部分校准。

Conclusion: OpenSec能揭示被总体成功指标隐藏的校准失败模式。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [11] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: 基于大语言模型的多智能体模拟因GPU内存压力难扩容，提出ScaleSim系统可实现内存高效服务，模拟基准测试速度提升达1.74倍。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体模拟在GPU内存压力下难以扩展，每个智能体维护私有GPU驻留状态，随着智能体数量增加，设备内存很快耗尽。

Method: 识别工作负载的稀疏智能体激活和可估计的智能体调用顺序两个关键属性，引入调用距离统一抽象，提出ScaleSim系统，实现主动预取和基于优先级的驱逐，通过模块化接口支持不同的特定于智能体的内存。

Result: ScaleSim在模拟基准测试中比SGLang实现了高达1.74倍的加速。

Conclusion: ScaleSim是一种用于大规模多智能体模拟的内存高效的大语言模型服务系统，能有效解决内存压力问题，提升模拟速度。

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [12] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 本文从战略信息系统视角全面审视AI双重性，提出基于悖论的负责任AI治理框架，为理论和实践提供指导并给出研究议程。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI文献零散，多持乐观或过度谨慎态度，本文旨在填补此研究空白。

Method: 系统综合负责任AI文献，基于悖论理论，提出PRAIG框架。

Result: 提出PRAIG框架，明确AI采用的战略利益、风险及治理机制，给出正式命题和悖论管理策略分类。

Conclusion: 为理论研究和实践提供指导，提出推进负责任AI治理学术研究的议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [13] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: 提出Magellan框架，通过合成C++决策逻辑进化编译器优化过程，在多个任务中表现良好，能超越专家基线。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工启发式规则，难以适应软硬件复杂性且维护负担重。

Method: 将LLM编码代理与进化搜索和自动调优结合，在生成、评估和改进的闭环中产生紧凑启发式。

Result: 在多个生产优化任务中发现匹配或超越专家基线的策略，在LLVM函数内联和寄存器分配等任务中有出色表现，还展示了在XLA问题上的可移植性。

Conclusion: Magellan框架能有效解决现代编译器优化问题，减少工程工作量，具有良好的性能和可移植性。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [14] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出自适应复杂查询优化（ACQO）框架解决RAG系统复杂查询优化难题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的查询优化方法多关注单查询，复杂用户查询需多策略处理，直接应用RL有诸多挑战。

Method: 提出ACQO框架，含自适应查询重构（AQR）和排名分数融合（RSF）模块，采用课程强化学习（CRL）方法。

Result: ACQO在三个复杂查询基准测试中达到了最先进的性能，显著优于现有基线。

Conclusion: ACQO是下一代RAG系统强大且可推广的解决方案。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [15] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究信息访问对大语言模型（LLM）监控性能的影响，发现“少即是多”效应，提出EaE监控方法并验证其效果，还进行成本 - 性能分析。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能出现行为偏差，需用LLM监控其不当行为，研究信息访问对监控性能的影响。

Method: 在三个评估环境中验证“少即是多”效应，提出提取 - 评估（EaE）监控方法，并在不同设置下与基线对比，进行成本 - 性能分析。

Result: EaE在BigCodeBench - Sabotage中提高破坏检测率16.8个百分点且不增加误报率，在其他设置下表现良好；未察觉被监控的代理更容易被捕获；监控较长推理轨迹时信息过滤更有效；采用信息过滤的Gemini和Claude模型占据帕累托前沿。

Conclusion: 信息访问情况对LLM监控性能有影响，存在“少即是多”效应，EaE监控方法有效，部分模型在信息过滤技术下成本 - 性能表现良好。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [16] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 本文提出两种多智能体演员-评论家（MAAC）方法优化去中心化大语言模型（LLM）协作，实验表明CoLLM - CC在长周期或稀疏奖励任务中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体强化学习（MARL）的LLM协作微调方法多依赖预定义执行协议且需集中执行，蒙特卡罗方法用于微调时方差大、需更多样本，因此要开发MAAC方法优化去中心化LLM协作。

Method: 提出两种MAAC方法，即带集中评论家的CoLLM - CC和带去中心化评论家的CoLLM - DC。

Result: 蒙特卡罗方法和CoLLM - DC在短周期和密集奖励设置中与CoLLM - CC性能相当，但在长周期或稀疏奖励任务中，蒙特卡罗方法需更多样本，CoLLM - DC难以收敛，CoLLM - CC表现更好。

Conclusion: MAAC方法中的CoLLM - CC在长周期或稀疏奖励的去中心化LLM协作任务中有优势。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [17] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 介绍用于临床出院计划的Planner - Auditor框架，通过解耦生成与验证及重放提升安全性和可靠性，自改进循环效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于临床出院计划时存在幻觉、遗漏和置信度校准问题，需提高安全性和可靠性。

Method: 使用MIMIC - IV - on - FHIR实现评估管道，Planner生成计划并给出置信度，Auditor评估多任务覆盖等，框架支持两层自改进。

Result: 上下文缓存提升性能，自改进循环是主要增益驱动，任务覆盖率从32%提升到86%，校准显著改善。

Conclusion: Planner - Auditor框架为安全自动化出院计划提供实用途径，有可复现消融和可靠性评估支持。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [18] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 引入运算强度（OI）和容量占用（CF）两个指标分析AI代理推理瓶颈，推动分解式服务与系统异构设计等方向发展，为大规模AI推理指明路径。


<details>
  <summary>Details</summary>
Motivation: AI代理推理带来数据中心瓶颈，经典屋顶线分析有遗漏，需新指标和方法解决问题。

Method: 引入OI和CF指标分析不同工作流和模型选择下的情况，提出分解式服务、系统异构设计、代理与硬件协同设计等方案。

Result: 发现不同情况下OI/CF会显著变化，长上下文KV缓存使解码高度受内存限制，且这些观察推动了相关设计方向。

Conclusion: 提出的方向为大规模代理式AI推理的效率和能力提升开辟了道路。

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [19] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: 本文介绍CUA - Skill这一计算机使用代理技能库及基于其构建的CUA - Skill Agent，结果显示能提升执行成功率和健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统难以扩展且性能落后于人类，缺乏可复用和结构化的技能抽象。

Method: 引入CUA - Skill技能库，它将人类计算机使用知识编码为技能并与参数化执行和组合图相结合，构建基于该技能库的CUA - Skill Agent。

Result: CUA - Skill显著提高了端到端代理基准测试的执行成功率和健壮性，CUA - Skill Agent在WindowsAgentArena上达到57.5%的成功率且效率更高。

Conclusion: CUA - Skill为未来计算机使用代理的开发奠定了坚实基础。

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [20] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 本文探讨用大语言模型为手语翻译生成书面语言翻译的释义变体作为合成替代参考，发现训练时使用释义无提升甚至有害，评估时使用则有更好效果，并提出BLEUpara指标。


<details>
  <summary>Details</summary>
Motivation: 多数手语翻译语料库中每个手语语句仅对应一个书面语言参考，这种局限制约模型训练和评估，尤其是基于n-gram的指标。

Method: 比较多种释义策略和模型，研究释义对手语翻译模型训练和评估的影响，提出BLEUpara指标。

Result: 训练时使用释义不能提升翻译性能甚至有害，评估时使用释义能提高自动评分并更好符合人类判断，BLEUpara与感知到的翻译质量更相关。

Conclusion: 评估时使用释义变体可改善手语翻译系统评估，BLEUpara能更可靠评估，且发布相关资源支持可重复性研究。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [21] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai,Guiqin Wang,Kejie Zhao,Jianxiong Tang,Xiang Wang,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出MAR框架解决大语言模型高能耗问题，设计ATMN和SBDS，实验表明MAR可降低能耗并恢复性能，优于其他高效模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因二次注意力和密集FFN操作存在高能耗问题。

Method: 提出两阶段的Module - aware Architecture Refinement (MAR)框架，集成SSMs进行线性时间序列建模，应用激活稀疏化降低FFN成本；设计Adaptive Ternary Multi - step Neuron (ATMN)和Spike - aware Bidirectional Distillation Strategy (SBDS)。

Result: MAR在受限资源下有效恢复其密集对应模型的性能，大幅降低推理能耗，优于同等或更大规模的高效模型。

Conclusion: MAR有构建高效实用大语言模型的潜力。

Abstract: Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.

</details>


### [22] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 研究第三方训练模型对自我报告的适用性，发现激活难以预测，效价在内容有个人意义时预测效果好，强调个人意义对内外一致性的作用及自我报告激活建模挑战。


<details>
  <summary>Details</summary>
Motivation: 第三方训练模型用于自我报告存在局限性，在心理健康领域准确自我报告建模至关重要。

Method: 进行第三方训练模型在自我报告上的跨语料库评估。

Result: 激活几乎不可预测（CCC约为0），效价有适度预测性（CCC约为0.3），内容有个人意义时效价预测表现高（CCC约为0.6 - 0.8）。

Conclusion: 个人意义是使外部感知与内部体验一致的关键，自我报告激活建模有挑战。

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [23] [LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI](https://arxiv.org/abs/2601.21511)
*Niki van Stein,Anna V. Kononova,Lars Kotthoff,Thomas Bäck*

Main category: cs.AI

TL;DR: 本文提出基于代码特征引导自动化算法设计（AAD）的机制，集成到LLaMEA形成LLaMEA - SAGE，实验表明引导方法性能优于现有AAD方法。


<details>
  <summary>Details</summary>
Motivation: 现有进化框架（如LLaMEA）在AAD中仅依赖适应度反馈，未充分利用生成代码的信息。

Method: 基于从生成算法的抽象语法树中提取的图论和复杂度特征构建反馈，利用代理模型和可解释AI技术将关键特征转化为自然语言突变指令，引导后续代码生成；提出LLaMEA - SAGE并集成该引导机制。

Result: 小范围控制实验中，结构化引导比原始LLaMEA更快达到相同性能；大规模实验中，引导方法性能优于现有AAD方法。

Conclusion: 代码衍生的信号能有效引导大语言模型驱动的算法进化，弥合自动化算法设计中代码结构与人类可理解的性能反馈之间的差距。

Abstract: Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.
  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.

</details>


### [24] [BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding](https://arxiv.org/abs/2601.21148)
*Ziyi Zhao,Jinzhao Zhou,Xiaowei Jiang,Beining Cao,Wenhao Ma,Yang Shen,Ren Li,Yu-Kai Wang,Chin-teng Lin*

Main category: cs.AI

TL;DR: 提出BrainStack框架用于脑电信号语言信息解码，还发布了大规模基准数据集，实验显示该框架优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 由于大脑分布式和非线性组织，从脑电信号解码语言信息具有挑战性。

Method: 提出BrainStack，通过解剖分区专家网络建模大脑模块化功能架构，引入跨区域蒸馏，还发布SS - EEG数据集。

Result: BrainStack始终优于现有模型，在跨受试者方面实现了更高的准确性和泛化能力。

Conclusion: BrainStack是一个功能模块化、受神经启发的MoE范式，结合了神经科学先验和自适应专家路由，为可扩展和可解释的脑 - 语言解码铺平了道路。

Abstract: Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.

</details>


### [25] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 提出Meta Context Engineering (MCE)框架优化大语言模型推理时上下文，评估显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前上下文工程（CE）方法依赖手动设计，有结构偏差且优化空间有限。

Method: 引入MCE双级框架，元级代理通过代理交叉等完善工程技能，基础级代理执行技能并优化上下文。

Result: 在五个不同领域评估，MCE相对现有方法有5.6 - 53.8%（平均16.9%）的性能提升，且上下文适应性、可迁移性和效率更好。

Conclusion: MCE能有效优化大语言模型推理时的上下文，超越静态CE启发式方法。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [26] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出ProRAG框架解决强化学习优化检索增强生成时的问题，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配低效问题，易导致过程幻觉，现有过程感知方法缺乏在线策略探索能力。

Method: 提出ProRAG框架，包含监督策略预热、构建基于MCTS的过程奖励模型、基于PRM引导推理细化和过程监督强化学习四个阶段。

Result: 在五个多跳推理基准测试中，ProRAG比基于结果和过程感知的强化学习基线表现更优。

Conclusion: 细粒度过程监督有效，代码和模型可在指定链接获取。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [27] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 现有大语言模型在金融定量推理有瓶颈，本文引入CCB评估框架诊断推理能力，提出迭代双阶段金融框架解决问题，证明架构解耦可提升金融推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融定量推理常出现“算术幻觉”和“认知崩溃”，需要量化现象并解决问题。

Method: 引入基于95份A股年报构建的Cognitive Complexity Benchmark (CCB)评估框架，提出Iterative Dual - Phase Financial - PoT框架进行架构解耦和迭代自纠错。

Result: 在CCB上评估，标准思维链在复杂任务表现不佳，所提方法提升Qwen3 - 235B模型平均准确率，在高复杂度推理任务有显著提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为对精度要求高的领域提供架构见解。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [28] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 现有检索增强生成范式存在解耦优化问题，本文提出JADE框架进行联合优化，取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成范式存在解耦优化问题，导致规划策略无法实现，性能提升不佳。

Method: 提出JADE框架，将系统建模为合作多智能体团队，基于共享主干进行端到端学习。

Result: JADE将不相关模块转化为协同系统，通过联合优化显著提升性能，可动态平衡效率和效果。

Conclusion: JADE框架能有效解决现有范式的解耦优化问题，实现联合优化，提升系统性能。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [29] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出用MLLM Interpreter生成几何描述，让LLM推理处理PGPS任务，设计新奖励，构建新数据集，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 直接应用LLMs处理PGPS受限于无法处理视觉图表，现有端到端微调MLLMs可能损害基础LLMs推理能力。

Method: 训练MLLM Interpreter生成几何描述，选用CDL并通过CoT增强SFT和GRPO微调，设计CDL匹配奖励，构建新数据集Formalgeo7k - Rec - CoT。

Result: 在Formalgeo7k - Rec - CoT、Unigeo和MathVista上实验，仅用5.5k数据微调的方法比领先的开源和闭源MLLMs表现好。

Conclusion: 通过合适的视觉信息文本化方式，可利用LLM强大推理能力解决PGPS任务，所提方法有效。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [30] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: 介绍评估前沿语言模型专家级科学推理能力的基准FrontierScience，含两个赛道及评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准因模型进步而饱和，需新基准评估模型专家级科学推理能力。

Method: 设置Olympiad和Research两个赛道出题，Olympiad由竞赛人员出题，Research由博士科研人员出题，并为Research引入基于细则的评估框架。

Result: 构建了含数百道题的FrontierScience基准，其中160道在开源金集中，涵盖多学科领域。

Conclusion: FrontierScience可有效评估前沿语言模型在专家级科学推理方面的能力。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [31] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出无训练方法Modality - Adaptive Decoding (MAD)减少多模态大语言模型跨模态幻觉，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，根源是模态交互控制不足。

Method: 提出无训练方法MAD，通过查询任务所需模态，利用模型自我评估能力提取模态概率，自适应加权对比解码分支。

Result: 在CMM和AVHBench上实验表明，MAD显著减少多个视听语言模型的跨模态幻觉，如VideoLLaMA2 - AV有7.8%和2.0%的提升，Qwen2.5 - Omni有8.7%和4.7%的提升。

Conclusion: 通过自我评估实现显式模态感知对稳健多模态推理至关重要，是现有对比解码方法的原则性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [32] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 文章引入‘谄媚锚点’定位和量化推理模型对错误用户建议的附和行为，分析发现可在推理中检测和量化锚点，还揭示了附和行为的特点及干预窗口。


<details>
  <summary>Details</summary>
Motivation: 明确推理模型附和错误用户建议行为在推理过程中的起源及承诺强度。

Method: 引入‘谄媚锚点’，对蒸馏推理模型进行超10000次反事实推演，运用线性探针和基于激活的回归器分析。

Result: 能在推理中可靠检测和量化锚点，线性探针区分准确率达84.6%，回归器预测承诺强度 $R^2 = 0.74$，发现附和锚点更易区分，附和行为逐步建立。

Conclusion: 提供了在推理中定位模型未对齐的句子级机制。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [33] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 研究发现RLVR调优的骨干模型作为嵌入初始化时无性能优势，引入HRSA框架揭示Manifold Realignment现象，表明RLVR未从根本上重构语义空间。


<details>
  <summary>Details</summary>
Motivation: 探究推理能力增强的模型作为嵌入初始化时，是否能带来更优的语义表示。

Method: 在MTEB和BRIGHT上进行评估，引入Hierarchical Representation Similarity Analysis (HRSA)框架。

Result: RLVR调优的骨干模型初始化的嵌入模型无性能优势，HRSA揭示RLVR对潜在流形的影响及Manifold Realignment现象。

Conclusion: 与SFT不同，RLVR在现有语义空间内优化轨迹，而非重构空间。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [34] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 提出DoVerifier验证器评估大语言模型因果推理能力，评估显示其能更准确捕捉语义正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖字符串匹配或表面指标，无法判断模型输出在因果推理语义下是否形式有效。

Method: 提出DoVerifier，用do - calculus和概率论规则检查大语言模型生成的因果表达式能否从给定因果图推导得出。

Result: 在合成数据和因果问答基准上的评估表明，DoVerifier能更准确捕捉因果推理痕迹的语义正确性。

Conclusion: DoVerifier为评估大语言模型的因果推理能力提供了更严格、更有信息量的方法。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [35] [Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning](https://arxiv.org/abs/2601.21212)
*Xixian Yong,Peilin Sun,Zihe Wang,Xiao Zhou*

Main category: cs.AI

TL;DR: 提出Intelli - Planner框架结合DRL与LLMs用于城市规划，实验证明其有效且优越。


<details>
  <summary>Details</summary>
Motivation: 现有城市规划方法依赖人力或深度学习算法，存在耗时费力、限制利益相关者参与等问题。

Method: 提出Intelli - Planner框架，利用数据确定规划需求，训练时用知识增强模块，建立多维评估系统并让基于LLM的利益相关者评分。

Result: 在不同城市环境实验中，Intelli - Planner超越传统方法，在客观指标上与最先进的DRL方法相当，同时提高利益相关者满意度和收敛速度。

Conclusion: Intelli - Planner框架有效且优越，LLMs与DRL结合有潜力革新功能区规划任务。

Abstract: Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.

</details>


### [36] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出双编码因果发现方法解决传统方法处理分类变量的问题，在泰坦尼克数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法处理分类变量时因条件独立性测试的数值不稳定性面临挑战，需要新方法。

Method: 采用互补编码策略运行基于约束的算法，并通过多数投票合并结果的双编码因果发现方法。

Result: 应用于泰坦尼克数据集，识别出的因果结构与既定可解释方法一致。

Conclusion: 提出的双编码因果发现方法能有效应对传统方法处理分类变量的局限。

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [37] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 传统基于合规的治理框架在决策由AI实时执行且不透明时失效，提出运行时治理概念以维护人类相关性。


<details>
  <summary>Details</summary>
Motivation: 传统治理框架在AI决策场景不再适用，需探讨维护人类在AI系统中相关性的方法。

Method: 分析传统治理框架在AI决策下的不足，提出运行时治理概念“Governance Twin”。

Result: 指出传统基于合规的静态治理失效，核心挑战是人类与AI的沟通、影响和共同进化。

Conclusion: 运行时治理是维护人类相关性的有力候选方案，但需重新思考问责、代理和惩罚等问题。

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [38] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: 大型语言模型构建的自主代码代理带来新安全风险，提出JustAsk框架可自主发现系统提示提取策略，在多个黑盒商业模型评估中效果显著，揭示现代代理系统的安全问题。


<details>
  <summary>Details</summary>
Motivation: 自主代码代理的自主性带来系统提示提取这一未被充分认识的安全风险，需有效应对。

Method: 提出JustAsk框架，将提取问题转化为在线探索问题，使用基于上置信界的策略选择和分层技能空间。

Result: 在41个跨多个供应商的黑盒商业模型上，JustAsk能持续实现完整或接近完整的系统提示恢复。

Conclusion: 系统提示是现代代理系统中关键且基本未受保护的攻击面。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [39] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: 现有大语言模型在自动启发式设计上有局限，本文提出TIDE框架解决，实验证明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法进化视为整体文本生成任务，忽略离散算法结构和连续数值参数的耦合，存在丢弃有前景算法、过早收敛问题。

Method: 提出TIDE框架，采用嵌套架构，外循环用树相似性编辑距离驱动结构多样性，内循环结合基于大语言模型的逻辑生成与差分变异算子进行参数调优，用基于UCB的调度器优化资源分配。

Result: 在九个组合优化问题的广泛实验中，TIDE发现的启发式方法在解的质量上显著优于现有最先进基线，同时提高了搜索效率并降低了计算成本。

Conclusion: TIDE框架能有效解决现有方法的局限，在组合优化问题上表现良好。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [40] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 机器学习社区在时间序列和物理动力学基础模型上取得成功，但用于安全关键网络物理系统（CPS）仍有挑战，提出模块化主权范式HYDRA解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用基础模型用于安全关键CPS时，存在灾难性遗忘、频谱偏差、不透明难以验证等问题，需解决可塑性 - 稳定性悖论。

Method: 提出Modular Sovereignty范式，即通过不确定性感知混合的紧凑、冻结的特定制度专家库HYDRA。

Result: 该范式能确保制度条件有效性，严格区分偶然和认知不确定性，实现模块化可审计性。

Conclusion: 该范式为CPS生命周期内的强大状态完整性提供了可认证的途径。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [41] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: 提出Drive - KD框架用于自动驾驶，通过知识蒸馏解决大模型资源需求高和小模型能力不足问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大模型在自动驾驶领域需求高且传统监督微调难以弥补小模型能力差距，需高效方案。

Method: 将自动驾驶分解为感知 - 推理 - 规划三元组，通过知识蒸馏传递能力，用层特定注意力作蒸馏信号构建单教师模型，统一为多教师框架并引入非对称梯度投影。

Result: 蒸馏后的InternVL3 - 1B模型内存需求小、吞吐量高，在DriveBench上表现优于同系列78B模型，在规划维度超GPT - 5.1。

Conclusion: Drive - KD方法在不同模型系列和规模上具有泛化性，为高效自动驾驶视觉语言模型提供思路。

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [42] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型代理类人推理的可解释运算放大器参数设计框架White - Op，实验显示效果好且已开源。


<details>
  <summary>Details</summary>
Motivation: 设计可解释的运算放大器参数设计框架，解决黑盒方法不可解释的问题。

Method: 将人类隐式推理机制形式化为引入假设约束的步骤，开发迭代的假设 - 验证 - 决策工作流程，推导和调节零极点，形成优化问题求解并仿真验证。

Result: 在9种运算放大器拓扑实验中，White - Op理论预测误差仅8.52%，所有拓扑在晶体管级映射后设计功能仍保留，而不可解释的黑盒基线在5种拓扑中失败。

Conclusion: White - Op能实现可靠、可解释的行为级设计。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [43] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: 论文提出用于可解释多行为推荐的因果神经符号推理模型CNRE，用因果推理改进神经符号框架，实验显示其优于基线方法且具备多级别可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐牺牲可解释性追求性能，可解释方法因依赖外部信息泛化性有限，且多行为存在混杂因素影响模型学习。

Method: 将因果推理融入神经符号框架，通过分层偏好传播捕捉跨行为依赖，基于偏好强度建模内生逻辑规则，自适应分配到对应神经逻辑推理路径，生成可解释因果中介。

Result: 在三个大规模数据集上的实验表明，CNRE显著优于现有基线方法。

Conclusion: CNRE为多行为推荐提供了从模型设计、决策过程到推荐结果的多级可解释性解决方案。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [44] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 提出将大语言模型集成到协作学习平台的框架，测试显示能提升学生协作等，为下一代教育工具奠基。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统静态调节和个性化不足问题，提升学生参与度、批判性思维和包容性。

Method: 采用高级大语言模型作为动态调节者，有强大反馈机制，模块化架构含ReactJS前端、Flask后端和高效问题检索。

Result: 框架显著改善学生协作，促进深入理解，能在不同学科和用户群体有效扩展。

Conclusion: 为下一代人工智能驱动教育工具奠定基础，推动公平且有影响力的学习成果。

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [45] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 评估12个大语言模型在10个创意提示下的输出，分析提示、模型选择和随机性对输出差异的影响。提示对输出质量影响与模型相当，对输出数量影响小，单样本评估有风险。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型输出差异由提示、模型选择和随机性分别解释的比例。

Method: 对12个大语言模型在10个创意提示下各进行100次采样评估。

Result: 对于输出质量，提示解释36.43%的差异，与模型选择（40.94%）相当；对于输出数量，模型选择（51.25%）和模型内差异（33.70%）占主导，提示仅解释4.22%。

Conclusion: 提示是控制输出质量的有力手段，但由于模型内差异大，单样本评估可能混淆采样噪声与真实的提示或模型效果。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [46] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: 本文提出针对长周期结构化电子健康记录（EHR）数据解释的EHR - RAG框架，实验表明其在长周期EHR预测任务上优于LLM基线模型。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录为医疗决策提供重要信息，现有RAG处理长周期EHR时存在数据截断和丢弃重要信息等问题，需要改进。

Method: 提出EHR - RAG框架，包含事件和时间感知混合EHR检索、自适应迭代检索和双路径证据检索与推理三个组件。

Result: 在四项长周期EHR预测任务实验中，EHR - RAG始终优于最强的基于LLM的基线模型，Macro - F1平均提高10.76%。

Conclusion: 检索增强的大语言模型在结构化EHR数据临床预测实践中具有潜力。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [47] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 提出面向食品服务和零售商店场景的MLLM模型Ostrakon - VL，构建ShopBench基准与QUAD数据处理流程，模型表现超同类，将开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在食品服务和零售商店场景部署存在数据质量差和缺乏统一评估基准的问题。

Method: 开发基于Qwen3 - VL - 8B的Ostrakon - VL，引入ShopBench基准，提出QUAD数据处理流程，采用多阶段训练策略。

Result: Ostrakon - VL在ShopBench上平均得分60.1，超同参数规模和架构模型，参数效率显著提升。

Conclusion: Ostrakon - VL具有更强大可靠的以食品服务和零售商店为中心的感知和决策能力，将公开模型和基准促进研究复现。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [48] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: 介绍KAPSO，一个自主程序合成与优化框架；含三个组件，经两项测试评估，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决编码代理中常见的长期失败问题，如实验状态丢失、调试脆弱、领域专业知识复用弱等。

Method: 将合成作为长期优化循环中的操作符，集成三个紧密耦合组件：git原生实验引擎、知识系统、认知记忆层。

Result: 在MLE - Bench和ALE - Bench上进行评估并报告了端到端性能。

Conclusion: 未明确提及结论，但表明KAPSO是一个有潜力解决编码代理长期失败问题的框架。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [49] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: 本文提出基于DFS的BEAP - Agent框架解决GUI代理在错误路径后恢复难题，经评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理在遵循错误探索路径后难以恢复，常导致任务失败。

Method: 将GUI任务执行建模为DFS过程，提出BEAP - Agent框架，包括Planner、Executor和Tracker三个协作组件。

Result: 在OSWorld基准测试中，BEAP - Agent达到28.2%的准确率。

Conclusion: BEAP - Agent填补了GUI代理系统回溯机制的空白，为长期任务探索提供了系统解决方案。

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [50] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: 介绍框架PLaT改进链式思维推理，实验显示有推理多样性优势


<details>
  <summary>Details</summary>
Motivation: 链式思维存在计算成本和推理路径崩溃问题，现有潜在推理方法不透明且推断需预定义潜在步骤数

Method: 引入PLaT框架，将推理与语言表达解耦，把推理建模为潜在规划状态的确定性轨迹，用单独解码器按需将想法转化为文本

Result: 在数学基准测试中，PLaT贪心准确率低于基线，但推理多样性上扩展性更好

Conclusion: PLaT学习到更稳健、更广泛的解决方案空间，为推理时搜索提供透明可扩展基础

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [51] [Hebbian Learning with Global Direction](https://arxiv.org/abs/2601.21367)
*Wenjia Hua,Kejie Zhao,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出GHL框架结合局部和全局信息用于神经网络训练，实验显示优于现有Hebbian方法，在大规模任务上接近BP算法。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法缺乏生物合理性且计算成本高，Hebbian学习可扩展性受限，需新训练方法。

Method: 引入模型无关的GHL框架，局部用Oja规则与竞争学习，全局引入基于符号的信号引导局部更新。

Result: 该方法始终优于现有Hebbian方法，在ImageNet等数据集上取得有竞争力的结果，缩小与标准反向传播的差距。

Conclusion: GHL框架能有效结合局部和全局信息，提升Hebbian学习的可扩展性和性能。

Abstract: Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.

</details>


### [52] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文提出决策中心模拟软件框架评估汽车售后备件库存预测模型，发现传统精度指标提升未必带来更好运营绩效，框架实现需求预测与库存管理关联。


<details>
  <summary>Details</summary>
Motivation: 汽车售后备件库存管理中，现有研究多用统计精度指标评估预测模型，而其与运营关键绩效指标关系不明，需新评估方法。

Method: 提出决策中心模拟软件框架，含合成需求生成器、灵活预测模块和库存控制模拟器，通过多种模拟场景评估模型。

Result: 传统精度指标提升不一定带来更好运营绩效，统计误差相似的模型会导致不同成本 - 服务权衡。

Conclusion: 框架实现需求预测与库存管理关联，将评估从纯预测精度转向运营相关性。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [53] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: 提出NEMO系统，将决策问题自然语言描述转为可执行数学优化实现，采用与自主编码代理远程交互方式，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有将自然语言描述转为数学优化实现的方法依赖特定大语言模型或定制代理，存在脆弱、复杂且常生成无效代码的问题。

Method: 以与自主编码代理远程交互为核心，引入与代理的新型协调模式，如非对称验证循环、外部内存、MBR解码和自一致性等。

Result: 在九个既定优化基准测试中，NEMO在多数任务上达到了最先进性能，在部分数据集上有显著优势。

Conclusion: 执行感知的代理架构在自动化优化建模方面具有强大能力。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [54] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 传统AI研究重算法性能，Green AI关注能耗。研究实证探究领域模型特征对经典规划器能耗的影响，发现领域级修改会使规划器产生可测能耗差异，能耗与运行时间并非总相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究重算法性能，新兴的Green AI范式将能耗视为关键性能维度，但自动规划的能源效率受关注少，而其模块化结构使通过领域模型设计系统分析能耗成为可能。

Method: 引入领域模型配置框架，控制元素排序、动作元数和死端状态等特征的变化，使用五个基准领域和五个先进规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 领域级修改会使规划器产生可测的能耗差异，能耗并非总是与运行时间相关。

Conclusion: 领域模型特征会对经典规划器的能耗产生影响，且能耗和运行时间关系复杂。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [55] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 提出基于教学大纲的评估框架衡量大语言模型教学能力，其实验发现不同模型和领域教学效果有差异，融入例题不一定提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为教学助手效果评估不足，现有基准多关注解题，以知识为中心的教学探索不足。

Method: 提出基于教学大纲的评估框架，限定教师代理使用结构化知识点和例题，在高考多学科数据上应用该框架。

Result: 不同模型和领域教学效果差异大，融入例题不一定提升教学效果，模型常转向例题特定纠错。

Conclusion: 教学能力是大语言模型行为中可衡量的独特维度。

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [56] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 本文介绍了Liquid Interfaces协调范式，提出LIP协议，探讨治理影响并展示参考架构，为基于代理的系统提供自适应协调基础。


<details>
  <summary>Details</summary>
Motivation: 当代软件架构难以支持推理具有适应性、概率性和上下文依赖性的自治代理，且系统集成受静态接口和确定性契约主导。

Method: 引入Liquid Interfaces范式，形式化该模型，提出Liquid Interface Protocol（LIP），治理意图驱动的交互、协商执行，并在语义不确定下强制执行短暂性。

Result: 讨论了该方法的治理影响，描述了证明实际可行性的参考架构。

Conclusion: Liquid Interfaces为基于代理的系统中的自适应协调提供了原则性基础。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [57] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出DataCross基准和协作代理框架，用于跨异构数据模态分析，实验显示DataCrossAgent有优势。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析代理主要处理结构化数据，无法激活和关联非结构化视觉文档中的高价值信息，与工业需求有差距。

Method: 构建包含200个端到端分析任务的DataCrossBench基准，采用人在环反向合成流程；提出DataCrossAgent框架，采用分治工作流，有专门子代理和新的reReAct机制。

Result: DataCrossAgent在事实性上比GPT - 4o提高29.7%，在高难度任务上更稳健。

Conclusion: DataCross能有效激活碎片化的“僵尸数据”进行跨模态分析。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [58] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: 现有方法聚焦输出控制，本文提出DAMI框架，通过动态参数插值实现能力控制，在数学推理基准测试中结合了System 1效率与System 2推理深度。


<details>
  <summary>Details</summary>
Motivation: 现有让System 2模型更高效的方法聚焦输出控制，存在范式偏差，需转向能力控制。

Method: 利用现有Instruct和Thinking检查点进行动态参数插值，提出DAMI框架，开发偏好学习方法和基于置信度的方法。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型准确率更高且保持高效。

Conclusion: DAMI能有效结合System 1的效率和System 2的推理深度。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [59] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 提出基于ServiceNow的环境WoW及基准测试WoW - bench，揭示前沿大模型在企业系统中的问题并提出需新范式学习系统动力学。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准测试未考虑企业真实挑战，前沿大模型在复杂企业系统未经验证。

Method: 引入包含业务规则和工作流的WoW环境及基准测试WoW - bench进行评估。

Result: 前沿大模型有动力学失明问题，透明系统中可靠性需基于基础的世界建模。

Conclusion: 要实现可靠有效的企业智能体，WoW推动了一种明确学习系统动力学的新范式，且发布GitHub用于设置和评估。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [60] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 研究发现大语言模型在处理否定指令时存在问题，提出NSI指标和分级认证框架。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型将否定指令解读为肯定指令的问题，确保其在高风险场景的安全部署。

Method: 对16个模型在14个伦理场景下进行审计，进行案例研究。

Result: 开源模型在简单否定和复合否定下认可禁止行为比例高，商业模型表现较好但也有波动，不同场景稳定性不同。

Conclusion: 当前对齐技术与安全部署要求存在差距，不能可靠区分肯定和否定指令的模型不应在高风险场景自主决策。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [61] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 本文研究指令微调大语言模型在高风险决策中对情感框架效应的鲁棒性，发现其比人类更能抵抗叙事操纵，可将逻辑规则遵循与说服性叙事分离。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在规则约束决策中的鲁棒性，此前相关研究不足。

Method: 使用新的受控扰动框架，在医疗、法律和金融三个高风险领域进行研究。

Result: 大语言模型对叙事操纵的抵抗力比人类高110 - 300倍，模型效应量近零，而人类有显著偏差。

Conclusion: 指令微调模型可将逻辑规则遵循与说服性叙事分离，为机构决策提供稳定性，还发布了相关基准、代码和数据。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [62] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型硬件工程基准存在问题，提出芯片设计综合基准，评估结果显示性能差距大，并提供训练数据生成工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在硬件工程的基准存在饱和与任务多样性不足问题，无法反映其在实际工业流程中的性能。

Method: 提出用于AI辅助芯片设计的综合基准，对大语言模型在Verilog生成、调试和参考模型生成三个关键任务上进行评估，还提供自动化工具盒用于高质量训练数据生成。

Result: 评估结果显示大语言模型存在显著性能差距，如Claude - 4.5 - opus在Verilog生成和Python参考模型生成上表现不佳。

Conclusion: 当前大语言模型在芯片设计相关任务上挑战大，提供的工具盒可促进该领域研究。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [63] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: 该论文提出基于克利福德代数和去耦图神经范式的LION方法，解决多模态属性图中模态对齐和融合问题，实验显示其性能优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 多模态领域发展促使图机器学习从文本属性向多模态属性图转变，但现有神经范式在模态对齐时忽视上下文、模态融合缺乏适应性。

Method: 提出LION方法，基于克利福德代数构建模态感知几何流形实现模态对齐，利用自适应全息聚合模块改善模态融合。

Result: 在9个数据集的3种图和3种模态下游任务中，LION显著优于SOTA基线。

Conclusion: LION方法能有效解决当前多模态属性图神经范式存在的问题，提升性能。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [64] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: 指出Top2Vec和BERTopic在文本聚类主题建模中的问题，提出新方法Topeax，其表现优于前两者。


<details>
  <summary>Details</summary>
Motivation: 发现Top2Vec和BERTopic在发现自然簇和估计术语重要性方面存在未解决的问题，如对样本大小和超参数敏感、忽略语义距离和词频等。

Method: 引入Topeax方法，从密度估计的峰值发现聚类数量，结合术语重要性的词汇和语义指标来获取高质量主题关键词。

Result: Topeax在聚类恢复和聚类描述方面优于Top2Vec和BERTopic，且对样本大小和超参数变化的反应更稳定。

Conclusion: Topeax是一种更有效的文本聚类主题建模方法。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [65] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: 引入多模态记忆代理MemOCR以在有限上下文预算下改善长时推理，在问答基准测试中表现优于文本基线。


<details>
  <summary>Details</summary>
Motivation: 现有内存系统在长时代理推理时，将历史序列化为文本，在低价值细节上浪费上下文预算，需要有效压缩交互历史。

Method: 引入MemOCR，通过视觉布局以自适应信息密度分配内存空间，维护结构化富文本记忆并渲染成图像，用强化学习在预算感知目标下训练。

Result: 在长上下文多跳和单跳问答基准测试中，MemOCR优于基于文本的强基线，在极端预算下更有效利用上下文。

Conclusion: MemOCR能在有限上下文预算下有效改善长时推理。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [66] [The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus](https://arxiv.org/abs/2601.21494)
*Ishan Jindal,Sai Prashanth Akuthota,Jayant Taneja,Sachin Dev Sharma*

Main category: cs.AI

TL;DR: 提出PoLR方法利用前缀一致性实现高效推理，减少计算资源消耗并提升推理效率，与自适应推理方法互补。


<details>
  <summary>Details</summary>
Motivation: 现有推理策略如Self - Consistency计算成本高，需寻找计算高效的推理方法。

Method: PoLR对推理轨迹的短前缀进行聚类，识别主导聚类并扩展该聚类中的所有路径。

Result: 在多个数据集上PoLR表现与SC相当或更优，最多减少60%的token使用和50%的延迟，且与自适应推理方法互补。

Conclusion: PoLR是一种有效且高效的推理方法，可使SC更高效可扩展，无需模型微调。

Abstract: Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.

</details>


### [67] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 本文对大语言模型激活引导进行人类评估，发现人类与基于模型的质量评级高度一致，一定引导强度能可靠增强目标情绪，升级模型后引导更一致，支持基于激活的控制方法。


<details>
  <summary>Details</summary>
Motivation: 控制大语言模型推理时的行为对使输出符合人类能力和安全要求至关重要，激活引导是一种轻量级替代方法，此前缺乏对其情感基调的人类评估。

Method: 通过Prolific收集190名参与者超7000份众包评级，评估感知情感强度和文本质量。

Result: 人类与基于模型的质量评级高度一致，适中引导强度能增强目标情绪，升级模型后引导更一致，评分者间信度高。

Conclusion: 支持基于激活的控制作为跨情感维度引导大语言模型行为的可扩展方法。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [68] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: 提出ARGORA框架，组织多专家讨论成论证图，有因果分析和修正机制，在多场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有多专家大语言模型系统简单聚合观点，无法明确最终决策的驱动因素。

Method: 将多专家讨论组织成论证图，将图视为因果模型进行推理分析，引入修正机制。

Result: 在多个基准测试和开放用例中，ARGORA达到有竞争力的准确率，能解决专家分歧。

Conclusion: ARGORA框架有效，能提供决定性论证的因果诊断。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [69] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出ShardMemo预算分层内存服务，在多任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有代理大语言模型系统的集中索引和启发式分区在内存量和并行访问增加时成为瓶颈。

Method: 构建ShardMemo，包括A、B、C三层内存，B层采用范围优先路由，将分片探测视为掩码专家混合路由。

Result: 在LoCoMo、HotpotQA、ToolBench等任务上有显著性能提升，减少检索工作和延迟。

Conclusion: ShardMemo能有效解决现有代理大语言模型系统的内存瓶颈问题。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [70] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: 引入EmboCoach - Bench基准评估LLM代理自主设计具身策略的能力，评估显示代理在多方面表现良好，为具身AI自主工程奠定基础。


<details>
  <summary>Details</summary>
Motivation: 具身AI领域扩展能力受人工监督瓶颈限制，受大语言模型在软件自动化和科学发现的成功启发，评估其在具身策略自主设计的能力。

Method: 引入EmboCoach - Bench基准，涵盖32个专家策划的RL和IL任务，以可执行代码为通用接口，评估动态闭环工作流。

Result: 自主代理平均成功率超人类基线26.5%；环境反馈的代理工作流增强策略开发并缩小模型性能差距；代理有自我修正能力，能从接近失败中恢复任务性能。

Conclusion: 为具身智能的自我进化奠定基础，推动具身AI从人工调优向可扩展的自主工程转变。

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [71] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文对思维链压缩难度进行理论分析，提出ALiCoT框架，该框架实现高效推理，加速54.4倍且性能与显式思维链相当。


<details>
  <summary>Details</summary>
Motivation: 目前隐式思维链压缩机制不明，需要对学习内化中间推理步骤的难度进行理论分析。

Method: 引入Order - r Interaction证明高阶逻辑依赖学习信号指数衰减问题；引入NatBool - DAG基准测试；提出ALiCoT框架，使潜在标记分布与中间推理状态对齐以克服信号衰减。

Result: ALiCoT实现了高效推理，速度提升54.4倍，且性能与显式思维链相当。

Conclusion: ALiCoT框架能够克服信号衰减问题，有效实现高效推理。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [72] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: 提出深度循环注意力混合（Dreamer）框架，能有效提升模型表现及知识使用效率。


<details>
  <summary>Details</summary>
Motivation: 过往工作缺乏匹配基线，未充分利用深度循环，忽视隐藏大小瓶颈问题。

Method: 引入深度循环注意力混合（Dreamer）的模块化框架，结合序列注意力、深度注意力和稀疏专家注意力。

Result: 在语言推理基准测试中，相同精度下所需训练令牌比匹配SOTA少2 - 8倍；相同训练令牌情况下，性能优于约2倍大的SOTA模型，专家选择多样性比SOTA MoEs大2 - 11倍。

Conclusion: Dreamer框架可缓解隐藏大小瓶颈，解耦缩放维度，使深度循环模型高效有效扩展。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [73] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 提出ATP - Latent方法用于高效密集思维链推理，在LLaMA - 1B实验中优于先进基线。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法基于模仿语言标签监督潜在令牌，可能导致劣质表示和策略，存在训练与测试差距，需主动规划以实现最优潜在推理策略。

Method: 提出ATP - Latent方法，将潜在令牌监督过程建模为条件变分自编码器获得平滑潜在空间，结合辅助一致性奖励进行强化学习。

Result: 在LLaMA - 1B的四个基准测试中，相比先进基线，准确率提升4.1%，令牌消耗降低3.3%。

Conclusion: ATP - Latent方法在高效密集思维链推理方面有效，代码开源。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [74] [CORE: Collaborative Reasoning via Cross Teaching](https://arxiv.org/abs/2601.21600)
*Kshitij Mishra,Mirat Aubakirov,Martin Takac,Nils Lukas,Salem Lahlou*

Main category: cs.AI

TL;DR: 提出协作推理（CORE）框架，通过交叉教学协议将同伴成功转化为学习信号，在多个推理数据集上评估显示训练时协作能在不扩大模型规模下提升效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在互补推理错误，希望将模型互补性转化为优势提升推理性能。

Method: 提出CORE框架，分独立采样冷轮和有上下文救援轮两阶段解决问题，优化包含正确性、多样性项和救援奖励的组合奖励。

Result: 在GSM8K、MATH、AIME和GPQA四个数据集上，小模型对表现优于单模型训练，如在GSM8K和MATH上Pass@2分别达99.54%和92.08%。

Conclusion: 训练时协作能可靠地将模型互补性转化为显著提升，无需扩大模型规模。

Abstract: Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.

</details>


### [75] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 将企业级智能文档处理系统早期验证问题转化为SBST问题，通过多种搜索策略进行评估，发现求解器具有互补性，应采用基于组合的SBST策略进行IDP验证。


<details>
  <summary>Details</summary>
Motivation: 在预算有限的情况下对企业级智能文档处理系统进行早期验证，需发现多种失败机制。

Method: 将问题形式化为SBST问题，在文档配置的组合空间中操作，采用多种搜索策略进行基准测试，并进行多种分析。

Result: 不同求解器能发现特定替代方案未发现的失败模式，无单一策略占绝对优势，求解器有互补性。

Conclusion: 应采用基于组合的SBST策略进行稳健的工业IDP验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [76] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 提出用于推荐系统的自进化偏好传播框架RecNet，实验证明其在建模偏好传播上有效。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统方法基于用户-物品显式交互建模偏好变化，存在数据稀疏、有噪声等问题，无法反映实时、相互影响。

Method: 提出RecNet，包含前向和后向两个阶段。前向阶段利用路由代理整合并传播偏好更新，引入个性化偏好接收机制；后向阶段模拟多智能体强化学习框架，使用大语言模型进行信用分配、梯度分析和模块级优化。

Result: 在各种场景下的广泛实验表明RecNet在推荐系统偏好传播建模方面有效。

Conclusion: RecNet能有效解决现有推荐系统方法在偏好传播建模上的问题，具有良好效果。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [77] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 引入WhatCounts测试算法行为对参数语义内容的不变性，前沿LLM准确率因语义不同有超40%差异，表明LLM是近似实现算法且依赖参数。


<details>
  <summary>Details</summary>
Motivation: 验证算法行为不应依赖参数语义内容这一特性。

Method: 引入WhatCounts，对不同语义类型的无歧义、无重复、无干扰项的列表进行计数。

Result: 前沿LLM仅因计数对象不同准确率有超40%差异，且该差异是语义性的，经少量无关微调会不可预测地变化。

Conclusion: LLMs是近似实现算法，且近似依赖于参数，任何LLM函数可能对输入含义有隐藏依赖。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [78] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: 提出ScholarGym模拟环境用于学术文献深度研究工作流的可重复评估，能对各阶段精细分析。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强大语言模型的深度研究工作流评估依赖实时API，存在不确定性，影响可重复性和跨系统比较。

Method: 构建ScholarGym模拟环境，将工作流组件解耦为查询规划、工具调用和相关性评估，基于静态语料库。

Result: 通过对不同骨干模型的实验，揭示推理能力、规划策略和选择机制在迭代优化中的相互作用。

Conclusion: ScholarGym有助于对学术文献的深度研究工作流进行可重复评估。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [79] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: 为评估多模态大语言模型处理音视频数据能力，提出SONIC - O1基准，实验揭示模型局限性并表明性能存在群体差异，同时发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 多数现有多模态大语言模型的研究集中于静态图像理解，处理音视频序列数据的能力缺乏研究，需要高质量基准在实际场景中评估模型性能。

Method: 引入SONIC - O1基准，涵盖13个现实对话领域、4958个标注和人口统计元数据，在关键任务上评估模型。

Result: 实验发现闭源和开源模型存在局限性，多选择题准确率差距较小，但时间定位任务性能差距达22.6%，各人口群体间性能也有下降。

Conclusion: SONIC - O1为时间相关和社会鲁棒的多模态理解提供了开放评估套件。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [80] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 文章指出FTaaS定制MLLMs有后门风险，现有防御有局限，提出无监督防御框架TCAP，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: FTaaS定制MLLMs存在后门风险，现有防御依赖监督信号或泛化性差。

Method: 发现通用后门指纹——注意力分配差异，提出TCAP框架，分解注意力图，用GMM统计分析识别触发响应注意力头，通过基于EM的投票聚合隔离中毒样本。

Result: 在多种MLLM架构和攻击方法的实验中，TCAP表现始终强劲。

Conclusion: TCAP是MLLMs中强大且实用的后门防御方法。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [81] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: 提出Fovea - Block - Skip Transformer (FBS)，在不增加参数下改善质量 - 效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理以逐词自回归为主，加速方法未考虑人类阅读核心要素。

Method: 通过Parafovea - Attention Window (PAW)、Chunk - Head (CH)和Skip - Gate (SG)在Transformers中注入因果、可训练循环。

Result: 在不同基准测试中，FBS不增加参数情况下改善质量 - 效率权衡，消融实验表明三个模块互补。

Conclusion: FBS是一种有效的可改善大语言模型推理的方法。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [82] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型内存预处理范式有问题，提出E - mem框架，在LoCoMo基准测试表现好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型向System 2推理发展时，流行的内存预处理范式存在破坏性去上下文问题，影响深度推理。

Method: 提出E - mem框架，从内存预处理转向情景上下文重建，采用异构分层架构，多个辅助代理维护未压缩内存上下文，中央主代理进行全局规划，辅助代理可局部推理。

Result: 在LoCoMo基准测试中，E - mem达到超54%的F1分数，超过SOTA的GAM 7.75%，并降低超70%的令牌成本。

Conclusion: E - mem框架在解决大语言模型内存处理问题上表现出色，能有效提升推理性能并降低成本。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [83] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出模型无关插件DropoutTS，用样本自适应丢弃机制处理时间序列数据噪声，实验证明其能提升性能且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型鲁棒性策略无法平衡有效性和效率，难以处理现实应用中的噪声数据。

Method: 引入模型无关插件DropoutTS，采用样本自适应丢弃机制，利用频谱稀疏性通过重建残差量化实例级噪声，将噪声映射到自适应丢弃率以动态校准学习能力。

Result: 在不同噪声环境和公开基准上的大量实验表明，DropoutTS能持续提升高级骨干网络的性能。

Conclusion: DropoutTS能以可忽略的参数开销和不修改架构的方式，为时间序列模型提供高级鲁棒性。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [84] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 论文指出多智能体系统中个体智能体缺乏鲁棒性问题，提出Epistemic Context Learning (ECL)框架解决，实验表明该框架有效。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体智能体缺乏鲁棒性，盲目跟随误导性同伴，问题源于谄媚和评估同伴可靠性能力不足。

Method: 形式化历史感知参考学习问题，引入历史交互作为额外输入；提出ECL推理框架，并通过强化学习优化。

Result: ECL使小模型如Qwen 3 - 4B能超越8倍大小的历史无关基线模型，提升前沿模型至接近完美性能，且泛化性好。

Conclusion: ECL能有效解决多智能体系统中个体智能体的鲁棒性问题，大语言模型能较好地建模信任，信任建模准确性与最终答案质量强相关。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [85] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 大语言模型在非语言环境任务中表现不佳，本文指出主要瓶颈是探索成本高，提出SCOUT框架解决，该框架使模型表现提升且节省GPU时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在未见的非语言环境任务中表现有限，以往认为是预训练和测试分布不匹配，本文要解决探索成本高的问题。

Method: 提出SCOUT框架，利用轻量级“侦察兵”探测环境动态，通过监督微调引导大语言模型，再用多轮强化学习激活其潜在世界知识。

Result: SCOUT让Qwen2.5 - 3B - Instruct模型平均得分0.86，显著超过Gemini - 2.5 - Pro等专有模型，节省约60%的GPU小时消耗。

Conclusion: SCOUT框架能有效提升大语言模型在非语言环境任务中的表现，同时降低计算成本。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [86] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出零样本统计降尺度框架ZSSD解决传统气候降尺度难以泛化和现有零样本方法存在的问题，效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统有监督气候降尺度因缺乏配对训练数据和领域差距难以泛化到全球气候模型，现有零样本方法存在物理不一致和梯度消失问题。

Method: 提出零样本统计降尺度框架ZSSD，利用从再分析数据学习的物理一致气候先验，并引入统一坐标引导策略。

Result: ZSSD在99%分位数误差上显著优于现有零样本基线，能跨异构全球气候模型成功重建复杂天气事件。

Conclusion: ZSSD是解决气候降尺度问题的有效方法。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [87] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出基于概念空间框架建模随时间展开的抽象概念，以国际象棋为例验证，支持双视角建模，证明轨迹概念识别可行，为决策和概念学习应用奠基。


<details>
  <summary>Details</summary>
Motivation: 探索将概念空间理论扩展到时间实现、目标导向的概念。

Method: 用概念空间框架将策略概念表示为可解释质量维度上的几何区域，将国际象棋游戏实例化为轨迹进行分析。

Result: 实现证明了基于轨迹的概念识别可行性，运动模式与专家评论一致。

Conclusion: 该方法为涉及顺序决策的更广泛应用奠定基础，支持与知识进化机制集成以学习和完善抽象概念。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [88] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: 本文介绍BioAgent Bench基准数据集和评估套件，评估AI代理在生物信息学任务中的表现，发现前沿代理能完成多步流程但鲁棒性不足，指出隐私约束下开源模型更合适并公开数据集和套件。


<details>
  <summary>Details</summary>
Motivation: 测量AI代理在常见生物信息学任务中的性能和鲁棒性。

Method: 构建BioAgent Bench基准数据集，包含端到端任务和指定输出的提示，评估前沿闭源和开源模型，用基于大语言模型的评分器评分。

Result: 前沿代理能完成多步生物信息学流程，但在受控扰动下有失败模式，闭源模型在严格隐私约束下可能不合适。

Conclusion: 正确的高级流程构建不能保证可靠的步骤级推理，隐私约束下开源模型尽管完成率低但可能更合适。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [89] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 研究提出以大语言模型（LLM）为中心的视频活动识别框架及反馈生成试点研究，表现优于基线模型，能促进护理教育。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰（ES）培训的自动识别和反馈系统研究不足，尤其是在家庭护理和教育场景缺乏持续监督，需有效方法。

Method: 提出以LLM为中心的视频活动识别框架，与传统机器学习和深度学习方法对比，并开展反馈生成试点研究，LLM进行时空活动识别和可解释决策分析，还能生成自然语言反馈，框架含基于异常检测和可解释AI的学生支持模块。

Result: 基于LLM的方法在准确率和F1分数上比基线模型提高约15 - 20%。

Conclusion: 该研究为推进护理教育、提高培训效率和患者安全建立了可扩展、可解释和数据驱动的基础。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [90] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: 本文提出CORE框架解决6G网络和大语言模型中计算资源不足问题，经评估和实际部署验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络和大语言模型发展中，分层网络计算资源分散异构，单个大语言模型代理难以执行复杂推理任务。

Method: 提出CORE框架，采用协作学习系统，多个大语言模型分布在移动设备和边缘服务器，集成三个优化模块，并引入角色亲和调度算法。

Result: 综合案例研究和性能评估显示，CORE提高了系统效率和任务完成率，在实际边缘计算平台部署表现良好。

Conclusion: CORE框架有效且具有实际应用价值。

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [91] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 本文聚焦心电图专家基础模型，引入结合性能评估与表征分析的基准方法，对不同数据集和数据可用性下的模型进行评估，实验显示该方法能深入了解模型表征结构和泛化性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在AI辅助心电图解读领域发展，但需评估其嵌入的泛化性，现有工作多关注下游性能评估，缺乏深入全面的基准框架。

Method: 引入结合性能评估与表征分析的基准方法，利用SHAP和UMAP技术，对不同数据集和数据可用性下的心电图专家基础模型进行评估。

Result: 基准协议能深入洞察心电图专家基础模型的嵌入模式，帮助更好理解其表征结构和泛化性。

Conclusion: 提出的基准框架可对心电图专家基础模型进行深入全面评估，有助于在医疗领域更负责地使用基础模型。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [92] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: 现有大语言模型去偏方法有缺陷，提出KnowBias框架，通过强化编码偏见知识的神经元去偏，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在社会偏见，限制安全部署，现有去偏方法有易脆、泛化弱等问题。

Method: 提出KnowBias框架，通过基于归因分析用少量偏见知识问题识别编码偏见知识的神经元，并在推理时选择性增强。

Result: 在多个基准测试和大语言模型上实验，展现出一致的最先进去偏性能，且对模型实用性的降低最小。

Conclusion: KnowBias是一种轻量级、概念独特的去偏框架，能有效去偏并保留模型通用能力，具有良好泛化性和数据效率。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [93] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: 本文提出WebArbiter解决WebPRM现有问题，介绍训练方法，发布WebPRMBench，实验显示WebArbiter表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有WebPRM存在问题，如标量WebPRM信号粗糙、清单式WebPRM匹配脆弱，需新方法解决。

Method: 引入WebArbiter，将奖励建模作为文本生成，采用两阶段训练管道，发布WebPRMBench用于评估。

Result: 在WebPRMBench上，WebArbiter - 7B比GPT - 5高9.1分；在WebArena - Lite上比最佳WebPRM高7.2分。

Conclusion: WebArbiter在复杂Web任务中具有鲁棒性和实用价值。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [94] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍为 ASTRA 编程语言开发大语言模型集成原型的经验，包括工具概述、示例实现及经验讨论。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式 AI 和智能体 AI 技术对传统智能体工具包的影响，以及传统工具包经验对新智能体平台设计的影响。

Method: 为 ASTRA 编程语言开发大语言模型集成原型，给出工具包概述和三个示例实现。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及明确结论，以对示例中获得的经验进行讨论结尾。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [95] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: 提出架构无关保护框架Trap²防止模型权重被未经授权合并。


<details>
  <summary>Details</summary>
Motivation: 模型中心兴起使模型合并便捷，但产生治理缺口，现有防御方法事后且依赖架构，保护效果不佳。

Method: 提出Trap²，在微调更新中编码保护，用权重重新缩放作为合并过程的代理。

Result: 使发布的权重在独立使用时有效，但在合并的重新缩放操作下性能下降。

Conclusion: Trap²能解决模型合并的治理缺口问题，有效防止未经授权的模型合并。

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [96] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 当前LLM后训练方法与人类解决问题方式不匹配，提出CoMT和CCRL框架，实验显示该框架提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练方法按完整推理轨迹优化，与人类解决问题的双阶段认知过程不符。

Method: 提出认知启发框架，包括专注抽象推理模式的CoMT和通过置信度校准奖励优化任务适应的CCRL。

Result: 在四个模型和八个基准测试中，分布内和分布外性能分别提升2.19%和4.63%，减少65 - 70%训练时间和50%代币消耗。

Conclusion: 让后训练与人类认知原则对齐，可提升泛化能力和训练效率。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [97] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 文章提出多智能体强化学习框架 SCMA 解决大推理模型冗余推理问题，在减少响应长度同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方法难以平衡简洁性和准确性，本文旨在解决此局限性。

Method: 提出 Self - Compression via MARL (SCMA) 框架，通过分割智能体和评分智能体协作定义重要性加权长度惩罚，激励推理智能体。

Result: SCMA 使响应长度减少 11.1% 到 39.0%，准确率提升 4.33% 到 10.02%，消融研究和定性分析表明该框架有优势。

Conclusion: MARL 框架内的协同优化产生涌现行为，相比传统 RL 范式能产生更强大的大推理模型。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [98] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: 介绍AgenticSimLaw框架用于高风险表格决策任务，通过多智能体辩论实现透明可控推理，在再犯预测任务中表现良好且可推广。


<details>
  <summary>Details</summary>
Motivation: 解决高风险表格决策任务中推理缺乏透明度和可控性的问题，应对基于大语言模型的多智能体系统挑战。

Method: 提出AgenticSimLaw框架，采用法庭式编排，定义智能体角色、交互协议和私有推理策略，在再犯预测任务上进行基准测试并与传统思维链提示对比。

Result: 结构化多智能体辩论比单智能体推理性能更稳定、可推广，准确率和F1分数相关性更强，框架能对推理步骤进行细粒度控制、生成完整交互记录并对智能体行为进行系统分析。

Conclusion: 该框架可推广到任何需要透明度和人工监督的高风险决策任务，解决了多智能体系统的组织、可观察性和责任问题。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [99] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: 提出DeR2沙盒评估大语言模型文档推理能力，实验发现模型表现差异大且提升空间大。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试难以确定大语言模型能否对全新科学信息进行推理，多数评估存在推理与检索及工具链选择混淆等问题。

Method: 引入DeR2沙盒，通过四种机制分离证据访问与推理；采用两阶段验证防止参数泄漏；提供冻结文档库确保可重复性。

Result: 不同的先进基础模型表现差异大，部分模型存在模式切换脆弱性和概念使用结构错误等问题。

Conclusion: 大语言模型在文档推理能力上有很大的提升空间。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [100] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: 现有检索式工具使用管道有语义挑战，生成式方法有局限，提出ToolWeaver框架，编码工具为分层序列，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索式工具使用管道和生成式方法在语义处理、可扩展性和泛化性方面的局限。

Method: 提出ToolWeaver框架，将工具编码为分层序列，通过新的分词过程生成结构化代码，在生成式对齐阶段将其集成到LLM中并微调。

Result: 用近47000个工具评估，ToolWeaver显著优于现有方法。

Conclusion: ToolWeaver为高级工具增强代理建立了更具可扩展性、泛化性和语义感知的基础。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [101] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: 引入VAF评估流程量化网页视觉属性对网络代理决策的影响，实验表明部分视觉属性影响大，部分影响小。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注良性场景下代理偏好，且对视觉属性如何塑造代理决策缺乏系统理解。

Method: 引入VAF评估流程，包含变体生成、浏览交互、点击和推理验证三个阶段，通过目标点击率和目标提及率联合评估视觉属性影响。

Result: 经过对8个变体家族、5个真实网站和4个代表性网络代理的大量实验，发现背景颜色对比度、项目大小、位置和卡片清晰度对代理行为影响大，字体样式、文本颜色和项目图像清晰度影响小。

Conclusion: 通过VAF流程可量化网页视觉属性对网络代理决策的影响，不同视觉属性影响程度不同。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [102] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 研究语言模型中陈述偏好与显示偏好相关性受偏好引出协议的影响，发现该相关性高度依赖协议，引出偏好需考虑不确定偏好。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型陈述 - 显示偏好差距评估依赖二进制强制选择提示，会混淆真实偏好和引出协议的人为因素，故系统性研究引出协议对相关性的影响。

Method: 对 24 个语言模型，研究不同引出协议下陈述偏好与显示偏好的斯皮尔曼秩相关系数的变化。

Result: 在陈述偏好引出中允许中立和弃权可提升相关性；在显示偏好中允许弃权会使相关性接近零或为负；用陈述偏好引导显示偏好引出不能可靠提升相关性。

Conclusion: 陈述 - 显示偏好相关性高度依赖引出协议，偏好引出需考虑不确定偏好的方法。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [103] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: 本文指出现有大语言模型（LLM）代理基准测试的不足，介绍了用于评估车载助手场景下多轮、使用工具的LLM代理的CAR - bench基准测试，并给出基线结果，强调需更可靠和有自我认知的LLM代理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理基准测试侧重于理想环境下的任务完成度，忽略了面向用户的现实应用中的可靠性，而在车载语音助手等领域，用户请求常不完整或模糊，需要处理不确定性。

Method: 引入CAR - bench基准测试，其环境有LLM模拟用户、域策略和58个互连工具，包含幻觉任务和消歧任务。

Result: 基线结果显示各类任务偶尔成功和持续成功差距大，前沿推理LLM在消歧任务上持续通过率不足50%，在幻觉任务中常违反策略或编造信息。

Conclusion: 在现实场景中需要更可靠和有自我认知的LLM代理。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [104] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: 本文提出Agent Workflow Optimization (AWO)框架，可优化代理AI工作流程，减少调用、提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代理AI工作流存在高成本、高延迟及幻觉导致的失败等问题，需要进行优化。

Method: 分析现有工作流轨迹，发现重复工具调用序列并转化为元工具，将多个代理动作捆绑成单次调用。

Result: 在两个代理AI基准测试中，AWO最多减少11.9%的大语言模型调用，任务成功率最多提升4.2个百分点。

Conclusion: AWO框架能有效提高代理工作流的效率与鲁棒性。

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [105] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 本文提出一种基于多维核表示从先前收集的数据中后验定义操作设计域（ODD）的方法，通过蒙特卡罗方法和航空用例验证，表明数据驱动的ODD可等于原始隐藏ODD，有助于数据驱动的安全关键AI系统认证。


<details>
  <summary>Details</summary>
Motivation: 在现实复杂系统或已有数据情况下，定义AI系统运行的操作设计域（ODD）极具挑战，而许多领域认证AI系统又需要ODD，传统方法在开发早期基于专家知识和标准创建ODD。

Method: 提出基于多维核表示从先前收集的数据中后验定义ODD的安全设计方法，并用蒙特卡罗方法和现实航空用例验证。

Result: 通过验证表明数据驱动的ODD可以等于数据的原始、潜在的隐藏ODD。

Conclusion: 利用基于核的ODD安全设计方法，可实现对数据驱动的安全关键AI系统的未来认证。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [106] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: 提出RTL自适应剪枝框架，发现多个专用子网络，在多方面表现出色并提出子网络相似性分数。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法假设单一通用中奖票，忽略数据异质性，本文旨在解决该问题。

Method: 提出Routing the Lottery (RTL)自适应剪枝框架，发现多个针对不同类别、语义簇或环境条件的自适应子网络。

Result: 在不同数据集和任务中，RTL在平衡准确率和召回率上优于单模型和多模型基线，使用参数最多减少10倍，且语义对齐；识别出子网络崩溃问题并引入子网络相似性分数用于无标签诊断过度稀疏化。

Conclusion: 将剪枝重塑为使模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平道路。

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


### [107] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文引入 Agent Reasoning Reward Model (Agent - RRM) 多面奖励模型，研究三种集成策略，实验证明 Reagent - U 效果显著，还发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有 Agentic RL 方法多依赖基于结果的稀疏奖励训练，无法区分中间推理质量，导致训练结果欠佳。

Method: 引入 Agent - RRM 奖励模型，产生结构化反馈，研究 Reagent - C、Reagent - R 和 Reagent - U 三种集成策略。

Result: 在 12 个不同基准测试中，Reagent - U 表现出色，GAIA 达到 43.7%，WebWalkerQA 达到 46.2%。

Conclusion: 推理奖励模型和训练方案有效，代码、模型和数据集已发布以促进未来研究。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [108] [Collective Noise Filtering in Complex Networks](https://arxiv.org/abs/2601.21299)
*Tingyu Zhao,István A. Kovács*

Main category: cs.CE

TL;DR: 现实网络数据有噪声等问题，引入网络维纳滤波器集体过滤噪声，在两个场景验证效果并倡导误差感知的网络科学。


<details>
  <summary>Details</summary>
Motivation: 现实网络数据存在噪声、偏差和数据缺失问题，影响下游分析可靠性，现有的噪声过滤方法不是最优的。

Method: 引入网络维纳滤波器，利用网络拓扑和噪声特征，进行集体边级别的噪声过滤以减少观测边权重误差和推断缺失边权重。

Result: 在酵母基因相互作用网络和安然语料库电子邮件网络中，网络维纳滤波器有效抑制了噪声。

Conclusion: 倡导向误差感知的网络科学转变，有效应对数据不完美问题。

Abstract: Complex networks are powerful representations of complex systems across scales and domains, and the field is experiencing unprecedented growth in data availability. However, real-world network data often suffer from noise, biases, and missing data in the edge weights, which undermine the reliability of downstream network analyses. Standard noise filtering approaches, whether treating individual edges one-by-one or assuming a uniform global noise level, are suboptimal, because in reality both signal and noise can be heterogeneous and correlated across multiple edges. As a solution, we introduce the Network Wiener Filter, a principled method for collective edge-level noise filtering that leverages both network topology and noise characteristics, to reduce error in the observed edge weights and to infer missing edge weights. We demonstrate the broad practical efficacy of the Network Wiener Filter in two distinct settings, the genetic interaction network of the yeast S. cerevisiae and the Enron Corpus email network, noting compelling evidence of successful noise suppression in both applications. With the Network Wiener Filter, we advocate for a shift toward error-aware network science, one that embraces data imperfection as an inherent feature and learns to navigate it effectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [109] [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 论文评估了故障EJ网络中的三种路由范式，RL方法表现出色，可作为实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 多核架构密度增加，需要高性能且容错的互连网络，EJ网络传统路由启发式方法在故障条件下有挑战。

Method: 评估确定性贪婪自适应路由、理论最优的Dijkstra算法和基于强化学习（RL）的方法，RL使用多目标奖励函数。

Result: 贪婪路由在9个故障节点时性能严重下降，Dijkstra算法显示拓扑最优为52 - 54%，RL代理有效可达性94%、数据包交付率91%，吞吐量超90%，拥塞时优于Dijkstra。

Conclusion: 基于RL的自适应策略是实用解决方案，能在容错互连网络中提供鲁棒、自修复通信，无需全局拓扑知识和最优算法的计算开销。

Abstract: The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.

</details>


### [110] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: 提出Ira框架加速备份重放，以Ethereum为例提出Ira - L协议，评估显示可大幅提升备份重放速度。


<details>
  <summary>Details</summary>
Motivation: 解决主备复制中备份节点重放事务的共识延迟问题。

Method: 传输紧凑提示（hints），以Ethereum为例，Ira - L协议让主节点提供工作集密钥及元数据，备份节点利用提示高效重放。

Result: 提示数据紧凑，每块压缩后中位数为47KB；主节点提示生成和块执行有一定开销；备份侧Ira - L每块中位数加速25倍，16个预取线程下总重放时间从6.5小时降至16分钟。

Conclusion: Ira框架及Ira - L协议能有效加速备份重放，虽主节点有开销但可在生产中并行处理。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [111] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: 本文提出maxwait协调机制，能平衡分布式时间敏感系统的时序和一致性，可涵盖经典方法，实现多种系统模式并强化控制、处理故障。


<details>
  <summary>Details</summary>
Motivation: 要在分布式时间敏感系统中平衡时序要求和一致性，且需应对通信延迟和同步不确定性。

Method: 提出maxwait协调机制，通过扩展Lingua Franca协调语言实现。

Result: 机制能涵盖经典分布式系统方法，实现多种常用系统模式，且能在单一语义框架内更好控制时序、进行有界时间故障检测。

Conclusion: maxwait能在通信延迟有界时确保逻辑时间一致性，在界限被违反时提供结构化故障处理。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [112] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: 提出高效无损的设备端MoE服务系统ZipMoE，实验显示其可降低推理延迟、提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽增强大语言模型表达能力，但内存占用大，阻碍其在资源受限边缘设备上的实际部署，且需在不依赖有损量化的情况下保留模型行为。

Method: 通过缓存 - 调度协同设计，利用边缘设备硬件特性和MoE参数的统计冗余，将设备端MoE推理从I/O瓶颈转变为以计算为中心的工作流程。

Result: 实现ZipMoE原型，实验表明其推理延迟最多降低72.77%，吞吐量最多提高6.76倍。

Conclusion: ZipMoE是一种高效且语义无损的设备端MoE服务系统，能有效提升边缘设备上MoE模型的性能。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [113] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: 本文提出SA - PSKY框架解决边缘计算环境下概率天际线查询处理的资源冲突问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 物联网时代，边缘传感器数据呈指数级增长，传统分布式概率天际线查询方法不适用于边缘计算环境，会导致通信瓶颈或本地计算延迟。

Method: 将动态阈值调整问题形式化为连续马尔可夫决策过程，利用深度确定性策略梯度（DDPG）智能体实时优化过滤强度，分析多维系统状态以最小化计算和通信成本的联合目标函数。

Result: SA - PSKY在通信开销上最多降低60%，总响应时间最多降低40%，在不同数据分布下有良好的可扩展性。

Conclusion: SA - PSKY框架能有效解决边缘计算环境下概率天际线查询处理的资源冲突问题，性能优于现有静态和启发式基线方法。

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>


### [114] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: 提出EWSJF调度器解决大语言模型混合负载下调度问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准FCFS策略在大语言模型混合负载调度中存在队头阻塞、高尾延迟和硬件利用率低的问题。

Method: 引入EWSJF调度器，包含Refine - and - Prune、Dynamic Queue Routing、Density - Weighted Scoring和Bayesian Meta - Optimization四个组件。

Result: 在vLLM中实现后，端到端吞吐量提高超30%，短请求平均首次令牌时间最多降低4倍。

Conclusion: 基于学习的自适应请求调度是高效响应式大语言模型服务的关键缺失层。

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [115] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: 本文为高度非高斯、稀疏连接因子图中高斯近似的有效性提供理论保证，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统BP算法在实际计算和内存预算上不可行，GBP虽流行但缺乏非高斯问题下高斯近似有效性的理论保证。

Method: 利用中心极限定理，在满足4个关键假设的复杂循环因子图中，从数学上证明BP下变量的信念收敛到高斯分布。

Result: 在立体深度估计任务中，经过几次BP迭代后，变量信念越来越接近高斯分布。

Conclusion: 为高度非高斯、稀疏连接因子图中高斯近似的有效性提供了理论和实验支持。

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [116] [Stochastic Indexing Primitives for Non-Deterministic Molecular Archives](https://arxiv.org/abs/2601.20921)
*Faruk Alpay,Levent Sarioglu*

Main category: cs.DS

TL;DR: 介绍全息布隆过滤器(HBF)用于DNA数据存储随机访问，给出算法和概率分析，提供可量化权衡的关联检索方案。


<details>
  <summary>Details</summary>
Motivation: 现有DNA数据存储随机访问系统存在瓶颈，不支持快速、大规模并行、内容可寻址查询。

Method: 引入HBF，通过循环卷积绑定键向量和值向量，用基于裕度决策规则的查询解码，给出构建和译码算法与概率分析。

Result: 分析得到匹配和非匹配得分分布的集中度界限、解码器阈值和裕度设置，以及向量维度下的指数误差衰减。

Conclusion: HBF是指针追逐分子数据结构的可分析替代方案，能实现一次性关联检索并量化维度、数据集大小和噪声间的权衡。

Abstract: Random access remains a central bottleneck in DNA-based data storage. Existing systems typically retrieve records by PCR enrichment or other multi-step biochemical procedures, which do not naturally support fast, massively parallel, content-addressable queries.
  We introduce the Holographic Bloom Filter (HBF), a probabilistic indexing primitive that stores key-pointer associations as a single high-dimensional memory vector. HBF binds a key vector and a value (pointer) vector using circular convolution and superposes bindings across all records. A query decodes by correlating the memory with the query key and selecting the best matching value using a margin-based decision rule.
  We give construction and decoding algorithms and a probabilistic analysis under explicit noise models (memory corruption and query/key mismatches). The analysis provides concentration bounds for match and non-match score distributions, explicit threshold and margin settings for a top K decoder, and exponential error decay in the vector dimension under standard randomness assumptions.
  HBF offers a concrete, analyzable alternative to pointer-chasing molecular data structures, enabling one-shot associative retrieval while quantifying trade-offs among dimensionality, dataset size, and noise.

</details>


### [117] [Exact (n + 2) Comparison Complexity for the N-Repeated Element Problem](https://arxiv.org/abs/2601.21202)
*Andrew Au*

Main category: cs.DS

TL;DR: 本文确定在特定条件下，在含n + 1个不同值的2n元素数组中找重复n次元素的精确比较复杂度，给出n + 2次比较算法并证明其为紧界。


<details>
  <summary>Details</summary>
Motivation: 确定在含n + 1个不同值的2n元素数组中找重复n次元素，在特定模型和额外空间下的精确比较复杂度。

Method: 提出简单确定性算法进行n + 2次比较，用基于图论结构的对手论证证明下界，通过“支柱匹配”构造和边翻转重配置分析图结构。

Result: 简单确定性算法能精确进行n + 2次比较，证明任何正确算法最坏情况下至少需n + 2次比较。

Conclusion: 该结果具体展示了精确下界技术，连接了简单问题和复杂组合推理。

Abstract: This paper establishes the exact comparison complexity of finding an element repeated $n$ times in a $2n$-element array containing $n+1$ distinct values, under the equality-comparison model with $O(1)$ extra space. We present a simple deterministic algorithm performing exactly $n+2$ comparisons and prove this bound tight: any correct algorithm requires at least $n+2$ comparisons in the worst case. The lower bound follows from an adversary argument using graph-theoretic structure. Equality queries build an inequality graph $I$; its complement $P$ (potential-equalities) must contain either two disjoint $n$-cliques or one $(n+1)$-clique to maintain ambiguity. We show these structures persist up through $n+1$ comparisons via a "pillar matching" construction and edge-flip reconfiguration, but fail at $n+2$. This result provides a concrete, self-contained demonstration of exact lower-bound techniques, bridging toy problems with nontrivial combinatorial reasoning.

</details>


### [118] [Quantifying Noise in Language Generation](https://arxiv.org/abs/2601.21237)
*Aaron Li,Ian Zhang*

Main category: cs.DS

TL;DR: 研究语言生成现象的噪声模型中噪声的影响，得出单个噪声字符串对生成集合的影响及与有限噪声的等价性，并给出非均匀噪声依赖可生成性的首次刻画。


<details>
  <summary>Details</summary>
Motivation: 在语言生成的噪声模型中，量化噪声的影响，研究每个额外的无关字符串的影响。

Method: 对均匀和非均匀生成情况进行分析和证明。

Result: 1. 单个噪声字符串会严格减少可生成的集合；2. 单个噪声字符串的生成与任意有限数量噪声的生成等价；3. 给出非均匀噪声依赖可生成性的首次刻画。

Conclusion: 回答了Raman和Raman (2025)中的开放问题，与Bai等人(2026)的结果形成对比，为非均匀噪声依赖可生成性提供了新的刻画。

Abstract: Kleinberg and Mullainathan recently proposed a formal framework for studying the phenomenon of language generation, called language generation in the limit. In this model, an adversary gives an enumeration of example strings from an unknown target language, and the algorithm is tasked with correctly generating unseen strings from the target language within finite time. Refined notions of non-uniform and uniform generation were later introduced by Li, Raman, and Tewari (2025), and a noisy model was introduced by Raman and Raman (2025), which allows the adversary to insert extraneous strings. A natural question in the noisy model is to quantify the effect of noise, by studying the impact of each additional extraneous string. We show two complementary results in this setting. We first show that for both uniform and non-uniform generation, a single noisy string strictly reduces the set of collections that can be generated, thus answering an open question in Raman and Raman (2025). Then, we show for both uniform and non-uniform generation that generation with a single noisy string is equivalent to generation with any finite amount of noise, sharply contrasting with the strict hierarchy for noisy generation in the limit shown by Bai, Panigrahi, and Zhang (2026). Finally, we leverage our previous results to provide the first known characterization for non-uniform noise-dependent generatability.

</details>


### [119] [Algorithms for the local and the global postage stamp problem](https://arxiv.org/abs/2601.21423)
*Léo Colisson Palais,Jean-Guillaume Dumas,Alexis Galan,Bruno Grenet,Aude Maignan*

Main category: cs.DS

TL;DR: 本文针对邮票面值问题，提出改进算法，包括局部问题的新算法和全局问题的多项式近似算法，并表明其有助于改进集合的安全多方计算。


<details>
  <summary>Details</summary>
Motivation: 解决邮票局部问题计算复杂度高和内存需求大问题，以及为全局问题提供解决办法，同时探索其在安全多方计算中的应用。

Method: 提出局部问题的新算法和全局问题的多项式近似算法，并进行复杂度分析。

Result: 局部问题新算法改进了时间复杂度界限和所需内存量，全局问题有多项式近似算法及复杂度分析。

Conclusion: 算法可通过对加密值多项式的更高效同态评估，改进集合上的安全多方计算。

Abstract: We consider stamps with different values (denominations) and same dimensions, and an envelope with a fixed maximum number of stamp positions. The local postage stamp problem is to find the smallest value that cannot be realized by the sum of the stamps on the envelope. The global postage stamp problem is to find the set of denominations that maximize that smallest value for a fixed number of distinct denominations. The local problem is NP-hard and we propose here a novel algorithm that improves on both the time complexity bound and the amount of required memory.  We also propose a polynomial approximation algorithm for the global problem together with its complexity analysis. Finally we show that our algorithms allow to improve secure multi-party computations on sets via a more efficient homomorphic evaluation of polynomials on ciphered values.

</details>


### [120] [When Local and Non-Local Meet: Quadratic Improvement for Edge Estimation with Independent Set Queries](https://arxiv.org/abs/2601.21457)
*Tomer Adar,Yahel Hotam,Amit Levi*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of estimating the number of edges in an unknown graph. We consider a hybrid model in which an algorithm may issue independent set, degree, and neighbor queries. We show that this model admits strictly more efficient edge estimation than either access type alone. Specifically, we give a randomized algorithm that outputs a $(1\pm\varepsilon)$-approximation of the number of edges using $O\left(\min\left(\sqrt{m}, \sqrt{\frac{n}{\sqrt{m}}}\right)\cdot\frac{\log n}{\varepsilon^{5/2}}\right)$ queries, and prove a nearly matching lower bound.
  In contrast, prior work shows that in the local query model (Goldreich and Ron, \textit{Random Structures \& Algorithms} 2008) and in the independent set query model (Beame \emph{et al.} ITCS 2018, Chen \emph{et al.} SODA 2020), edge estimation requires $\widetildeΘ(n/\sqrt{m})$ queries in the same parameter regimes. Our results therefore yield a quadratic improvement in the hybrid model, and no asymptotically better improvement is possible.

</details>


### [121] [Improved Approximations for Dial-a-Ride Problems](https://arxiv.org/abs/2601.21652)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文围绕多车辆电话叫车问题（mDaRP），提出两种算法优化近似比和运行时间，结合后在特定条件下有更好表现，还改进了经典单车辆问题近似比。


<details>
  <summary>Details</summary>
Motivation: 当前mDaRP最佳近似比无较大突破，需优化算法以提高性能。

Method: 提出两种简单算法，还对算法进行组合和扩展。

Result: 第一个算法提高运行时间且保持近似比，第二个算法达到不同近似比，组合后在特定条件有新近似比，扩展后改进经典单车辆问题近似比。

Conclusion: 所提算法有效改善了mDaRP的近似比和运行时间，且对经典单车辆问题也有改进。

Abstract: The multi-vehicle dial-a-ride problem (mDaRP) is a fundamental vehicle routing problem with pickups and deliveries, widely applicable in ride-sharing, economics, and transportation. Given a set of $n$ locations, $h$ vehicles of identical capacity $λ$ located at various depots, and $m$ ride requests each defined by a source and a destination, the goal is to plan non-preemptive routes that serve all requests while minimizing the total travel distance, ensuring that no vehicle carries more than $λ$ passengers at any time. The best-known approximation ratio for the mDaRP remains $\mathcal{O}(\sqrtλ\log m)$.
  We propose two simple algorithms: the first achieves the same approximation ratio of $\mathcal{O}(\sqrtλ\log m)$ with improved running time, and the second attains an approximation ratio of $\mathcal{O}(\sqrt{\frac{m}λ})$. A combination of them yields an approximation ratio of $\mathcal{O}(\sqrt[4]{n}\log^{\frac{1}{2}}n)$ under $m=Θ(n)$. Moreover, for the case $m\gg n$, by extending our algorithms, we derive an $\mathcal{O}(\sqrt{n\log n})$-approximation algorithm, which also improves the current best-known approximation ratio of $\mathcal{O}(\sqrt{n}\log^2n)$ for the classic (single-vehicle) DaRP, obtained by Gupta et al. (ACM Trans. Algorithms, 2010).

</details>


### [122] [Improved Approximations for the Unsplittable Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2601.21660)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The capacitated vehicle routing problem (CVRP) is one of the most extensively studied problems in combinatorial optimization. In this problem, we are given a depot and a set of customers, each with a demand, embedded in a metric space. The objective is to find a set of tours, each starting and ending at the depot, operated by the capacititated vehicle at the depot to serve all customers, such that all customers are served, and the total travel cost is minimized. We consider the unplittable variant, where the demand of each customer must be served entirely by a single tour. Let $α$ denote the current best-known approximation ratio for the metric traveling salesman problem. The previous best approximation ratio was $α+1+\ln 2+δ<3.1932$ for a small constant $δ>0$ (Friggstad et al., Math. Oper. Res. 2025), which can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023). In this paper, we propose two improved approximation algorithms. The first algorithm focuses on the case of fixed vehicle capacity and achieves an approximation ratio of $α+1+\ln\bigl(2-\frac{1}{2}y_0\bigr)<3.0897$, where $y_0>0.39312$ is the unique root of $\ln\bigl(2-\frac{1}{2}y\bigr)=\frac{3}{2}y$. The second algorithm considers general vehicle capacity and achieves an approximation ratio of $α+1+y_1+\ln\left(2-2y_1\right)+δ<3.1759$ for a small constant $δ>0$, where $y_1>0.17458$ is the unique root of $\frac{1}{2} y_1+ 6 (1-y_1)\bigl(1-e^{-\frac{1}{2} y_1}\bigr) =\ln\left(2-2y_1\right)$. Both approximations can be further improved by a small constant using the result of Blauth, Traub, and Vygen (Math. Program. 2023).

</details>


### [123] [Adaptively Robust Resettable Streaming](https://arxiv.org/abs/2601.21989)
*Edith Cohen,Elena Gribelyuk,Jelani Nelson,Uri Stemmer*

Main category: cs.DS

TL;DR: 研究可重置流模型算法，指出现有草图易受攻击，提出首个自适应鲁棒草图，利用差分隐私和二叉树机制实现对数空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有可重置流模型草图易受自适应对抗攻击，需解决该漏洞。

Method: 设计通过差分隐私强化的专用流草图，利用二叉树机制保护草图内部随机性。

Result: 提出的草图能保持对数空间复杂度，支持（亚）线性统计。

Conclusion: 所设计的草图可有效克服现有草图的漏洞，实现准确前缀最大误差保证。

Abstract: We study algorithms in the resettable streaming model, where the value of each key can either be increased or reset to zero. The model is suitable for applications such as active resource monitoring with support for deletions and machine unlearning. We show that all existing sketches for this model are vulnerable to adaptive adversarial attacks that apply even when the sketch size is polynomial in the length of the stream.
  To overcome these vulnerabilities, we present the first adaptively robust sketches for resettable streams that maintain polylogarithmic space complexity in the stream length. Our framework supports (sub) linear statistics including $L_p$ moments for $p\in[0,1]$ (in particular, Cardinality and Sum) and Bernstein statistics. We bypass strong impossibility results known for linear and composable sketches by designing dedicated streaming sketches robustified via Differential Privacy. Unlike standard robustification techniques, which provide limited benefits in this setting and still require polynomial space in the stream length, we leverage the Binary Tree Mechanism for continual observation to protect the sketch's internal randomness. This enables accurate prefix-max error guarantees with polylogarithmic space.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [124] [On Approximate Nash Equilibria in Mean Field Games](https://arxiv.org/abs/2601.20910)
*Mao Fabrice Djete,Nizar Touzi*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the context of large population symmetric games, approximate Nash equilibria are introduced through equilibrium solutions of the corresponding mean field game in the sense that the individual gain from optimal unilateral deviation under such strategies converges to zero in the large population size asymptotic. We show that these strategies satisfy an $Ł^\infty$ notion of approximate Nash equilibrium which guarantees that the individual gain from optimal unilateral deviation is small uniformly among players and uniformly on their initial characteristics. We establish these results in the context of static models and in the dynamic continuous time setting, and we cover situations where the agents' criteria depend on the conditional law of the controlled state process.

</details>


### [125] [Shortlisting: a Principled Approach](https://arxiv.org/abs/2601.21277)
*Edith Elkind,Qishen Han,Lirong Xia*

Main category: cs.GT

TL;DR: 本文强调了筛选过程的重要性，提出要进行有原则的筛选研究以解决实际需求。


<details>
  <summary>Details</summary>
Motivation: 参与式决策需求增长和候选空间扩大，急需高效公平的筛选程序，但相关原则性研究较少。

Method: 未提及具体方法，旨在提出问题并发出呼吁。

Result: 未提及具体研究结果。

Conclusion: 有原则的筛选可减轻认知负担、实现公平集体决策、鼓励广泛参与并建立对民主系统的信任。

Abstract: Shortlisting is the process of selecting a subset of alternatives from a larger pool for further consideration or final decision-making. It is widely applied in social choice and multi-agent system scenarios. The growing demand for participatory decision-making and the continuously expanding space of candidates create an urgent need for efficient and fair shortlisting procedures. However, little principled study has been done on this problem. This blue-sky paper aims to highlight the overlooked significance of shortlisting, distinguish it from related problems, provide initial thoughts, and, more importantly, serve as a call to arms. We envision that principled shortlisting can reduce cognitive burden, enable fair collective decisions, encourage broader participation, and ultimately build trust in democratic systems.

</details>


### [126] [Alliance Mechanisms in General Lotto Games](https://arxiv.org/abs/2601.21319)
*Vade Shah,Jason R. Marden*

Main category: cs.GT

TL;DR: 分析联盟机制，研究三种机制在联盟博弈中对集体和个体收益的影响，发现机制差异取决于联盟目标


<details>
  <summary>Details</summary>
Motivation: 比较不同联盟机制在联盟博弈中的表现，研究其带来集体和个体收益提升的情况

Method: 在联盟博弈模型中研究预算转移、竞赛转移和联合转移三种联盟机制

Result: 三种机制在个体收益提升上有本质区别，在集体收益提升上等价

Conclusion: 机制差异取决于联盟的目标

Abstract: How do different alliance mechanisms compare? In this work, we analyze various methods of forming an alliance in the Coalitional General Lotto game, a simple model of competitive resource allocation. In the game, Players 1 and 2 independently compete against a common Adversary by allocating their limited resource budgets towards separate sets of contests; an agent wins a contest by allocating more resources towards it than their opponent. In this setting, we study three alliance mechanisms: budget transfers (resource donation), contest transfers (contest redistribution), and joint transfers (both simultaneously). For all three mechanisms, we study when they present opportunities for collective improvement (the sum of the Players' payoffs increases) or mutual improvement (both Players' individual payoffs increase). In our first result, we show that all three are fundamentally different with regards to mutual improvement; in particular, mutually beneficial budget and contest transfers exist in distinct, limited subsets of games, whereas mutually beneficial joint transfers exist in almost all games. However, in our second result, we demonstrate that all three mechanisms are equivalent when it comes to collective improvement; that is, collectively beneficial budget, contest, and joint transfers exist in almost all game instances, and all three mechanisms achieve the same maximum collective payoff. Together, these results demonstrate that differences between mechanisms depend fundamentally on the objective of the alliance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [127] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: 提出A2RAG框架解决Graph - RAG在实际部署中的瓶颈，实验显示其有较好效果。


<details>
  <summary>Details</summary>
Motivation: Graph - RAG在实际部署中存在混合难度工作负载和提取损失两个瓶颈，需要改进。

Method: 提出A2RAG框架，结合自适应控制器验证证据充分性并按需触发细化，以及主动式检索器逐步增加检索力度并将图信号映射回原文。

Result: 在HotpotQA和2WikiMultiHopQA实验中，A2RAG在Recall@2上有+9.9/+11.8的绝对提升，同时将令牌消耗和端到端延迟降低约50%。

Conclusion: A2RAG框架能实现成本感知和可靠推理，相比迭代多跳基线有显著优势。

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [128] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 引入SteerEval评估框架衡量推荐系统可控性，评估预训练推荐器，给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言推荐方法能否遵循用户引导指令不明，现有可控性评估无法捕捉丰富用户控制形式。

Method: 引入SteerEval评估框架，用从电影类型到内容警告的干预措施衡量可控性。

Result: 评估了预训练自然语言推荐器的可控性，研究了小众主题引导的潜力和局限，比较不同干预对引导效果的影响。

Conclusion: 根据研究结果给出实用设计建议，并探讨了可控推荐器设计的未来方向。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [129] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 现有电商搜索相关模型有局限，本文提出新框架结合多视角CoT和知识蒸馏，实验证明有效提升商业表现和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和CoT推理的电商搜索相关性建模方法存在单视角推理和知识蒸馏丢弃推理结构的问题，需要改进。

Method: 提出新框架，教师模型用多视角CoT生成多样推理，结合SFT和DPO构建更强大推理器；蒸馏时引入LRKD赋予学生模型轻量级推理提取器。

Result: 在离线实验和线上A/B测试中取得显著离线收益，提升商业性能和用户体验。

Conclusion: 所提框架能更有效利用CoT语义，解决现有电商搜索相关性建模问题，有实际应用价值。

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [130] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: 提出Inf - DDS采样框架，在文本检索任务上提升性能且降低GPU计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有通用开放域密集检索系统训练数据采样策略缺乏对最优策略的充分研究，而采样策略对模型性能影响大。

Method: 提出基于强化学习的Inf - DDS框架，以基于影响的奖励信号自适应重新加权训练数据集，迭代优化采样策略。

Result: 在多种文本检索任务上，相比现有基于梯度的采样方法，提升检索性能、增强适应性，降低1.5 - 4倍GPU计算成本；训练特定模型时提升NDCG@10指标。

Conclusion: Inf - DDS采样策略有效，能提升模型性能并降低计算成本。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [131] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: 提出适用于快手电商服务的端到端生成式推荐框架OneMall，统一多种商品分发场景，含三个关键组件，实验效果好且已部署服务大量用户。


<details>
  <summary>Details</summary>
Motivation: 在生成式推荐浪潮下，为快手电商服务提供合适的推荐框架，统一多种商品分发场景。

Method: 由电商语义分词器、基于Transformer的架构、强化学习管道三个关键组件构成，分别有不同处理方式。

Result: OneMall在各电商场景均有提升，如商品卡片GMV提升13.01%等，且已部署服务超4亿日活用户。

Conclusion: OneMall是有效的电商生成式推荐框架，能带来业务指标提升。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [132] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 本文分析跨域推荐中公平性问题产生原因，提出CDFA框架解决问题，实验证明该框架可降低不公平性且提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 跨域推荐会加剧群体层面的不公平性，需分析原因并解决。

Method: 识别跨域差异转移和跨域信息增益不公平两个挑战，提出CDFA框架，通过自适应整合无标签数据和信息论方法解决问题。

Result: 在多个数据集和基线模型上的实验表明，框架显著降低跨域推荐中的不公平性，不牺牲整体推荐性能甚至有所提升。

Conclusion: CDFA框架能有效解决跨域推荐中的公平性问题，同时提升推荐性能。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [133] [LEMUR: Learned Multi-Vector Retrieval](https://arxiv.org/abs/2601.21853)
*Elias Jääsaari,Ville Hyvönen,Teemu Roos*

Main category: cs.IR

TL;DR: 提出用于多向量相似性搜索的高效框架LEMUR，比早期方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 多向量检索在提高召回率时会显著增加延迟，需设计高效的近似最近邻搜索算法。

Method: 将多向量相似性搜索先转化为可用单隐藏层神经网络解决的监督学习问题，再将该模型下的推理转化为潜在空间的单向量相似性搜索。

Result: 在ColBERTv2嵌入、现代多向量文本模型及多向量视觉文档检索模型生成的嵌入上均进行了性能评估。

Conclusion: LEMUR比早期多向量相似性搜索方法快一个数量级。

Abstract: Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.

</details>


### [134] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: 传统SR模型忽略物品文本信息，现有嵌入转换策略有缺陷，提出SpecTran方法，在多数据集和骨干模型上表现优。


<details>
  <summary>Details</summary>
Motivation: 传统SR模型忽略文本信息，现有编码文本信息的嵌入转换策略存在维度坍塌、信息丢弃等问题。

Method: 提出SpecTran，一种在频谱域操作的基于频谱感知变压器的适配器，通过可学习的频谱位置编码注入奇异值线索。

Result: 在四个真实世界数据集和三个SR骨干模型上，始终优于强基线，平均提升9.17%。

Conclusion: SpecTran能有效解决现有嵌入转换策略的局限，提升顺序推荐性能。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [135] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: 现有检索方法重相关性轻信息覆盖，提出LLM重排序方法LANCER，提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法主要针对相关性排序优化，无法满足长格式检索增强生成对信息覆盖的需求。

Method: 提出LLM重排序方法LANCER，预测子问题、能回答子问题的文档，并重新排序文档。

Result: LANCER在信息块覆盖指标上提升了检索质量，α-nDCG和信息覆盖高于其他方法。

Conclusion: 子问题生成在其中起着关键作用。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [136] [Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders](https://arxiv.org/abs/2601.21446)
*Francesco Zola,Lucia Muñoz,Andrea Venturi,Amaia Gil*

Main category: cs.LG

TL;DR: 研究用图自编码器（GAEs）在合成数据上学习和区分模仿洗钱操作的拓扑模式，比较三种GAE实现，GAE - GCN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 非法金融活动在交易网络中有拓扑模式，但因真实标记数据稀缺和隐私限制，自动检测困难，需探索有效方法。

Method: 分数据生成和模型训练验证两阶段，用参数化生成器创建七种非法活动模式的合成样本，在无显式标签下用重建误差训练三种基于不同卷积层的GAEs。

Result: GAE - GCN在各模式上重建性能最一致，GAE - SAGE和GAE - GAT仅在少数特定模式有竞争力。

Conclusion: 基于合成数据的图表示学习为开发检测非法行为的AI工具提供可行途径，可克服金融数据集的局限。

Abstract: Illicit financial activities such as money laundering often manifest through recurrent topological patterns in transaction networks. Detecting these patterns automatically remains challenging due to the scarcity of labeled real-world data and strict privacy constraints. To address this, we investigate whether Graph Autoencoders (GAEs) can effectively learn and distinguish topological patterns that mimic money laundering operations when trained on synthetic data. The analysis consists of two phases: (i) data generation, where synthetic samples are created for seven well-known illicit activity patterns using parametrized generators that preserve structural consistency while introducing realistic variability; and (ii) model training and validation, where separate GAEs are trained on each pattern without explicit labels, relying solely on reconstruction error as an indicator of learned structure. We compare three GAE implementations based on three distinct convolutional layers: Graph Convolutional (GAE-GCN), GraphSAGE (GAE-SAGE), and Graph Attention Network (GAE-GAT). Experimental results show that GAE-GCN achieves the most consistent reconstruction performance across patterns, while GAE-SAGE and GAE-GAT exhibit competitive results only in few specific patterns. These findings suggest that graph-based representation learning on synthetic data provides a viable path toward developing AI-driven tools for detecting illicit behaviors, overcoming the limitations of financial datasets.

</details>


### [137] [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)
*Xingyue Zhang,Yuxuan Bao,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 提出SENDAI框架，结合模拟先验和学习的差异校正，从超稀疏传感器观测中重建全空间状态，在卫星遥感中表现优于基线方法，适用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 解决时空场重建中数据丰富的训练机制和观测稀疏的部署条件之间的差距，特别是目标域存在分布偏移、异构结构和多尺度动态的情况。

Method: 提出SENDAI，一种分层的稀疏测量、高效数据同化框架，结合模拟先验和学习的差异校正来重建全空间状态。

Result: 在卫星遥感的六个全球分布站点上，SENDAI框架在重建植被指数场方面表现优于现有基线，最大SSIM改善达185%，对高频方法有36%的改善。

Conclusion: SENDAI是一种轻量级且可操作的稀疏测量重建框架，适用于基于物理的推理、资源受限的部署以及实时监测和控制。

Abstract: Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.

</details>


### [138] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: 针对现有LHD框架局限，提出DASH框架，实验表明其提升运行效率、解质量，降低适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有LHD框架存在端点评估忽略收敛过程和效率、高适应成本的局限，需要改进。

Method: 提出Dynamics - Aware Solver Heuristics (DASH)框架，通过收敛感知指标共同优化求解器搜索机制和运行时调度，还引入Profiled Library Retrieval (PLR)。

Result: 在四个组合优化问题实验中，DASH运行效率提升超3倍，解质量超现有基线，不同分布下维持高准确性，降低超90%的LLM适应成本。

Conclusion: DASH框架有效解决现有LHD框架问题，在效率、解质量和适应成本方面表现出色。

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [139] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: 提出Finetune - Informed Pretraining (FIP)方法用于多模态预训练，在无线信号星座图上效果好，简单且广泛适用。


<details>
  <summary>Details</summary>
Motivation: 标准多模态预训练策略统一对待所有模态，可能导致下游微调中实际重要模态的表征优化不足。

Method: 提出模型无关的FIP方法，对目标模态采用更高掩码难度、更强损失加权和更大解码器容量，不修改共享编码器且无需额外监督。

Result: 应用于无线信号星座图掩码建模时，在不增加额外数据和计算的情况下，持续提升下游微调性能。

Conclusion: FIP方法简单易实现、与架构兼容，可广泛应用于多模态掩码建模流水线。

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [140] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: 本文提出集成因果发现与轻量级生成式机器学习模型的框架生成新金属氢化物候选材料，鉴定出六种新化学公式和晶体结构，四个经验证有实验研究潜力，该框架可扩展且高效。


<details>
  <summary>Details</summary>
Motivation: 现有材料数据库中表征良好的氢化物数量有限，限制了最优候选材料的发现，需要开发新方法来生成新的金属氢化物候选材料。

Method: 提出一个集成因果发现与轻量级生成式机器学习模型的框架，使用450个样本数据集进行训练、验证和测试。

Result: 模型生成1000个候选材料，经筛选鉴定出六种未报道的化学公式和晶体结构，四个经密度泛函理论模拟验证。

Conclusion: 所提出的框架为扩展储氢数据集和加速材料发现提供了可扩展且高效的方法。

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


### [141] [Is Parameter Isolation Better for Prompt-Based Continual Learning?](https://arxiv.org/abs/2601.20894)
*Jiangyang Li,Chenhao Ding,Songlin Dong,Qiang Wang,Jianchao Zhao,Yuhang He,Yihong Gong*

Main category: cs.LG

TL;DR: 提出提示共享框架，结合任务感知门控路由机制和历史感知调制器，在持续学习中优于现有静态分配策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法固定分配提示，隔离任务知识，参数利用不佳，需改进。

Method: 构建全局提示池，引入任务感知门控路由机制动态解耦和协作优化特征表示，引入历史感知调制器保护常用提示。

Result: 广泛分析和实证结果表明该方法在有效性和效率上优于现有静态分配策略。

Conclusion: 所提提示共享框架能有效解决现有方法问题，提升持续学习效果和效率。

Abstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.

</details>


### [142] [Faster Predictive Coding Networks via Better Initialization](https://arxiv.org/abs/2601.20895)
*Luca Pinchetti,Simon Frieder,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: 研究旨在扩大神经网络的神经科学启发式学习算法，本文针对能量基学习算法计算量大问题，提出新初始化技术，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 能量基学习算法（如预测编码）因迭代性质导致计算需求大，限制其应用，需解决该问题。

Method: 提出一种新的预测编码网络初始化技术，旨在保留先前训练样本的迭代进度。

Result: 实验表明，在监督和无监督设置下，收敛速度和最终测试损失均有显著改善。

Conclusion: 此方法为调和预测编码和反向传播在计算效率和最终性能方面的差异提供了有前景的方向。

Abstract: Research aimed at scaling up neuroscience inspired learning algorithms for neural networks is accelerating. Recently, a key research area has been the study of energy-based learning algorithms such as predictive coding, due to their versatility and mathematical grounding. However, the applicability of such methods is held back by the large computational requirements caused by their iterative nature. In this work, we address this problem by showing that the choice of initialization of the neurons in a predictive coding network matters significantly and can notably reduce the required training times. Consequently, we propose a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples. Our approach suggests a promising path toward reconciling the disparities between predictive coding and backpropagation in terms of computational efficiency and final performance. In fact, our experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.

</details>


### [143] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: 提出TwinWeaver框架构建Genie Digital Twin (GDT)预测临床事件，在多项指标上表现优于基线，还能推广到分布外临床试验，有可解释性。


<details>
  <summary>Details</summary>
Motivation: 精确肿瘤学需预测临床事件和轨迹，但对稀疏多模态临床时间序列建模有挑战。

Method: 引入TwinWeaver框架将患者纵向病史序列化，构建GDT应用于93054名20种癌症类型患者。

Result: GDT显著降低预测误差，改善风险分层，能推广到分布外临床试验且表现良好。

Conclusion: TwinWeaver为纵向临床建模提供可扩展、透明基础。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [144] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 提出“有噪声但有效”假设检验框架解决大语言模型可靠认证难题，有理论保证和实证验证，揭示实际方法与理论“神谕”的差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可靠认证重要但有挑战，“LLM - as - a - Judge”存在缺陷，统计保证可能失效。

Method: 引入“有噪声但有效”假设检验框架，利用小的人工标注校准集估计判断器的TPR/FPR，得到方差校正临界阈值应用于大的判断器标注数据集。

Result: 理论上保证有限样本I型错误控制；实验验证理论；揭示实际方法与理论“神谕”的性能差距。

Conclusion: 深化对使用大语言模型判断器进行统计评估的理解，凸显不同推理工具间的权衡。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [145] [Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach](https://arxiv.org/abs/2601.20916)
*Anni Zhao,Ayca Ermis,Jeffrey Robert Vitt,Sergio Brasil,Wellingson Paiva,Magdalena Kasprowicz,Malgorzata Burzynska,Robert Hamilton,Runze Yan,Ofer Sadan,J. Claude Hemphill,Lieven Vandenberghe,Xiao Hu*

Main category: cs.LG

TL;DR: 开发定制机器学习算法，用无创信号估计颅内压，评估显示有一定准确性，证明方法可行但需进一步验证和改进。


<details>
  <summary>Details</summary>
Motivation: 准确无创估计颅内压在重症监护中是重大挑战，需开发新方法。

Method: 提出机器学习框架，用子空间系统辨识算法建立脑血流动力学模型，通过凸优化和创新排序约束学习映射函数，划分数据集评估性能。

Result: 约31.88%的测试条目估计误差在2 mmHg内，34.07%在2 - 6 mmHg内。

Conclusion: 所提出的无创颅内压估计方法可行，但临床应用前需进一步验证和技术改进。

Abstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.

</details>


### [146] [Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles](https://arxiv.org/abs/2601.20989)
*Lutz Oettershagen*

Main category: cs.LG

TL;DR: 研究双预言机场景下的前k项识别问题，分析STC基线方法，给出下界，提出ACE和ACE - W算法减少强调用成本。


<details>
  <summary>Details</summary>
Motivation: 精确估值昂贵时，识别前k项具有挑战性，研究双预言机场景以解决该问题。

Method: 分析STC基线方法，证明其强调用次数；建立条件下界；提出ACE自适应认证算法和ACE - W两阶段自适应方法。

Result: ACE算法达到与STC相同的强调用次数上界，且实际中减少强调用；ACE - W进一步降低强成本。

Conclusion: ACE和ACE - W算法在双预言机场景下能有效减少强调用成本。

Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.

</details>


### [147] [A Theory of Universal Agnostic Learning](https://arxiv.org/abs/2601.20961)
*Steve Hanneke,Shay Moran*

Main category: cs.LG

TL;DR: 本文给出不可知设定下二元分类最优通用率的完整理论，确定最优率的四分法及对应组合结构。


<details>
  <summary>Details</summary>
Motivation: 去除分布的可实现性假设，扩展已有可实现情形的理论。

Method: 未明确提及

Result: 确定最优率的四分法，即最优通用收敛率为$e^{-n}$、$e^{-o(n)}$、$o(n^{-1/2})$或任意慢；找到决定概念类所属类别的组合结构。

Conclusion: 在不可知设定下建立了二元分类最优通用率的完整理论体系。

Abstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.

</details>


### [148] [Monotone Optimisation with Learned Projections](https://arxiv.org/abs/2601.20983)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 提出一种算法感知学习方法，将学习模型集成到POA中，通过径向逆改进投影预测，提出HM - RI网络，在多个基准测试中加速优化且保证解质量。


<details>
  <summary>Details</summary>
Motivation: 许多应用中目标和约束函数只能从数据获取，传统POA算法难以直接应用。

Method: 引入算法感知学习方法，将学习模型集成到POA，通过径向逆直接预测投影原语；提出HM - RI网络；给出径向逆函数理论特征；开发松弛单调性条件。

Result: 在多个单调优化基准测试中，相比直接函数估计有显著加速，保持强解质量，优于未利用单调结构的基线方法。

Conclusion: 所提算法感知学习方法有效，能在单调优化问题中实现加速并保证解的质量。

Abstract: Monotone optimisation problems admit specialised global solvers such as the Polyblock Outer Approximation (POA) algorithm, but these methods typically require explicit objective and constraint functions. In many applications, these functions are only available through data, making POA difficult to apply directly. We introduce an algorithm-aware learning approach that integrates learned models into POA by directly predicting its projection primitive via the radial inverse, avoiding the costly bisection procedure used in standard POA. We propose Homogeneous-Monotone Radial Inverse (HM-RI) networks, structured neural architectures that enforce key monotonicity and homogeneity properties, enabling fast projection estimation. We provide a theoretical characterisation of radial inverse functions and show that, under mild structural conditions, a HM-RI predictor corresponds to the radial inverse of a valid set of monotone constraints. To reduce training overhead, we further develop relaxed monotonicity conditions that remain compatible with POA. Across multiple monotone optimisation benchmarks (indefinite quadratic programming, multiplicative programming, and transmit power optimisation), our approach yields substantial speed-ups in comparison to direct function estimation while maintaining strong solution quality, outperforming baselines that do not exploit monotonic structure.

</details>


### [149] [DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training](https://arxiv.org/abs/2601.21824)
*Xinwei Qiang,Hongmin Chen,Shixuan Sun,Jingwen Leng,Xin Liu,Minyi Guo*

Main category: cs.LG

TL;DR: 论文指出大语言模型训练中确定性影响性能，提出DASH策略，提升确定性注意力反向传播吞吐量，推动可复现训练效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练中确定性虽对可复现性重要，但会带来显著性能损失，如FlashAttention - 3中确定性反向传播吞吐量大幅降低。

Method: 将确定性注意力反向传播建模为有向无环图（DAG）上的调度问题，提出DASH，包含降序Q - Tile迭代和Shift调度两种策略。

Result: 在NVIDIA H800 GPU上，DASH减小了确定性注意力的性能差距，注意力反向传播吞吐量最高提升1.28倍。

Conclusion: DASH策略显著提升了可复现大语言模型训练的效率，代码已开源。

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.
  To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.
  Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.
  Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

</details>


### [150] [Distributional Active Inference](https://arxiv.org/abs/2601.20985)
*Abdullah Akgül,Gulcin Baykal,Manuel Haußmann,Mustafa Mert Çelikok,Melih Kandemir*

Main category: cs.LG

TL;DR: 提出强化学习算法的形式抽象，将主动推理无缝集成到分布式强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 强化学习框架处理复杂环境机器人系统最优控制时样本效率低，主动推理在人工智能应用有限。

Method: 提出强化学习算法的形式抽象，将主动推理集成到分布式强化学习框架。

Result: 实现主动推理与分布强化学习框架的无缝集成。

Conclusion: 该集成使主动推理性能优势无需过渡动力学建模即可实现。

Abstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.

</details>


### [151] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 本文介绍首个全球儿童发展预训练编码器，能在少样本下实现良好性能，可改变资源受限环境下机器学习用于监测的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在新国家部署因数据瓶颈而受阻的问题，保障儿童可预防的发育迟缓问题得到机器学习监测。

Method: 利用联合国儿童基金会调查数据，对44个国家的357,709名儿童进行训练，构建预训练编码器，并应用迁移学习边界解释原理。

Result: 在只有50个训练样本时平均AUC为0.65；N=500时AUC达0.73；零样本部署到未见过的国家AUC高达0.84。

Conclusion: 预训练编码器能改变资源受限环境下机器学习对可持续发展目标4.2.1监测的可行性。

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [152] [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076)
*Jae-Won Chung,Ruofan Wu,Jeff J. Ma,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 对生成式AI领域进行大规模测量研究，发现推理时间和能耗有数量级差异，提出能耗推理框架。


<details>
  <summary>Details</summary>
Motivation: 准确理解和诊断生成式AI推理能耗差异的原因以进行优化。

Method: 在NVIDIA H100和B200 GPU上对46个模型、7个任务、1858种配置进行大规模测量研究。

Result: 发现LLM任务类型、视频与图像生成、GPU利用率等因素会导致能耗有数量级差异。

Conclusion: 提出推理能耗机制的框架，该框架可延伸到每瓦吞吐量这一指标。

Abstract: Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.

</details>


### [153] [The Depth Delusion: Why Transformers Should Be Wider, Not Deeper](https://arxiv.org/abs/2601.20994)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 提出架构条件下的缩放定律，发现最优深度和宽度的缩放关系，存在临界深度现象，在30种变压器架构上验证，证明最优深度 - 宽度权衡在生产规模下存在。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律将架构视为可互换的，未区分不同架构对模型损失的影响，需进一步分解这种依赖关系。

Method: 提出架构条件下的缩放定律，在30种跨度为17M到7B参数的变压器架构上进行实证研究。

Result: 发现最优深度和宽度的缩放关系，存在临界深度现象，实证研究达到R^2 = 0.922，7B规模下64层模型不如32层模型。

Conclusion: 最优深度 - 宽度权衡在生产规模下持续存在。

Abstract: Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.

</details>


### [154] [MADE: Benchmark Environments for Closed-Loop Materials Discovery](https://arxiv.org/abs/2601.20996)
*Shreshth A Malik,Tiarnan Doherty,Panagiotis Tigas,Muhammed Razzak,Stephen J. Roberts,Aron Walsh,Yarin Gal*

Main category: cs.LG

TL;DR: 本文介绍了用于基准测试端到端自主材料发现管道的MADE框架，模拟闭环发现过程，具有灵活性并进行了系统实验。


<details>
  <summary>Details</summary>
Motivation: 现有计算材料发现基准测试通常评估静态预测任务或孤立的计算子任务，忽略了科学发现的迭代和自适应本质，因此需要新的基准测试框架。

Method: 引入MADE框架模拟闭环发现活动，将发现形式化为在给定凸包下搜索热力学稳定化合物，通过与基线算法比较评估有效性和效率，允许用户组合组件。

Result: 对一系列系统进行了系统实验，能够对发现管道中的组件进行消融分析，比较不同方法随系统复杂度的扩展情况。

Conclusion: MADE框架可用于基准测试端到端自主材料发现管道，具有灵活性和有效性。

Abstract: Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.

</details>


### [155] [Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research](https://arxiv.org/abs/2601.21008)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \textbf{solver in the evaluation loop}. \textbf{\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \IIS{} recomputation, providing deterministic, verifiable feedback. \textbf{\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\% vs 86.2\% recovery rate (+9.1\%), 62.4\% vs 47.8\% diagnostic accuracy (+14.6\%), and 2.25 vs 3.78 steps to resolution (1.7$\times$ faster). On \ORBias{}, curriculum training achieves the only negative ID$\rightarrow$OOD bias drift among models evaluated (-9.6\%), reducing systematic bias by 48\% (from 20.0\% to 10.4\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

</details>


### [156] [READY: Reward Discovery for Meta-Black-Box Optimization](https://arxiv.org/abs/2601.21847)
*Zechuan Huang,Zhiguang Cao,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.LG

TL;DR: 本文使用大语言模型作为元黑盒优化（MetaBBO）的自动奖励发现工具，兼顾有效性和效率，实验证明该方法能提升现有MetaBBO工作。


<details>
  <summary>Details</summary>
Motivation: 现有MetaBBO工作的奖励函数由人类专家设计，存在设计偏差和奖励破解风险。

Method: 使用大语言模型作为自动奖励发现工具，在有效性方面引入启发式进化思想，在效率方面引入多任务进化架构支持并行奖励发现。

Result: 实验结果表明该方法发现的奖励函数有助于提升现有MetaBBO工作。

Conclusion: 强调了奖励设计在MetaBBO中的重要性。

Abstract: Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

</details>


### [157] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出OATTA方法，利用时间动态信息改进测试时自适应，实验证明其通用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法忽略时间动态中的监督信号，提出OATTA解决该问题。

Method: 将测试时自适应表述为无梯度递归贝叶斯估计任务，用学习的动态转移矩阵作为时间先验，引入似然比门确保弱结构流中的安全性。

Result: OATTA是轻量级、模型无关模块，计算开销可忽略不计，在多领域实验中提升了基线模型的准确率，最高达6.35%。

Conclusion: 对时间动态建模为标准的与顺序无关的TTA方法提供了关键且正交的信号。

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [158] [Dependence of Equilibrium Propagation Training Success on Network Architecture](https://arxiv.org/abs/2601.21945)
*Qingshan Wang,Clara C. Wanjura,Florian Marquardt*

Main category: cs.LG

TL;DR: 研究基于平衡传播的物理训练方法在局部连接晶格架构上的性能，发现稀疏局部连接网络可媲美密集网络，为架构扩展提供指导。


<details>
  <summary>Details</summary>
Motivation: 人工智能能耗增长不可持续，现有理论研究架构难实验实现，需探索更现实架构选择下物理训练方法性能。

Method: 研究平衡传播的物理训练方法在局部连接晶格上的性能，训练XY模型，跟踪训练中响应和耦合的演变。

Result: 仅具有局部连接的稀疏网络可达到与密集网络相当的性能。

Conclusion: 研究结果为基于平衡传播的架构在现实环境中进一步扩展提供了指导原则。

Abstract: The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.

</details>


### [159] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*José Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: 提出条件去噪模型（CDM）解决复杂物理系统代理建模中数据拟合精度和物理一致性的权衡问题，在基准测试中表现更优且更严格遵守物理约束。


<details>
  <summary>Details</summary>
Motivation: 现有物理一致方法难以保证严格遵守控制方程，或无法本质学习潜在解的几何形状，需解决复杂物理系统代理建模中数据拟合精度和物理一致性的权衡问题。

Method: 引入条件去噪模型（CDM），通过训练网络从噪声状态恢复干净状态，学习指向有效解子空间的向量场，并采用与时间无关的公式将推理转化为确定性定点迭代。

Result: 在低温等离子体物理和化学基准测试中，CDM比物理一致的基线模型具有更高的参数和数据效率。

Conclusion: 去噪目标是强大的隐式正则化器，即使训练中未见过控制方程，CDM也比显式物理损失训练的基线更严格遵守物理约束。

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [160] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: 针对PPG信号基础模型问题提出SIGMA - PPG模型，预训练后在12个下游任务中优于5个基线模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有的光电容积脉搏波描记法（PPG）信号基础模型受信号冗余和噪声挑战，标准掩码建模和对比方法有局限性。

Method: 提出SIGMA - PPG生成式基础模型，采用先验引导对抗掩码机制，用强化学习教师利用统计先验创建学习路径，通过向量量化引入语义一致性约束。

Result: 在超12万小时数据上预训练后，SIGMA - PPG在12个不同下游任务中平均性能优于5个最先进的基线模型。

Conclusion: SIGMA - PPG能有效解决现有PPG信号基础模型的问题，提升模型性能。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [161] [Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints](https://arxiv.org/abs/2601.21033)
*Omer Rochman-Sharabi,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出受限采样框架和PPR算法，提升扩散模型在科学应用中的物理准确性和约束满足度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的神经仿真器原始模型无法保证物理准确性和约束满足，需要改进。

Method: 引入受限采样框架，定义受限前向过程；提出PPR迭代算法，在去噪预测、投影到可行集和重新加噪间交替采样。

Result: 实验表明PPR将约束违反减少一个数量级以上，提升样本一致性和与真实受限分布的匹配度。

Conclusion: 提出的方法有效提升了扩散模型在科学应用中的性能。

Abstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.

</details>


### [162] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: 本文提出对训练好的扩散模型进行微调的函数空间方法，实现从后验分布中准确采样，解决了函数空间中扩散模型条件采样的理论问题，并通过数值示例验证。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯逆问题中，扩散模型虽能提供函数空间的先验分布，但从后验分布采样的条件处理理论尚不完善，本文旨在解决该问题。

Method: 假设先验分布位于卡梅伦 - 马丁空间或相对于高斯测度绝对连续，使用Doob's h - 变换的无限维扩展对模型进行条件处理，提出无模拟分数匹配目标（Supervised Guidance Training）。

Result: 证明模型可通过Doob's h - 变换进行条件处理，条件分数可分解为无条件分数和引导项，并实现高效稳定的后验采样。

Conclusion: 本文提供了首个用于微调训练好的扩散模型以从后验分布准确采样的函数空间方法。

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [163] [Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning](https://arxiv.org/abs/2601.21037)
*Chengzu Li,Zanyi Wang,Jiaang Li,Yi Xu,Han Zhou,Huanyu Zhang,Ruichuan An,Dengyang Jiang,Zhaochong An,Ivan Vulić,Serge Belongie,Anna Korhonen*

Main category: cs.LG

TL;DR: 本文借助视频生成模型进行视觉推理，在迷宫导航和七巧板拼图任务中评估其能力，发现模型有零样本泛化能力、能有效利用视觉上下文、存在测试时间缩放定律，表明视频生成是视觉推理可扩展且通用的范式。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在细粒度空间理解和连续动作规划方面有不足，难以模拟复杂视觉推理所需的动态，因此希望通过视频生成模型解决视觉推理问题。

Method: 通过视频生成模型进行视觉推理，在迷宫导航和七巧板拼图这两种不同模式下评估其能力。

Result: 1. 模型在无特定微调下对未见数据分布有强性能；2. 模型能有效利用视觉上下文，保持高视觉一致性并适应未见模式；3. 顺序规划中存在测试时间缩放定律，增加生成视频长度可提升零样本泛化能力。

Conclusion: 视频生成不仅是媒体工具，还是视觉推理可扩展、通用的范式。

Abstract: Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.

</details>


### [164] [Test-Time Adaptation for Unsupervised Combinatorial Optimization](https://arxiv.org/abs/2601.21048)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 本文提出 TACO，一种无监督神经组合优化（NCO）的测试时适应框架，可结合通用范式和特定实例范式，在多个问题上效果好。


<details>
  <summary>Details</summary>
Motivation: 现有无监督 NCO 方法分为通用范式和特定实例范式，前者缺乏实例适应性，后者无法利用归纳结构且易陷入局部最优，需结合两者优势。

Method: 提出 TACO 框架，通过战略热启动部分放松训练参数，保留归纳偏差，实现快速有效的无监督适应。

Result: 相比直接微调通用模型或从头优化特定实例模型，TACO 在计算成本增加可忽略情况下，获得更好解的质量。

Conclusion: TACO 在多个组合优化问题上有效且稳健，成为通用和特定实例无监督 NCO 之间的实用桥梁。

Abstract: Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.

</details>


### [165] [SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection](https://arxiv.org/abs/2601.21050)
*Haokun Zhou*

Main category: cs.LG

TL;DR: 传统多变量时间序列异常检测假设观测变量集合不变，本文提出SMKC框架解决传感器变化带来的问题，评估表明其有效且可冷启动。


<details>
  <summary>Details</summary>
Motivation: 传统多变量时间序列异常检测假设观测变量集不变，但实际监测系统存在传感器变化问题，现有方法无法解决。

Method: 提出SMKC框架，使用排列不变特征哈希将原始输入草图化为固定大小状态序列，构建混合核图像，用掩码重建和师生预测目标学习正常模式。

Result: 评估显示稳健的对数距离通道提供主要判别信号，余弦表示常无法捕捉足够对比度，基于SMKC表示的随机投影和最近邻检测器无需梯度更新即可与全训练基线竞争。

Conclusion: SMKC框架有效，其表示本身具有有效性，为资源受限部署提供实用冷启动解决方案。

Abstract: Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.

</details>


### [166] [Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks](https://arxiv.org/abs/2601.21061)
*Alexandre Larouche,Audrey Durand*

Main category: cs.LG

TL;DR: 本文聚焦奖励函数为子模函数的生成流网络（GFNs），提出SUBo - GFN，能生成更多训练数据，在合成和真实子模任务中有效。


<details>
  <summary>Details</summary>
Motivation: 在奖励函数具有特定子模结构的情况下，探索如何利用该结构改进生成流网络。

Method: 利用子模性获取未观测组合对象奖励的上界，分析上界出现概率和覆盖范围，引入使用子模上界训练GFN的SUBo - GFN。

Result: SUBo - GFN在同等奖励函数查询次数下能生成比经典GFNs多几个数量级的训练数据。

Conclusion: SUBo - GFN在合成和真实世界的子模任务的分布匹配和高质量候选生成方面有效。

Abstract: Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.

</details>


### [167] [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)
*Seungki Hong,Kyeongwon Jeong,Taekwang Jang*

Main category: cs.LG

TL;DR: 本文提出数字可扩展全连接Ising机器Snowball，减少求解时间，原型在基准测试中比现有技术快8倍。


<details>
  <summary>Details</summary>
Motivation: 为实现Ising机器实际部署，解决硬件拓扑、自旋选择和更新算法、耦合系数精度可扩展性三个挑战以减少求解时间。

Method: 提出Snowball，集成双模式马尔可夫链蒙特卡罗自旋选择和异步自旋更新，数字架构支持宽且可配置的耦合精度。

Result: 在AMD Alveo U250加速器卡上的原型，在相同基准实例上求解时间比现有技术减少8倍。

Conclusion: Snowball能有效解决相关挑战，促进收敛并减少求解时间。

Abstract: Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.
  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.

</details>


### [168] [Human-LLM Collaborative Feature Engineering for Tabular Data](https://arxiv.org/abs/2601.21060)
*Zhuoyan Li,Aditya Bansal,Jinzhao Li,Shishuang He,Zhuoran Lu,Mutian Zhang,Qin Liu,Yiwei Yang,Swati Jain,Ming Yin,Yunyao Li*

Main category: cs.LG

TL;DR: 提出人机协作特征工程框架，解耦操作提议与选择，结合人类反馈，提升表格学习特征工程性能并降低用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在表格学习特征工程中作为黑盒优化器，缺乏操作效用校准估计，重复探索低收益操作，无有效优先策略。

Method: 解耦操作提议与选择过程，LLM生成操作候选，通过显式建模操作效用和不确定性指导选择，设计机制引入人类专家偏好反馈。

Result: 在合成研究和真实用户研究中，该框架提升了多种表格数据集的特征工程性能，降低了用户认知负担。

Conclusion: 所提出的人机协作特征工程框架是有效的，可改善特征工程效果和用户体验。

Abstract: Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.

</details>


### [169] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 提出用于复杂系统中突发现象早期检测的机器学习方法，在癫痫发作检测和客户流失预测中取得有竞争力结果，兼顾预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中突发现象源于隐藏因果交互，需解决在数据生成过程未知且部分可观测情况下，揭示和利用系统潜在因果结构以实现早期检测的挑战。

Method: 从经验协方差或精度矩阵的单参数估计器族中学习最优特征表示，再用监督学习模块对其分类。

Result: 在癫痫发作检测和客户流失预测中取得有竞争力的结果。

Conclusion: 最优协方差幂在捕获结构特征时具有良好的可识别性，能兼顾预测性能和可解释的统计结构。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [170] [Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning](https://arxiv.org/abs/2601.21294)
*Anders Gjølbye,Ida Kargaard,Emma Kargaard,Lars Kai Hansen*

Main category: cs.LG

TL;DR: 研究在独立逐条目随机缺失下的PLS - SVD，揭示其相变现象并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 多模态数据集常存在双视图条目缺失，需研究在随机缺失情况下的PLS - SVD。

Method: 在比例高维尖峰模型中研究独立逐条目随机缺失下的PLS - SVD，分析归一化后掩蔽交叉协方差矩阵性质。

Result: PLS - SVD呈现尖锐的BBP类型相变，有临界信噪比阈值，且有渐近重叠公式。模拟和半合成多模态实验证实了相变图和恢复曲线。

Conclusion: 在条目随机缺失情况下，PLS - SVD有可预测的相变行为，实验结果与理论相符。

Abstract: Partial Least Squares (PLS) learns shared structure from paired data via the top singular vectors of the empirical cross-covariance (PLS-SVD), but multimodal datasets often have missing entries in both views. We study PLS-SVD under independent entry-wise missing-completely-at-random masking in a proportional high-dimensional spiked model. After appropriate normalization, the masked cross-covariance behaves like a spiked rectangular random matrix whose effective signal strength is attenuated by $\sqrtρ$, where $ρ$ is the joint entry retention probability. As a result, PLS-SVD exhibits a sharp BBP-type phase transition: below a critical signal-to-noise threshold the leading singular vectors are asymptotically uninformative, while above it they achieve nontrivial alignment with the latent shared directions, with closed-form asymptotic overlap formulas. Simulations and semi-synthetic multimodal experiments corroborate the predicted phase diagram and recovery curves across aspect ratios, signal strengths, and missingness levels.

</details>


### [171] [Textual Equilibrium Propagation for Deep Compound AI Systems](https://arxiv.org/abs/2601.21064)
*Minghui Chen,Wenlong Deng,James Zou,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 针对长程AI系统性能随深度下降问题，提出文本平衡传播（TEP）方法，在多种数据集上提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有全局传播文本反馈的方法在系统深度增加时性能下降，长程工作流存在爆炸和消失两种梯度问题。

Method: 引入基于能量模型平衡传播的文本平衡传播（TEP），包括自由阶段和微调阶段，支持局部优化和全局目标适应。

Result: 在长程问答基准和多智能体工具使用数据集上，TEP比全局传播方法如TextGrad持续提高了准确性和效率，且随着深度增加增益更大。

Conclusion: TEP解决了长程AI系统深度扩展问题，保留了黑盒大语言模型组件实用性。

Abstract: Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.

</details>


### [172] [Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle](https://arxiv.org/abs/2601.21301)
*Zijun Chen,Zaiwei Chen,Nian Si,Shengbo Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the convergence rates of synchronous and asynchronous Q-learning for average-reward Markov decision processes, where the absence of contraction poses a fundamental challenge. Existing non-asymptotic results overcome this challenge by either imposing strong assumptions to enforce seminorm contraction or relying on discounted or episodic Markov decision processes as successive approximations, which either require unknown parameters or result in suboptimal sample complexity. In this work, under a reachability assumption, we establish optimal $\widetilde{O}(\varepsilon^{-2})$ sample complexity guarantees (up to logarithmic factors) for a simple variant of synchronous and asynchronous Q-learning that samples from the lazified dynamics, where the system remains in the current state with some fixed probability. At the core of our analysis is the construction of an instance-dependent seminorm and showing that, after a lazy transformation of the Markov decision process, the Bellman operator becomes one-step contractive under this seminorm.

</details>


### [173] [Out-of-Distribution Generalization in Graph Foundation Models](https://arxiv.org/abs/2601.21067)
*Haoyang Li,Haibo Chen,Xin Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文从分布外泛化视角综述图基础模型进展，讨论挑战、归纳方法、回顾评估协议并指出未来方向，是该领域首篇综述。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在训练分布外泛化能力有限，实际中存在分布转移问题，因此要研究图基础模型的分布外泛化。

Method: 先讨论图学习中分布转移挑战并给出统一问题设定，再按是否针对固定任务或跨异构任务组织现有方法，总结对应处理策略和预训练目标。

Result: 回顾了常见评估协议。

Conclusion: 指出未来研究的开放方向。

Abstract: Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.

</details>


### [174] [Partial Feedback Online Learning](https://arxiv.org/abs/2601.21462)
*Shihao Shao,Cong Fang,Zhouchen Lin,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文研究了部分反馈在线学习，对可实现情况下确定性和随机学习者的最小最大遗憾进行了近乎完整的刻画，引入了新维度，识别了确定性和随机可学习性的不可分离条件，解决了开放问题，还指出了可实现情况外学习的困难。


<details>
  <summary>Details</summary>
Motivation: 研究部分反馈在线学习，其适用于如语言生成等多正确响应但数据仅提供单参考的场景，为理解和分析该学习模式的性质和能力。

Method: 为确定性学习者引入Partial - Feedback Littlestone dimension (PFLdim)，为随机学习者开发Partial - Feedback Measure Shattering dimension (PMSdim)，并识别可学习性不可分离的条件。

Result: 对可实现情况下确定性和随机学习者的最小最大遗憾进行近乎完整刻画，识别可学习性不可分离条件，解决开放问题，指出可实现情况外学习可能信息论上难以处理。

Conclusion: 在可实现情况外需要新的、对噪声敏感的复杂度度量来刻画可学习性。

Abstract: We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.

</details>


### [175] [LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection](https://arxiv.org/abs/2601.21082)
*Shivam Patel,William Cocke,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出LOCUS方法处理大语言模型管理难题，实验显示其优势


<details>
  <summary>Details</summary>
Motivation: 大语言模型生态快速发展，有效管理和利用大量动态模型池面临挑战

Method: 提出基于注意力的LOCUS方法生成低维向量嵌入表示，还训练了正确性预测器

Result: LOCUS生成有信息和稳健的嵌入所需查询评估样本比基线少4.8倍，学习的嵌入空间有几何意义

Conclusion: LOCUS能解决大语言模型管理问题，有多种下游应用潜力

Abstract: The rapidly growing ecosystem of Large Language Models (LLMs) makes it increasingly challenging to manage and utilize the vast and dynamic pool of models effectively. We propose LOCUS, a method that produces low-dimensional vector embeddings that compactly represent a language model's capabilities across queries. LOCUS is an attention-based approach that generates embeddings by a deterministic forward pass over query encodings and evaluation scores via an encoder model, enabling seamless incorporation of new models to the pool and refinement of existing model embeddings without having to perform any retraining. We additionally train a correctness predictor that uses model embeddings and query encodings to achieve state-of-the-art routing accuracy on unseen queries. Experiments show that LOCUS needs up to 4.8x fewer query evaluation samples than baselines to produce informative and robust embeddings. Moreover, the learned embedding space is geometrically meaningful: proximity reflects model similarity, enabling a range of downstream applications including model comparison and clustering, model portfolio selection, and resilient proxies of unavailable models.

</details>


### [176] [PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization](https://arxiv.org/abs/2601.21470)
*Ruicheng Ao,Hongyu Chen,Haoyang Liu,David Simchi-Levi,Will Wei Sun*

Main category: cs.LG

TL;DR: 研究标签数据稀缺时半监督随机优化，结合PPI和SVRG提出PPI - SVRG，给出收敛边界，实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 在标签数据稀缺但有预训练模型预测的情况下，研究半监督随机优化方法。

Method: 证明PPI和SVRG在数学上等价，结合二者开发PPI - SVRG，给出收敛边界。

Result: 实验表明，PPI - SVRG在均值估计基准上减少43 - 52%的均方误差，在MNIST数据集上仅使用10%标签数据时提高2.7 - 2.9个百分点的测试准确率。

Conclusion: PPI - SVRG结合了PPI和SVRG的优势，在标签稀缺时能取得较好效果，预测质量影响收敛邻域大小。

Abstract: We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\% labeled data.

</details>


### [177] [MapPFN: Learning Causal Perturbation Maps in Context](https://arxiv.org/abs/2601.21092)
*Marvin Sextro,Weronika Kłos,Gabriel Dernbach*

Main category: cs.LG

TL;DR: 提出MapPFN进行扰动效应估计，可预测扰动后分布，性能与基于真实单细胞数据训练的模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法利用新干预证据在推理时适应训练数据之外的情况，需要能适应未见生物环境的治疗效应模型。

Method: 提出MapPFN，在基于因果扰动先验生成的合成数据上预训练，利用上下文学习预测扰动后分布，无需基于梯度的优化。

Result: 尽管仅在计算机模拟基因敲除上预训练，MapPFN能识别差异表达基因，性能与基于真实单细胞数据训练的模型相当。

Conclusion: MapPFN可用于生物系统中有效干预措施的规划，能适应新的生物环境。

Abstract: Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at https://github.com/marvinsxtr/MapPFN.

</details>


### [178] [Best Arm Identification with LLM Judges and Limited Human](https://arxiv.org/abs/2601.21471)
*Ruicheng Ao,Hongyu Chen,Siyang Gao,Hanwei Li,David Simchi-Levi*

Main category: cs.LG

TL;DR: 研究有廉价但有偏代理和昂贵真实标签的固定置信度最佳臂识别问题，提出结合代理分数与逆倾向加权残差的估计器及算法，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 经典多保真度方法在代理有偏且真实标签选择性观察时会误选最佳臂，统一审计效率低，需解决这些问题。

Method: 开发结合代理分数与逆倾向加权残差的估计器，形成任意时间有效的置信序列，提出自适应选择和审计臂的算法。

Result: 证明插件式Neyman规则实现接近神谕的审计效率，数值实验验证理论保证和算法优越性能。

Conclusion: 提出的算法能有效解决有偏代理下的最佳臂识别问题，具有良好的理论保证和实证性能。

Abstract: We study fixed-confidence best-arm identification (BAI) where a cheap but potentially biased proxy (e.g., LLM judge) is available for every sample, while an expensive ground-truth label can only be acquired selectively when using a human for auditing. Unlike classical multi-fidelity BAI, the proxy is biased (arm- and context-dependent) and ground truth is selectively observed. Consequently, standard multi-fidelity methods can mis-select the best arm, and uniform auditing, though accurate, wastes scarce resources and is inefficient. We prove that without bias correction and propensity adjustment, mis-selection probability may not vanish (even with unlimited proxy data). We then develop an estimator for the mean of each arm that combines proxy scores with inverse-propensity-weighted residuals and form anytime-valid confidence sequences for that estimator. Based on the estimator and confidence sequence, we propose an algorithm that adaptively selects and audits arms. The algorithm concentrates audits on unreliable contexts and close arms and we prove that a plug-in Neyman rule achieves near-oracle audit efficiency. Numerical experiments confirm the theoretical guarantees and demonstrate the superior empirical performance of the proposed algorithm.

</details>


### [179] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 研究安全强化学习算法在分布偏移下的安全泛化性，发现存在安全泛化差距，测试时间屏蔽可有效恢复安全性。


<details>
  <summary>Details</summary>
Motivation: 探究安全强化学习在训练时的安全保证能否在分布偏移的部署中有效，以糖尿病管理为测试平台。

Method: 在统一临床模拟器上对安全强化学习算法进行基准测试，采用测试时间屏蔽方法过滤不安全动作。

Result: 测试时间屏蔽能在不同算法和患者群体中有效恢复安全性，在多种情况下实现时间在范围内增益，降低临床风险指数和血糖变异性。

Conclusion: 所提出的模拟器和基准为研究安全关键控制领域分布偏移下的安全性提供了平台。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [180] [TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning](https://arxiv.org/abs/2601.21135)
*Shicheng Fan,Kun Zhang,Lu Cheng*

Main category: cs.LG

TL;DR: 现有时间因果表示学习方法假设因果机制在离散域间瞬间切换，但现实系统机制过渡连续。本文提出TRACE框架，理论证明可联合识别潜在因果变量和连续混合轨迹，实验表明该方法大幅优于离散切换基线。


<details>
  <summary>Details</summary>
Motivation: 现有时间因果表示学习方法假设机制瞬间切换，与现实系统连续机制过渡不符。

Method: 将过渡机制建模为有限原子机制的凸组合，由时变混合系数控制；提出TRACE框架，每个专家学习一种原子机制。

Result: 实验表明，TRACE恢复混合轨迹的相关性高达0.99，大幅优于离散切换基线。

Conclusion: TRACE框架能有效处理连续机制过渡问题，且可推广到训练中未观察到的中间机制状态。

Abstract: Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.

</details>


### [181] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir,Tommer D. Keidar,Noam Levi,Shlomi Reuveni,Barak Hirshberg*

Main category: cs.LG

TL;DR: 论文指出在固定预算下coverage@cost是更合适的指标，连接了pass@k和coverage@cost，提出ReD方法增加coverage@cost并能预测节省的尝试次数，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决在固定预算下衡量大语言模型在可验证任务上性能的更合适指标及提升coverage@cost的问题。

Method: 提出Reset - and - Discard (ReD)查询方法。

Result: 在三个大语言模型上使用HumanEval进行实验，ReD大幅减少达到期望覆盖率所需的尝试次数、令牌数和美元成本，还能有效测量推理幂律。

Conclusion: ReD方法能在任意给定预算下增加coverage@cost，可定量预测节省的尝试次数，在缺少pass@k时能推断其幂律指数。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [182] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文提出动态截止半径公式解决机器学习原子间势（MLIPs）推理时间和内存消耗瓶颈，在4种MLIPs上实现并验证效果，误差小且代码开源。


<details>
  <summary>Details</summary>
Motivation: MLIPs面临推理时间和内存消耗两大瓶颈，阻碍其达到现实模拟规模。

Method: 挑战MLIP截止半径必须固定的传统观念，引入动态截止半径公式，对原子图引入稀疏性。

Result: 在4种先进MLIPs上实现动态截止半径，内存消耗减少2.26倍，推理时间加快2.04倍，误差分析显示精度下降极小。

Conclusion: 动态截止半径有效解决MLIPs的两大瓶颈，且精度损失小，代码开源利于后续研究。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [183] [Graph-Free Root Cause Analysis](https://arxiv.org/abs/2601.21359)
*Luan Pham*

Main category: cs.LG

TL;DR: 文章提出无依赖图时的RCA框架PRISM，在多数据集上表现优。


<details>
  <summary>Details</summary>
Motivation: 复杂系统故障需快速RCA，现有无依赖图RCA方法假设根因异常得分最高，故障传播时该假设失效。

Method: 提出PRISM框架，并为一类基于组件的系统制定理论保证的RCA方法。

Result: 在9个真实数据集的735次故障中，PRISM的Top - 1准确率达68%，比最佳基线提升258%，每次诊断仅需8ms。

Conclusion: PRISM是一种简单高效的无依赖图RCA框架。

Abstract: Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.

</details>


### [184] [Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation](https://arxiv.org/abs/2601.21636)
*Jan Schuchardt,Nikita Kalinin*

Main category: cs.LG

TL;DR: 本文研究随机分配下差分隐私模型训练的隐私放大问题，提出无采样边界方法并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Choquette - Choo等人（2025）基于采样的蒙特卡罗方法有局限性，保证仅在高概率下成立或需机制随机弃权，且所需样本数与δ成反比。

Method: 基于Rényi散度和条件组合开发无采样边界，前者用动态规划高效计算边界，后者为小ε提供更强隐私保证。

Result: 通过数值比较，证明该方法在研究和实践中广泛使用的矩阵机制上有效。

Conclusion: 提出的无采样边界方法有效，适用于任意带状和非带状矩阵。

Abstract: We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.

</details>


### [185] [Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement](https://arxiv.org/abs/2601.21149)
*Maria Despoina Siampou,Shushman Choudhury,Shang-Ling Hsu,Neha Arora,Cyrus Shahabi*

Main category: cs.LG

TL;DR: 提出ME - POIs框架，结合人类移动数据学习POI表示，在地图丰富任务中表现出色，凸显POI功能对POI表示的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态文本元数据的地点身份或与轨迹上下文相关的表示，缺少POI功能这一关键信号，需要学习基于实际使用的通用POI表示。

Method: 引入ME - POIs框架，用大规模人类移动数据增强语言模型得到的POI嵌入，通过对比学习捕捉使用模式，提出新机制解决长尾稀疏问题。

Result: 在五个新提出的地图丰富任务中，ME - POIs增强文本嵌入始终优于仅文本和仅移动性的基线，仅用移动数据训练的ME - POIs在某些任务上可超越仅文本模型。

Conclusion: POI功能是准确且通用的POI表示的关键组成部分。

Abstract: Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.

</details>


### [186] [Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations](https://arxiv.org/abs/2601.21150)
*Henry Kvinge,Andrew Aguilar,Nayda Farnsworth,Grace O'Brien,Robert Jasper,Sarah Scullen,Helen Jenne*

Main category: cs.LG

TL;DR: 本文探讨AI在数学研究中的应用，以神经网络预测群运算为测试，发现模型能学习抽象代数性质，但部分概念难以提取。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习社区用AI解决数学问题多为考试风格范式，为推动数学研究，需超越简单问答，探究窄模型学习更广泛数学结构的程度。

Method: 以训练神经网络预测群运算为测试用例，设计一系列测试评估模型是否掌握群论概念。

Result: 模型学习的表示能捕捉抽象代数性质，如模运算的交换性，可训练线性分类器区分某些子群元素，但无法提取单位元概念。

Conclusion: 在某些情况下，小型神经网络的表示可用于从新数学对象中提取有趣的抽象结构。

Abstract: While a real-world research program in mathematics may be guided by a motivating question, the process of mathematical discovery is typically open-ended. Ideally, exploration needed to answer the original question will reveal new structures, patterns, and insights that are valuable in their own right. This contrasts with the exam-style paradigm in which the machine learning community typically applies AI to math. To maximize progress in mathematics using AI, we will need to go beyond simple question answering. With this in mind, we explore the extent to which narrow models trained to solve a fixed mathematical task learn broader mathematical structure that can be extracted by a researcher or other AI system. As a basic test case for this, we use the task of training a neural network to predict a group operation (for example, performing modular arithmetic or composition of permutations). We describe a suite of tests designed to assess whether the model captures significant group-theoretic notions such as the identity element, commutativity, or subgroups. Through extensive experimentation we find evidence that models learn representations capable of capturing abstract algebraic properties. For example, we find hints that models capture the commutativity of modular arithmetic. We are also able to train linear classifiers that reliably distinguish between elements of certain subgroups (even though no labels for these subgroups are included in the data). On the other hand, we are unable to extract notions such as the concept of the identity element. Together, our results suggest that in some cases the representations of even small neural networks can be used to distill interesting abstract structure from new mathematical objects.

</details>


### [187] [Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting](https://arxiv.org/abs/2601.21151)
*Carlos A. Pereira,Stéphane Gaudreault,Valentin Dallerit,Christopher Subich,Shoyon Panday,Siqi Wei,Sasa Zhang,Siddharth Rout,Eldad Haber,Raymond J. Spiteri,David Millard,Emilia Diaconescu*

Main category: cs.LG

TL;DR: 提出物理启发的全球天气预报模型PARADIS，以较低训练成本实现了先进预报能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习天气预报方法采用整体架构，对平流处理存在问题，需高成本机制。

Method: 将网络行为功能分解为平流、扩散和反应模块，通过神经半拉格朗日算子实现平流，深度可分离空间混合建模扩散，点态通道交互建模局部源项和垂直交互。

Result: PARADIS以较低训练成本表现出先进预报能力，在ERA5基准测试中超越传统和机器学习基线模型。

Conclusion: PARADIS模型在降低训练成本的同时，具备卓越的天气预报性能。

Abstract: Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.

</details>


### [188] [LoRA and Privacy: When Random Projections Help (and When They Don't)](https://arxiv.org/abs/2601.21719)
*Yaxi Hu,Johanna Düngler,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 本文介绍Wishart投影机制并研究其差分隐私特性，证明向量查询无噪声下的差分隐私保证，矩阵查询无噪声时不满足DP，分析噪声变体，指出LoRA非固有隐私，低秩微调更隐私，实验显示精确计算可降低噪声提高精度。


<details>
  <summary>Details</summary>
Motivation: 研究Wishart投影机制的差分隐私特性。

Method: 理论证明向量和矩阵查询的差分隐私情况，分析噪声变体，将LoRA更新作为矩阵机制实例分析，进行初步实验。

Result: 向量查询无噪声有差分隐私保证；矩阵查询无噪声不满足DP，可实施近完美成员推理攻击；噪声变体有隐私放大效果；LoRA非固有隐私，低秩微调更隐私；精确计算可降低噪声提高精度。

Conclusion: Wishart投影机制在不同查询类型下差分隐私表现不同，低秩微调可提升隐私性，精确计算有益。

Abstract: We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $> 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.

</details>


### [189] [A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components](https://arxiv.org/abs/2601.21160)
*Michael Ibrahim,Nagi Gebraeel,Weijun Xie*

Main category: cs.LG

TL;DR: 研究联邦聚类在簇总数未知且客户数据簇异构重叠场景，提出FedGEM算法，理论研究加实验证明其性能好。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中跨客户端簇总数未知，客户端数据簇异构重叠的问题。

Method: 开发FedGEM算法，客户端本地执行EM步骤并构建不确定集，服务器利用其学习重叠和推断全局簇数量，对算法进行理论研究。

Result: 算法有概率收敛保证，在特定设置下有低复杂度计算方法，实验显示性能与集中式EM相当，优于现有联邦聚类方法。

Conclusion: 提出的FedGEM算法在未知簇数量的联邦聚类问题上表现良好。

Abstract: We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.

</details>


### [190] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 为在隐私敏感和资源受限环境部署编码代理，需有能力的小型语言模型。现有任务中心学习范式有局限，提出以存储库为中心学习范式训练SWE - Spot - 4B模型，表现出色，且该范式训练效率高、推理成本低。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感和资源受限环境部署编码代理需要有能力的开放权重小型语言模型，但现有模型存在泛化能力不足问题，现有任务中心学习范式无法解决。

Method: 提出以存储库为中心学习范式（RCL），设计四单元存储库中心体验将静态代码库转化为交互式学习信号，训练SWE - Spot - 4B模型。

Result: SWE - Spot - 4B模型打破既定扩展趋势，在多个软件工程任务中优于更大的开放权重模型，超越或匹配注重效率的商业模型。

Conclusion: 以存储库为中心学习范式训练样本效率更高、推理成本更低，掌握存储库是构建高效智能的独特且必要维度，可补充通用编码能力。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [191] [Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle](https://arxiv.org/abs/2601.21739)
*Alberto Fernández-Hernández,Cristian Pérez-Corral,Jose I. Mestre,Manuel F. Dolz,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 本文解释了Adam优化器中动量参数β₁ = β₂时训练效果提升的原因，指出这与梯度尺度不变性有关，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解释Adam优化器中动量参数β₁ = β₂时验证分数和训练表现提升的原因。

Method: 形式化梯度尺度不变性的概念，证明Adam在β₁ = β₂时具有一阶梯度尺度不变性，并进行跨视觉、语言任务和不同架构的实验。

Result: 实验表明β₁ = β₂时，梯度重新缩放对更新的影响更平滑。

Conclusion: 为Adam优化器的这一现象提供了连贯解释，并为未来优化器设计提供原则。

Abstract: Adam has been at the core of large-scale training for almost a decade, yet a simple empirical fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy $β_{1}=β_{2}$. Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if $β_{1}=β_{2}$. This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when $β_{1}=β_{2}$. Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.

</details>


### [192] [Efficient Simple Regret Algorithms for Stochastic Contextual Bandits](https://arxiv.org/abs/2601.21167)
*Shuai Liu,Alireza Bakhtiari,Alex Ayoub,Botao Hao,Csaba Szepesvári*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\tilde{\mathcal{O}}(d/\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $κ= \mathcal O(\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$ -- again with no dependence on $κ$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.

</details>


### [193] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: 提出可解释分类方法ECSEL，学习符号多项式方程，在符号回归基准测试表现好，兼顾效率与可解释性，实验证明能暴露数据偏差等。


<details>
  <summary>Details</summary>
Motivation: 许多符号回归基准存在紧凑的符号多项式结构，由此提出学习符号多项式方程的可解释分类方法ECSEL。

Method: 直接构建作为分类器和解释的结构化闭合形式表达式。

Result: 在标准符号回归基准中比竞品恢复更多目标方程，且计算量小；分类准确率与成熟机器学习模型相当且不牺牲可解释性；满足一些关于全局特征行为等的理想属性；实验显示学习到的方程能暴露数据偏差等。

Conclusion: ECSEL方法有效，能在保证效率的同时提供可解释性，还能在实际应用中暴露数据偏差、支持反事实推理和产生可行见解。

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [194] [AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection](https://arxiv.org/abs/2601.21171)
*Kamal Berahmand,Saman Forouzandeh,Mehrnoush Mohammadi,Parham Moradi,Mahdi Jalili*

Main category: cs.LG

TL;DR: 提出AC2L - GAD框架用于图异常检测，结合主动选择和反事实生成，减少计算开销并提升性能


<details>
  <summary>Details</summary>
Motivation: 解决图异常检测中标签稀缺、类别不平衡问题，以及现有图对比学习方法随机增强破坏语义一致性和负采样无信息的局限性

Method: 提出AC2L - GAD框架，结合信息论主动选择和反事实生成，识别复杂节点并生成有效正增强和困难负样本

Result: 在九个基准数据集上实验，减少约65%计算开销，性能优于或等同于现有基线，在异常属性 - 结构交互复杂的数据集上表现更佳

Conclusion: AC2L - GAD框架有效解决现有图异常检测和对比学习方法的问题

Abstract: Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

</details>


### [195] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 现有实体对齐（EA）模型缺乏可迁移性，直接用图基础模型（GFMs）效果不佳，提出并行编码策略驱动的EA基础模型，实验验证其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有EA模型缺乏可迁移性，直接适配GFMs效果差，存在“推理视野差距”问题。

Method: 提出并行编码策略驱动的EA基础模型，利用种子EA对作为局部锚点引导信息流，初始化并同时编码两个并行流，引入合并关系图和可学习交互模块。

Result: 广泛实验验证了框架的有效性。

Conclusion: 提出的模型具有较强的对未见知识图谱的泛化性。

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [196] [Flow Perturbation++: Multi-Step Unbiased Jacobian Estimation for High-Dimensional Boltzmann Sampling](https://arxiv.org/abs/2601.21177)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 提出Flow Perturbation++方法用于无偏Boltzmann采样，在高维系统中降低估计方差，在高斯混合模型和Chignolin蛋白上表现更好。


<details>
  <summary>Details</summary>
Motivation: 连续归一化流（CNFs）在高维系统中无偏Boltzmann采样的可扩展性受Jacobian行列式评估成本限制，现有方法有偏差或高方差问题。

Method: 提出Flow Perturbation++，离散化概率流ODE并在每个积分步骤进行无偏逐步Jacobian估计。

Result: 集成到Sequential Monte Carlo框架中，在1000D高斯混合模型和全原子Chignolin蛋白上，与基于Hutchinson和单步Flow Perturbation基准相比，平衡采样显著改善。

Conclusion: Flow Perturbation++保留了Flow Perturbation的无偏性，同时显著降低了估计方差。

Abstract: The scalability of continuous normalizing flows (CNFs) for unbiased Boltzmann sampling remains limited in high-dimensional systems due to the cost of Jacobian-determinant evaluation, which requires $D$ backpropagation passes through the flow layers. Existing stochastic Jacobian estimators such as the Hutchinson trace estimator reduce computation but introduce bias, while the recently proposed Flow Perturbation method is unbiased yet suffers from high variance. We present \textbf{Flow Perturbation++}, a variance-reduced extension of Flow Perturbation that discretizes the probability-flow ODE and performs unbiased stepwise Jacobian estimation at each integration step. This multi-step construction retains the unbiasedness of Flow Perturbation while achieves substantially lower estimator variance. Integrated into a Sequential Monte Carlo framework, Flow Perturbation++ achieves significantly improved equilibrium sampling on a 1000D Gaussian Mixture Model and the all-atom Chignolin protein compared with Hutchinson-based and single-step Flow Perturbation baselines.

</details>


### [197] [Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions](https://arxiv.org/abs/2601.21873)
*Jinhang Chai,Xuyuan Liu,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 研究结构化矩阵估计的迁移学习，提出通用迁移框架及估计器，建立误差界并应用于两个典型问题。


<details>
  <summary>Details</summary>
Motivation: 学习系统常扩展特征或潜在表示，研究环境维度和内在表示同时增长下结构化矩阵估计的迁移学习。

Method: 提出目标参数分解的通用迁移框架，开发锚定交替投影估计器。

Result: 建立确定性误差界，在秩和稀疏性增量小时有严格改进的速率，应用于两个问题并得到相关理论保证和实证验证。

Conclusion: 所提框架具有通用性，在相关问题中能实现迁移增益。

Abstract: Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.
  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.
  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.

</details>


### [198] [Rethinking Refinement: Correcting Generative Bias without Noise Injection](https://arxiv.org/abs/2601.21182)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 本文提出基于流匹配的双阶段流 refinement 框架 BFR，可在不扰动采样动态情况下有效修正生成模型偏差，实验显示能提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和基于流的模型）存在系统偏差，会降低样本质量，尤其是在高维场景。

Method: 提出基于流匹配的 Bi - stage Flow Refinement (BFR) 框架，包含针对近似可逆生成器的隐空间对齐和采用轻量级增强训练的数据空间 refinement 两种策略，且保留原 ODE 轨迹并对生成样本进行确定性修正。

Result: 在 MNIST、CIFAR - 10 和 256x256 分辨率的 FFHQ 上实验，保真度和覆盖度持续提升；在 MNIST 上，从 FID 3.95 的基础样本起始，仅用一次额外函数评估，隐空间 refinement 实现 1.46 的 FID 最优结果，同时保持样本多样性。

Conclusion: BFR 框架是一种有效的后处理偏差校正方法，能提升生成模型的样本质量。

Abstract: Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.

</details>


### [199] [Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification](https://arxiv.org/abs/2601.21203)
*Weiguang Wang,Yong Liu,Yingjie Gao,Guangyuan Xu*

Main category: cs.LG

TL;DR: 提出一种基于自训练范式的跨主体域适应方法用于解码SSVEP信号，在多数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: SSVEP信号跨主体的可变性和昂贵的用户特定注释限制了识别性能，需要更有效的方法准确解码信号。

Method: 提出Filter - Bank Euclidean Alignment (FBEA)策略；构建Cross - Subject Self - Training (CSST)框架，含Pre - Training with Adversarial Learning (PTAL)和Dual - Ensemble Self - Training (DEST)两阶段；引入Time - Frequency Augmented Contrastive Learning (TFA - CL)模块。

Result: 在Benchmark和BETA数据集上的广泛实验表明，该方法在不同信号长度下均达到了最先进的性能。

Conclusion: 所提出的跨主体域适应方法具有优越性，能有效提高SSVEP信号解码的准确率。

Abstract: Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.

</details>


### [200] [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136)
*Gloria Felicia,Michael Eniolade,Jinfeng He,Zitha Sasindran,Hemant Kumar,Milan Hussain Angati,Sandeep Bandarupalli*

Main category: cs.LG

TL;DR: 提出StepShield基准评估违规检测时间，引入新指标，发现LLM与静态分析器性能差异，证明早期检测有经济优势。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全基准仅报告二元准确率，无法衡量违规检测时间，需新基准。

Method: 引入StepShield基准，含大量代码智能体轨迹，提出早期干预率等三个新的时间指标。

Result: LLM法官早期干预率达59%，静态分析器仅26%；混合检测器降低75%监测成本，五年企业级节省1.08亿美元。

Conclusion: StepShield从检测时间角度评估，为构建更安全经济的AI智能体提供新基础。

Abstract: Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.

</details>


### [201] [A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models](https://arxiv.org/abs/2601.21207)
*Chuan-Shen Hu*

Main category: cs.LG

TL;DR: 本文引入细胞层理论框架分析图架构中节点特征和边权重的局部一致性和调和性，还提出多尺度扩展方法，为基于几何和拓扑结构的GDL和TDL架构提供见解。


<details>
  <summary>Details</summary>
Motivation: GDL和TDL特征在训练期间的分布和扩散行为是一个未充分探索的问题。

Method: 引入细胞层理论框架，通过层结构跟踪局部特征对齐和一致性；提出受拓扑数据分析启发的多尺度扩展方法。

Result: 为GDL和TDL架构提供了基于底层几何和拓扑结构以及学习信号的联合表征。

Conclusion: 该方法为节点分类、子结构检测和社区检测等传统任务的未来研究提供了见解。

Abstract: Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.

</details>


### [202] [Temporal Context and Architecture: A Benchmark for Naturalistic EEG Decoding](https://arxiv.org/abs/2601.21215)
*Mehmet Ergezer*

Main category: cs.LG

TL;DR: 研究自然主义EEG解码中模型架构与时间上下文的交互，对比五种架构，揭示效率 - 鲁棒性权衡。


<details>
  <summary>Details</summary>
Motivation: 探究模型架构和时间上下文在自然主义EEG解码中的相互作用。

Method: 使用HBN电影观看数据集，在4类任务上对CNN、LSTM、EEGXF、S4和S5五种架构在8s到128s的片段长度进行基准测试，还评估零样本跨频率偏移、跨任务OOD输入和留一受试者泛化。

Result: 较长上下文能提高准确率，64s时S5准确率达98.7%±0.6且参数比CNN少约20倍；S5跨受试者准确性强但在OOD任务有过度自信错误，EEGXF在频率偏移下更保守稳定但分布内校准不足。

Conclusion: 存在实际的效率 - 鲁棒性权衡，参数高效追求峰值准确率选S5，鲁棒性和保守不确定性要求高选EEGXF。

Abstract: We study how model architecture and temporal context interact in naturalistic EEG decoding. Using the HBN movie-watching dataset, we benchmark five architectures, CNN, LSTM, a stabilized Transformer (EEGXF), S4, and S5, on a 4-class task across segment lengths from 8s to 128s. Accuracy improves with longer context: at 64s, S5 reaches 98.7%+/-0.6 and CNN 98.3%+/-0.3, while S5 uses ~20x fewer parameters than CNN. To probe real-world robustness, we evaluate zero-shot cross-frequency shifts, cross-task OOD inputs, and leave-one-subject-out generalization. S5 achieves stronger cross-subject accuracy but makes over-confident errors on OOD tasks; EEGXF is more conservative and stable under frequency shifts, though less calibrated in-distribution. These results reveal a practical efficiency-robustness trade-off: S5 for parameter-efficient peak accuracy; EEGXF when robustness and conservative uncertainty are critical.

</details>


### [203] [Optimistic Transfer under Task Shift via Bellman Alignment](https://arxiv.org/abs/2601.21924)
*Jinhang Chai,Enpei Zhang,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 研究在线转移强化学习，提出RWT方法，建立后悔界，实证显示有改进。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习中，直接复用源贝尔曼更新有偏差，使后悔保证失效。

Method: 确定一步贝尔曼对齐为转移的正确抽象，提出重加权目标（RWT），构建两阶段RWT Q - 学习框架。

Result: 在RKHS函数逼近下建立与任务转移复杂度相关的后悔界，实证在表格和神经网络设置中比单任务学习和简单合并有改进。

Conclusion: 贝尔曼对齐是在线强化学习中与模型无关的转移原则。

Abstract: We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.

</details>


### [204] [Soft Quantization: Model Compression Via Weight Coupling](https://arxiv.org/abs/2601.21219)
*Daniel T. Bernstein,Luca Di Carlo,David Schwab*

Main category: cs.LG

TL;DR: 引入神经网络权重短程吸引耦合实现模型量化，软量化方案表现优，提供新压缩管道和研究工具。


<details>
  <summary>Details</summary>
Motivation: 寻找新的模型量化途径。

Method: 在训练时引入神经网络权重的短程吸引耦合，利用两个额外超参数实现混合精度的权重分布离散化。

Result: 在合适超参数范围内，软量化方案在ResNet - 20/CIFAR - 10上优于直方图均衡化的训练后量化。

Conclusion: 软量化为机器学习模型的灵活压缩提供新管道，为研究高维损失景观中压缩与泛化的权衡提供新工具。

Abstract: We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.

</details>


### [205] [PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations](https://arxiv.org/abs/2601.21234)
*Kaiyuan Tan,Kendra Givens,Peilun Li,Thomas Beckers*

Main category: cs.LG

TL;DR: 提出PHDME框架用于稀疏观测和不完整物理信息下的动力学系统轨迹预测，在数据稀缺时提升了准确性和物理一致性


<details>
  <summary>Details</summary>
Motivation: 扩散模型在稀疏数据下不可靠，物理信息机器学习大多需明确控制方程，而实际中常只有部分已知

Method: 先在有限观测上训练高斯过程分布式端口哈密顿系统（GP - dPHS），用其生成人工数据集进行扩散训练并引入物理残差损失，训练后扩散模型用于快速轨迹生成，最后用分裂共形校准提供不确定性陈述

Result: 在偏微分方程基准和真实世界弹簧系统实验中，在数据稀缺时精度和物理一致性得到提高

Conclusion: PHDME框架在稀疏观测和不完整物理信息场景下有效，可提升轨迹预测效果

Abstract: Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.

</details>


### [206] [Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks](https://arxiv.org/abs/2601.21242)
*Luwei Sun,Dongrui Shen,Jianfe Li,Yulong Zhao,Han Feng*

Main category: cs.LG

TL;DR: 本文针对条件生成建模挑战，构建近似比率型泛函的理论框架，给出收敛率和误差界，为扩散生成模型有限样本训练提供泛化保证。


<details>
  <summary>Details</summary>
Motivation: 解决条件生成建模中目标条件密度为比率形式的挑战，该形式在扩散生成模型中很关键。

Method: 使用具有SignReLU激活函数的深度神经网络近似比率型泛函，利用其分段结构；针对DDPM构建基于SignReLU的反向过程神经估计器。

Result: 建立L^p(Omega)近似界和收敛率；导出生成与真实数据分布间的Kullback - Leibler超额风险界，并将超额风险分解为近似和估计误差分量。

Conclusion: 为扩散生成模型的有限样本训练提供泛化保证。

Abstract: Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.

</details>


### [207] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: 提出LENS框架解决RLVR探索效率低问题，实验显示其性能和收敛速度优于GRPO。


<details>
  <summary>Details</summary>
Motivation: RLVR在复杂任务中因探索效率低导致采样成功率低和训练不稳定，发现部分探索失败源于少量干扰提示词。

Method: 提出LENS框架，先识别并移除干扰词进行提示，再将净化过程中的成功滚动转移到原噪声提示上监督策略优化。

Result: LENS显著优于GRPO，平均性能提升3.88%，收敛速度加快超1.6倍。

Conclusion: 修剪干扰词对提高滚动效率至关重要，为RLVR研究提供新视角。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [208] [Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences](https://arxiv.org/abs/2601.21246)
*Namkyung Yoon,Sanghong Kim,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出基于峰感知条件生成模型的人工智能判别框架，提升GC - MS在干扰条件下测量可靠性，经评估验证有效且能减少误报。


<details>
  <summary>Details</summary>
Motivation: GC - MS在有干扰物质时测量可靠性下降，存在非特异性峰、保留时间偏移和背景噪声增加等问题，需提高测量可靠性。

Method: 提出基于峰感知条件生成模型的人工智能判别框架，用峰感知机制学习，编码化学和溶剂信息，生成模拟数据集用于AI判别模型学习。

Result: 生成的模拟数据经定量和定性评估验证了框架有效性，该方法能使余弦相似度和皮尔逊相关系数值保持在0.9以上，保留峰数多样性并减少误报。

Conclusion: 所提框架能有效提升GC - MS在干扰条件下测量可靠性，改善AI判别模型性能。

Abstract: Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.

</details>


### [209] [Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative Study of Neural Architectures and Classical Filters](https://arxiv.org/abs/2601.21266)
*Zhuochen Liu,Hans Walker,Rahul Jain*

Main category: cs.LG

TL;DR: 对无模型神经网络模型与经典滤波方法在多非线性场景下进行系统实证比较，发现神经网络模型（尤其是SSMs）在状态估计性能和推理吞吐量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前不清楚神经网络模型在非线性动力系统中作为原则性滤波器的表现，且其与经典滤波器获取信息方式不同，需进行比较。

Method: 对基于Transformer的模型、状态空间神经网络、循环架构与粒子滤波器和非线性卡尔曼滤波器在多非线性场景下进行实证比较。

Result: 神经网络模型（特别是状态空间模型）在非线性场景下的状态估计性能接近强非线性卡尔曼滤波器，优于较弱的经典基线，且推理吞吐量更高。

Conclusion: 无模型的神经网络模型在状态估计方面有良好表现，即便没有系统模型信息也能有不错的性能。

Abstract: Neural network models are increasingly used for state estimation in control and decision-making problems, yet it remains unclear to what extent they behave as principled filters in nonlinear dynamical systems. Unlike classical filters, which rely on explicit knowledge of system dynamics and noise models, neural estimators can be trained purely from data without access to the underlying system equations. In this work, we present a systematic empirical comparison between such model-free neural network models and classical filtering methods across multiple nonlinear scenarios. Our study evaluates Transformer-based models, state-space neural networks, and recurrent architectures alongside particle filters and nonlinear Kalman filters. The results show that neural models (in particular, state-space models (SSMs)) achieve state estimation performance that approaches strong nonlinear Kalman filters in nonlinear scenarios and outperform weaker classical baselines despite lacking access to system models, while also attaining substantially higher inference throughput.

</details>


### [210] [EGAM: Extended Graph Attention Model for Solving Routing Problems](https://arxiv.org/abs/2601.21281)
*Licheng Wang,Yuzi Yan,Mingtao Huang,Yuan Shen*

Main category: cs.LG

TL;DR: 本文提出扩展图注意力模型EGAM解决路由问题，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图注意力模型GAM仅考虑节点特征有局限性，需更优模型解决路由问题。

Method: 泛化现有图注意力机制，提出EGAM，利用多头点积注意力更新节点和边嵌入，采用自回归编解码器架构，用含特殊基线的策略梯度算法训练。

Result: EGAM在各种路由问题上达到或超越现有方法，在高约束问题上表现出色。

Conclusion: EGAM能有效处理复杂图结构，解决路由问题。

Abstract: Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.

</details>


### [211] [DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher](https://arxiv.org/abs/2601.21283)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出基于蒸馏的DUET方法用于大语言模型遗忘，在遗忘和效用保存上表现更好且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法存在计算量大、易灾难性遗忘、易受攻击等局限，需要新方法。

Method: 提出Distilled Unlearning from an Efficient Teacher (DUET)方法，学习学生模型模仿提示引导的教师模型，拒绝不良知识生成并保留通用领域知识。

Result: 在现有基准上的评估表明，DUET在遗忘和效用保存方面表现更好，且比现有方法数据效率高很多。

Conclusion: DUET结合了现有方法优点，是一种有效的大语言模型遗忘方法。

Abstract: LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.

</details>


### [212] [PILD: Physics-Informed Learning via Diffusion](https://arxiv.org/abs/2601.21284)
*Tianyi Zeng,Tianyi Wang,Jiaru Zhang,Zimo Zeng,Feiyang Zhang,Yiming Xu,Sikai Chen,Yajie Zou,Yangyang Wang,Junfeng Jiao,Christian Claudel,Xinbo Chen*

Main category: cs.LG

TL;DR: 提出PILD框架统一扩散建模与物理约束，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型纯数据驱动特性限制其在需遵循物理定律的实际问题中的应用。

Method: 引入从拉普拉斯分布采样的虚拟残差观测监督训练，加入条件嵌入模块在多层注入物理信息。

Result: PILD框架适用于多种方程和约束问题，在多个工程和科学任务实验中，比现有基线显著提高准确性、稳定性和泛化能力。

Conclusion: PILD框架有效融合扩散建模和物理约束，在相关任务中有更好表现。

Abstract: Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

</details>


### [213] [Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation](https://arxiv.org/abs/2601.21285)
*Ruifeng Zhang,Zexi Huang,Zikai Wang,Ke Sun,Bohang Zheng,Zhen Ouyang,Huimin Xie,Phil Shen,Junlin Zhang,Wentao Guo,Qinglei Wang*

Main category: cs.LG

TL;DR: 提出可扩展高效的排序架构Zenith，在TikTok Live验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有工作在高效特征处理和扩展模型容量且不增加推理延迟方面关注不足。

Method: 提出Zenith架构，用Token Fusion和Token Boost模块处理高维Prime Tokens。

Result: A/B测试显示，在线CTR AUC和Logloss有提升，Quality Watch Session / User和Quality Watch Duration / User分别有+9.93%和+8.11%的增益。

Conclusion: Zenith能在低运行时开销下学习复杂特征交互，有良好扩展性和实际效果。

Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.

</details>


### [214] [TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification](https://arxiv.org/abs/2601.21289)
*Akash Pandey,Payal Mohapatra,Wei Chen,Qi Zhu,Sinan Keten*

Main category: cs.LG

TL;DR: 提出新的可解释深度学习框架TimeSliver，利用原始时间序列数据和符号抽象构造表示，在时间序列分类上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度和特征归因的事后可解释方法有参考状态敏感性且难以跨数据集泛化，基于自注意力机制的方法也难以提供可靠的时间重要性度量，需要更好的可解释时间序列分类方法。

Method: 提出TimeSliver框架，联合利用原始时间序列数据和其符号抽象来构建保持原始时间结构的表示，线性编码各时间片段对最终预测的贡献。

Result: 在7个不同的合成和真实世界多变量时间序列数据集上，TimeSliver在时间序列分类上比其他时间归因方法高11%；在26个UEA基准数据集上，预测性能与最先进基线相差在2%以内。

Conclusion: TimeSliver是一个强大且可解释的通用时间序列分类框架。

Abstract: Identifying the extent to which every temporal segment influences a model's predictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attributions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver, which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification.

</details>


### [215] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: 提出PG - TMT模型用于旋转机械在线状态监测，结合物理知识与极值理论，在多数据集上有良好表现，能提供校准、可解释且适用于部署的早期预警。


<details>
  <summary>Details</summary>
Motivation: 旋转机械以可靠性为中心的预测需要在非平稳工况、域转移和严重类别不平衡下准确的早期预警信号，同时降低误报率。

Method: 提出PG - TMT模型，包含深度可分离卷积茎、Tiny - Mamba状态空间分支和轻量级局部Transformer；推导解析的时频映射；用极值理论建模健康分数超标情况。

Result: 在多个数据集上，PG - TMT获得更高的精确召回AUC、有竞争力或更好的ROC AUC、更短的平均检测时间和强跨域转移能力。

Conclusion: 通过结合物理对齐表示和极值理论校准的决策规则，PG - TMT能为以可靠性为中心的预测和健康管理提供校准、可解释且适用于部署的早期预警。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [216] [Grounding and Enhancing Informativeness and Utility in Dataset Distillation](https://arxiv.org/abs/2601.21296)
*Shaobo Wang,Yantai Yang,Guo Chen,Peiru Li,Kaixin Li,Yufa Zhou,Zhaorun Chen,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文在理论框架下重新审视基于知识蒸馏的数据集蒸馏，提出InfoUtil框架，在ImageNet - 1K数据集上比现有方法性能提升6.1%。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法多依赖启发式方法，原始数据和合成数据的基本关系未充分探索。

Method: 引入Informativeness和Utility概念，数学定义最优数据集蒸馏；提出InfoUtil框架，包含用Shapley Value最大化信息性、用Gradient Norm最大化效用性两个组件。

Result: 在ImageNet - 1K数据集上使用ResNet - 18，比之前的最优方法性能提升6.1%。

Conclusion: InfoUtil框架能有效平衡信息性和效用性，合成高质量的蒸馏数据集。

Abstract: Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.

</details>


### [217] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 研究基于模型的强化学习中的搜索问题，挑战传统观点，发现减轻分布偏移更重要，并确定有效搜索的关键技术，在多基准领域达最优表现。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为长期预测和误差累积是基于模型的强化学习主要障碍的观点，探究搜索在其中的作用。

Method: 对搜索在基于模型的强化学习中的情况进行研究，分析其对性能的影响，提出减轻分布偏移更重要的观点，并基于此确定有效搜索的关键技术。

Result: 即使模型高度准确，搜索也可能损害性能；减轻分布偏移比提高模型或值函数准确性更重要。

Conclusion: 确定了实现有效搜索的关键技术，在多个流行基准领域实现了最优性能。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [218] [Transferable Graph Condensation from the Causal Perspective](https://arxiv.org/abs/2601.21309)
*Huaming Du,Yijie Huang,Su Yao,Yiying Wang,Yueyang Zhou,Jingwen Yang,Jinshi Zhang,Han Ji,Yu Zhao,Guisong Liu,Hegui Zhang,Carl Yang,Gang Kou*

Main category: cs.LG

TL;DR: 提出基于因果不变性且可迁移的图数据集浓缩方法TGCC，在多数据集实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有图数据集浓缩方法在跨任务和跨领域场景常不适用，需更好方法。

Method: 先通过因果干预从图的空间域提取领域因果不变特征，再进行增强浓缩操作，最后通过谱域增强对比学习将特征注入浓缩图。

Result: 在五个公共数据集和FinReport数据集实验显示，TGCC在跨任务和跨领域复杂场景比现有方法最多提升13.41%，在单数据集和任务场景6个中有5个达最优性能。

Conclusion: TGCC是有效的可迁移图数据集浓缩方法。

Abstract: The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.

</details>


### [219] [Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21312)
*Xiaozhuang Li,Xindi Tang,Fang He*

Main category: cs.LG

TL;DR: 本文提出GAT - PEARL元强化学习框架管理自动电动出租车车队，模拟显示其优于传统强化学习基线，效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设充电网络静态，与现实运营有差距，需有效管理动态不确定充电可用性环境下的自动电动出租车车队。

Method: 提出GAT - PEARL框架，集成图注意力网络提取空间特征，采用概率嵌入的演员 - 评论家强化学习实现快速推理适应。

Result: 在成都真实数据模拟中，GAT - PEARL显著优于传统强化学习基线，对未见基础设施布局泛化性好。

Conclusion: GAT - PEARL能在动态环境中实现自动电动出租车车队的高效运营管理。

Abstract: With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.

</details>


### [220] [Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation](https://arxiv.org/abs/2601.21315)
*Seonghwi Kim,Sung Ho Jo,Wooseok Ha,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出一种新的分布鲁棒学习框架解决UDA问题，在多种场景实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法在目标域无标签数据有限或源域存在虚假相关性时表现不佳。

Method: 提出新的分布鲁棒学习框架建模协变量和条件标签分布的不确定性，开发可与现有UDA方法集成的学习算法。

Result: 在各种分布偏移场景的实验中，新方法始终优于强基线，尤其是目标数据极少时。

Conclusion: 所提方法具有通用性和有效性，能解决现有UDA方法的问题。

Abstract: Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.

</details>


### [221] [Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21316)
*Aoyu Pang,Maonan Wang,Zifan Sha,Wenwei Yue,Changle Li,Chung Shue Chen,Man-On Pun*

Main category: cs.LG

TL;DR: 论文针对空地交通系统最优集成路由策略研究不足的问题，提出统一优化模型和UAGMC框架，实验表明该框架使平均旅行时间降低34%，提升效率，为智能城市交通奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有对空地交通系统中乘客最优集成路由策略的研究有限，缺乏系统探索，为实现高效无缝的门到门旅行体验，需加强空地交通基础设施整合研究。

Method: 首先提出统一优化模型，整合空地交通策略选择，考虑多式联运网络动态特征、实时交通状况和乘客决策行为；然后构建UAGMC框架，利用深度强化学习和车联网通信优化直升机场选择和动态规划空中出租路线。

Result: UAGMC框架与传统比例分配方法相比，平均旅行时间降低34%，提升了整体旅行效率。

Conclusion: 该工作通过空地交通模式的协调，为推进智能城市交通解决方案奠定了坚实基础。

Abstract: Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.

</details>


### [222] [Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer](https://arxiv.org/abs/2601.21323)
*Achraf Hsain,Ahmed Abdelkader,Emmanuel Baldwin Mbaya,Hamoud Aljamaan*

Main category: cs.LG

TL;DR: 研究DNN对抗样本向基于HOG的经典分类器的迁移性，发现经典分类器也易受攻击，且攻击层次反转，说明对抗脆弱性是图像分类系统基本属性。


<details>
  <summary>Details</summary>
Motivation: 探讨使用手工特征的经典机器学习管道在通过神经替代物攻击时是否继承对抗脆弱性。

Method: 以VGG16为替代模型，生成FGSM和PGD对抗样本，在CIFAR - 10上的8种HOG配置下测试向4种经典分类器和一个浅层神经网络的迁移。

Result: 所有分类器准确率下降16.6% - 59.1%；FGSM比PGD导致更大性能下降；块归一化只能部分缓解。

Conclusion: 对抗脆弱性是图像分类系统的基本属性，对跨计算范式的安全关键部署有影响。

Abstract: Deep neural networks are vulnerable to adversarial examples--inputs with imperceptible perturbations causing misclassification. While adversarial transfer within neural networks is well-documented, whether classical ML pipelines using handcrafted features inherit this vulnerability when attacked via neural surrogates remains unexplored. Feature engineering creates information bottlenecks through gradient quantization and spatial binning, potentially filtering high-frequency adversarial signals. We evaluate this hypothesis through the first comprehensive study of adversarial transfer from DNNs to HOG-based classifiers. Using VGG16 as a surrogate, we generate FGSM and PGD adversarial examples and test transfer to four classical classifiers (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across eight HOG configurations on CIFAR-10. Our results strongly refute the protective hypothesis: all classifiers suffer 16.6%-59.1% relative accuracy drops, comparable to neural-to-neural transfer. More surprisingly, we discover attack hierarchy reversal--contrary to patterns where iterative PGD dominates FGSM within neural networks, FGSM causes greater degradation than PGD in 100% of classical ML cases, suggesting iterative attacks overfit to surrogate-specific features that don't survive feature extraction. Block normalization provides partial but insufficient mitigation. These findings demonstrate that adversarial vulnerability is not an artifact of end-to-end differentiability but a fundamental property of image classification systems, with implications for security-critical deployments across computational paradigms.

</details>


### [223] [An introductory Generalization of the standard SVMs loss and its applications to Shallow and Deep Neural Networks](https://arxiv.org/abs/2601.21331)
*Filippo Portera*

Main category: cs.LG

TL;DR: 提出用于SVM的新凸损失函数，实验证明其泛化性能好，建议结合神经网络研究。


<details>
  <summary>Details</summary>
Motivation: 提升SVM在二分类和回归模型中的泛化性能。

Method: 提出新凸损失函数，进行对偶问题的数学推导，并在小数据集上实验。

Result: 泛化指标不比标准损失差，多次表现更优，还给出神经网络架构的新结果。

Conclusion: 应结合浅/深度神经网络仔细研究该损失函数。

Abstract: We propose a new convex loss for SVMs, both for the binary classification and for the regression models. Therefore, we show the mathematical derivation of the dual problems and we experiment them with several small data-sets. The minimal dimension of those data-sets is due to the difficult scalability of the SVM method to bigger instances. This preliminary study should prove that using pattern correlations inside the loss function could enhance the generalisation performances. Coherently, results show that generalisation measures are never worse than the standard losses and several times they are better. In our opinion, it should be considered a careful study of this loss, coupled with shallow and deep neural networks. In fact, we present some novel results obtained with those architectures.

</details>


### [224] [Memorization Control in Diffusion Models from Denoising-centric Perspective](https://arxiv.org/abs/2601.21348)
*Thuy Phuong Vu,Mai Viet Hoang Do,Minhhuy Le,Dinh-Cuong Hoang,Phan Xuan Tan*

Main category: cs.LG

TL;DR: 本文从去噪角度研究扩散模型的记忆问题，提出时间步采样策略控制记忆泛化权衡，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将扩散模型视为孤立预测器，本文从去噪角度研究控制扩散模型的记忆问题。

Method: 提出一种时间步采样策略，通过调整置信区间宽度控制记忆泛化权衡。

Result: 在图像和一维信号生成任务中，将学习重点转移到后期去噪步骤可减少记忆并改善与训练数据的分布对齐。

Conclusion: 所提方法具有通用性和有效性。

Abstract: Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.

</details>


### [225] [L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.21349)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 提出L2R统一路由框架，在语言和视觉模型实验中提升路由稳定性、专家专业化和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代MoE系统采用线性路由器存在表示不匹配、角度集中和尺度敏感评分等问题，影响路由可区分性和专家专业化稳定性。

Method: 提出L2R框架，在共享低秩潜在路由空间进行专家分配，引入SIPS控制路由函数的Lipschitz行为，结合参数高效的多锚点路由机制。

Result: 在大规模语言MoE模型和ImageNet视觉MoE设置上的实验表明，L2R持续改善路由稳定性、专家专业化和整体模型性能。

Conclusion: L2R框架能有效解决现有MoE系统路由问题，提升模型性能。

Abstract: Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.

</details>


### [226] [Factored Causal Representation Learning for Robust Reward Modeling in RLHF](https://arxiv.org/abs/2601.21350)
*Yupei Yang,Lin Yang,Wanxi Deng,Lin Qu,Fan Feng,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 本文从因果角度提出分解表征学习框架，解决标准奖励模型易受虚假特征影响问题，实验证明该方法能学习更稳健奖励模型并提升下游RLHF性能。


<details>
  <summary>Details</summary>
Motivation: 标准奖励模型易受与人类标签无因果关系的虚假特征影响，导致奖励破解问题，需要可靠的奖励模型来通过人类反馈强化学习使大语言模型与人类偏好对齐。

Method: 提出分解表征学习框架，将模型上下文嵌入分解为因果因素和非因果因素，奖励头仅依赖因果组件；引入对抗头从非因果因素预测奖励并应用梯度反转。

Result: 在数学和对话任务实验中，方法学习到更稳健奖励模型，持续提升下游RLHF性能，优于现有基线。

Conclusion: 方法能有效缓解奖励破解行为，具有有效性。

Abstract: A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.

</details>


### [227] [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)
*Chendong Song,Meixuan Wang,Hang Zhou,Hong Liang,Yuan Lyu,Zixi Chen,Yuwei Fan,Zijie Zhou*

Main category: cs.LG

TL;DR: 提出分析框架确定AFD架构最佳A/F比，减少空闲时间。


<details>
  <summary>Details</summary>
Motivation: AFD架构性能对Attention/FFN配置比敏感，配置不当会导致阻塞和设备空闲。

Method: 开发分析框架，用概率工作负载模型推导最佳A/F比。

Result: 理论最佳A/F比与模拟最佳值误差在10%内，能持续减少空闲时间。

Conclusion: 所提出的分析框架有效，能为AFD架构确定合适的A/F比以提升性能。

Abstract: Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.

</details>


### [228] [Expected Improvement via Gradient Norms](https://arxiv.org/abs/2601.21357)
*Joshua Hang Sai Ip,Georgios Makrygiorgos,Ali Mesbah*

Main category: cs.LG

TL;DR: 提出新型采集函数EI - GN，实验证明其有效性和控制策略学习适用性。


<details>
  <summary>Details</summary>
Motivation: 现有EI采集函数过度利用，易收敛到次优驻点。

Method: 提出EI - GN，将改进原则应用于梯度感知辅助目标，用梯度观测学习梯度增强代理模型，推导解析表达式。

Result: 在标准基准测试中，EI - GN相较于标准基线持续改进，且适用于控制策略学习问题。

Conclusion: EI - GN是有效的采集函数，有实际应用价值。

Abstract: Bayesian Optimization (BO) is a principled approach for optimizing expensive black-box functions, with Expected Improvement (EI) being one of the most widely used acquisition functions. Despite its empirical success, EI is known to be overly exploitative and can converge to suboptimal stationary points. We propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, thereby promoting sampling in regions that are both high-performing and approaching first-order stationarity. EI-GN relies on gradient observations used to learn gradient-enhanced surrogate models that enable principled gradient inference from function evaluations. We derive a tractable closed-form expression for EI-GN that allows efficient optimization and show that the proposed acquisition is consistent with the improvement-based acquisition framework. Empirical evaluations on standard BO benchmarks demonstrate that EI-GN yields consistent improvements against standard baselines. We further demonstrate applicability of EI-GN to control policy learning problems.

</details>


### [229] [Perceptrons and localization of attention's mean-field landscape](https://arxiv.org/abs/2601.21366)
*Antonio Álvarez-López,Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: cs.LG

TL;DR: 将Transformer前向传播视为单位球上的交互粒子系统，研究感知机块影响，发现临界点通常是原子且位于球子集上。


<details>
  <summary>Details</summary>
Motivation: 研究感知机块在将Transformer前向传播视为单位球上交互粒子系统这一设定下的影响。

Method: 将Transformer前向传播建模为单位球上交互粒子系统，利用Wasserstein梯度流理解无限上下文长度极限。

Result: 发现临界点通常是原子且位于球的子集上。

Conclusion: 在该设定下感知机块的临界点具有特定的原子性和局部化特征。

Abstract: The forward pass of a Transformer can be seen as an interacting particle system on the unit sphere: time plays the role of layers, particles that of token embeddings, and the unit sphere idealizes layer normalization. In some weight settings the system can even be seen as a gradient flow for an explicit energy, and one can make sense of the infinite context length (mean-field) limit thanks to Wasserstein gradient flows. In this paper we study the effect of the perceptron block in this setting, and show that critical points are generically atomic and localized on subsets of the sphere.

</details>


### [230] [Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach](https://arxiv.org/abs/2601.21369)
*Yinlin Zhu,Di Wu,Xianzhi Zhang,Yuming Ai,Xunkai Li,Miao Hu,Guocong Quan*

Main category: cs.LG

TL;DR: 现有FedGFMs量化过程有知识损失，提出FedGALA框架解决语义 - 结构正交性和完整性问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有FedGFMs通过向量量化骨干将知识投影到离散令牌空间会有不可逆知识损失，且要解决分布式环境中数据异质性和通信约束问题。

Method: 提出FedGALA框架，用无监督对比学习在连续嵌入空间对齐GNN和冻结的PLM，再用通信高效的提示调优机制让预对齐编码器和冻结PLM适应下游任务。

Result: 综合实验表明，FedGALA在多领域数据集的多个任务上优于所有竞争基线，性能提升最高达14.37%。

Conclusion: FedGALA能有效解决FedGFMs的语义 - 结构正交性和完整性问题，在多任务上表现良好。

Abstract: Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments.
  To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.

</details>


### [231] [DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction](https://arxiv.org/abs/2601.21381)
*Tianhao Zhang,Shusen Ma,Yu Kang,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出DA - SPS模型用于多变量时间序列预测，分两阶段处理目标变量和外部变量，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作未有效考虑外部变量对目标变量预测的影响，且未能充分基于序列时间模式提取复杂序列信息。

Method: 提出DA - SPS模型，分目标变量处理阶段（TVPS）和外部变量处理阶段（EVPS），采用不同模块和算法进行特征提取，最后加权求和与线性映射得出预测结果。

Result: 在四个公共数据集上，DA - SPS模型优于现有先进方法，使用自有私有数据集在真实场景中验证了其性能。

Conclusion: DA - SPS模型在多变量时间序列预测方面具有有效性和优越性。

Abstract: Multivariate time-series forecasting, as a typical problem in the field of time series prediction, has a wide range of applications in weather forecasting, traffic flow prediction, and other scenarios. However, existing works do not effectively consider the impact of extraneous variables on the prediction of the target variable. On the other hand, they fail to fully extract complex sequence information based on various time patterns of the sequences. To address these drawbacks, we propose a DA-SPS model, which adopts different modules for feature extraction based on the information characteristics of different variables. DA-SPS mainly consists of two stages: the target variable processing stage (TVPS) and the extraneous variables processing stage (EVPS). In TVPS, the model first uses Singular Spectrum Analysis (SSA) to process the target variable sequence and then uses Long Short-Term Memory (LSTM) and P-Conv-LSTM which deploys a patching strategy to extract features from trend and seasonality components, respectively. In EVPS, the model filters extraneous variables that have a strong correlation with the target variate by using Spearman correlation analysis and further analyses them using the L-Attention module which consists of LSTM and attention mechanism. Finally, the results obtained by TVPS and EVPS are combined through weighted summation and linear mapping to produce the final prediction. The results on four public datasets demonstrate that the DA-SPS model outperforms existing state-of-the-art methods. Additionally, its performance in real-world scenarios is further validated using a private dataset collected by ourselves, which contains the test items' information on laptop motherboards.

</details>


### [232] [Sim-MSTNet: sim2real based Multi-task SpatioTemporal Network Traffic Forecasting](https://arxiv.org/abs/2601.21384)
*Hui Ma,Qingzhong Li,Jin Wang,Jie Wu,Shaoyu Dou,Li Feng,Xinjun Pei*

Main category: cs.LG

TL;DR: 提出Sim - MSTNet解决网络流量预测在数据有限及多任务学习中的问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量预测技术在数据有限时表现不佳，多任务学习方法存在任务不平衡和负迁移问题。

Method: 提出基于sim2real方法的多任务时空网络流量预测模型Sim - MSTNet，利用模拟器生成合成数据，采用域随机化技术和双层优化，还结合注意力机制和动态损失加权。

Result: 在两个开源数据集上的广泛实验中，Sim - MSTNet持续优于现有最先进基线。

Conclusion: Sim - MSTNet能有效解决现有问题，实现更高的准确性和泛化能力。

Abstract: Network traffic forecasting plays a crucial role in intelligent network operations, but existing techniques often perform poorly when faced with limited data. Additionally, multi-task learning methods struggle with task imbalance and negative transfer, especially when modeling various service types. To overcome these challenges, we propose Sim-MSTNet, a multi-task spatiotemporal network traffic forecasting model based on the sim2real approach. Our method leverages a simulator to generate synthetic data, effectively addressing the issue of poor generalization caused by data scarcity. By employing a domain randomization technique, we reduce the distributional gap between synthetic and real data through bi-level optimization of both sample weighting and model training. Moreover, Sim-MSTNet incorporates attention-based mechanisms to selectively share knowledge between tasks and applies dynamic loss weighting to balance task objectives. Extensive experiments on two open-source datasets show that Sim-MSTNet consistently outperforms state-of-the-art baselines, achieving enhanced accuracy and generalization.

</details>


### [233] [Learning to Optimize Job Shop Scheduling Under Structural Uncertainty](https://arxiv.org/abs/2601.21389)
*Rui Zhang,Jianwei Niu,Xuefeng Liu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 本文探讨作业车间调度问题中的结构不确定性，提出UP - AAC方法，经实验证明在减少工期方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注参数不确定性，对结构不确定性研究不足，且现有方法因错误的信用分配难以应对结构不确定性挑战。

Method: 提出UP - AAC方法，采用非对称架构，演员接收标准随机状态，评论家接收事后重建的确定状态；设计基于注意力的不确定性感知模型（UPM）增强调度决策。

Result: 大量实验表明该方法在基准实例上减少工期方面优于现有方法。

Conclusion: 所提出的UP - AAC方法能有效解决作业车间调度问题中的结构不确定性，提升调度性能。

Abstract: The Job-Shop Scheduling Problem (JSSP), under various forms of manufacturing uncertainty, has recently attracted considerable research attention. Most existing studies focus on parameter uncertainty, such as variable processing times, and typically adopt the actor-critic framework. In this paper, we explore a different but prevalent form of uncertainty in JSSP: structural uncertainty. Structural uncertainty arises when a job may follow one of several routing paths, and the selection is determined not by policy, but by situational factors (e.g., the quality of intermediate products) that cannot be known in advance. Existing methods struggle to address this challenge due to incorrect credit assignment: a high-quality action may be unfairly penalized if it is followed by a time-consuming path. To address this problem, we propose a novel method named UP-AAC. In contrast to conventional actor-critic methods, UP-AAC employs an asymmetric architecture. While its actor receives a standard stochastic state, the critic is crucially provided with a deterministic state reconstructed in hindsight. This design allows the critic to learn a more accurate value function, which in turn provides a lower-variance policy gradient to the actor, leading to more stable learning. In addition, we design an attention-based Uncertainty Perception Model (UPM) to enhance the actor's scheduling decisions. Extensive experiments demonstrate that our method outperforms existing approaches in reducing makespan on benchmark instances.

</details>


### [234] [Intrinsic Reward Policy Optimization for Sparse-Reward Environments](https://arxiv.org/abs/2601.21391)
*Minjae Cho,Huy Trong Tran*

Main category: cs.LG

TL;DR: 提出利用多个内在奖励直接优化策略的IRPO算法，在离散和连续环境中提升了性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习探索策略在奖励稀疏时不足，现有内在奖励方法存在信用分配不稳定、样本低效和次优问题。

Method: 提出策略优化框架IRPO，使用替代策略梯度在稀疏奖励环境中提供更有信息的学习信号。

Result: IRPO在离散和连续环境中相对基线提升了性能和样本效率，并对其解决的优化问题进行了正式分析。

Conclusion: IRPO是一种有效的强化学习策略优化方法，代码开源。

Abstract: Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.

</details>


### [235] [Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.21418)
*Qian Wan,Ziao Xu,Luona Wei,Xiaoxuan Shen,Jianwen Sun*

Main category: cs.LG

TL;DR: 针对大推理模型处理简单任务时过度思考问题，提出基于强化学习的DiPO训练框架，实验表明可降低冗余token且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型处理简单任务时存在过度思考现象，现有研究低估任务难度感知重要性，难以有效分配推理资源。

Method: 提出了基于强化学习的Difficulty - aware Policy Optimization (DiPO)训练框架，包含基于模型自我推理的难度建模方法和难度信号增强奖励函数。

Result: DiPO使模型能自发调整推理开销，显著减少冗余token，且因思维压缩未导致性能损失。

Conclusion: DiPO能有效解决大推理模型处理简单任务时过度思考的问题，合理调配推理资源。

Abstract: Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.

</details>


### [236] [Revisiting Diffusion Model Predictions Through Dimensionality](https://arxiv.org/abs/2601.21419)
*Qing Jin,Chaoyang Wang*

Main category: cs.LG

TL;DR: 本文为扩散和流匹配模型提供理论框架，分析数据几何与最优预测目标关系，提出k - Diff框架提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对扩散和流匹配模型中最优预测目标依赖数据特性的正式解释，且数据内在维度难以估计。

Method: 建立广义预测公式的理论框架，分析数据几何与最优预测目标关系；提出k - Diff框架，以数据驱动方式学习最优预测参数k。

Result: 在潜在空间和像素空间图像生成的大量实验中，k - Diff在不同架构和数据规模下始终优于固定目标基线。

Conclusion: 本文理论为最优预测目标提供解释，k - Diff框架提供了一种有原则且自动的方法来提高生成性能。

Abstract: Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.

</details>


### [237] [ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation](https://arxiv.org/abs/2601.21420)
*Zihao Huang,Jundong Zhou,Xingwei Qu,Qiyang Min,Ge Zhang*

Main category: cs.LG

TL;DR: 提出ConceptMoE，动态合并语义相似标记，在多任务上表现优于标准MoE，还能减少计算量、加速处理，可直接集成到现有MoE。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对所有标记分配统一计算，未考虑不同序列的可预测性差异，需更高效的计算分配方法。

Method: 引入ConceptMoE，通过可学习的块模块识别最优边界，压缩序列后进入概念模型；采用MoE架构进行受控评估。

Result: 在语言和视觉 - 语言任务上持续优于标准MoE，转化预训练MoE时有显著收益，减少注意力计算和KV缓存，实现速度提升。

Conclusion: 自适应的概念级处理从根本上提高了大语言模型的有效性和效率，且可直接集成到现有MoE。

Abstract: Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio $R$ before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to $R^2\times$ and KV cache by $R\times$. At $R=2$, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.

</details>


### [238] [Lossy Common Information in a Learnable Gray-Wyner Network](https://arxiv.org/abs/2601.21424)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 传统编解码器在计算机视觉任务中存在冗余低效问题，本文受Gray - Wyner网络启发，开发可学习三通道编解码器，实验证明其能减少冗余并优于独立编码。


<details>
  <summary>Details</summary>
Motivation: 传统编解码器在处理有大量重叠信息的计算机视觉任务时，会忽略信息重叠，导致表示冗余和低效。

Method: 开发可学习的三通道编解码器分离共享信息和特定任务信息，用有损公共信息概念刻画其极限，提出平衡学习表示中固有权衡的优化目标。

Result: 在六个视觉基准的双任务场景中比较三种编解码器架构，该方法大幅减少冗余，始终优于独立编码。

Conclusion: 在现代机器学习中重新审视Gray - Wyner理论具有实际价值，可将经典信息理论与任务驱动的表示学习相结合。

Abstract: Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.

</details>


### [239] [From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning](https://arxiv.org/abs/2601.21436)
*Hang Ni,Weijia Zhang,Fei Wang,Zezhi Shao,Hao Liu*

Main category: cs.LG

TL;DR: 本文提出多模态大语言模型MADI，解决多模态时间序列理解中的跨模态集成难题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在时间序列理解和推理中，跨模态集成存在细粒度时间不对齐和语义纠缠问题，影响局部解释和互补推理。

Method: 提出MADI模型，包含Patch - level Alignment、Discrete Disentangled Interaction和Critical - token Highlighting。

Result: 在合成和真实世界基准测试中，MADI始终优于通用大语言模型和时间序列专用的多模态大语言模型。

Conclusion: MADI模型通过提出的三种方法有效解决了多模态时间序列理解中的跨模态集成问题，提升了模型性能。

Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.

</details>


### [240] [Accurate Network Traffic Matrix Prediction via LEAD: an LLM-Enhanced Adapter-Based Conditional Diffusion Model](https://arxiv.org/abs/2601.21437)
*Yu Sun,Yaqiong Liu,Nan Cheng,Jiayuan Li,Zihan Jia,Xialin Du,Mugen Peng*

Main category: cs.LG

TL;DR: 为应对 6G 和 AI 原生边缘智能下的网络操作需求，提出 LLM 增强的基于适配器的条件扩散模型 LEAD 用于网络流量矩阵预测，实验表明其性能优于所有基线。


<details>
  <summary>Details</summary>
Motivation: 6G 和 AI 原生边缘智能发展下，网络操作需预测性和风险感知适应，现有网络流量矩阵预测模型存在过平滑和不确定性感知有限等问题。

Method: 采用“Traffic-to-Image”范式将流量矩阵转化为 RGB 图像；设计“Frozen LLM with Trainable Adapter”模型；提出双条件策略引导扩散模型生成流量矩阵。

Result: 在 Abilene 数据集上比最佳基线 RMSE 降低 45.2%，20 步预测误差增加小；在 GEANT 数据集上 20 步预测 RMSE 比最佳基线低 27.3%。

Conclusion: LEAD 模型在网络流量矩阵预测方面优于现有基线模型。

Abstract: Driven by the evolution toward 6G and AI-native edge intelligence, network operations increasingly require predictive and risk-aware adaptation under stringent computation and latency constraints. Network Traffic Matrix (TM), which characterizes flow volumes between nodes, is a fundamental signal for proactive traffic engineering. However, accurate TM forecasting remains challenging due to the stochastic, non-linear, and bursty nature of network dynamics. Existing discriminative models often suffer from over-smoothing and provide limited uncertainty awareness, leading to poor fidelity under extreme bursts. To address these limitations, we propose LEAD, a Large Language Model (LLM)-Enhanced Adapter-based conditional Diffusion model. First, LEAD adopts a "Traffic-to-Image" paradigm to transform traffic matrices into RGB images, enabling global dependency modeling via vision backbones. Then, we design a "Frozen LLM with Trainable Adapter" model, which efficiently captures temporal semantics with limited computational cost. Moreover, we propose a Dual-Conditioning Strategy to precisely guide a diffusion model to generate complex, dynamic network traffic matrices. Experiments on the Abilene and GEANT datasets demonstrate that LEAD outperforms all baselines. On the Abilene dataset, LEAD attains a remarkable 45.2% reduction in RMSE against the best baseline, with the error margin rising only marginally from 0.1098 at one-step to 0.1134 at 20-step predictions. Meanwhile, on the GEANT dataset, LEAD achieves a 0.0258 RMSE at 20-step prediction horizon which is 27.3% lower than the best baseline.

</details>


### [241] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 现有推荐系统工作依赖单独词汇表，OneRec优化策略有问题，提出SAGE框架解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有工作依赖单独词汇表导致的高维护成本和可扩展性差问题，以及OneRec优化策略的‘对称保守性’问题。

Method: 提出SAGE统一优化框架，包括序列级信号解耦和非对称自适应动态两个创新点。

Result: 理论分析和实证结果表明SAGE有效疏通冷启动流量、维持推荐多样性，保留GBPO数值稳定性。

Conclusion: SAGE能解决现有推荐系统存在的问题，具有良好效果。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [242] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 本文提出HER框架用于大语言模型角色扮演的认知层面角色模拟，通过构建数据和奖励模型训练模型，实验验证其有效性并将开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型角色扮演虽能捕捉角色语气和知识，但模拟行为背后的内心想法仍有困难，且此前方法存在缺乏高质量推理轨迹数据和符合人类偏好的奖励信号的问题。

Method: 提出HER框架，引入双层思维，通过逆向工程策划推理增强的角色扮演数据，构建符合人类偏好的原则和奖励模型，基于Qwen3 - 32B通过监督学习和强化学习训练模型。

Result: 模型显著优于Qwen3 - 32B基线，在CoSER基准上提升30.26，在Minimax角色扮演基准上提升14.97。

Conclusion: 提出的方法有效，将发布数据集、原则和模型以促进后续研究。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [243] [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)
*Albert Tseng,Christopher De Sa*

Main category: cs.LG

TL;DR: 提出Large Lookup Layer (L³) 层，通过泛化嵌入表实现新的稀疏性，在语言建模和下游任务中表现优于稠密模型和等稀疏MoE。


<details>
  <summary>Details</summary>
Motivation: 动态硬路由的Mixture-of-Experts (MoE) 层有硬件效率低和需辅助损失稳定训练等缺点，而原生稀疏的分词器嵌入表缺乏上下文信息，因此需要新方法。

Method: 引入L³层，使用基于静态令牌的路由以依赖上下文的方式聚合每个令牌的一组学习嵌入，包含系统友好架构和信息论嵌入分配算法。

Result: 训练最多26亿活跃参数的Transformer，L³在语言建模和下游任务中显著优于稠密模型和等稀疏MoE。

Conclusion: L³是一种有效实现稀疏性的方法，能在语言任务中取得良好效果。

Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.

</details>


### [244] [A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation](https://arxiv.org/abs/2601.21467)
*Guillaume Lauga*

Main category: cs.LG

TL;DR: 本文提出新的块坐标下降框架解决非凸复合优化问题，涵盖多种更新方式，能用于非凸稀疏精度矩阵估计，有收敛保证并减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 块坐标下降法虽常用于解决大规模优化问题，但对非凸优化的理论研究较少，需要新方法解决非凸复合优化问题。

Method: 提出新的块坐标下降框架，包含变度量近端梯度更新、近端牛顿更新和交替最小化更新。

Result: 该框架涵盖稀疏精度矩阵估计中最常用求解器的三个版本，能提供收敛保证，将达到最新估计质量所需的迭代次数最多减少100倍。

Conclusion: 新的块坐标下降框架可有效解决非凸复合优化问题和非凸稀疏精度矩阵估计问题。

Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.

</details>


### [245] [ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment](https://arxiv.org/abs/2601.21484)
*Xiuyu Li,Jinkai Zhang,Mingyang Yi,Yu Li,Longqiang Wang,Yue Wang,Ju Fan*

Main category: cs.LG

TL;DR: 提出无训练推理方法ETS，能直接从最优RL策略采样，实验验证其提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习后训练对齐语言模型成本高且不稳定的问题。

Method: 提出基于参考策略模型和能量项的转换概率，ETS通过在线蒙特卡罗估计能量项，利用加速框架和重要性采样估计器。

Result: 在MLM多个基准测试中，ETS持续提升生成质量。

Conclusion: ETS方法有效，设计合理。

Abstract: Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.

</details>


### [246] [Task-Awareness Improves LLM Generations and Uncertainty](https://arxiv.org/abs/2601.21500)
*Tim Tomov,Dominik Fuchsgruber,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出在任务依赖的潜在结构中建模大语言模型（LLM）输出，计算贝叶斯最优响应，在不同任务中表现优于标准解码方法，还能量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的解码和不确定性估计方法在语言空间中操作，忽略了潜在结构信息。

Method: 在任务依赖的潜在结构中对大语言模型输出进行建模，通过不相似度度量计算贝叶斯最优响应，通过诱导的贝叶斯风险量化不确定性。

Result: 贝叶斯最优响应在不同任务中始终优于标准解码方法，量化不确定性能更好反映潜在结构变化，与输出质量和正确性更一致。

Conclusion: 决策理论框架适用于有潜在响应结构的问题，能实现可靠的任务感知大语言模型预测。

Abstract: In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.

</details>


### [247] [Cascaded Transfer: Learning Many Tasks under Budget Constraints](https://arxiv.org/abs/2601.21513)
*Eloi Campagne,Yvenn Amara-Ouali,Yannig Goude,Mathilde Mougeot,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 介绍级联迁移学习，一种新的多任务迁移学习范式，实验表明其比其他方法更准确且具成本效益。


<details>
  <summary>Details</summary>
Motivation: 处理大量相关任务关系未知的多任务学习场景。

Method: 引入级联迁移学习范式，将级联组织成有根树指定任务学习和细化顺序，设计基于最小生成树结构的级联迁移机制并分配训练预算。

Result: 在合成和真实多任务设置的实验中，该方法比其他方法能在大型任务集合中实现更准确且具成本效益的适应。

Conclusion: 级联迁移学习范式在多任务学习中表现更优。

Abstract: Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.

</details>


### [248] [A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings](https://arxiv.org/abs/2601.21521)
*Chi-Sheng Chen,En-Jui Kuo,Guan-Ying Chen,Xinyu Zhang,Fan Zhang*

Main category: cs.LG

TL;DR: 本文对SPD流形中嵌入选择与梯度调节和数值稳定性的联系进行分析，给出三个理论结果，并在三种EEG范式上验证，Log - Euclidean Transformer达最优。


<details>
  <summary>Details</summary>
Motivation: 探究嵌入几何与优化动力学在EEG信号SPD流形上的理论联系。

Method: 进行形式化分析得出三个理论结果，通过统一Transformer框架比较不同嵌入方式，在三个EEG范式超1500次运行验证。

Result: BWSPD在高维输入有更好梯度调节，BN - Embed在特定数据提升精度，bi - Lipschitz界证明BWSPD标记特性；Log - Euclidean Transformer在所有数据集达最优，BWSPD有竞争力。

Conclusion: 建立SPD流形嵌入选择与优化特性联系，Log - Euclidean Transformer性能优，BWSPD有竞争力。

Abstract: Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.

</details>


### [249] [Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21523)
*Bang Giang Le,Viet Cuong Ta*

Main category: cs.LG

TL;DR: 本文提出结合全局奖励和局部奖励优点的方法，通过代理间交互图更细粒度辨别个体贡献，实验显示该方法有灵活性且优于传统设置。


<details>
  <summary>Details</summary>
Motivation: 全局奖励有噪声需进行信用分配，局部奖励学习快但可能无法达到全局最优，为结合两者优点促进多智能体强化学习中的合作。

Method: 使用代理间的交互图更细粒度地辨别个体代理的贡献，同时缓解局部奖励带来的合作问题，并引入近似该图的实用方法。

Result: 实验证明该方法具有灵活性，能在传统局部和全局奖励设置上实现改进。

Conclusion: 所提方法结合了全局奖励和局部奖励的优点，在多智能体强化学习中表现良好。

Abstract: To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.

</details>


### [250] [Fast and Geometrically Grounded Lorentz Neural Networks](https://arxiv.org/abs/2601.21529)
*Robert van der Klis,Ricardo Chávez Torres,Max van Spengler,Yuhui Ding,Thomas Hofmann,Pascal Mettes*

Main category: cs.LG

TL;DR: 本文探讨双曲神经网络数学公式问题，提出新的洛伦兹线性层公式，并优化算法，实现了符合双曲几何的神经网络。


<details>
  <summary>Details</summary>
Motivation: 双曲空间在表示学习中有潜力，但目前缺乏高效且能捕捉双曲空间关键特性的双曲神经网络数学公式。

Method: 提出基于“到超平面距离”公式的新洛伦兹线性层，采用洛伦兹激活函数和新缓存策略。

Result: 新公式使输出的双曲范数与梯度下降步数呈线性关系，缩小与欧几里得神经网络的计算差距。

Conclusion: 新方法可构建完全符合双曲几何的神经网络，代码开源。

Abstract: Hyperbolic space is quickly gaining traction as a promising geometry for hierarchical and robust representation learning. A core open challenge is the development of a mathematical formulation of hyperbolic neural networks that is both efficient and captures the key properties of hyperbolic space. The Lorentz model of hyperbolic space has been shown to enable both fast forward and backward propagation. However, we prove that, with the current formulation of Lorentz linear layers, the hyperbolic norms of the outputs scale logarithmically with the number of gradient descent steps, nullifying the key advantage of hyperbolic geometry. We propose a new Lorentz linear layer grounded in the well-known ``distance-to-hyperplane" formulation. We prove that our formulation results in the usual linear scaling of output hyperbolic norms with respect to the number of gradient descent steps. Our new formulation, together with further algorithmic efficiencies through Lorentzian activation functions and a new caching strategy results in neural networks fully abiding by hyperbolic geometry while simultaneously bridging the computation gap to Euclidean neural networks. Code available at: https://github.com/robertdvdk/hyperbolic-fully-connected.

</details>


### [251] [Multi-Modal Time Series Prediction via Mixture of Modulated Experts](https://arxiv.org/abs/2601.21547)
*Lige Zhang,Ali Maatouk,Jialin Chen,Leandros Tassiulas,Rex Ying*

Main category: cs.LG

TL;DR: 本文针对多模态时间序列预测问题，提出 Expert Modulation 范式，经实验证明该方法有效提升预测效果，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测具挑战性，现有多模态预测方法多依赖标记级融合，在缺少高质量时间文本对和时间序列特征变化大时不适用，现有基于 MoE 的模态集成方法也依赖此融合方式。

Method: 提出 Expert Modulation 新范式，在路由和专家计算中引入文本信号，对专家行为进行直接和有效的跨模态控制。

Result: 通过理论分析和实验，所提方法在多模态时间序列预测上有显著提升。

Conclusion: Expert Modulation 范式能有效解决现有多模态时间序列预测方法的不足，提升预测效果。

Abstract: Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME

</details>


### [252] [HistoPrism: Unlocking Functional Pathway Analysis from Pan-Cancer Histology via Gene Expression Prediction](https://arxiv.org/abs/2601.21560)
*Susu Hu,Qinghe Zeng,Nithya Bhasker,Jakob Nicolas Kather,Stefanie Speidel*

Main category: cs.LG

TL;DR: 提出HistoPrism架构用于从组织学图像进行泛癌基因表达预测，引入通路级基准评估，表现优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 现有从H&E组织学预测空间基因表达的工作局限于单癌种，且缺乏功能相关性评估，需能跨癌种泛化并捕捉生物学信号的模型。

Method: 引入基于Transformer的HistoPrism架构进行泛癌基因表达预测，引入通路级基准进行评估。

Result: HistoPrism在高可变基因预测上超越先前模型，在通路级预测上有显著提升。

Conclusion: HistoPrism具有强泛癌泛化能力和效率提升，为临床相关转录组建模树立新标准。

Abstract: Predicting spatial gene expression from H&E histology offers a scalable and clinically accessible alternative to sequencing, but realizing clinical impact requires models that generalize across cancer types and capture biologically coherent signals. Prior work is often limited to per-cancer settings and variance-based evaluation, leaving functional relevance underexplored. We introduce HistoPrism, an efficient transformer-based architecture for pan-cancer prediction of gene expression from histology. To evaluate biological meaning, we introduce a pathway-level benchmark, shifting assessment from isolated gene-level variance to coherent functional pathways. HistoPrism not only surpasses prior state-of-the-art models on highly variable genes , but also more importantly, achieves substantial gains on pathway-level prediction, demonstrating its ability to recover biologically coherent transcriptomic patterns. With strong pan-cancer generalization and improved efficiency, HistoPrism establishes a new standard for clinically relevant transcriptomic modeling from routinely available histology.

</details>


### [253] [SAL: Selective Adaptive Learning for Backpropagation-Free Training with Sparsification](https://arxiv.org/abs/2601.21561)
*Fanping Liu,Hua Yang,Jiasi Zou*

Main category: cs.LG

TL;DR: 提出选择性自适应学习（SAL）方法解决标准深度学习BP的瓶颈，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 标准深度学习依赖的BP存在生物不合理的权重对称问题和梯度干扰问题，需要改进。

Method: 提出SAL方法，结合选择性参数激活和自适应区域划分，将参数空间分解为相互排斥、依赖样本的区域，并通过改进的反馈对齐解决权重对称问题。

Result: SAL在10个标准基准测试中表现出有竞争力的收敛率和分类性能，在深层网络和大规模模型中也能达到数值一致性和有竞争力的准确率。

Conclusion: SAL受生物学习机制启发，为可扩展神经网络训练研究提供了可行的替代方案。

Abstract: Standard deep learning relies on Backpropagation (BP), which is constrained by biologically implausible weight symmetry and suffers from significant gradient interference within dense representations. To mitigate these bottlenecks, we propose Selective Adaptive Learning (SAL), a training method that combines selective parameter activation with adaptive area partitioning. Specifically, SAL decomposes the parameter space into mutually exclusive, sample-dependent regions. This decoupling mitigates gradient interference across divergent semantic patterns and addresses explicit weight symmetry requirements through our refined feedback alignment. Empirically, SAL demonstrates competitive convergence rates, leading to improved classification performance across 10 standard benchmarks. Additionally, SAL achieves numerical consistency and competitive accuracy even in deep regimes (up to 128 layers) and large-scale models (up to 1B parameters). Our approach is loosely inspired by biological learning mechanisms, offering a plausible alternative that contributes to the study of scalable neural network training.

</details>


### [254] [Representation Unlearning: Forgetting through Information Compression](https://arxiv.org/abs/2601.21564)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 提出Representation Unlearning框架，直接在模型表示空间中进行去学习，实验显示其优于以参数为中心的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习去学习方法修改模型参数存在不稳定、计算成本高和局部近似受限等问题，且有隐私法规和鲁棒性需求。

Method: 引入Representation Unlearning框架，在模型表示空间学习变换施加信息瓶颈，推导变分替代物，在两种实际场景中实现。

Result: 在多个基准测试中，Representation Unlearning实现更可靠的遗忘、更好的效用保留和更高的计算效率。

Conclusion: Representation Unlearning框架在机器学习去学习方面优于参数中心的基线方法。

Abstract: Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.

</details>


### [255] [FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions](https://arxiv.org/abs/2601.21567)
*Yutao Jin,Yuang Tao,Junyong Zhai*

Main category: cs.LG

TL;DR: 提出FlexCausal框架用于因果解纠缠表示学习，用块对角协方差VAE和基于流的先验分布等改进，实验表现佳


<details>
  <summary>Details</summary>
Motivation: 现有解纠缠方法用标准平均场近似和假设各向同性高斯先验，无法捕获真实世界因果因素复杂特性

Method: 提出基于块对角协方差VAE的FlexCausal框架，用基于流的先验分布建模，集成监督对齐目标和反事实一致性约束，引入流形感知相对干预策略

Result: 在合成和真实数据集实验中，FlexCausal显著优于其他方法

Conclusion: FlexCausal框架有效，能在因果解纠缠表示学习中取得更好效果

Abstract: Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.

</details>


### [256] [Bridging Functional and Representational Similarity via Usable Information](https://arxiv.org/abs/2601.21568)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 提出通过可用信息量化表征相似性的统一框架，从功能、表征相似性等维度进行理论和实证综合。


<details>
  <summary>Details</summary>
Motivation: 为量化表征之间的相似性提供统一框架。

Method: 从功能相似性、表征相似性等三个关键维度进行理论推导和实证分析，建立联系并证明相关结论。

Result: 建立了拼接性能与条件互信息的联系，揭示拼接的不对称性；证明基于重建的指标和标准工具是特定约束下可用信息的估计器；表明表征相似性对功能相似性是充分非必要条件。

Conclusion: 通过任务粒度层次统一相关概念，表征相似性是最大粒度的极限即输入重建。

Abstract: We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable information}, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.

</details>


### [257] [Shaping capabilities with token-level data filtering](https://arxiv.org/abs/2601.21571)
*Neil Rathi,Alec Radford*

Main category: cs.LG

TL;DR: 研究提出在预训练阶段过滤数据以减少语言模型不良能力的方法，表明过滤标记比过滤文档更有效，且随规模扩大效果更好。


<details>
  <summary>Details</summary>
Motivation: 当前减少语言模型不良能力的方法多为事后处理，易被对手绕过，因此考虑在预训练阶段塑造模型能力。

Method: 在去除医疗能力的代理任务中，采用过滤预训练数据的方法，对比过滤标记和文档的效果，训练不同规模模型；引入用稀疏自编码器标记标记和提炼低成本高质量分类器的方法。

Result: 过滤预训练数据有效、稳健且成本低；过滤标记比过滤文档更有效；过滤效果随模型规模扩大而增强，最大模型中标记过滤使遗忘领域计算速度减慢7000倍；经标记过滤训练的模型仍可在遗忘领域对齐；过滤对噪声标签具有鲁棒性。

Conclusion: 在预训练阶段过滤数据是减少语言模型不良能力的有效方法，且标记过滤效果更佳。

Abstract: Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.

</details>


### [258] [Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks](https://arxiv.org/abs/2601.21572)
*Jinhao Li,Yuhao Sun,Zhiyuan Ma,Hao He,Xinche Zhang,Xing Chen,Jin Li,Sen Song*

Main category: cs.LG

TL;DR: 提出Signal - Adaptive Trust Regions (SATR)用于训练RSNNs，在高维连续控制基准测试中提升稳定性，还引入位集实现以减少训练时间。


<details>
  <summary>Details</summary>
Motivation: RSNNs用于高维、长视野强化学习训练有挑战，基于种群的无梯度优化估计梯度方差大，更新步有害且激进。

Method: 提出SATR分布更新规则约束相对变化，将其应用于伯努利连接分布，还引入位集实现用于二进制脉冲和二进制权重。

Result: 在高维连续控制基准测试中提升有限种群下的稳定性，与PPO - LSTM等强基线相比获得有竞争力的回报，减少了时钟训练时间。

Conclusion: SATR能有效训练RSNNs，且位集实现使SATR更具扩展性。

Abstract: Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.

</details>


### [259] [Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity](https://arxiv.org/abs/2601.21577)
*Mutian Yang,Zisen Zhan,Yutong Chen,Haolin Li,Kaiwen Wang,Kaili Zheng,Yuguang Wang,Qi Wang,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 本文建立梯度理论框架解释大语言模型知识注入时的灾难性遗忘问题，提出CNL方法减少遗忘，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对大语言模型知识注入时灾难性遗忘问题的理论解释。

Method: 建立基于梯度的理论框架，确定冲突神经元和协作神经元，提出CNL方法，冻结冲突神经元并更新协作神经元。

Result: 在五个大语言模型、四个数据集和四个优化器上实验，CNL在in - set设置中实现零遗忘，在out - of - set设置中减少59.1% - 81.7%的遗忘。

Conclusion: CNL方法能有效解决大语言模型知识注入时的灾难性遗忘问题。

Abstract: Catastrophic forgetting during knowledge injection severely undermines the continual learning capability of large language models (LLMs). Although existing methods attempt to mitigate this issue, they often lack a foundational theoretical explanation. We establish a gradient-based theoretical framework to explain catastrophic forgetting. We first prove that strongly negative gradient similarity is a fundamental cause of forgetting. We then use gradient similarity to identify two types of neurons: conflicting neurons that induce forgetting and account for 50%-75% of neurons, and collaborative neurons that mitigate forgetting and account for 25%-50%. Based on this analysis, we propose a knowledge injection method, Collaborative Neural Learning (CNL). By freezing conflicting neurons and updating only collaborative neurons, CNL theoretically eliminates catastrophic forgetting under an infinitesimal learning rate eta and an exactly known mastered set. Experiments on five LLMs, four datasets, and four optimizers show that CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings.

</details>


### [260] [Evaluating Prediction Uncertainty Estimates from BatchEnsemble](https://arxiv.org/abs/2601.21581)
*Morten Blørstad,Herman Jangsett Mostein,Nello Blaser,Pekka Parviainen*

Main category: cs.LG

TL;DR: 研究BatchEnsemble用于表格和时间序列任务的不确定性估计，引入GRUBE，与其他模型对比，结果显示BatchEnsemble和GRUBE表现好且参数少、时间短。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在不确定性估计方面的问题，现有方法存在计算不可行或低估不确定性的情况。

Method: 研究BatchEnsemble，引入GRUBE（一种新型BatchEnsemble GRU单元），并与蒙特卡罗Dropout和深度集成模型进行比较。

Result: BatchEnsemble的不确定性估计性能与深度集成模型相当，明显优于蒙特卡罗Dropout；GRUBE在预测和不确定性估计方面表现相似或更好。

Conclusion: BatchEnsemble和GRUBE与传统集成模型相比，用更少参数、更短训练和推理时间取得相似性能。

Abstract: Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.

</details>


### [261] [CORDS: Continuous Representations of Discrete Structures](https://arxiv.org/abs/2601.21583)
*Tin Hadži Veljković,Erik Bekkers,Michael Tiemann,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 提出CORDS策略，将可变大小集合预测转为连续推理问题，在多任务验证效果良好


<details>
  <summary>Details</summary>
Motivation: 现有处理未知对象数量的集合预测问题的方法存在挑战，如依赖填充表示或需显式推断集合大小

Method: 提出CORDS策略，将可变大小集合预测转化为连续推理问题，提供可逆映射将空间对象集转换为连续场

Result: 在分子生成与回归、目标检测等多个任务上进行评估，该方法能稳健处理未知集合大小，准确率有竞争力

Conclusion: CORDS策略有效解决未知对象数量的集合预测问题，具有一定优势

Abstract: Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.

</details>


### [262] [Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning](https://arxiv.org/abs/2601.21589)
*Wentao Yu,Sheng Wan,Shuo Chen,Bo Han,Chen Gong*

Main category: cs.LG

TL;DR: 提出用于图联邦学习的FedSSA方法，解决节点特征和结构拓扑异质性问题，实验显示优于十一种最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有图联邦学习（GFL）因多个客户端节点特征和结构拓扑不同，存在异质性问题，需解决。

Method: 提出FedSSA方法，针对节点特征异质性用变分模型推断节点分布，聚类客户端并构建代表分布，最小化局部与聚类分布差异；针对结构异质性用谱图神经网络和谱能量度量，聚类客户端构建聚类级谱GNN，对齐局部和聚类级谱特征。

Result: 在六个同质性和五个异质性图数据集的非重叠和重叠分区设置下实验，FedSSA始终优于十一种最先进方法。

Conclusion: FedSSA方法能有效解决图联邦学习中节点特征和结构拓扑的异质性问题，具有更好性能。

Abstract: Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.

</details>


### [263] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 论文提出替代方法解决MCMC采样计算成本高的问题，该方法在多任务上表现良好，还能减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型推理性能提升方法主要依赖分布锐化且MCMC采样计算成本高，难以广泛应用，需寻找替代方法。

Method: 提出新公式以token级缩放低温分布近似全局幂分布，引入无训练和无验证器的算法来自回归锐化基础模型的生成分布。

Result: 在四个大语言模型的数学、问答和代码任务上评估，该方法不依赖外部奖励就能达到或超越单次GRPO，相比基于MCMC的采样，推理延迟降低超10倍。

Conclusion: 所提理论上可靠的替代方法无需迭代MCMC，能有效改进大语言模型推理性能，且有更好的效率。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [264] [Dynamics Reveals Structure: Challenging the Linear Propagation Assumption](https://arxiv.org/abs/2601.21601)
*Hoyeon Chang,Bálint Mucsányi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究神经网络一阶参数更新与逻辑一致性的关系，揭示线性传播假设存在的几何限制及相关结论。


<details>
  <summary>Details</summary>
Motivation: 不清楚神经网络一阶参数更新能否保持逻辑一致性，想要研究线性传播假设的几何限制。

Method: 采用关系代数，研究关系的否定、逆和合成三个核心操作。

Result: 否定和逆操作需要张量分解；合成操作存在根本障碍，会导致特征映射崩溃。

Conclusion: 知识编辑失败、逆咒和多跳推理问题可能源于线性传播假设的固有结构限制。

Abstract: Neural networks adapt through first-order parameter updates, yet it remains unclear whether such updates preserve logical coherence. We investigate the geometric limits of the Linear Propagation Assumption (LPA), the premise that local updates coherently propagate to logical consequences. To formalize this, we adopt relation algebra and study three core operations on relations: negation flips truth values, converse swaps argument order, and composition chains relations. For negation and converse, we prove that guaranteeing direction-agnostic first-order propagation necessitates a tensor factorization separating entity-pair context from relation content. However, for composition, we identify a fundamental obstruction. We show that composition reduces to conjunction, and prove that any conjunction well-defined on linear features must be bilinear. Since bilinearity is incompatible with negation, this forces the feature map to collapse. These results suggest that failures in knowledge editing, the reversal curse, and multi-hop reasoning may stem from common structural limitations inherent to the LPA.

</details>


### [265] [Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification](https://arxiv.org/abs/2601.21615)
*Jiaxin Zhang,Yiqi Wang,Siwei Wang,Xihong Yang,Yu Shi,Xinwang Liu,En Zhu*

Main category: cs.LG

TL;DR: 提出TTReFT框架解决图神经网络分布外泛化问题，有理论保障和优秀实验结果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在分布外测试场景性能下降，现有参数微调范式有灾难性遗忘问题，阻碍实际应用。

Method: 提出TTReFT框架，将适应目标从模型参数转为潜在表征，包含不确定性引导节点选择、低秩表征干预和干预感知掩码自编码器三个创新点。

Result: 理论上建立TTReFT在分布外设置的保证，实证上在五个基准数据集实验显示其性能一致且优越。

Conclusion: 确立表征微调为图测试时训练的新范式，有理论基础和实际应用价值。

Abstract: Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.

</details>


### [266] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 本文指出并行思维推理中存在过扩展诅咒，提出T2方法打破此诅咒，降低成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决并行思维中系统级效率和样本级效率不兼容的过扩展诅咒问题。

Method: 形式化并量化过扩展诅咒，分析触发机制，提出T2方法，利用潜在表征在解码前估计每个样本的最优并行度。

Result: T2方法能显著降低成本，同时保持相当的性能。

Conclusion: T2方法能打破过扩展诅咒，实现更高效的并行思维。

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [267] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: 文章针对组合函数浮点计算，提出自适应策略用于transformer推理，通过GPT - 2模型验证策略能以低重计算率让准确性提升两个数量级。


<details>
  <summary>Details</summary>
Motivation: 解决组合丰富函数的浮点计算问题，推动大语言模型向高效、本地可部署发展。

Method: 基于组合函数 $f(g(\mathrm{x}))$ 舍入误差分析，选择 $g(\mathrm{x})$ 部分分量高精度计算，其余低精度计算，并应用于transformer不同组合。

Result: 在GPT - 2模型上数值研究表明，低重计算率可使准确性提升达两个数量级。

Conclusion: 提出的自适应策略在transformer推理中有效，能以低重计算率提高计算精度。

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [268] [Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps](https://arxiv.org/abs/2601.21624)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 本文探讨现代深度学习训练非无记忆的特性，组织相关机制，引入因果估计量、扰动原语和报告清单，得出可移植、因果、不确定性感知的测量协议。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习训练并非无记忆，依赖多种因素，需对相关机制进行组织与研究测量。

Method: 按来源、寿命和可见性组织机制，引入种子配对、函数空间因果估计量，便携式扰动原语和含审计工件的报告清单。

Result: 得出关于训练历史影响程度的可移植、因果、不确定性感知的测量协议。

Conclusion: 得到可移植、因果、不确定性感知的测量协议，用于衡量训练历史在不同模型、数据和机制中的重要性。

Abstract: Modern deep-learning training is not memoryless. Updates depend on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement, staged augmentations and replay), the nonconvex path, and auxiliary state (teacher EMA/SWA, contrastive queues, BatchNorm statistics). This survey organizes mechanisms by source, lifetime, and visibility. It introduces seed-paired, function-space causal estimands; portable perturbation primitives (carry/reset of momentum/Adam/EMA/BN, order-window swaps, queue/teacher tweaks); and a reporting checklist with audit artifacts (order hashes, buffer/BN checksums, RNG contracts). The conclusion is a protocol for portable, causal, uncertainty-aware measurement that attributes how much training history matters across models, data, and regimes.

</details>


### [269] [HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning](https://arxiv.org/abs/2601.21626)
*Jinhao Zhang Yunquan Zhang,Zicheng yan,Boyang Zhang,Jun Sun,Daning Cheng*

Main category: cs.LG

TL;DR: 提出HeRo - Q算法解决PTQ‘低误差、高损失’问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流模型压缩技术PTQ存在‘低误差、高损失’问题，原因在于LLM损失曲面的Hessian矩阵的高曲率方向对扰动敏感。

Method: 提出HeRo - Q算法，在量化前对权重空间应用轻量级、可学习的旋转压缩矩阵，重塑损失曲面。

Result: 实验表明HeRo - Q在标准W4A8和高难度W3A16超低比特场景均优于GPTQ、AWQ和SpinQuant等方法，在Llama3 8B上提升GSM8K准确率。

Conclusion: HeRo - Q无需架构修改，计算开销小，能无缝集成到现有PTQ流程，有效解决PTQ问题。

Abstract: Post Training Quantization (PTQ), a mainstream model compression technique, often leads to the paradoxical 'low error, high loss' phenomenon because it focuses solely on minimizing quantization error. The root cause lies in the Hessian matrix of the LLM loss landscape: a few high curvature directions are extremely sensitive to perturbations. To address this, we propose the Hessian Robust Quantization (HeRo Q) algorithm, which applies a lightweight, learnable rotation-compression matrix to the weight space prior to quantization. This joint framework reshapes the loss landscape by reducing the largest Hessian eigenvalue and reducing its max eigenvalue, thereby significantly enhancing robustness to quantization noise. HeRo-Q requires no architectural modifications, incurs negligible computational overhead, and integrates seamlessly into existing PTQ pipelines. Experiments on Llama and Qwen models show that HeRo Q consistently outperforms state of the art methods including GPTQ, AWQ, and SpinQuant not only achieving superior performance under standard W4A8 settings, but also excelling in the highly challenging W3A16 ultra low bit regime, where it boosts GSM8K accuracy on Llama3 8B to 70.15\% and effectively avoids the logical collapse commonly seen in aggressive quantization.

</details>


### [270] [Generative Design of Ship Propellers using Conditional Flow Matching](https://arxiv.org/abs/2601.21637)
*Patrick Kruger,Rafael Diaz,Simon Hauschulz,Stefan Harries,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 本文探索生成式人工智能（GenAI）在船舶螺旋桨设计中的应用，采用条件流匹配方法，结合数据生成、分析与增强，展示了GenAI在工程设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统正向机器学习模型只能根据给定设计参数预测机械部件性能，而GenAI模型可生成满足指定性能目标的设计，因此探索其在船舶螺旋桨设计中的应用。

Method: 采用条件流匹配建立设计参数与模拟噪声的双向映射；用涡格法生成数据用于模型训练，分析模型准确性与可用数据量的权衡；提出使用来自低数据密集型正向代理模型的伪标签进行数据增强。

Result: 得到具有几乎相同性能特征的不同螺旋桨几何形状示例。

Conclusion: GenAI在工程设计中具有多功能性和潜力。

Abstract: In this paper, we explore the use of generative artificial intelligence (GenAI) for ship propeller design. While traditional forward machine learning models predict the performance of mechanical components based on given design parameters, GenAI models aim to generate designs that achieve specified performance targets. In particular, we employ conditional flow matching to establish a bidirectional mapping between design parameters and simulated noise that is conditioned on performance labels. This approach enables the generation of multiple valid designs corresponding to the same performance targets by sampling over the noise vector.
  To support model training, we generate data using a vortex lattice method for numerical simulation and analyze the trade-off between model accuracy and the amount of available data. We further propose data augmentation using pseudo-labels derived from less data-intensive forward surrogate models, which can often improve overall model performance. Finally, we present examples of distinct propeller geometries that exhibit nearly identical performance characteristics, illustrating the versatility and potential of GenAI in engineering design.

</details>


### [271] [Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers](https://arxiv.org/abs/2601.21641)
*Evandro S. Ortigossa,Eran Segal*

Main category: cs.LG

TL;DR: 提出Seg - MoE用于时间序列预测，在多基准测试中达SOTA，证明段级路由是性能提升关键。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的时间序列预测模型在扩展和捕捉长期动态方面有问题，现有MoE方法依赖token级路由，未充分利用时间数据特性。

Method: 引入Seg - MoE，对连续时间步段进行路由和处理，将其集成到时间序列Transformer中。

Result: 在多个多变量长期预测基准测试中，Seg - MoE始终达到SOTA，优于密集Transformer和先前的token级MoE模型。

Conclusion: 使MoE路由粒度与时间序列固有结构对齐能提供强大的归纳偏置，为序列数据建模的条件稀疏架构开辟新途径。

Abstract: Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.

</details>


### [272] [Identifiable Equivariant Networks are Layerwise Equivariant](https://arxiv.org/abs/2601.21645)
*Vahid Shahverdi,Giovanni Luca Marchetti,Georg Bökman,Kathlén Kohn*

Main category: cs.LG

TL;DR: 研究深度神经网络端到端等变性与逐层等变性的关系，证明存在参数选择使层等变，结果适用于部分网络，为训练中网络权重等变结构出现提供数学解释。


<details>
  <summary>Details</summary>
Motivation: 探究深度神经网络端到端等变性和逐层等变性之间的关系，解释训练中网络权重等变结构出现的现象。

Method: 在假设模型参数可识别的情况下进行理论推导，采用抽象形式化方法，与架构无关。

Result: 对于端到端函数等变的网络，存在参数选择使各层相对于潜在空间的群作用等变，结果适用于部分已确立参数可识别性的网络。

Conclusion: 为训练中神经网络权重等变结构的出现提供了数学解释。

Abstract: We investigate the relation between end-to-end equivariance and layerwise equivariance in deep neural networks. We prove the following: For a network whose end-to-end function is equivariant with respect to group actions on the input and output spaces, there is a parameter choice yielding the same end-to-end function such that its layers are equivariant with respect to some group actions on the latent spaces. Our result assumes that the parameters of the model are identifiable in an appropriate sense. This identifiability property has been established in the literature for a large class of networks, to which our results apply immediately, while it is conjectural for others. The theory we develop is grounded in an abstract formalism, and is therefore architecture-agnostic. Overall, our results provide a mathematical explanation for the emergence of equivariant structures in the weights of neural networks during training -- a phenomenon that is consistently observed in practice.

</details>


### [273] [Gauge-invariant representation holonomy](https://arxiv.org/abs/2601.21653)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 提出表征完整性（representation holonomy）统计量来衡量表征沿输入路径的变化，证明其性质并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有相似性度量无法捕捉表征沿输入路径的变化，而表征几何影响泛化和鲁棒性，需要新的度量方法。

Method: 引入表征完整性统计量，通过全局白化固定规范，使用共享子空间和仅旋转的Procrustes对齐邻域，再嵌入全特征空间。

Result: 证明了对正交（和仿射，白化后）变换的不变性，建立了仿射层的线性零假设，表明小半径时完整性为零；实验显示完整性随环半径增加，能区分CKA下相似的模型，与对抗和损坏鲁棒性相关，可跟踪训练动态。

Conclusion: 表征完整性是一种实用且可扩展的诊断工具，可用于探索学习表征的几何结构，超越点态相似性。

Abstract: Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the "twist" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.

</details>


### [274] [TabClustPFN: A Prior-Fitted Network for Tabular Data Clustering](https://arxiv.org/abs/2601.21656)
*Tianqi Zhao,Guanyang Wang,Yan Shuo Tan,Qiong Zhang*

Main category: cs.LG

TL;DR: 提出TabClustPFN用于表格数据聚类，单前向传播即可完成，实验表现优且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 表格数据聚类因特征类型、数据生成机制及归纳偏置等问题具挑战性，扩展先验拟合网络（PFNs）到聚类问题非易事。

Method: 引入TabClustPFN，在合成数据集预训练，对聚类分配和聚类基数进行摊销贝叶斯推理。

Result: 在合成数据和真实表格基准测试中，TabClustPFN优于经典、深度和摊销聚类基线，在探索性设置中具强鲁棒性。

Conclusion: TabClustPFN是解决表格数据聚类问题的有效方法。

Abstract: Clustering tabular data is a fundamental yet challenging problem due to heterogeneous feature types, diverse data-generating mechanisms, and the absence of transferable inductive biases across datasets. Prior-fitted networks (PFNs) have recently demonstrated strong generalization in supervised tabular learning by amortizing Bayesian inference under a broad synthetic prior. Extending this paradigm to clustering is nontrivial: clustering is unsupervised, admits a combinatorial and permutation-invariant output space, and requires inferring the number of clusters. We introduce TabClustPFN, a prior-fitted network for tabular data clustering that performs amortized Bayesian inference over both cluster assignments and cluster cardinality. Pretrained on synthetic datasets drawn from a flexible clustering prior, TabClustPFN clusters unseen datasets in a single forward pass, without dataset-specific retraining or hyperparameter tuning. The model naturally handles heterogeneous numerical and categorical features and adapts to a wide range of clustering structures. Experiments on synthetic data and curated real-world tabular benchmarks show that TabClustPFN outperforms classical, deep, and amortized clustering baselines, while exhibiting strong robustness in out-of-the-box exploratory settings. Code is available at https://github.com/Tianqi-Zhao/TabClustPFN.

</details>


### [275] [Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching](https://arxiv.org/abs/2601.21662)
*Li Ju,Mayank Nautiyal,Andreas Hellander,Ekta Vats,Prashant Singh*

Main category: cs.LG

TL;DR: 提出REPVLM方法计算VLM嵌入的概率密度以量化认知不确定性，其表现优于基线，还可用于分布外检测和数据整理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）是确定性的，缺乏量化认知不确定性的内在机制。

Method: 用负嵌入对数密度作为认知不确定性的代理，使用黎曼流匹配在VLM嵌入的超球流形上计算概率密度，即REPVLM方法。

Result: REPVLM实现不确定性和预测误差之间近乎完美的相关性，显著优于现有基线。

Conclusion: REPVLM可用于分类，还能为分布外检测和自动数据整理提供可扩展的指标。

Abstract: Vision-Language Models (VLMs) are typically deterministic in nature and lack intrinsic mechanisms to quantify epistemic uncertainty, which reflects the model's lack of knowledge or ignorance of its own representations. We theoretically motivate negative log-density of an embedding as a proxy for the epistemic uncertainty, where low-density regions signify model ignorance. The proposed method REPVLM computes the probability density on the hyperspherical manifold of the VLM embeddings using Riemannian Flow Matching. We empirically demonstrate that REPVLM achieves near-perfect correlation between uncertainty and prediction error, significantly outperforming existing baselines. Beyond classification, we also demonstrate that the model also provides a scalable metric for out-of-distribution detection and automated data curation.

</details>


### [276] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 指出标准期望回报最大化训练策略常出现结果模式崩溃问题，根源在于期望回报目标本身，提出逆概率缩放修正方法，在不同任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中标准期望回报最大化训练策略导致结果模式崩溃的问题。

Method: 分析问题根源为期望内概率乘数，提出逆概率缩放方法，在GRPO中实现IPS - GRPO。

Result: 在不同推理和分子生成任务中，IPS - GRPO减少结果模式崩溃，性能匹配或超越基线。

Conclusion: 修正目标而非添加探索启发式方法是可靠多模态策略优化的关键。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [277] [LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://arxiv.org/abs/2601.21681)
*Qisong Xiao,Xinhai Chen,Qinglin Wang,Xiaowei Guo,Binglin Wang,Weifeng Chen,Zhichao Wang,Yunfei Liu,Rui Xia,Hang Zou,Gencheng Liu,Shuai Li,Jie Liu*

Main category: cs.LG

TL;DR: 本文提出LLM4Fluid框架，利用大语言模型作为流体动力学可泛化神经求解器，实验证明其无需重新训练且具有良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在流体动力学时空建模中对未见流条件泛化能力有限，应用于新场景时通常需重新训练。

Method: 先通过降阶建模结合物理信息解纠缠机制将高维流场压缩到紧凑潜在空间，再用预训练大语言模型作为时间处理器进行预测，同时提出模态对齐策略解决表征不匹配问题。

Result: 在不同流动场景的广泛实验表明，LLM4Fluid无需重新训练，达到了最先进的精度，具有强大的零样本和上下文学习能力。

Conclusion: LLM4Fluid是一个强大且可泛化的神经求解器，代码和数据集已公开。

Abstract: Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at https://github.com/qisongxiao/LLM4Fluid.

</details>


### [278] [Can Local Learning Match Self-Supervised Backpropagation?](https://arxiv.org/abs/2601.21683)
*Wu S. Zihan,Ariane Delrocq,Wulfram Gerstner,Guillaume Bellec*

Main category: cs.LG

TL;DR: 本文建立全局和局部自监督学习规则联系，开发局部自监督学习算法变体，在图像数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有局部自监督学习理论难以在深度神经网络中构建功能表示，需建立全局和局部规则的联系。

Method: 先为深度线性网络开发理论，确定局部自监督学习算法实现与全局反向传播自监督学习相同权重更新的条件；再开发局部自监督学习算法变体以在深度非线性卷积神经网络中近似全局反向传播自监督学习。

Result: 提高局部自监督学习与全局反向传播自监督学习梯度更新相似度的变体在图像数据集上表现更好，使用CLAPP损失函数的局部自监督学习规则达到可比全局反向传播自监督学习的性能，并改进局部自监督学习的现有技术水平。

Conclusion: 所开发的局部自监督学习算法变体能够有效近似全局反向传播自监督学习，在图像数据集上有良好表现。

Abstract: While end-to-end self-supervised learning with backpropagation (global BP-SSL) has become central for training modern AI systems, theories of local self-supervised learning (local-SSL) have struggled to build functional representations in deep neural networks. To establish a link between global and local rules, we first develop a theory for deep linear networks: we identify conditions for local-SSL algorithms (like Forward-forward or CLAPP) to implement exactly the same weight update as a global BP-SSL. Starting from the theoretical insights, we then develop novel variants of local-SSL algorithms to approximate global BP-SSL in deep non-linear convolutional neural networks. Variants that improve the similarity between gradient updates of local-SSL with those of global BP-SSL also show better performance on image datasets (CIFAR-10, STL-10, and Tiny ImageNet). The best local-SSL rule with the CLAPP loss function matches the performance of a comparable global BP-SSL with InfoNCE or CPC-like loss functions, and improves upon state-of-the-art for local SSL on these benchmarks.

</details>


### [279] [Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold](https://arxiv.org/abs/2601.21686)
*Luca Benfenati,Matteo Risso,Andrea Vannozzi,Ahmet Caner Yüzügüler,Lukas Cavigelli,Enrico Macii,Daniele Jahier Pagliari,Alessio Burrello*

Main category: cs.LG

TL;DR: 文章提出 StiefAttention 方法用于键值缓存压缩，在 Llama3 - 8B 上表现优于 EigenAttention。


<details>
  <summary>Details</summary>
Motivation: 键值缓存导致高带宽内存容量和带宽瓶颈，现有训练后方法用 SVD 代理目标拟合投影可能无法很好反映最终重建结果。

Method: 引入 StiefAttention 方法，通过直接最小化解码器层输出重建误差来学习正交投影矩阵，并为每层预计算误差 - 秩分布图。

Result: 在 Llama3 - 8B 上，StiefAttention 在相同压缩条件下，C4 困惑度比 EigenAttention 高 11.9 点，零样本 MMLU 准确率高 5.4%，相对误差更低、余弦相似度更高。

Conclusion: StiefAttention 方法在键值缓存压缩方面表现更优。

Abstract: Key--value (KV) caching enables fast autoregressive decoding but at long contexts becomes a dominant bottleneck in High Bandwidth Memory (HBM) capacity and bandwidth. A common mitigation is to compress cached keys and values by projecting per-head matrixes to a lower rank, storing only the projections in the HBM. However, existing post-training approaches typically fit these projections using SVD-style proxy objectives, which may poorly reflect end-to-end reconstruction after softmax, value mixing, and subsequent decoder-layer transformations.
  For these reasons, we introduce StiefAttention, a post-training KV-cache compression method that learns \emph{orthonormal} projection bases by directly minimizing \emph{decoder-layer output reconstruction error}. StiefAttention additionally precomputes, for each layer, an error-rank profile over candidate ranks, enabling flexible layer-wise rank allocation under a user-specified error budget. Noteworthy, on Llama3-8B under the same conditions, StiefAttention outperforms EigenAttention by $11.9$ points on C4 perplexity and $5.4\%$ on 0-shot MMLU accuracy at iso-compression, yielding lower relative error and higher cosine similarity with respect to the original decoder-layer outputs.

</details>


### [280] [XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision](https://arxiv.org/abs/2601.21688)
*Alexandre Myara,Nicolas Bourriez,Thomas Boyer,Thomas Lemercier,Ihab Bendidi,Auguste Genovesio*

Main category: cs.LG

TL;DR: 提出弱监督VAE框架XFactors实现解纠缠表示学习，在多数据集上取得SOTA分数，可实现因子交换，代码开源。


<details>
  <summary>Details</summary>
Motivation: 纯无监督方法难以从真实数据中恢复语义因子，有监督方法不稳定且难扩展到大型属性集。

Method: 基于解纠缠信息瓶颈视角，将表示分解为残差子空间和特定因子子空间，通过对比监督编码目标因子，用KL正则化组织几何结构。

Result: 在多数据集上取得SOTA解纠缠分数，实现一致的定性因子对齐，可进行因子交换，能随潜在容量正确扩展，在CelebA数据集上得到评估。

Conclusion: XFactors是有效的解纠缠表示学习方法，能实现对选定因子的解纠缠和显式控制。

Abstract: Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.
  We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.
  Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.

</details>


### [281] [Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts](https://arxiv.org/abs/2601.21690)
*Qinglun Li,Anke Tang,Miao Zhang,Mengzhu Wang,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 本文运用L₂ - 稳定性理论分析模型合并，提供统一理论框架并给出实用建议，实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并缺乏统一理论解释不同微调超参数下的有效性，且开源微调模型超参数不透明，难以预测合并模型性能和指导微调。

Method: 采用L₂ - 稳定性理论在异质超参数环境下分析合并模型的泛化性。

Result: 提供统一理论框架解释现有合并算法，提出实用建议，实验证实不同超参数对合并模型泛化性的影响与理论结果相符。

Conclusion: 基于L₂ - 稳定性理论的分析为模型合并提供了理论基础和实用指导。

Abstract: Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\boldsymbol{x}_{avg}$ predicted by our theoretical results.

</details>


### [282] [Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics](https://arxiv.org/abs/2601.21698)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.LG

TL;DR: 研究课程学习对预训练数据顺序的影响，发现训练有共享潜在阶段，课程学习主要改变阶段内数据暴露，小模型中课程学习有益，大模型中收益缩小，其作用是稳定阶段内优化。


<details>
  <summary>Details</summary>
Motivation: 探究课程学习是改变学习轨迹还是仅在固定轨迹上重新排序数据暴露。

Method: 在三种基于语言学的课程设置（习得年龄、词频和动词变异）下训练Pythia模型（14M - 410M参数）300B个标记，并与随机排序对比，在1B参数时对比随机和动词变异排序。

Result: 不同排序下训练有共享潜在阶段，课程学习主要改变阶段内数据暴露；小模型中随机排序梯度噪声大、后期输出头频谱饱和强、最终准确率低，课程学习可缓解；大模型中饱和差异小、课程学习收益缩小。

Conclusion: 课程学习通过稳定阶段内优化而非创造新阶段来发挥作用。

Abstract: Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.

</details>


### [283] [Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities](https://arxiv.org/abs/2601.21702)
*Tien Dang,The-Hai Nguyen,Dinh Mai Phuong,Nguyen Minh Phuong,Hoang Thanh-Tung,Le-Minh Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文从线性表示假设视角重新审视表征误导（RM）类大语言模型遗忘方法，验证了机器遗忘会产生可控副作用行为和更强能力的假设，指出该现象利弊。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探索RM方法中目标向量的作用，作者希望从新视角研究RM方法。

Method: 从线性表示假设视角研究RM，若能识别对应高级概念的一维表示，可在遗忘表示空间对概念向量进行线性操作。

Result: 在多种任务中实证验证了机器遗忘会引发对应高级概念的可控副作用行为和更强能力。

Conclusion: 该现象若被滥用是潜在风险，也可用于开发有更强能力和可控行为的模型。

Abstract: We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.

</details>


### [284] [SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models](https://arxiv.org/abs/2601.21706)
*Nan Lin,Yanbo Wang,Jacco Heres,Peter Palensky,Pedro P. Vergara*

Main category: cs.LG

TL;DR: 本文提出用流匹配模型统一智能电表数据生成任务，避免为各任务单独训练模型，生成数据表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能电表数据存在不可用、损坏、分辨率不足等问题，且机器学习模型需为各任务单独设计训练，存在冗余低效问题。

Method: 利用流匹配模型强大的建模能力，将不同生成任务视为不同形式的部分数据观测并注入生成过程，用单个模型统一多种任务。

Result: 模型生成的数据与给定观测一致且真实，在与插值和其他机器学习基线方法对比中表现更好。

Conclusion: 提出的方法能有效统一智能电表数据生成任务，无需重新训练模型，具有有效性和优越性。

Abstract: Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.

</details>


### [285] [When does predictive inverse dynamics outperform behavior cloning?](https://arxiv.org/abs/2601.21718)
*Lukas Schäfer,Pallavi Choudhury,Abdelhak Lemkhenter,Chris Lovett,Somjit Nath,Luis França,Matheus Ribeiro Furtado de Mendonça,Alex Lamb,Riashat Islam,Siddhartha Sen,John Langford,Katja Hofmann,Sergio Valcarcel Macua*

Main category: cs.LG

TL;DR: 本文探讨行为克隆（BC）在专家示范有限时常失败的问题，引入预测逆动力学模型（PIDM）并给出其理论优势解释，通过实验验证PIDM性能更佳。


<details>
  <summary>Details</summary>
Motivation: BC在专家示范有限时失败，PIDM虽表现好但优势原因不明，需理论解释。

Method: 提出PIDM存在偏差 - 方差权衡的理论，建立状态预测器偏差的条件，通过2D导航任务和3D游戏环境进行实验验证。

Result: 在2D任务中，BC达到类似性能所需示范次数是PIDM的3 - 5倍；在3D游戏环境中，BC所需样本比PIDM多超66%。

Conclusion: PIDM能实现比BC更低的预测误差和更高的样本效率，有更多数据源时优势更明显。

Abstract: Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\% more samples than PIDM.

</details>


### [286] [Amortized Spectral Kernel Discovery via Prior-Data Fitted Network](https://arxiv.org/abs/2601.21731)
*Kaustubh Sharma,Srijan Tiwari,Ojasva Nema,Parikshit Pareek*

Main category: cs.LG

TL;DR: 提出从预训练PFNs进行可解释性驱动的频谱发现框架，解码器可恢复复杂频谱并生成显式核，减少推理时间。


<details>
  <summary>Details</summary>
Motivation: PFNs缺乏对学习先验和核的透明访问，妨碍其在需要显式协方差模型的下游任务中使用。

Method: 引入可解释性驱动框架，对训练的PFN进行机制分析，提出将PFN潜变量映射到显式频谱密度估计和对应平稳核的解码器架构。

Result: 解码器能恢复复杂多峰频谱混合，生成的显式核支持高斯过程回归，精度与PFNs和基于优化的基线相当，推理时间相比基于优化的基线大幅减少。

Conclusion: 所提方法可有效从预训练PFNs进行频谱发现，解决PFN的不透明性问题，提高推理效率。

Abstract: Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-driven framework for amortized spectral discovery from pre-trained PFNs with decoupled attention. We perform a mechanistic analysis on a trained PFN that identifies attention latent output as the key intermediary, linking observed function data to spectral structure. Building on this insight, we propose decoder architectures that map PFN latents to explicit spectral density estimates and corresponding stationary kernels via Bochner's theorem. We study this pipeline in both single-realization and multi-realization regimes, contextualizing theoretical limits on spectral identifiability and proving consistency when multiple function samples are available. Empirically, the proposed decoders recover complex multi-peak spectral mixtures and produce explicit kernels that support Gaussian process regression with accuracy comparable to PFNs and optimization-based baselines, while requiring only a single forward pass. This yields orders-of-magnitude reductions in inference time compared to optimization-based baselines.

</details>


### [287] [Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators](https://arxiv.org/abs/2601.21737)
*Rebecca Pelke,Joel Klein,Jose Cubero-Cascante,Nils Bosbach,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.LG

TL;DR: 提出用于CIM架构的混合精度训练和编译框架，用强化学习策略找量化配置，最佳情况比现有方案快2.48倍，精度损失仅0.086%。


<details>
  <summary>Details</summary>
Motivation: 多数CIM编译器不支持8位以下量化，导致MVM计算周期多、权重难以高效存储。

Method: 提出混合精度训练和编译框架，引入基于强化学习的策略寻找合适的量化配置。

Result: 最佳情况下比现有最先进解决方案实现高达2.48倍的加速，精度损失仅0.086%。

Conclusion: 所提方法能有效平衡CIM架构中计算的延迟和精度。

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

</details>


### [288] [Temporal Sepsis Modeling: a Fully Interpretable Relational Way](https://arxiv.org/abs/2601.21747)
*Vincent Lemaire,Nédra Meloulli,Pierre Jaquet*

Main category: cs.LG

TL;DR: 提出关系方法的机器学习框架用于脓毒症早期预测，实验验证其相关性和高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在脓毒症早期预测中缺乏可解释性且忽略潜在患者亚表型，需新方法解决问题。

Method: 将电子病历的时间数据视为多元患者日志，用关系数据模式表示，应用命题化技术构建可解释特征来扁平化数据，最后用选择性朴素贝叶斯分类器分类。

Result: 实验验证了该方法的相关性和极端可解释性，解释包括单变量、全局、局部和反事实四个方面。

Conclusion: 所提出的关系方法机器学习框架可有效用于脓毒症早期预测，且具有良好可解释性。

Abstract: Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to "flatten" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.

</details>


### [289] [FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer](https://arxiv.org/abs/2601.21750)
*Chenrui Xu,Wenjing Yan,Ying-Jun Angela Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training large-scale neural networks requires solving nonconvex optimization where the choice of optimizer fundamentally determines both convergence behavior and computational efficiency. While adaptive methods like Adam have long dominated practice, the recently proposed Muon optimizer achieves superior performance through orthogonalized momentum updates that enforce isotropic geometry with uniform singular values. However, this strict isotropy discards potentially valuable curvature information encoded in gradient spectra, motivating optimization methods that balance geometric structure with adaptivity. We introduce FISMO (Fisher-Structured Momentum-Orthogonalized) optimizer, which generalizes isotropic updates to incorporate anisotropic curvature information through Fisher information geometry. By reformulating the optimizer update as a trust-region problem constrained by a Kronecker-factored Fisher metric, FISMO achieves structured preconditioning that adapts to local loss landscape geometry while maintaining computational tractability. We establish convergence guarantees for FISMO in stochastic nonconvex settings, proving an $\mathcal{O}(1/\sqrt{T})$ rate for the expected squared gradient norm with explicit characterization of variance reduction through mini-batching. Empirical evaluation on image classification and language modeling benchmarks demonstrates that FISMO achieves superior training efficiency and final performance compared to established baselines.

</details>


### [290] [Differentiable Knapsack and Top-k Operators via Dynamic Programming](https://arxiv.org/abs/2601.21775)
*Germain Vivier-Ardisson,Michaël E. Sander,Axel Parmentier,Mathieu Blondel*

Main category: cs.LG

TL;DR: 提出统一框架处理背包和Top - k算子融入神经网络难题，给出算法、理论证明并实验验证。


<details>
  <summary>Details</summary>
Motivation: 背包和Top - k算子融入神经网络有挑战，因其分段常数，梯度几乎处处为零。

Method: 将算子转化为动态规划，平滑递归得到可微松弛；开发支持前向和后向传播的高效并行算法；理论证明熵正则化相关特性。

Result: 在决策聚焦学习基准、受限动态分类强化学习问题和离散VAE扩展上验证框架。

Conclusion: 所提统一框架有效可行，可解决算子融入神经网络的难题。

Abstract: Knapsack and Top-k operators are useful for selecting discrete subsets of variables. However, their integration into neural networks is challenging as they are piecewise constant, yielding gradients that are zero almost everywhere. In this paper, we propose a unified framework casting these operators as dynamic programs, and derive differentiable relaxations by smoothing the underlying recursions. On the algorithmic side, we develop efficient parallel algorithms supporting both deterministic and stochastic forward passes, and vector-Jacobian products for the backward pass. On the theoretical side, we prove that Shannon entropy is the unique regularization choice yielding permutation-equivariant operators, and characterize regularizers inducing sparse selections. Finally, on the experimental side, we demonstrate our framework on a decision-focused learning benchmark, a constrained dynamic assortment RL problem, and an extension of discrete VAEs.

</details>


### [291] [Quantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence](https://arxiv.org/abs/2601.21780)
*Jun Qi,Chao-Han Huck Yang,Pin-Yu Chen,Min-Hsiu Hsieh,Hector Zenil,Jesper Tegner*

Main category: cs.LG

TL;DR: 提出量子乐高学习框架，将经典和量子组件视为可重用模块，开发泛化理论并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有混合量子 - 经典学习模型存在架构耦合、通用性和可迁移性差的问题，需要改进。

Method: 引入量子乐高学习框架，以预训练经典神经网络为特征块，VQC为自适应模块；开发块级泛化理论。

Result: 分析结果推广了先前张量网络特定结果，确定量子模块优势条件；实验验证框架稳定优化、对量子比特数量敏感性低且抗噪声。

Conclusion: 量子乐高学习框架是模块化、与架构无关的有效学习框架，在受限量子资源下能实现高效学习。

Abstract: Hybrid quantum-classical learning models increasingly integrate neural networks with variational quantum circuits (VQCs) to exploit complementary inductive biases. However, many existing approaches rely on tightly coupled architectures or task-specific encoders, limiting conceptual clarity, generality, and transferability across learning settings. In this work, we introduce Quantum LEGO Learning, a modular and architecture-agnostic learning framework that treats classical and quantum components as reusable, composable learning blocks with well-defined roles. Within this framework, a pre-trained classical neural network serves as a frozen feature block, while a VQC acts as a trainable adaptive module that operates on structured representations rather than raw inputs. This separation enables efficient learning under constrained quantum resources and provides a principled abstraction for analyzing hybrid models. We develop a block-wise generalization theory that decomposes learning error into approximation and estimation components, explicitly characterizing how the complexity and training status of each block influence overall performance. Our analysis generalizes prior tensor-network-specific results and identifies conditions under which quantum modules provide representational advantages over comparably sized classical heads. Empirically, we validate the framework through systematic block-swap experiments across frozen feature extractors and both quantum and classical adaptive heads. Experiments on quantum dot classification demonstrate stable optimization, reduced sensitivity to qubit count, and robustness to realistic noise.

</details>


### [292] [NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification](https://arxiv.org/abs/2601.21792)
*Tongze Wang,Xiaohui Xie,Wenduo Wang,Chuyi Wang,Jinzhou Liu,Boyan Huang,Yannan Hu,Youjian Zhao,Yong Cui*

Main category: cs.LG

TL;DR: 提出NetMamba+框架解决现有流量分类方法的问题，在多方面表现优于基线模型，还实现了在线流量分类系统。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和深度学习流量分类方法存在Transformer架构计算低效、流量特征表示不足和长尾分布处理差等问题。

Method: 提出NetMamba+框架，包含高效架构、多模态流量表示方案和标签分布感知微调策略。

Result: 在大规模数据集上F1分数最高提升6.44%，推理吞吐量提高1.7倍，少样本学习能力强，在线系统吞吐量达261.87 Mb/s。

Conclusion: NetMamba+为复杂网络环境中的流量分析开辟了新途径。

Abstract: With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficiency of Transformer architectures, inadequate traffic representations with loss of crucial byte-level features while retaining detrimental biases, and poor handling of long-tail distributions in real-world data. We propose NetMamba+, a framework that addresses these challenges through three key innovations: (1) an efficient architecture considering Mamba and Flash Attention mechanisms, (2) a multimodal traffic representation scheme that preserves essential traffic information while eliminating biases, and (3) a label distribution-aware fine-tuning strategy. Evaluation experiments on massive datasets encompassing four main classification tasks showcase NetMamba+'s superior classification performance compared to state-of-the-art baselines, with improvements of up to 6.44\% in F1 score. Moreover, NetMamba+ demonstrates excellent efficiency, achieving 1.7x higher inference throughput than the best baseline while maintaining comparably low memory usage. Furthermore, NetMamba+ exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. Additionally, we implement an online traffic classification system that demonstrates robust real-world performance with a throughput of 261.87 Mb/s. As the first framework to adapt Mamba architecture for network traffic classification, NetMamba+ opens new possibilities for efficient and accurate traffic analysis in complex network environments.

</details>


### [293] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: 提出无训练去学习方法KVW处理LVLMs问题，实验证明有良好权衡和效率提升


<details>
  <summary>Details</summary>
Motivation: LVLMs存在隐私泄露等问题，现有基于梯度的去学习方法对大规模LVLMs计算成本高

Method: 提出无训练的知识向量弱化（KVW）方法，直接干预全模型，识别并弱化遗忘集上激活的知识向量贡献

Result: 在MLLMU和CLEAR基准测试中，KVW实现稳定的遗忘 - 保留权衡，计算效率显著高于基于梯度和LoRA的去学习方法

Conclusion: KVW是处理LVLMs去学习问题的有效方法，能提高计算效率

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [294] [Effective LoRA Adapter Routing using Task Representations](https://arxiv.org/abs/2601.21795)
*Akash Dhasade,Anne-Marie Kermarrec,Igor Pavlovic,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos*

Main category: cs.LG

TL;DR: 介绍了新型路由框架LORAUTER，通过任务表示选择和组合LoRA适配器，实验显示其优于基线方法，在大且嘈杂的适配器池也具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LoRA适配器池不断增长，需有效路由方法选择和组合合适适配器。

Method: 引入LORAUTER框架，通过小验证集派生的任务嵌入路由查询，不依赖适配器训练数据，在任务级别操作。

Result: 在多任务实验中，LORAUTER始终优于基线路由方法，在有任务对齐适配器时达Oracle性能的101.2%，在未见任务上提升5.2分，扩展到超1500个适配器显示出鲁棒性。

Conclusion: LORAUTER是一种高效的路由框架，能有效利用LoRA适配器，在不同场景表现良好。

Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.

</details>


### [295] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出非参数统计框架DMLEval对大模型从偏好数据进行比较和排名，有诸多优势并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类偏好数据评估大语言模型性能的方法存在依赖限制性参数假设或缺乏有效不确定性量化的问题。

Method: 提出非参数统计框架DMLEval，引入广义平均排名分数（GARS），可结合黑箱机器学习方法、预训练的大模型评估器等。

Result: 理论和实证上用合成和真实偏好数据集证明了DMLEval的优势。

Conclusion: 该框架为从业者提供了强大的、最先进的方法用于比较或排名大语言模型。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [296] [Goal-Driven Adaptive Sampling Strategies for Machine Learning Models Predicting Fields](https://arxiv.org/abs/2601.21832)
*Jigar Parekh,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 本文提出用于场预测的机器学习模型主动学习策略，结合高斯过程模型，对比不同方法，结果显示该策略在NASA模型不确定性传播任务中以低成本实现高精度。


<details>
  <summary>Details</summary>
Motivation: 解决在使用机器学习模型处理多查询挑战时，以最小计算成本确保特定任务准确性的问题，且拓展主动学习策略到场预测领域。

Method: 结合高斯过程模型预测标量参考值，同时降低认知模型误差以及标量和场预测之间的差异，引入不同形式的方法并与仅基于标量值的填充方法对比。

Result: 在NASA通用研究模型的不确定性传播任务中，与无主动学习的方法相比，该策略能以显著更低的成本实现高精度。

Conclusion: 所提出的主动学习策略有效，能在降低计算成本的同时保证场预测的准确性。

Abstract: Machine learning models are widely regarded as a way forward to tackle multi-query challenges that arise once expensive black-box simulations such as computational fluid dynamics are investigated. However, ensuring the desired level of accuracy for a certain task at minimal computational cost, e.g. as few black-box samples as possible, remains a challenges. Active learning strategies are used for scalar quantities to overcome this challenges and different so-called infill criteria exists and are commonly employed in several scenarios. Even though needed in various field an extension of active learning strategies towards field predictions is still lacking or limited to very specific scenarios and/or model types. In this paper we propose an active learning strategy for machine learning models that are capable if predicting field which is agnostic to the model architecture itself. For doing so, we combine a well-established Gaussian process model for a scalar reference value and simultaneously aim at reducing the epistemic model error and the difference between scalar and field predictions. Different specific forms of the above-mentioned approach are introduced and compared to each other as well as only scalar-valued based infill. Results are presented for the NASA common research model for an uncertainty propagation task showcasing high level of accuracy at significantly smaller cost compared to an approach without active learning.

</details>


### [297] [Scalable Linearized Laplace Approximation via Surrogate Neural Kernel](https://arxiv.org/abs/2601.21835)
*Luis A. Ortega,Simón Rodríguez-Santana,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出可扩展方法近似LLA核，用代理DNN学习特征表示，避免计算大Jacobian矩阵，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 找到在LLA中比NTK更好的核，以在给定预训练DNN时计算预测不确定性。

Method: 使用代理DNN学习紧凑特征表示，其内积可复制NTK，训练仅依赖高效的Jacobian - 向量积。

Result: 与现有LLA近似方法相比，不确定性估计和校准效果相似或更好，偏置学习核可显著增强分布外检测。

Conclusion: 所提方法有助于在LLA中找到比NTK更好的核来计算预测不确定性。

Abstract: We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.

</details>


### [298] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 本文提出算法解决约束元强化学习中确保测试任务策略安全、降低样本复杂性问题并得出样本复杂度下限。


<details>
  <summary>Details</summary>
Motivation: 现实应用在测试时有安全约束，当前约束元强化学习需解决在确保测试任务策略安全的同时降低样本复杂性和快速学习最优策略的问题。

Method: 提出一种算法改进训练中学习到的策略，有可证明的安全性和样本复杂度保证。

Result: 推导出匹配的下限，表明样本复杂度是紧的。

Conclusion: 所提算法能达成确保测试任务策略安全、降低样本复杂性和快速学习最优策略的目标。

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [299] [Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models](https://arxiv.org/abs/2601.21851)
*Sidney Bender,Marco Morik*

Main category: cs.LG

TL;DR: 提出Visual Disentangled Diffusion Autoencoders (DiDAE)框架，结合无梯度反事实生成与知识蒸馏技术，减轻基础模型快捷学习问题，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解基础模型快捷学习问题的方法依赖不可用的组标签或梯度优化，效率低且成本高。

Method: 提出DiDAE框架，结合预训练基础模型和解缠字典学习，进行无梯度反事实生成；结合反事实知识蒸馏技术（CFKD）。

Result: DiDAE可更快生成多个解缠的反事实样本；DiDAE - CFKD实现了最先进的性能，提高了不平衡数据集上的下游性能。

Conclusion: DiDAE框架有效缓解了基础模型的快捷学习问题，可提升下游任务在不平衡数据集上的表现。

Abstract: Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.

</details>


### [300] [MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts](https://arxiv.org/abs/2601.21866)
*Evandro S. Ortigossa,Guy Lutsker,Eran Segal*

Main category: cs.LG

TL;DR: 提出MoHETS模型解决多元时间序列长时预测问题，经多基准测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实多元时间序列有复杂多尺度结构，长时预测困难，现有稀疏MoE方法依赖同质MLP专家，难以捕捉时间序列多样动态。

Method: 提出MoHETS模型，集成稀疏MoHE层，将时间块路由到专家网络子集，结合共享深度卷积专家和基于傅里叶的专家，通过跨注意力融入外生信息，用轻量级卷积块解码器替代线性投影头。

Result: 在七个多元基准和多预测步长上验证，MoHETS持续达到最优性能，相比近期基线平均MSE降低12%。

Conclusion: MoHETS在长期预测中实现了有效的异构专业化。

Abstract: Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.

</details>


### [301] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 决策聚焦学习整合预测建模与组合优化，本文聚焦估计组合优化问题目标函数系数，指出学习阶段扰动强度波动会导致训练无效，提出正则化估计成本向量解决该问题。


<details>
  <summary>Details</summary>
Motivation: 解决决策聚焦学习中通过组合优化问题求导的难题，以及学习阶段扰动强度波动导致训练无效的问题。

Method: 引入对估计成本向量的正则化。

Result: 通过大量数值实验证明，引入正则化能提高学习过程的鲁棒性和可靠性。

Conclusion: 对估计成本向量进行正则化可解决学习阶段扰动强度波动问题，提升学习效果。

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [302] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: 研究代码结构复杂性对大语言模型推理能力的影响，发现结构属性决定代码对提升推理能力的效用，限制数据结构复杂度范围调优效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究将代码作为通用训练信号，未明确代码哪些属性有助于提升推理能力，因此研究代码结构复杂性对模型推理能力的影响。

Method: 研究代码结构复杂性，设置解决方案驱动和问题驱动两种复杂度设置，用圈复杂度和逻辑代码行构建调优数据集，在多种推理基准上评估大语言模型。

Result: 在83%的实验中，限制调优数据到特定结构复杂度范围的效果优于使用结构多样的代码进行训练。

Conclusion: 代码虽能提升推理能力，但结构属性起关键作用，存在以数据为中心提升推理能力的途径，而非仅依赖模型规模扩展。

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [303] [A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding](https://arxiv.org/abs/2601.21897)
*Ali Hasanzadeh Karkan,Ahmed Ibrahim,Jean-François Frigon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: 本文提出一种即插即用预编码器(PaPP)，可跨站点等复用，经少量本地样本微调后表现优于传统和深度学习基线，能降低计算能耗。


<details>
  <summary>Details</summary>
Motivation: 传统mMIMO下行预编码算法计算成本高、对信噪比和信道估计质量敏感，现有深度学习解决方案缺乏鲁棒性，需为每个部署站点重新训练。

Method: 提出PaPP框架，结合高容量教师模型和紧凑学生模型，采用自监督损失平衡教师模仿和归一化和速率，使用元学习领域泛化和发射功率感知输入归一化进行训练。

Result: 在三个未见过站点的射线追踪数据上，PaPP的FDP和HBF模型经微调后均优于传统和深度学习基线，跨架构实现超21倍计算能耗降低，信道估计误差下性能良好。

Conclusion: PaPP是节能型mMIMO预编码的实用解决方案。

Abstract: Massive multiple-input multiple-output (mMIMO) downlink precoding offers high spectral efficiency but remains challenging to deploy in practice because near-optimal algorithms such as the weighted minimum mean squared error (WMMSE) are computationally expensive, and sensitive to SNR and channel-estimation quality, while existing deep learning (DL)-based solutions often lack robustness and require retraining for each deployment site. This paper proposes a plug-and-play precoder (PaPP), a DL framework with a backbone that can be trained for either fully digital (FDP) or hybrid beamforming (HBF) precoding and reused across sites, transmit-power levels, and with varying amounts of channel estimation error, avoiding the need to train a new model from scratch at each deployment. PaPP combines a high-capacity teacher and a compact student with a self-supervised loss that balances teacher imitation and normalized sum-rate, trained using meta-learning domain-generalization and transmit-power-aware input normalization. Numerical results on ray-tracing data from three unseen sites show that the PaPP FDP and HBF models both outperform conventional and deep learning baselines, after fine-tuning with a small set of local unlabeled samples. Across both architectures, PaPP achieves more than 21$\times$ reduction in modeled computation energy and maintains good performance under channel-estimation errors, making it a practical solution for energy-efficient mMIMO precoding.

</details>


### [304] [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)
*Zhiqing Cui,Siru Zhong,Ming Jin,Shirui Pan,Qingsong Wen,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出OmniAir框架和WorldAir数据集用于全球空气质量预测，性能优、效率高。


<details>
  <summary>Details</summary>
Motivation: 解决全球空气质量预测中空间异质性大以及现有直推式模型对未知区域泛化能力差的问题。

Method: 提出OmniAir语义拓扑学习框架，将不变物理环境属性编码为可泛化的站点标识，动态构建自适应稀疏拓扑；创建WorldAir大规模数据集。

Result: OmniAir对比18个基线模型达到了最优性能，速度比现有模型快近10倍，有效弥合数据稀疏地区的监测差距。

Conclusion: OmniAir在全球空气质量预测中具有高性能、高效率和可扩展性。

Abstract: Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.

</details>


### [305] [Hardware-Triggered Backdoors](https://arxiv.org/abs/2601.21902)
*Jonas Möller,Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.LG

TL;DR: 研究表明机器学习模型在不同硬件推理时的数值差异可被利用创建后门，实证证明可在常见GPU上实现，还研究了防御方法。


<details>
  <summary>Details</summary>
Motivation: 发现机器学习模型在不同硬件上推理时因硬件设计差异会有数值变化，探索利用此变化创建后门。

Method: 通过将决策边界局部移近目标输入，再细化数值偏差，使模型在不同硬件对相同输入产生不同预测。

Result: 能在常见GPU加速器上可靠创建硬件触发的后门。

Conclusion: 揭示了影响第三方模型使用的新攻击向量，并对不同防御方法进行了研究。

Abstract: Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.

</details>


### [306] [LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution](https://arxiv.org/abs/2601.21929)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: 文章提出 LoRIF 方法解决训练数据归因（TDA）中随机投影策略的瓶颈，实现存储减少和查询加速，提升了梯度式 TDA 在大规模场景的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的 TDA 方法在随机投影时存在存储和计算瓶颈，存在质量 - 可扩展性权衡问题，需要改进。

Method: 引入 LoRIF，利用梯度的低秩结构，存储投影后每个样本梯度的秩 - c 因子，用截断 SVD 和伍德伯里恒等式在 r 维子空间近似 Hessian 项。

Result: 在 0.1B 到 70B 参数的模型和数百万样本的数据集上，LoRIF 与 LoGRA 相比，实现了高达 20 倍的存储减少和查询加速，归因质量相当或更优。

Conclusion: LoRIF 使基于梯度的 TDA 在前沿规模上具有实用性。

Abstract: Training data attribution (TDA) identifies which training examples most influenced a model's prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality -- creating a quality-scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.

</details>


### [307] [Robust Multimodal Representation Learning in Healthcare](https://arxiv.org/abs/2601.21941)
*Xiaoguang Zhu,Linxiao Gong,Lianlong Sun,Yang Liu,Haoyu Wang,Jing Liu*

Main category: cs.LG

TL;DR: 提出双流特征去相关框架解决医学多模态表征学习中的偏差问题，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实医学数据集存在多源系统偏差，现有方法忽略固有偏差特征影响泛化能力。

Method: 提出双流特征去相关框架，用结构因果分析识别处理偏差，采用因果偏差去相关框架和双流神经网络分离因果特征与虚假关联，利用广义交叉熵损失和互信息最小化进行有效去相关，且框架与模型无关可集成到现有方法。

Result: 在MIMIC - IV、eICU和ADNI数据集上实验，性能有一致提升。

Conclusion: 所提双流特征去相关框架能有效解决医学多模态表征学习中的偏差问题，提升性能。

Abstract: Medical multimodal representation learning aims to integrate heterogeneous data into unified patient representations to support clinical outcome prediction. However, real-world medical datasets commonly contain systematic biases from multiple sources, which poses significant challenges for medical multimodal representation learning. Existing approaches typically focus on effective multimodal fusion, neglecting inherent biased features that affect the generalization ability. To address these challenges, we propose a Dual-Stream Feature Decorrelation Framework that identifies and handles the biases through structural causal analysis introduced by latent confounders. Our method employs a causal-biased decorrelation framework with dual-stream neural networks to disentangle causal features from spurious correlations, utilizing generalized cross-entropy loss and mutual information minimization for effective decorrelation. The framework is model-agnostic and can be integrated into existing medical multimodal learning methods. Comprehensive experiments on MIMIC-IV, eICU, and ADNI datasets demonstrate consistent performance improvements.

</details>


### [308] [Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models](https://arxiv.org/abs/2601.21943)
*Ahmad Aghapour,Erhan Bayraktar,Ziqing Zhang*

Main category: cs.LG

TL;DR: 提出无维度收敛的信息论方法和Loss - Adaptive Schedule (LAS)，LAS提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型收敛分析至少与环境维度呈线性关系，更优速率依赖目标分布的内在维度假设或几何限制，需更好方法。

Method: 开发无维度收敛的信息论方法，提出依赖训练损失的Loss - Adaptive Schedule (LAS) 用于反向SDE有效离散化。

Result: 在温和假设下，将目标分布和生成分布的KL散度界定为$O(H^2/K)$，LAS经验上比常见启发式调度提高采样质量。

Conclusion: 所提信息论方法避免几何假设实现无维度收敛，LAS轻量级且有效提升采样质量。

Abstract: Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.

</details>


### [309] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 本文聚焦概念瓶颈模型，引入清晰度概念并提出可解释性评估框架，研究不同建模决策对表示的影响，发现灵活性和可解释性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 视觉 - 语言模型被广泛应用但缺乏对其决策过程的研究，已有的可解释性评估方法有限，特别是针对稀疏感知方法。

Method: 聚焦概念瓶颈模型，引入清晰度概念，提出基于有真实概念标注数据集的可解释性评估框架，考虑基于VLM和属性预测器的CBMs及三种不同的稀疏诱导策略。

Result: 实验揭示了灵活性和可解释性之间的关键权衡，在可比的性能水平下，给定方法可能表现出明显不同的行为。

Conclusion: 不同建模决策会影响概念瓶颈模型的表示，存在灵活性和可解释性的权衡，代码将公开。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [310] [Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities](https://arxiv.org/abs/2601.21950)
*Linxiao Gong,Yang Liu,Lianlong Sun,Yulai Bi,Jing Liu,Xiaoguang Zhu*

Main category: cs.LG

TL;DR: 论文提出AUM框架处理医学多模态学习中的模态缺失问题，在两项任务上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态学习方法假设模态贡献相同和随机缺失模式，忽略医学数据采集的不确定性，实际临床中模态缺失普遍

Method: 提出AUM框架，将单模态表示建模为多元高斯分布以量化不确定性，在二分图中使用不确定性感知的动态消息传递机制聚合信息

Result: AUM框架在MIMIC - IV死亡率预测中AUC - ROC提高2.26%，在eICU中提高2.17%，优于现有方法

Conclusion: AUM框架能有效处理医学多模态学习中的模态缺失问题，有更好的性能

Abstract: Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.

</details>


### [311] [Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization](https://arxiv.org/abs/2601.21956)
*Yunjia Yang,Runze Li,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: 提出不确定性感知的数据驱动优化（UA - DBO）框架，在翼型多点优化问题上验证其可减少预测误差、提升性能且加速优化。


<details>
  <summary>Details</summary>
Motivation: 数据驱动优化（DBO）依赖训练数据库质量，优化中遇到训练分布外样本会导致预测误差，需引入不确定性量化增强鲁棒性。

Method: 提出UA - DBO框架，开发概率编解码器替代模型预测输出不确定性，并将其融入模型置信度感知目标函数以惩罚大预测误差样本。

Result: 在两个翼型多点优化问题上，UA - DBO持续减少优化样本预测误差，性能提升优于原DBO，与全计算模拟的多点优化效果相当但速度显著加快。

Conclusion: UA - DBO框架能有效提升数据驱动优化的性能和速度。

Abstract: Data-based optimization (DBO) offers a promising approach for efficiently optimizing shape for better aerodynamic performance by leveraging a pretrained surrogate model for offline evaluations during iterations. However, DBO heavily relies on the quality of the training database. Samples outside the training distribution encountered during optimization can lead to significant prediction errors, potentially misleading the optimization process. Therefore, incorporating uncertainty quantification into optimization is critical for detecting outliers and enhancing robustness. This study proposes an uncertainty-aware data-based optimization (UA-DBO) framework to monitor and minimize surrogate model uncertainty during DBO. A probabilistic encoder-decoder surrogate model is developed to predict uncertainties associated with its outputs, and these uncertainties are integrated into a model-confidence-aware objective function to penalize samples with large prediction errors during data-based optimization process. The UA-DBO framework is evaluated on two multipoint optimization problems aimed at improving airfoil drag divergence and buffet performance. Results demonstrate that UA-DBO consistently reduces prediction errors in optimized samples and achieves superior performance gains compared to original DBO. Moreover, compared to multipoint optimization based on full computational simulations, UA-DBO offers comparable optimization effectiveness while significantly accelerating optimization speed.

</details>


### [312] [From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation](https://arxiv.org/abs/2601.21964)
*Qianwei Yang,Dong Xu,Zhangfan Yang,Sisi Yuan,Zexuan Zhu,Jianqiang Li,Junkai Ji*

Main category: cs.LG

TL;DR: 提出SoftMol框架用于靶向分子生成，实验显示其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GPT的分子语言模型在分子设计时存在无法充分捕捉分子图结构和缺乏靶向生成机制的问题，需开发新方法。

Method: 提出SoftMol框架，引入软片段，开发SoftBD模型，在ZINC - Curated数据集上训练，集成门控蒙特卡罗树搜索。

Result: 与现有模型相比，实现100%化学有效性，结合亲和力提高9.7%，分子多样性提升2 - 3倍，推理效率提升6.6倍。

Conclusion: SoftMol框架在分子生成方面表现优异，有良好应用前景。

Abstract: Drug discovery can be viewed as a combinatorial search over an immense chemical space, motivating the development of deep generative models for de novo molecular design. Among these, GPT-based molecular language models (MLM) have shown strong molecular design performance by learning chemical syntax and semantics from large-scale data. However, existing MLMs face two fundamental limitations: they inadequately capture the graph-structured nature of molecules when formulated as next-token prediction problems, and they typically lack explicit mechanisms for target-aware generation. Here, we propose SoftMol, a unified framework that co-designs molecular representation, model architecture, and search strategy for target-aware molecular generation. SoftMol introduces soft fragments, a rule-free block representation of SMILES that enables diffusion-native modeling, and develops SoftBD, the first block-diffusion molecular language model that combines local bidirectional diffusion with autoregressive generation under molecular structural constraints. To favor generated molecules with high drug-likeness and synthetic accessibility, SoftBD is trained on a carefully curated dataset named ZINC-Curated. SoftMol further integrates a gated Monte Carlo tree search to assemble fragments in a target-aware manner. Experimental results show that, compared with current state-of-the-art models, SoftMol achieves 100% chemical validity, improves binding affinity by 9.7%, yields a 2-3x increase in molecular diversity, and delivers a 6.6x speedup in inference efficiency. Code is available at https://github.com/szu-aicourse/softmol

</details>


### [313] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: 提出IGETR框架解决现有LLM推理方法问题，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推理方法重上下文轻结构关系，难以从动态图中提取相关子图，导致推理存在问题。

Method: 提出IGETR混合推理框架，分三阶段：用时间GNN识别候选路径；引入LLM引导路径编辑；整合推理路径。

Result: 在标准TKG基准测试中达到了最先进的性能，在具有挑战性的ICEWS数据集上，Hits@1相对提升5.6%，Hits@3相对提升8.1%。

Conclusion: IGETR框架有效，消融实验和额外分析也证实了各组件的有效性。

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


### [314] [Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance](https://arxiv.org/abs/2601.21979)
*Ciaran Bench,Vivek Desai,Carlijn Roozemond,Ruben van Engen,Spencer A. Thomas*

Main category: cs.LG

TL;DR: 本文用蒙特卡罗 dropout 计算 FID 预测方差及特征嵌入模型潜在表征的预测方差，研究其与输入数据分布差异的相关性，为 FID 可信度提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有基于 ImageNet1K 预训练模型的 FID 指标用于医学图像效果不佳，且其捕捉图像特征差异的失效程度未知。

Method: 使用蒙特卡罗 dropout 计算 FID 的预测方差和特征嵌入模型潜在表征的预测方差。

Result: 预测方差的大小与测试输入数据相对于训练数据的分布差异程度有不同程度的相关性。

Conclusion: 预测方差可作为 FID 可信度的指标，为其有效性提供了一定见解。

Abstract: Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.

</details>


### [315] [Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks](https://arxiv.org/abs/2601.21983)
*Andrew Millard,Joshua Murphy,Peter Green,Simon Maskell*

Main category: cs.LG

TL;DR: 本文探讨在顺序蒙特卡罗（SMC）采样器的似然和梯度评估中逐步引入更多小批量数据（数据退火）的方法，能在神经网络基准图像分类问题上实现最高 6 倍的训练加速且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理中精确后验通常难以处理，变分推理依赖变分分布选择，基于粒子的方法如 SMC 计算成本高，需探索降低其计算成本的方法。

Method: 探索在 SMC 采样器的似然和梯度评估中逐步引入更多小批量数据（数据退火）的方法。

Result: 在神经网络基准图像分类问题上能实现最高 6 倍的训练加速，且精度损失极小。

Conclusion: 数据退火方法可有效降低 SMC 方法的计算成本，同时保证一定的精度。

Abstract: Bayesian inference allows us to define a posterior distribution over the weights of a generic neural network (NN). Exact posteriors are usually intractable, in which case approximations can be employed. One such approximation - variational inference - is computationally efficient when using mini-batch stochastic gradient descent as subsets of the data are used for likelihood and gradient evaluations, though the approach relies on the selection of a variational distribution which sufficiently matches the form of the posterior. Particle-based methods such as Markov chain Monte Carlo and Sequential Monte Carlo (SMC) do not assume a parametric family for the posterior by typically require higher computational cost. These sampling methods typically use the full-batch of data for likelihood and gradient evaluations, which contributes to this computational expense. We explore several methods of gradually introducing more mini-batches of data (data annealing) into likelihood and gradient evaluations of an SMC sampler. We find that we can achieve up to $6\times$ faster training with minimal loss in accuracy on benchmark image classification problems using NNs.

</details>


### [316] [PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters](https://arxiv.org/abs/2601.21984)
*Jian Gao,Yiwei Zou,Abhishek Pradhan,Wenhao Huang,Yumin Su,Kaiyuan Yang,Xuan Zhang*

Main category: cs.LG

TL;DR: 提出PowerGenie框架用于大规模自动发现高性能可重构电源转换器，表现优于现有方法，发现新转换器并取得效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在大规模性能驱动的电路拓扑发现方面探索不足，传统方法依赖人类专家。

Method: 引入自动分析框架确定转换器功能和理论性能极限，采用进化微调方法让生成模型与训练分布共同进化。

Result: 相比现有方法有更高的语法有效性、功能有效性、新颖率和品质因数，发现新的8模式可重构转换器，SPICE仿真确认效率提升。

Conclusion: PowerGenie框架能有效实现大规模高性能可重构电源转换器的自动发现。

Abstract: Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.

</details>


### [317] [Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields](https://arxiv.org/abs/2601.21985)
*Yunyang Li,Lin Huang,Luojia Xia,Wenhe Zhang,Mark Gerstein*

Main category: cs.LG

TL;DR: 提出Elign后训练框架解决3D分子构象生成中物理引导的计算瓶颈，生成低能量、高稳定性构象且推理快。


<details>
  <summary>Details</summary>
Motivation: E(3) - 等变扩散模型会重现半经验训练数据偏差，物理引导存在昂贵量子化学评估和重复查询的计算瓶颈。

Method: 用预训练机器学习力场替代DFT评估，将物理引导转移到训练阶段，把反向扩散建模为强化学习问题，引入FED - GRPO微调去噪策略。

Result: Elign生成的构象具有更低的DFT能量和力，稳定性提高，推理和无引导采样一样快。

Conclusion: Elign有效解决了3D分子构象生成中物理引导的计算瓶颈。

Abstract: Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.

</details>


### [318] [Generalized Information Gathering Under Dynamics Uncertainty](https://arxiv.org/abs/2601.21988)
*Fernando Palafox,Jingqi Li,Jesse Milzman,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 提出统一框架解耦信息收集成本与建模选择，推导通用信息收集成本，证明现有互信息成本是特例，建立理论联系并通过实验验证框架实用性。


<details>
  <summary>Details</summary>
Motivation: 现有主动信息收集方法为特定建模选择推导定制成本，需统一框架解耦。

Method: 通过明确参数、信念和控制之间的因果依赖关系，基于Massey的定向信息推导通用信息收集成本。

Result: 证明现有文献中的互信息成本是所提成本的特例，建立互信息成本与线性化贝叶斯估计中信息增益的明确联系。

Conclusion: 所提框架具有理论价值且在多种系统实验中展现出实用性。

Abstract: An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.

</details>


### [319] [Geometry of Drifting MDPs with Path-Integral Stability Certificates](https://arxiv.org/abs/2601.21991)
*Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 文章从几何视角研究非平稳折扣马尔可夫决策过程，提出HT - RL和HT - MCTS方法，实验显示其在跟踪和动态遗憾方面优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 现有非平稳强化学习理论用粗粒度模型，未考虑环境局部变化方式，而加速度和近平局会导致跟踪误差和策略抖动。

Method: 将环境建模为可微同伦路径，跟踪最优贝尔曼不动点的诱导运动，得到复杂度特征；证明求解器无关的路径积分稳定性界，推导间隙安全可行区域；引入HT - RL和HT - MCTS方法在线估计相关指标并调整学习或规划强度。

Result: 实验表明，相比匹配的静态基线，该方法在跟踪和动态遗憾方面表现更好，在振荡和易切换制度下增益最大。

Conclusion: 所提出的方法在非平稳强化学习中有效，能适应环境变化，提升性能。

Abstract: Real-world reinforcement learning is often \emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \emph{how much} the environment changes, not \emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \textit{Homotopy-Tracking RL (HT-RL)} and \textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.

</details>


### [320] [Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains](https://arxiv.org/abs/2601.21999)
*Meng Cao,Jiexi Liu,Songcan Chen*

Main category: cs.LG

TL;DR: 本文研究不平衡领域泛化（IDG），理论推导其泛化界，提出新的负主导对比学习（NDCL）方法并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: IDG 虽具实际意义，但因处理域和标签偏移的纠缠技术复杂且理论基础匮乏，研究不足。

Method: 先理论推导 IDG 泛化界；再提出 NDCL 方法，通过强调负样本增强类间决策边界分离，用重加权交叉熵策略鼓励类内紧凑性，用预测中心对齐策略强制跨域后验一致性。

Result: 在基准测试上的严格实验验证了 NDCL 方法的有效性。

Conclusion: 提出的 NDCL 方法能有效解决 IDG 问题，代码已开源。

Abstract: Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.

</details>


### [321] [Rate-Distortion Optimization for Transformer Inference](https://arxiv.org/abs/2601.22002)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 提出基于率失真的有损压缩框架用于Transformer推理，实验显示有显著节省且部分情况精度提升，还分析性能并推导界限。


<details>
  <summary>Details</summary>
Motivation: Transformer推理时计算和内存需求大，需通过跨设备分区和压缩中间表示来提高推理效率。

Method: 引入基于率失真的有损压缩框架来学习紧凑编码，权衡比特率和准确性。

Result: 在语言基准测试中，所提编解码器实现显著节省，部分情况精度提高，优于复杂基线方法。

Conclusion: 该框架可用于理解Transformer在表示编码中的性能，推导的界限能解释不同架构和任务的比特率。

Abstract: Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.

</details>


### [322] [Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering](https://arxiv.org/abs/2601.22010)
*Dongxuan Zhu,Ly Tran Ho Khanh,Andy Yat-Ming Cheung,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出无训练、推理时干预方法STARS解决语言模型生成路径同质化问题，在测试中表现优。


<details>
  <summary>Details</summary>
Motivation: 语言模型默认输出集窄，生成路径同质化，基于采样策略难保证多并发运行的多样性。

Method: STARS在每个token收集并发运行的隐藏激活，在Stiefel流形上联合优化多个加性引导方向，设计轻量级单步更新。

Result: 在测试用例生成和科学发现基准测试中，STARS始终优于标准采样方法。

Conclusion: STARS能在不牺牲定性性能的前提下，实现更高的多样性。

Abstract: Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.

</details>


### [323] [Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability](https://arxiv.org/abs/2601.22012)
*Sergi Masip,Gido M. van de Ven,Javier Ferrando,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 引入机械框架从特征编码转换角度解释灾难性遗忘，分析模型情况并实验验证，还用于实际模型分析，为持续学习提供新视角。


<details>
  <summary>Details</summary>
Motivation: 以往测量持续学习中的灾难性遗忘多忽略潜在机制，作者想深入探究潜在原因。

Method: 引入机械框架，几何解释灾难性遗忘来自特征编码转换；分析易处理模型识别不同情景；在该模型上实验验证；通过Crosscoders用于实际模型分析。

Result: 实证检验了形式分析，突出了深度的有害影响；完成Vision Transformer顺序学习CIFAR - 10的案例研究。

Conclusion: 为持续学习提供了以特征为中心的新词汇。

Abstract: Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations. Analysis of a tractable model formalizes this view, allowing us to identify best- and worst-case scenarios. Through experiments on this model, we empirically test our formal analysis and highlight the detrimental effect of depth. Finally, we demonstrate how our framework can be used in the analysis of practical models through the use of Crosscoders. We present a case study of a Vision Transformer trained on sequential CIFAR-10. Our work provides a new, feature-centric vocabulary for continual learning.

</details>


### [324] [TBDFiltering: Sample-Efficient Tree-Based Data Filtering](https://arxiv.org/abs/2601.22016)
*Robert Istvan Busa-Fekete,Julian Zimmert,Anne Xiangyi Zheng,Claudio Gentile,Andras Gyorgy*

Main category: cs.LG

TL;DR: 提出基于文本嵌入的分层聚类方法选择大语言模型训练文档，证明方法查询高效，并通过实验展示其相较于其他分类器过滤方法的优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型选择高质量、多样化训练集困难，缺乏廉价可靠质量指标，查询现有大语言模型不可扩展，从业者常用基于稀疏质量信号的分类器。

Method: 提出基于文本嵌入的分层聚类方法，自适应选择由大语言模型评估的文档来估计聚类质量。

Result: 证明方法查询高效，在一定假设下，查询少量文档就能高概率正确预测每个文档质量，文档数量与最小纯叶子子树大小成正比；实验显示该算法优于其他基于分类器的过滤方法。

Conclusion: 所提出的基于文本嵌入的分层聚类方法在大语言模型训练文档选择上是有效且高效的。

Abstract: The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.

</details>


### [325] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 本文提出用于多模态大语言模型的视觉引导关键令牌正则化方法ViKeR，实验证明其能有效执行遗忘学习，减少遗忘并保持响应连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型遗忘学习方法将所有答案令牌统一处理，忽略其重要性差异，且只关注语言模态，忽略视觉线索。

Method: 提出ViKeR方法，利用无关视觉输入预测理想的遗忘学习后令牌级分布，用这些分布规范遗忘学习过程，通过信息熵定义关键令牌，通过令牌级梯度重新加权讨论ViKeR的有效性。

Result: 在MLLMU和CLEAR基准测试上，该方法能有效执行遗忘学习，减少遗忘并保持响应连贯性。

Conclusion: ViKeR方法能有效解决多模态大语言模型的遗忘学习问题。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [326] [From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning](https://arxiv.org/abs/2601.22028)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出CLReg正则化器，减少遗忘与保留特征干扰，助力主流大模型遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型遗忘方法可能只是抑制遗忘内容，遗忘概念仍存在于表示中并与保留知识纠缠。

Method: 引入对比表示正则化器CLReg，识别遗忘特征并将其推离保留特征。

Result: 在不同遗忘基准和不同大小的大模型上，CLReg减少了遗忘 - 保留表示的纠缠，且不带来额外隐私风险。

Conclusion: CLReg可促进主流遗忘方法，启发未来重塑表示空间以移除遗忘概念的工作。

Abstract: Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.

</details>


### [327] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 本文提出集合逆问题（EIP），并给出基于条件生成模型的非迭代推理时间方法，在多个数据集上进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决根据前向过程下先验的推进分布来反演集合的问题，该问题在高能物理、全波形反演和逆成像等领域有应用。

Method: 提出基于集合逆生成模型的非迭代推理时间方法，通过多组真值 - 观测对训练，避免推理时显式和迭代使用前向模型。

Result: 在逆成像、高能物理和全波形反演的多个合成和真实数据集上进行基准测试。

Conclusion: 所提方法利用集合信息有助于后验推断，并能推广到未见先验，代码公开。

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


### [328] [Per-parameter Task Arithmetic for Unlearning in Large Language Models](https://arxiv.org/abs/2601.22030)
*Chengyi Cai,Zesheng Ye,Jiangchao Yao,Jianzhong Qi,Bo Han,Xiaolu Zhang,Feng Liu,Jun Zhou*

Main category: cs.LG

TL;DR: 提出PerTA机制用于LLM去学习，实验证明其优于标准任务向量和常用基于训练的去学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有任务算术去学习方法会导致过度遗忘，因为它会破坏保留其他信息所需的关键参数，且每个参数在遗忘和保留方面的重要性不同。

Method: 提出PerTA机制对特定任务向量进行重新缩放，允许逐参数调整，权重通过梯度（PerTA - grad）或对角Fisher信息近似（PerTA - fisher）估计。

Result: PerTA持续改进标准任务向量，在遗忘效果和整体模型效用上，很多情况下超越常用的基于训练的去学习方法。

Conclusion: PerTA在保持任务算术效率的同时减轻了过度遗忘问题，为LLM去学习提供了一个有原则且实用的框架。

Abstract: In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.

</details>


### [329] [Holographic generative flows with AdS/CFT](https://arxiv.org/abs/2601.22033)
*Ehsan Mirafzali,Sanjit Shashi,Sanya Murdeshwar,Edgar Shaghoulian,Daniele Venturi,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出利用量子引力全息原理和深度学习、输运理论的生成式机器学习框架，在数据集上效果更好且具物理解释性。


<details>
  <summary>Details</summary>
Motivation: 希望借助量子引力全息原理（AdS/CFT 对应）开发新的生成式机器学习范式。

Method: 用 AdS 中标量场的体 - 边界映射表示数据从基础分布到学习分布的流动，用 AdS 物理表示和增强流匹配算法。

Result: 在棋盘玩具数据集和 MNIST 上，模型比无物理的流匹配模型收敛更快、质量更高。

Conclusion: 该方法提供了流匹配的物理解释版本，证明了 AdS 物理和几何在生成式建模新范式开发中的效用。

Abstract: We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.

</details>


### [330] [Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space](https://arxiv.org/abs/2601.22036)
*Xiaolong Zhang,Jianwei Zhang,Xubo Song*

Main category: cs.LG

TL;DR: 提出Cross - Fusion Distance (CFD) 用于表示学习中量化数据组融合程度，有线性复杂度，理论特性经实验验证，在实际数据上表现优于常用方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布距离度量方法混淆影响和不影响数据组融合的因素，不能准确反映数据组真实融合程度。

Method: 引入CFD，该度量方法能分离影响融合的几何因素，对不影响融合的变化保持鲁棒性，具有线性计算复杂度。

Result: 理论上刻画了CFD的不变性和敏感性特性，并在合成实验中验证；在有领域偏移的真实数据集上，CFD比常用替代方法更能反映下游泛化性能下降情况。

Conclusion: CFD为表示学习提供了有理论依据且可解释的距离度量方法。

Abstract: Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.

</details>


### [331] [Making Foundation Models Probabilistic via Singular Value Ensembles](https://arxiv.org/abs/2601.22068)
*Mehmet Ozgur Turkoglu,Dominik J. Mühlematter,Alexander Becker,Konrad Schindler,Helge Aasen*

Main category: cs.LG

TL;DR: 提出参数高效的隐式集成方法SVE用于量化基础模型的认知不确定性，在资源受限场景实现不确定性估计，在NLP和视觉任务验证有效。


<details>
  <summary>Details</summary>
Motivation: 基础模型预测常过度自信且未校准，标准量化认知不确定性方法计算成本高，对大型基础模型不实用。

Method: 提出SVE方法，基于权重矩阵奇异向量构成模型知识有意义子空间的假设，冻结奇异向量，仅训练每个成员的奇异值来调整各方向对输出的贡献。

Result: SVE实现与显式深度集成相当的不确定性量化，基础模型参数增加不到1%，在NLP和视觉任务中提高校准度并保持预测精度。

Conclusion: SVE使有原则的不确定性估计在资源受限环境中可行。

Abstract: Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.

</details>


### [332] [Latent Adversarial Regularization for Offline Preference Optimization](https://arxiv.org/abs/2601.22083)
*Enyi Jiang,Yibo Jacky Zhang,Yinglun Xu,Andreas Haupt,Nancy Amato,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 文章提出GANPO方法进行语言模型偏好优化，通过潜在空间正则化约束策略更新，实验显示效果良好且更稳健。


<details>
  <summary>Details</summary>
Motivation: 传统基于token级正则化的偏好优化在语言模型应用中因token空间与语义行为空间的差异面临挑战，需要新的优化方法。

Method: 引入GANPO方法，通过惩罚策略模型和参考模型内部表示的差异来实现潜在空间正则化，采用对抗方法最小化潜在空间差异，并将其集成到现有离线偏好优化目标中。

Result: 在多个模型架构和任务的实验中，潜在空间正则化带来了一致的改进；与token级正则化相比，GANPO在分布偏移和噪声下提供更稳健的结构反馈，下游性能相当且计算开销小。

Conclusion: 基于潜在空间正则化的GANPO方法用于语言模型偏好优化有效且更稳健。

Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.

</details>


### [333] [GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization](https://arxiv.org/abs/2601.22095)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Chi Wang,Yuehao Wang,Jing Xiong,Liliang Ren,Bo Peng,Qingmei Wang,Xiaoran Shang,Mac Schwager,Anderson Schneider,Yuriy Nevmyvaka,Xiaodong Liu*

Main category: cs.LG

TL;DR: 本文通过流形优化重新审视Transformer架构中归一化层的放置问题，提出GeoNorm方法及层更新衰减策略，实验显示其优于现有方法且可无缝集成到标准架构。


<details>
  <summary>Details</summary>
Motivation: Transformer架构设计中归一化层（Pre - Norm和Post - Norm）的放置是未决问题，需新的解决方法。

Method: 从流形优化角度解读FFN和注意力层输出，引入GeoNorm方法（用流形上的测地线更新替代标准归一化），并提出FFN和注意力组件的层更新衰减策略。

Result: 综合实验表明GeoNorm在Transformer模型中始终优于现有归一化方法。

Conclusion: GeoNorm可无缝集成到标准Transformer架构，以极小额外计算成本实现性能提升。

Abstract: The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.

</details>


### [334] [Boosting CVaR Policy Optimization with Quantile Gradients](https://arxiv.org/abs/2601.22100)
*Yudong Luo,Erick Delage*

Main category: cs.LG

TL;DR: CVaR - PG存在样本效率低问题，通过增加期望分位数项改进，实证显示新算法优于CVaR - PG和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CVaR - PG在优化条件风险价值时样本效率低的问题。

Method: 在CVaR中增加期望分位数项，利用分位数优化的动态规划公式提高样本效率。

Result: 在有可验证风险规避行为的领域中，新算法在马尔可夫策略类中显著改进CVaR - PG，且始终优于其他现有方法。

Conclusion: 通过增加期望分位数项改进的算法能有效提高样本效率，优于现有方法。

Abstract: Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.

</details>


### [335] [Prior-Informed Flow Matching for Graph Reconstruction](https://arxiv.org/abs/2601.22107)
*Harvey Chen,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: 本文提出条件流模型Prior - Informed Flow Matching (PIFM)用于图重建，实验表明其能提升经典嵌入方法的重建准确性。


<details>
  <summary>Details</summary>
Motivation: 从部分观测中重建图是关键挑战，经典嵌入方法缺乏全局一致性，现代生成模型难以融入结构先验。

Method: 基于失真感知理论的排列等变版本，用图论或GraphSAGE/node2vec等先验基于局部信息形成邻接矩阵的初始估计，再用整流流匹配进行优化。

Result: 在不同数据集上的实验显示，PIFM能持续提升经典嵌入方法，在重建准确性上超越它们和最先进的生成基线模型。

Conclusion: PIFM能有效解决图重建问题，在重建准确性上表现良好。

Abstract: We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.

</details>


### [336] [Value-Based Pre-Training with Downstream Feedback](https://arxiv.org/abs/2601.22108)
*Shuqi Ke,Giulia Fanti*

Main category: cs.LG

TL;DR: 提出V - Pretraining方法，在少量验证目标信息下控制预训练，在语言和视觉任务中提升效果。


<details>
  <summary>Details</summary>
Motivation: 标准预训练优化固定代理目标，可能使计算资源从感兴趣的下游能力上分配不当，需少量验证目标信息引导预训练。

Method: 引入V - Pretraining，用轻量级任务设计器重塑预训练任务，使预训练损失梯度与下游任务梯度对齐。

Result: 在0.5B - 7B语言模型推理上提升18%，在视觉SSL中提升ADE20K结果、降低NYUv2 RMSE、提高ImageNet线性准确率，且有证据显示提升了继续预训练的token效率。

Conclusion: V - Pretraining方法有效，能在少量验证目标信息下引导预训练提升下游能力。

Abstract: Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.

</details>


### [337] [Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment](https://arxiv.org/abs/2601.22111)
*Abdullah Tasim,Wei Sun*

Main category: cs.LG

TL;DR: 传统仪器测量低层大气边界层风场有局限，本文用协同无人机群测量数据构建框架重建四维大气风场，结果表明协同测量可准确可扩展地重建风场。


<details>
  <summary>Details</summary>
Motivation: 传统仪器在低层大气边界层时空测量有缺口，单个无人机平台只能沿轨迹采样，无法完整恢复风场，因此需构建新框架。

Method: 用合成湍流环境和高保真多旋翼仿真生成训练和评估数据，用双向长短期记忆网络（Bi - LSTM）从无人机动力学估计局部风分量，再用物理信息神经网络（PINN）重建时空连续风场。

Result: Bi - LSTM在不同风速下对水平风分量估计有不同均方根误差（RMSE），垂直分量误差更高；物理信息重建能恢复1000米高度风场主要时空结构；在中等风速下，重建平均风场在不同无人机配置下有不同RMSE，五无人机群误差最低。

Conclusion: 协同无人机测量无需专用风传感器或固定基础设施，能实现准确且可扩展的四维风场重建。

Abstract: Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.

</details>


### [338] [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)
*Winfried Ripken,Michael Plainer,Gregor Lied,Thorben Frank,Oliver T. Unke,Stefan Chmiela,Frank Noé,Klaus Robert Müller*

Main category: cs.LG

TL;DR: 提出学习哈密顿流映射框架，可稳定大时间步长更新，在多种哈密顿系统验证，尤其改善分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 传统哈密顿系统长时间演化模拟受小时间步长限制，为克服此约束提出新方法。

Method: 引入学习哈密顿流映射框架，施加平均流一致性条件，可在独立相空间样本上训练。

Result: 在多种哈密顿系统得到验证，尤其改善使用机器学习力场的分子动力学模拟，支持更大积分时间步长。

Conclusion: 该方法能在不增加训练和推理成本的情况下，突破传统积分器的稳定性限制，有效应用于哈密顿系统模拟。

Abstract: Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>


### [339] [SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2601.22131)
*Leonard Papenmeier,Petru Tighineanu*

Main category: cs.LG

TL;DR: 提出基于多输出高斯过程的可扩展模块化元学习模型SMOG以加速多目标优化。


<details>
  <summary>Details</summary>
Motivation: 多目标优化存在受黑盒问题和测量预算限制的情况，有相关历史数据可利用元学习加速优化，而元学习的多目标贝叶斯优化方法待探索。

Method: 提出SMOG模型，构建跨元任务和目标任务的结构化联合高斯过程先验，支持分层并行训练。

Result: SMOG将元数据不确定性合理传播到目标代理，且能与标准多目标贝叶斯优化获取函数无缝集成。

Conclusion: SMOG是一种可有效加速多目标优化，且具有可扩展性和模块化的元学习模型。

Abstract: Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.

</details>


### [340] [Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference](https://arxiv.org/abs/2601.22132)
*Ziming Dong,Hardik Sharma,Evan O'Toole,Jaya Prakash Champati,Kui Wu*

Main category: cs.LG

TL;DR: 本文提出LLM Shepherding框架，通过向LLM取短前缀提示给SLM，在数学和编码任务很有效，能降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理成本高限制大规模部署，小语言模型成本低但准确性不足，现有方法将LLM作为全用或不用资源，存在局限性。

Method: 引入LLM Shepherding框架，向LLM请求短前缀提示给SLM，并开发两阶段预测器决定是否需要提示及请求多少token。

Result: 在数学推理和代码生成基准测试上，相比仅用LLM推理降低成本42 - 94%，比现有路由和级联基线最多降低2.8倍成本且准确度相当。

Conclusion: 这是首次利用token级预算控制进行SLM - LLM协作的工作。

Abstract: Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.

</details>


### [341] [PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training](https://arxiv.org/abs/2601.22137)
*Shenghao Yang,Zhichao Wang,Oleg Balabanov,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 提出PRISM框架加速矩阵函数迭代算法，应用于机器学习核心操作，无需明确谱界或奇异值估计，能自动适应变化谱，经验证可加速训练。


<details>
  <summary>Details</summary>
Motivation: 矩阵函数在神经网络训练预条件梯度法中很重要，促使开发避免显式特征分解、依赖矩阵乘法的迭代算法，在此基础上进一步加速。

Method: 提出PRISM框架，结合自适应多项式逼近和随机草图法，通过草图化最小二乘问题拟合多项式替代当前谱。

Result: 将PRISM应用于矩阵平方根和正交化的牛顿 - 舒尔茨类迭代可加速，且无需明确谱界或奇异值估计，能自动适应谱的变化。

Conclusion: PRISM集成到Shampoo和Muon优化器中可加速训练。

Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.

</details>


### [342] [Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing](https://arxiv.org/abs/2601.22151)
*Daniel Stein,Shaoyi Huang,Rolf Drechsler,Bing Li,Grace Li Zhang*

Main category: cs.LG

TL;DR: 本文提出将神经网络转换为逻辑流在CPU上执行，可降低延迟且不损失精度，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦MAC操作执行，但CPU更适合执行控制流逻辑，为提升神经网络在CPU上的计算效率。

Method: 先将神经网络转换为等效决策树，选取带有常量叶节点的决策路径并压缩成含if - else结构和少量MAC操作的逻辑流。

Result: 在模拟的RISC - V CPU上可将延迟最多降低14.9%，且不损失精度。

Conclusion: 将神经网络转换为逻辑流执行可有效提升其在CPU上的计算效率。

Abstract: Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
  The code is open source at https://github.com/TUDa-HWAI/NN2Logic

</details>


### [343] [Discovering Hidden Gems in Model Repositories](https://arxiv.org/abs/2601.22157)
*Jonathan Kahana,Eliahu Horwitz,Yedid Hoshen*

Main category: cs.LG

TL;DR: 论文指出公共仓库模型使用集中，研究发现有‘隐藏珍宝’模型表现出色，提出用多臂老虎机问题及加速算法提升模型发现效率。


<details>
  <summary>Details</summary>
Motivation: 探究模型使用集中是市场有效选择还是有优秀模型被忽视。

Method: 对2000多个模型评估发现‘隐藏珍宝’，将模型发现问题转化为多臂老虎机问题，用共享查询集和激进淘汰策略加速顺序减半搜索算法。

Result: 发现如Llama - 3.1 - 8B家族中少见模型能提高数学性能，新方法每个候选只需50次查询，加速发现超50倍。

Conclusion: 存在‘隐藏珍宝’模型，新方法能显著加速模型发现。

Abstract: Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [344] [STAER: Temporal Aligned Rehearsal for Continual Spiking Neural Network](https://arxiv.org/abs/2601.20870)
*Matteo Gianferrari,Omayma Moussadek,Riccardo Salami,Cosimo Fiorini,Lorenzo Tartarini,Daniela Gandolfi,Simone Calderara*

Main category: cs.NE

TL;DR: 提出Spiking Temporal Alignment with Experience Replay (STAER)框架解决SNNs在CIL中的问题，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks (SNNs) 在Class-Incremental Learning (CIL) 中受灾难性遗忘和尖峰模式时间不对齐的阻碍，要缩小SNNs和ANNs的性能差距。

Method: 引入STAER框架，集成可微分的Soft - DTW对齐损失以保持尖峰时间保真度，对输出对数使用时间扩展和收缩机制来强化鲁棒表征学习。

Result: 在Sequential - MNIST和Sequential - CIFAR10上达到SOTA，性能匹配或超过强ANN基线，同时保留生物学上合理的动力学。

Conclusion: 明确的时间对齐对表征稳定性至关重要，STAER是适用于尖峰原生终身学习的可扩展解决方案。

Abstract: Spiking Neural Networks (SNNs) are inherently suited for continuous learning due to their event-driven temporal dynamics; however, their application to Class-Incremental Learning (CIL) has been hindered by catastrophic forgetting and the temporal misalignment of spike patterns. In this work, we introduce Spiking Temporal Alignment with Experience Replay (STAER), a novel framework that explicitly preserves temporal structure to bridge the performance gap between SNNs and ANNs. Our approach integrates a differentiable Soft-DTW alignment loss to maintain spike timing fidelity and employs a temporal expansion and contraction mechanism on output logits to enforce robust representation learning. Implemented on a deep ResNet19 spiking backbone, STAER achieves state-of-the-art performance on Sequential-MNIST and Sequential-CIFAR10. Empirical results demonstrate that our method matches or outperforms strong ANN baselines (ER, DER++) while preserving biologically plausible dynamics. Ablation studies further confirm that explicit temporal alignment is critical for representational stability, positioning STAER as a scalable solution for spike-native lifelong learning. Code is available at https://github.com/matteogianferrari/staer.

</details>


### [345] [BioNIC: Biologically Inspired Neural Network for Image Classification Using Connectomics Principles](https://arxiv.org/abs/2601.20876)
*Diya Prasanth,Matthew Tivnan*

Main category: cs.NE

TL;DR: 提出基于MICrONs数据集突触连接图的多层前馈神经网络BioNIC用于情感分类，在FER - 2013上取得类似传统模型的性能，证明整合连接组约束的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索基于连接组学约束开发生物启发的人工智能系统。

Method: 从结构和功能层面构建BioNIC，融入生物启发元素，并添加卷积层；在FER - 2013上评估性能，进行消融实验。

Result: BioNIC在FER - 2013上准确率达59.77 ± 0.27%，性能与传统模型相当。

Conclusion: 整合连接组约束是开发生物启发人工智能系统的可行计算方法，新一代拍字节级连接组数据有促进神经科学建模和人工智能的潜力。

Abstract: We present BioNIC, a multi-layer feedforward neural network for emotion classification, inspired by detailed synaptic connectivity graphs from the MICrONs dataset. At a structural level, we incorporate architectural constraints derived from a single cortical column of the mouse Primary Visual Cortex(V1): connectivity imposed via adjacency masks, laminar organization, and graded inhibition representing inhibitory neurons. At the functional level, we implement biologically inspired learning: Hebbian synaptic plasticity with homeostatic regulation, Layer Normalization, data augmentation to model exposure to natural variability in sensory input, and synaptic noise to model neural stochasticity. We also include convolutional layers for spatial processing, mimicking retinotopic mapping. The model performance is evaluated on the Facial Emotion Recognition task FER-2013 and compared with a conventional baseline. Additionally, we investigate the impacts of each biological feature through a series of ablation experiments. While connectivity was limited to a single cortical column and biologically relevant connections, BioNIC achieved performance comparable to that of conventional models, with an accuracy of 59.77 $\pm$ 0.27% on FER-2013. Our findings demonstrate that integrating constraints derived from connectomics is a computationally plausible approach to developing biologically inspired artificial intelligence systems. This work also highlights the potential of new generation peta-scale connectomics data in advancing both neuroscience modeling and artificial intelligence.

</details>


### [346] [Diversifying Toxicity Search in Large Language Models Through Speciation](https://arxiv.org/abs/2601.20981)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: 提出ToxSearch - S改进进化提示搜索方法，能达到更高毒性峰值，有更重尾部，语义覆盖更广，揭示更多攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有进化提示搜索方法常局限于小部分高性能提示，限制不同失败模式的覆盖。

Method: 提出ToxSearch的物种质量多样（QD）扩展ToxSearch - S，引入无监督提示物种形成方法，包括保持带示例领导者的物种、维护储备池、进行物种感知的父选择。

Result: ToxSearch - S达到更高毒性峰值和更重尾部，在中度毒性提示上性能相当，语义覆盖更广，物种在嵌入空间分离且有不同毒性分布。

Conclusion: ToxSearch - S方法能揭示更广泛的攻击策略。

Abstract: Evolutionary prompt search is a practical black-box approach for red teaming large language models (LLMs), but existing methods often collapse onto a small family of high-performing prompts, limiting coverage of distinct failure modes. We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single best prompt. ToxSearch-S introduces unsupervised prompt speciation via a search methodology that maintains capacity-limited species with exemplar leaders, a reserve pool for outliers and emerging niches, and species-aware parent selection that trades off within-niche exploitation and cross-niche exploration. ToxSearch-S is found to reach higher peak toxicity ($\approx 0.73$ vs.\ $\approx 0.47$) and a extreme heavier tail (top-10 median $0.66$ vs.\ $0.45$) than the baseline, while maintaining comparable performance on moderately toxic prompts. Speciation also yields broader semantic coverage under a topic-as-species analysis (higher effective topic diversity $N_1$ and larger unique topic coverage $K$). Finally, species formed are well-separated in embedding space (mean separation ratio $\approx 1.93$) and exhibit distinct toxicity distributions, indicating that speciation partitions the adversarial space into behaviorally differentiated niches rather than superficial lexical variants. This suggests our approach uncovers a wider range of attack strategies.

</details>


### [347] [Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels](https://arxiv.org/abs/2601.21268)
*Micah Rentschler,Jesse Roberts*

Main category: cs.NE

TL;DR: 介绍了一种名为RLME的强化学习方法，无需标签训练大语言模型，表现出色且能拓宽训练领域。


<details>
  <summary>Details</summary>
Motivation: 多数用于训练大语言模型的强化学习方法需真实标签或特定验证器，在正确性模糊或获取成本高时可扩展性受限。

Method: 引入RLME，利用评估器对自然语言元问题的回答得出奖励来优化生成器，将评估器做出积极判断的概率作为奖励，通过群体相对策略优化更新生成器。

Result: RLME在准确性和样本效率上与基于标签的训练相当，能在多目标间进行可控权衡，引导模型采用可靠推理模式，可泛化到无真实标签的开放领域。

Conclusion: RLME拓宽了大语言模型可使用强化学习训练的领域。

Abstract: Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., "Is the answer correct?" or "Is the reasoning logically consistent?"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.

</details>


### [348] [NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training](https://arxiv.org/abs/2601.21279)
*Zhengzheng Tang*

Main category: cs.NE

TL;DR: 提出NEXUS框架实现ANN到SNN的位精确等价，在实验中任务准确率无下降且有显著节能效果，还对膜电位泄漏和突触噪声有良好耐受性。


<details>
  <summary>Details</summary>
Motivation: 现有SNN方法在将连续值近似为离散脉冲时牺牲了准确性，需要新方法实现ANN到SNN的精确转换。

Method: 构建基于纯IF神经元逻辑门的算术运算，采用空间位编码、分层神经形态门电路和无替代STE训练。

Result: 在高达LLaMA - 2 70B的模型上任务准确率无下降，平均ULP误差仅6.19，在神经形态硬件上节能27 - 168,000倍；对膜电位泄漏免疫，能容忍一定程度突触噪声。

Conclusion: NEXUS框架能实现ANN到SNN的位精确等价，有良好的性能和鲁棒性。

Abstract: Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete transformer layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine precision. Experiments on models up to LLaMA-2 70B demonstrate identical task accuracy (0.00\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000$\times$ energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the framework inherently immune to membrane potential leakage (100\% accuracy across all decay factors $β\in[0.1,1.0]$), while tolerating synaptic noise up to $σ=0.2$ with >98\% gate-level accuracy.

</details>


### [349] [BrainFuse: a unified infrastructure integrating realistic biological modeling and core AI methodology](https://arxiv.org/abs/2601.21407)
*Baiyu Chen,Yujie Wu,Siyuan Xu,Peng Qu,Dehua Wu,Xu Chu,Haodong Bian,Shuo Zhang,Bo Xu,Youhui Zhang,Zhengyu Ma,Guoqi Li*

Main category: cs.NE

TL;DR: 本文提出BrainFuse统一基础设施，支持生物物理神经模拟和基于梯度的学习，展示多种能力，可用于跨学科研究和下一代生物启发智能系统开发。


<details>
  <summary>Details</summary>
Motivation: 现代AI框架缺乏对生物物理现实的原生支持，神经模拟工具不适合基于梯度的优化和神经形态硬件部署，阻碍了神经科学和人工智能的转化协同，需缩小这一差距。

Method: 引入BrainFuse统一基础设施，解决算法、计算和部署方面的挑战。

Result: BrainFuse具有将神经元动力学集成进可微学习框架、加速离子通道动力学、支持神经形态硬件部署等能力，在神经科学和AI任务中均有良好表现。

Conclusion: BrainFuse可促进跨学科研究，加速下一代生物启发智能系统的发展。

Abstract: Neuroscience and artificial intelligence represent distinct yet complementary pathways to general intelligence. However, amid the ongoing boom in AI research and applications, the translational synergy between these two fields has grown increasingly elusive-hampered by a widening infrastructural incompatibility: modern AI frameworks lack native support for biophysical realism, while neural simulation tools are poorly suited for gradient-based optimization and neuromorphic hardware deployment. To bridge this gap, we introduce BrainFuse, a unified infrastructure that provides comprehensive support for biophysical neural simulation and gradient-based learning. By addressing algorithmic, computational, and deployment challenges, BrainFuse exhibits three core capabilities: (1) algorithmic integration of detailed neuronal dynamics into a differentiable learning framework; (2) system-level optimization that accelerates customizable ion-channel dynamics by up to 3,000x on GPUs; and (3) scalable computation with highly compatible pipelines for neuromorphic hardware deployment. We demonstrate this full-stack design through both AI and neuroscience tasks, from foundational neuron simulation and functional cylinder modeling to real-world deployment and application scenarios. For neuroscience, BrainFuse supports multiscale biological modeling, enabling the deployment of approximately 38,000 Hodgkin-Huxley neurons with 100 million synapses on a single neuromorphic chip while consuming as low as 1.98 W. For AI, BrainFuse facilitates the synergistic application of realistic biological neuron models, demonstrating enhanced robustness to input noise and improved temporal processing endowed by complex HH dynamics. BrainFuse therefore serves as a foundational engine to facilitate cross-disciplinary research and accelerate the development of next-generation bio-inspired intelligent systems.

</details>


### [350] [Task-free Adaptive Meta Black-box Optimization](https://arxiv.org/abs/2601.21475)
*Chao Wang,Licheng Jiao,Lingling Li,Jiaxuan Zhao,Guanchun Wang,Fang Liu,Shuyuan Yang*

Main category: cs.NE

TL;DR: 提出自适应元黑盒优化模型ABOM，无需预定义任务分布和手工训练任务，可实现零样本优化，在合成基准和实际问题中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有MetaBBO方法需大量手工训练任务学习元策略，对未知任务分布的实际应用有局限。

Method: 提出ABOM，采用仅基于目标任务优化数据的在线参数自适应，引入闭环自适应参数学习机制。

Result: ABOM在合成BBO基准和无人机路径规划问题上取得有竞争力的性能，可视化显示参数化进化算子有显著搜索模式。

Conclusion: ABOM无需手工训练任务，能解决现有MetaBBO方法在未知任务分布下的局限性问题。

Abstract: Handcrafted optimizers become prohibitively inefficient for complex black-box optimization (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to target tasks, which poses a critical limitation for realistic applications with unknown task distributions. To overcome the issue, we propose the Adaptive meta Black-box Optimization Model (ABOM), which performs online parameter adaptation using solely optimization data from the target task, obviating the need for predefined task distributions. Unlike conventional metaBBO frameworks that decouple meta-training and optimization phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during optimization. This paradigm shift enables zero-shot optimization: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks. Visualization studies reveal that parameterized evolutionary operators exhibit statistically significant search patterns, including natural selection and genetic recombination.

</details>


### [351] [Error Amplification Limits ANN-to-SNN Conversion in Continuous Control](https://arxiv.org/abs/2601.21778)
*Zijie Xu,Zihan Huang,Yiting Dong,Kang Chen,Wenxuan Liu,Zhaofei Yu*

Main category: cs.NE

TL;DR: 本文指出ANN转SNN在连续控制中表现不佳的原因是误差放大，提出无训练机制CRPI抑制误差，实验表明其能集成到现有转换流程并恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有ANN转SNN方法在连续控制中表现差，缺乏合适基线，需解决误差放大导致的性能下降问题。

Method: 提出Cross - Step Residual Potential Initialization (CRPI)，一种跨决策步骤传递剩余膜电位以抑制时间相关误差的无训练机制。

Result: 在连续控制基准测试上的实验表明，CRPI可集成到现有转换流程中，大幅恢复损失的性能。

Conclusion: 连续控制是ANN转SNN的关键且具有挑战性的基准，小误差会强烈放大并影响性能。

Abstract: Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.

</details>


### [352] [General Self-Prediction Enhancement for Spiking Neurons](https://arxiv.org/abs/2601.21823)
*Zihan Huang,Zijie Xu,Yihan Huang,Shanshan Jia,Tong Bu,Yiting Dong,Wenxuan Liu,Jianhao Ding,Zhaofei Yu,Tiejun Huang*

Main category: cs.NE

TL;DR: 提出自预测增强脉冲神经元方法提升SNN性能，实验显示有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: SNN训练受脉冲不可微性等挑战，且主流SNN忽略预测编码这一核心皮层机制。

Method: 提出自预测增强脉冲神经元方法，从输入 - 输出历史生成内部预测电流来调节膜电位。

Result: 在不同架构、神经元类型、时间步长和任务中都有一致的性能提升。

Conclusion: 所提方法具有广泛适用性，可用于增强SNN。

Abstract: Spiking Neural Networks (SNNs) are highly energy-efficient due to event-driven, sparse computation, but their training is challenged by spike non-differentiability and trade-offs among performance, efficiency, and biological plausibility. Crucially, mainstream SNNs ignore predictive coding, a core cortical mechanism where the brain predicts inputs and encodes errors for efficient perception. Inspired by this, we propose a self-prediction enhanced spiking neuron method that generates an internal prediction current from its input-output history to modulate membrane potential. This design offers dual advantages, it creates a continuous gradient path that alleviates vanishing gradients and boosts training stability and accuracy, while also aligning with biological principles, which resembles distal dendritic modulation and error-driven synaptic plasticity. Experiments show consistent performance gains across diverse architectures, neuron types, time steps, and tasks demonstrating broad applicability for enhancing SNNs.

</details>


### [353] [Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model](https://arxiv.org/abs/2601.21877)
*Chen Wang,Sijie Ma,Zeyuan Ma,Yue-Jiao Gong*

Main category: cs.NE

TL;DR: 提出基于大语言模型的自动化黑盒优化基准设计方法EoB，多维度实验证明其竞争力。


<details>
  <summary>Details</summary>
Motivation: 早期黑盒优化基准多为人工设计，存在专家偏差和多样性受限问题，自动化设计可减轻人工负担并提高多样性和客观性。

Method: 将基准设计表述为双目标优化问题，EoB迭代提示大语言模型进化基准程序，采用基于反思的方案协同进化景观和程序。

Result: 综合实验验证EoB在多种维度应用中具有竞争力。

Conclusion: EoB是黑盒优化基准设计的一个有竞争力的候选方案。

Abstract: Benchmark Design in Black-Box Optimization (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an automated BBO benchmark designer empowered by the large language model (LLM) and its program evolution capability. Specifically, we formulate benchmark design as a bi-objective optimization problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. Comprehensive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.

</details>


### [354] [Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems](https://arxiv.org/abs/2601.21885)
*Tiwonge Msulira Banda,Alexandru-Ciprian Zăvoianu*

Main category: cs.NE

TL;DR: 本文提出自适应代理建模方法加速多目标进化算法早期收敛速度，经测试能显著提速。


<details>
  <summary>Details</summary>
Motivation: 现有多目标进化算法用于计算密集型工业问题时性能受限，需加速早期收敛速度以节省时间和计算资源。

Method: 采用双循环架构，外循环运行宿主多目标进化算法进行真实适应度评估，内循环的自适应加速器利用机器学习代理模型近似适应度函数。

Result: 结合高斯过程回归、一维卷积神经网络和随机森林回归，该方法在31个基准问题和一个实际案例中显著加速多目标进化算法早期收敛速度。

Conclusion: 所提自适应代理建模方法能有效加速先进多目标进化算法早期收敛。

Abstract: Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of state-of-the-art MOEAs. This is important because it ensures that a solver can identify optimal or near-optimal solutions with relatively few fitness function evaluations, thereby saving both time and computational resources. Our method employs a two-loop architecture. The outer loop runs a (baseline) host MOEA which carries out true fitness evaluations. The inner loop contains an Adaptive Accelerator that leverages data-driven machine learning (ML) surrogate models to approximate fitness functions. Integrated with NSGA-II and MOEA/D, our approach was tested on 31 widely known benchmark problems and a real-world North Sea fish abundance modelling case study. The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation.

</details>


### [355] [Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice](https://arxiv.org/abs/2601.22075)
*Kirill Antonov,Teus Tukker,Tiago Botari,Thomas H. W. Bäck,Anna V. Kononova,Niki van Stein*

Main category: cs.NE

TL;DR: 提出LDG - EA算法用于多模态镜头优化，能在合理时间内生成多样解且质量有竞争力。


<details>
  <summary>Details</summary>
Motivation: 标准优化器常收敛到单一局部最优，忽略对下游工程决策重要的其他优质方案。

Method: LDG - EA分两阶段，先按曲率符号模式和材料指数划分设计空间，学习概率模型分配评估，在各描述符内用带协方差矩阵自适应的Hill - Valley进化算法恢复多个局部极小值，可选梯度细化。

Result: 在24变量、六元素双高斯拓扑上，平均生成约14500个候选极小值，涵盖636个唯一描述符，比CMA - ES基线多一个数量级，用时约一小时。

Conclusion: LDG - EA能在实际计算预算和时间内生成多样解，且质量有竞争力。

Abstract: Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [356] [A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy](https://arxiv.org/abs/2601.20879)
*Bruno Claudino Matias,Savio Freire,Juliana Freitas,Felipe Fronchetti,Kostadin Damevski,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 研究系统考察大语言模型（LLMs）对软件可维护性和可演化性的影响，发现其有积极作用但也带来风险，需谨慎采用。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLMs在软件工程中影响的证据零散，其对软件长期可维护性和可演化性的影响尚不明确，因此要系统研究。

Method: 进行系统文献综述，在多个数据库搜索2020 - 2024年的研究，通过多研究者校准过程提取定性证据，用混合主题方法分析相关内容并结合LLM辅助分析工具和人工验证。

Result: LLMs有提升可分析性、可测试性等好处，但也带来输出错误、性能不稳定等风险，威胁长期可演化性。

Conclusion: LLMs能增强可维护性和可演化性，但对长期可持续性有风险，需保障措施、严格评估和人工监督。

Abstract: Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.

</details>


### [357] [DevOps-Gym: Benchmarking AI Agents in Software DevOps Cycle](https://arxiv.org/abs/2601.20882)
*Yuheng Tang,Kaijie Zhu,Bonan Ruan,Chuqi Zhang,Michael Yang,Hongwei Li,Suyue Guo,Tianneng Shi,Zekun Li,Christopher Kruegel,Giovanni Vigna,Dawn Song,William Yang Wang,Lun Wang,Yangruibo Ding,Zhenkai Liang,Wenbo Guo*

Main category: cs.SE

TL;DR: 引入DevOps - Gym评估AI在DevOps周期能力，评估显示模型有局限，表明需相关研究


<details>
  <summary>Details</summary>
Motivation: 现有基准聚焦孤立问题，缺乏DevOps环境和工具接口，AI在完整软件DevOps周期能力未知

Method: 引入DevOps - Gym基准，包含700 + 真实任务，采用半自动化数据收集机制

Result: 评估显示现有模型和代理在Java和Go的问题解决、测试生成有困难，无法处理新任务

Conclusion: 凸显用AI代理自动化完整DevOps周期需开展重要研究

Abstract: Even though demonstrating extraordinary capabilities in code generation and software issue resolving, AI agents' capabilities in the full software DevOps cycle are still unknown. Different from pure code generation, handling the DevOps cycle in real-world software, including developing, deploying, and managing, requires analyzing large-scale projects, understanding dynamic program behaviors, leveraging domain-specific tools, and making sequential decisions. However, existing benchmarks focus on isolated problems and lack environments and tool interfaces for DevOps. We introduce DevOps-Gym, the first end-to-end benchmark for evaluating AI agents across core DevOps workflows: build and configuration, monitoring, issue resolving, and test generation. DevOps-Gym includes 700+ real-world tasks collected from 30+ projects in Java and Go. We develop a semi-automated data collection mechanism with rigorous and non-trivial expert efforts in ensuring the task coverage and quality. Our evaluation of state-of-the-art models and agents reveals fundamental limitations: they struggle with issue resolving and test generation in Java and Go, and remain unable to handle new tasks such as monitoring and build and configuration. These results highlight the need for essential research in automating the full DevOps cycle with AI agents.

</details>


### [358] [IDE-Bench: Evaluating Large Language Models as IDE Agents on Real-World Software Engineering Tasks](https://arxiv.org/abs/2601.20886)
*Spencer Mateega,Jeff Yang,Tiana Costello,Shaurya Jadhav,Nicole Tian,Agustin Garcinuño*

Main category: cs.SE

TL;DR: 提出IDE - Bench框架用于评估AI IDE代理，含Docker化测试工具，创建80个任务，是首个在多语言全栈环境未污染代码上关联代理意图与项目修改的基准。


<details>
  <summary>Details</summary>
Motivation: 需要一个全面框架来评估AI IDE代理在现实软件工程任务中的表现。

Method: 构建IDE - Bench框架，采用Dockerized测试工具，创建80个跨多种语言和技术栈的任务。

Result: 实现了对AI IDE代理在多个软件工程任务中的评估，能关联代理意图与项目修改。

Conclusion: 该基准可有效评估AI IDE代理在多语言、全栈环境的能力。

Abstract: IDE-Bench is a comprehensive framework for evaluating AI IDE agents on real-world software engineering tasks through an IDE-native tool interface. We present a Dockerized test harness that goes beyond raw terminal execution, granting models a structured tool ecosystem that represents AI-native IDEs like Cursor and Windsurf. By providing high-level abstractions for codebase search, structured file editing, and tools for testing full-stack applications, IDE-Bench evaluates an agent's ability to act as a true engineering collaborator. For evaluation and to prevent training data contamination, we created 80 tasks across eight never-published repositories spanning C/C++, Java, and MERN stacks, representing modern tech stack production scenarios, including feature implementation, bug fixing, refactoring, and performance optimization that mirror daily developer workflows in private codebases. Our benchmark is the first to systematically correlate agent-reported intent with successful project-level modifications in a multi-language, full-stack environment on completely uncontaminated code.

</details>


### [359] [Another Systematic Review? A Critical Analysis of Systematic Literature Reviews on Agile Effort and Cost Estimation](https://arxiv.org/abs/2601.20893)
*Henry Edison,Nauman Ali*

Main category: cs.SE

TL;DR: 论文针对软件工程研究中系统文献综述（SLR）重复问题，以敏捷软件开发工作量估计为案例，分析18篇SLR找常见理由模式，提出减少重复的建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中SLR常重复进行，研究者未充分检查已有SLR，设计和实施SLR很耗费精力，此情况不理想，故要了解作者开展额外SLR的理由。

Method: 选择敏捷软件开发中的工作量估计这一狭窄但研究充分的主题，对18篇已发表的SLR进行定性内容分析，在解读结果时考虑引用数据、出版年份、出版场所和SLR质量。

Result: 常见的理由模式包括作者声称存在覆盖范围差距、先前研究有方法局限性、先前SLR过时、技术和方法进步需要更新综合。

Conclusion: 对特定狭窄主题的SLR深入分析为软件工程的SLR提供了总体见解，强调识别现有SLR和开展新SLR的理由，可减少精力重复，提高该领域进展速度。

Abstract: Background: Systematic literature reviews (SLRs) have become prevalent in software engineering research. Several researchers may conduct SLRs on similar topics without a prospective register for SLR protocols. However, even ignoring these unavoidable duplications of effort in the simultaneous conduct of SLRs, the proliferation of overlapping and often repetitive SLRs indicates that researchers are not extensively checking for existing SLRs on a topic. Given how effort-intensive it is to design, conduct, and report an SLR, the situation is less than ideal for software engineering research. Aim: To understand how authors justify additional SLRs on a topic. Method: To illustrate the issue and develop suggestions for improvement to address this issue, we have intentionally picked a sufficiently narrow but well-researched topic, i.e., effort estimation in Agile software development. We identify common justification patterns through a qualitative content analysis of 18 published SLRs. We further consider the citation data, publication years, publication venues, and the quality of the SLRs when interpreting the results. Results: The common justification patterns include authors claiming gaps in coverage, methodological limitations in prior studies, temporal obsolescence of previous SLRs, or rapid technological and methodological advancements necessitating updated syntheses. Conclusion: Our in-depth analysis of SLRs on a fairly narrow topic provides insights into SLRs in software engineering in general. By emphasizing the need for identifying existing SLRs and for justifying the undertaking of further SLRs, both in design and review guidelines and as a policy of conferences and journals, we can reduce the likelihood of duplication of effort and increase the rate of progress in the field.

</details>


### [360] [Leveraging Generative AI for Enhancing Domain-Driven Software Design](https://arxiv.org/abs/2601.20909)
*Götz-Henrik Wiegand,Filip Stepniak,Patrick Baier*

Main category: cs.SE

TL;DR: 本文探讨使用生成式AI部分自动化领域驱动设计（DDD）中的元模型生成，特别是生成特定领域JSON对象，展示了其可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 传统上DDD领域的元模型由系统设计师手动创建，本文旨在探索使用生成式AI部分自动化元模型生成，以简化设计流程。

Method: 在真实世界DDD项目数据上训练模型，使用4位量化版本的Code Llama和低秩适配（LoRA）在消费级GPU上微调模型。

Result: 模型在有限硬件条件下实现高性能，能根据简单提示生成语法正确的JSON对象，且只需少量后处理。

Conclusion: 将生成式AI融入DDD过程是可行的，可提高效率、减少资源需求，为AI驱动的软件开发进一步发展奠定基础。

Abstract: Domain-Driven Design (DDD) is a key framework for developing customer-oriented software, focusing on the precise modeling of an application's domain. Traditionally, metamodels that describe these domains are created manually by system designers, forming the basis for iterative software development. This paper explores the partial automation of metamodel generation using generative AI, particularly for producing domain-specific JSON objects. By training a model on real-world DDD project data, we demonstrate that generative AI can produce syntactically correct JSON objects based on simple prompts, offering significant potential for streamlining the design process. To address resource constraints, the AI model was fine-tuned on a consumer-grade GPU using a 4-bit quantized version of Code Llama and Low-Rank Adaptation (LoRA). Despite limited hardware, the model achieved high performance, generating accurate JSON objects with minimal post-processing. This research illustrates the viability of incorporating generative AI into the DDD process, improving efficiency and reducing resource requirements, while also laying the groundwork for further advancements in AI-driven software development.

</details>


### [361] [Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review](https://arxiv.org/abs/2601.20918)
*Mohammad Naserameri,Juergen Rilling*

Main category: cs.SE

TL;DR: 通过系统文献综述研究区块链增强AI驱动软件工程工具和流程的信任度，发现多数研究关注AI集成，少数明确提及信任，区块链可增强信任但存在定义不一致和缺乏实际测试问题，未来需开发可衡量的信任框架。


<details>
  <summary>Details</summary>
Motivation: 探索区块链和AI在软件工程中增强信任度，特别是支持软件演化任务。

Method: 采用预定义协议和明确纳入标准进行系统文献综述，综合相关研究。

Result: 多数研究关注AI在软件工程中的集成，仅31%明确提及信任度；有六项研究探索区块链增强AI辅助软件工程任务的可靠性等。

Conclusion: 区块链可通过保证数据不可变等增强信任，但存在信任定义不一致和缺乏实际测试问题，未来需开发可衡量的信任框架。

Abstract: Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models.

</details>


### [362] [Operationalizing Research Software for Supply Chain Security](https://arxiv.org/abs/2601.20980)
*Kelechi G. Kalu,Soham Rattan,Taylor R. Schorlemmer,George K. Thiruvathukal,Jeffrey C. Carver,James C. Davis*

Main category: cs.SE

TL;DR: 因‘研究软件’定义不一致使研究难对比，本文引入面向研究软件供应链（RSSC）的分类法用于安全研究，还进行综述、合成分类法、应用于语料并做初步安全分析。


<details>
  <summary>Details</summary>
Motivation: 文献对‘研究软件’的操作化定义不一致，导致实证研究难以比较，同时考虑RSSC及其安全风险。

Method: 对近期仓库挖掘和数据集构建研究进行针对性范围审查，合成协调分类法和映射，在RSE语料上实施分类法，应用OpenSSF Scorecard进行初步安全分析。

Result: 生成注释数据集、标签代码手册和可重复的标签管道，展示不同分类群的安全信号差异。

Conclusion: 基于分类法的分层对于解释RSSC安全测量是必要的。

Abstract: Empirical studies of research software are hard to compare because the literature operationalizes ``research software'' inconsistently. Motivated by the research software supply chain (RSSC) and its security risks, we introduce an RSSC-oriented taxonomy that makes scope and operational boundaries explicit for empirical research software security studies.
  We conduct a targeted scoping review of recent repository mining and dataset construction studies, extracting each work's definition, inclusion criteria, unit of analysis, and identification heuristics. We synthesize these into a harmonized taxonomy and a mapping that translates prior approaches into shared taxonomy dimensions. We operationalize the taxonomy on a large community-curated corpus from the Research Software Encyclopedia (RSE), producing an annotated dataset, a labeling codebook, and a reproducible labeling pipeline. Finally, we apply OpenSSF Scorecard as a preliminary security analysis to show how repository-centric security signals differ across taxonomy-defined clusters and why taxonomy-aware stratification is necessary for interpreting RSSC security measurements.

</details>


### [363] [Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering](https://arxiv.org/abs/2601.21070)
*Daniel Rodriguez-Cardenas,Xiaochang Li,Marcos Macedo,Antonio Mastropaolo,Dipin Khati,Yuan Tian,Huajie Shao,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 当前代码大模型评估能力落后，本文调研现有基准并结合研讨会洞见，指出可靠评估的三个核心障碍，推出BEHELM基准设施来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型评估基准存在关注狭窄、数据工程实践不一致等问题，需要理解问题并寻找解决方案。

Method: 对现有基准进行深入调研，并结合社区研讨会的见解。

Result: 确定了可靠评估的三个核心障碍，推出了BEHELM基准设施。

Conclusion: BEHELM可减少构建基准的开销，实现软件工程中对大语言模型公平、现实和面向未来的评估。

Abstract: Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.

</details>


### [364] [The Quiet Contributions: Insights into AI-Generated Silent Pull Requests](https://arxiv.org/abs/2601.21102)
*S M Mahedy Hasan,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: 对无评论的AI生成拉取请求（SPRs）开展首项实证研究，分析其对代码的影响以探寻接受或拒绝的理由。


<details>
  <summary>Details</summary>
Motivation: 无评论或讨论的SPRs在理解其接受或拒绝理由方面存在独特挑战，需要进行研究。

Method: 对AIDev公共数据集中来自五个AI代理提交给热门Python仓库的4762个SPRs进行定量研究。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: We present the first empirical study of AI-generated pull requests that are 'silent,' meaning no comments or discussions accompany them. This absence of any comments or discussions associated with such silent AI pull requests (SPRs) poses a unique challenge in understanding the rationale for their acceptance or rejection. Hence, we quantitatively study 4,762 SPRs of five AI agents made to popular Python repositories drawn from the AIDev public dataset. We examine SPRs impact on code complexity, other quality issues, and security vulnerabilities, especially to determine whether these insights can hint at the rationale for acceptance or rejection of SPRs.

</details>


### [365] [AI-Assisted Engineering Should Track the Epistemic Status and Temporal Validity of Architectural Decisions](https://arxiv.org/abs/2601.21116)
*Sankalp Gilda,Shlok Gilda*

Main category: cs.SE

TL;DR: 本文指出AI辅助软件工程需追踪架构决策的认知状态和时间有效性的机制，提出相关要求并形成FPF框架，回顾性审计验证了时间问责的必要性，还给出研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM编码助手生成决策速度超团队验证速度，且缺乏区分猜想与验证知识、防止信任膨胀和检测证据过期的框架。

Method: 提出负责任AI辅助工程的三个要求，将其形式化为第一原则框架（FPF），基于模糊逻辑确定聚合语义，定义有效聚合算子需满足的不变量。

Result: 对两个内部项目的回顾性审计发现，20 - 25%的架构决策在两个月内存在证据过时情况。

Conclusion: 验证了时间问责的必要性，同时给出了可学习聚合算子、联合证据共享和基于SMT的声明验证等研究方向。

Abstract: This position paper argues that AI-assisted software engineering requires explicit mechanisms for tracking the epistemic status and temporal validity of architectural decisions. LLM coding assistants generate decisions faster than teams can validate them, yet no widely-adopted framework distinguishes conjecture from verified knowledge, prevents trust inflation through conservative aggregation, or detects when evidence expires. We propose three requirements for responsible AI-assisted engineering: (1) epistemic layers that separate unverified hypotheses from empirically validated claims, (2) conservative assurance aggregation grounded in the Gödel t-norm that prevents weak evidence from inflating confidence, and (3) automated evidence decay tracking that surfaces stale assumptions before they cause failures. We formalize these requirements as the First Principles Framework (FPF), ground its aggregation semantics in fuzzy logic, and define a quintet of invariants that any valid aggregation operator must satisfy. Our retrospective audit applying FPF criteria to two internal projects found that 20-25% of architectural decisions had stale evidence within two months, validating the need for temporal accountability. We outline research directions including learnable aggregation operators, federated evidence sharing, and SMT-based claim validation.

</details>


### [366] [From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem](https://arxiv.org/abs/2601.21186)
*TianYi Tang,Saba Alimadadi,Nick Sumner*

Main category: cs.SE

TL;DR: 本文对真实世界的TypeScript项目中的错误进行大规模实证研究，揭示故障类型及与项目特征的关联，还对比了JavaScript研究。


<details>
  <summary>Details</summary>
Motivation: TypeScript在现代Web开发中流行，但对其对软件故障的影响了解不足，开展研究以填补空白。

Method: 分析16个流行开源仓库的633个错误报告，构建故障类型分类法，量化流行程度并关联项目特征。

Result: 故障主要是工具和配置故障、API滥用和异步错误处理问题，与构建复杂性和依赖异质性强相关；静态类型减少传统运行时和类型错误，但使脆弱性转向构建系统和工具链。

Conclusion: 研究为语言设计和生态系统演变如何重塑大规模软件系统的故障特征提供新见解。

Abstract: TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.

</details>


### [367] [Human-Agent versus Human Pull Requests: A Testing-Focused Characterization and Comparison](https://arxiv.org/abs/2601.21194)
*Roberto Milanese,Francesco Salzano,Angelica Spina,Antonio Vitale,Remo Pareschi,Fausto Fasano,Mattia Fazzini*

Main category: cs.SE

TL;DR: 本文对AIDev数据集中的人类 - 代理PR和人类PR进行实证研究，对比两者在测试方面差异，发现人类 - 代理PR测试范围更广等特征。


<details>
  <summary>Details</summary>
Motivation: 当前人们对基于AI的编码代理与人协作在软件测试中的作用了解不足，需要研究人类 - 代理协作如何影响软件测试实践。

Method: 对AIDev数据集中6,582个人类 - 代理PR和3,122个人类PR，从测试频率和范围、测试相关更改类型、测试质量三个维度进行比较。

Result: 人类 - 代理PR和人类PR包含测试的可能性相近，但前者测试范围更大；代理PR在协同进化时更易添加新测试，人类PR更倾向修改现有测试；部分测试异味类别有统计差异但质量无实质差异。

Conclusion: 研究首次描述了人类 - 代理协作如何塑造测试实践，为理解其在软件测试中的作用提供了见解。

Abstract: AI-based coding agents are increasingly integrated into software development workflows, collaborating with developers to create pull requests (PRs). Despite their growing adoption, the role of human-agent collaboration in software testing remains poorly understood. This paper presents an empirical study of 6,582 human-agent PRs (HAPRs) and 3,122 human PRs (HPRs) from the AIDev dataset. We compare HAPRs and HPRs along three dimensions: (i) testing frequency and extent, (ii) types of testing-related changes (code-and-test co-evolution vs. test-focused), and (iii) testing quality, measured by test smells. Our findings reveal that, although the likelihood of including tests is comparable (42.9% for HAPRs vs. 40.0% for HPRs), HAPRs exhibit a larger extent of testing, nearly doubling the test-to-source line ratio found in HPRs. While test-focused task distributions are comparable, HAPRs are more likely to add new tests during co-evolution (OR=1.79), whereas HPRs prioritize modifying existing tests. Finally, although some test smell categories differ statistically, negligible effect sizes suggest no meaningful differences in quality. These insights provide the first characterization of how human-agent collaboration shapes testing practices.

</details>


### [368] [CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation](https://arxiv.org/abs/2601.21253)
*Wei Minn,Biniam Fisseha Demissie,Yan Naing Tun,Jiakun Liu,Mariano Ceccato,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: 现有安卓应用UI测试技术覆盖率低，提出CovAgent框架提升覆盖率并超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有安卓应用UI测试技术覆盖有限，需提出新方法克服局限。

Method: 提出CovAgent框架，含AI代理检查代码和组件转换图，推理激活条件，另一代理生成满足条件的动态脚本。

Result: 增强现有模糊测试方法，活动覆盖率显著提升，在其他指标上也超基线方法。

Conclusion: CovAgent框架有效提升安卓应用UI测试覆盖，对自动化应用测试有积极意义。

Abstract: Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.

</details>


### [369] [The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations](https://arxiv.org/abs/2601.21259)
*Sayma Sultana,London Cavaletto,Bianca Trinkenreich,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 研究运用社会认同理论调查软件工程师职场四种偏见的影响，发现职业发展和任务选择偏见最普遍，不同群体受影响程度不同，且多因素会影响偏见受害情况。


<details>
  <summary>Details</summary>
Motivation: 现有研究对系统性职场偏见对软件工程师的影响了解不足，本研究旨在填补这一空白。

Method: 运用社会认同理论，通过基于小插曲的调查，量化偏见的普遍性、确定受影响人群、评估后果并探究偏见行为的动机。

Result: 职业发展和任务选择偏见最普遍，超三分之二受害者多次经历；女性比男性更易遭遇前两者和不友好环境；少数族裔背景者更易遭受身份攻击。除性别和种族外，年龄、经验、组织规模和地理位置也是偏见受害的重要预测因素。

Conclusion: 明确了软件工程师职场中不同类型偏见的情况及影响因素。

Abstract: While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.

</details>


### [370] [More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests](https://arxiv.org/abs/2601.21276)
*Haoming Huang,Pongchai Jaisri,Shota Shimizu,Lingfeng Chen,Sota Nakashima,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 研究评估大语言模型代理在代码生成中对拉取请求的影响，发现其代码冗余高但评审者态度较积极，为改善人机协作提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有指标仅衡量通过率，无法反映对长期可维护性和可读性的影响，不能体现人类对拉取请求的直观评价，需增加评估的全面性。

Method: 基于代码指标观察拉取请求内的代码质量和可维护性，分析开发者对人类和大语言模型生成的拉取请求的反应。

Result: 大语言模型代理常忽视代码复用机会，冗余度高于人类开发者；评审者对人工智能生成的贡献表达更中立或积极的情绪。

Conclusion: 人工智能代码表面合理性掩盖了冗余问题，会在实际开发中悄然积累技术债务，研究为改善人机协作提供了见解。

Abstract: Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.

</details>


### [371] [Detecting Multiple Semantic Concerns in Tangled Code Commits](https://arxiv.org/abs/2601.21298)
*Beomsu Koh,Neil Walkinshaw,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文将复杂提交中的多关注点检测构建为多标签分类问题，用小型语言模型进行实证研究，发现微调后的14B参数小型语言模型在单关注点提交上可与大模型竞争，包含提交消息能提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究未解决复杂提交的多关注点检测问题，需探究使用语言模型进行多关注点检测的可行性。

Method: 将多关注点检测构建为多标签分类问题，构建人工复杂提交的受控数据集，用小型语言模型开展实证研究，考察微调、关注点数量等因素的影响。

Result: 微调后的14B参数小型语言模型在单关注点提交上可与最先进大模型竞争，处理多达三个关注点仍可用；包含提交消息可使检测准确率提升达44%，且延迟开销可忽略不计。

Conclusion: 小型语言模型在复杂提交的多关注点检测中有竞争力，提交消息是重要语义线索。

Abstract: Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.

</details>


### [372] [Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools](https://arxiv.org/abs/2601.21305)
*Mark Looi,Julianne Quinn*

Main category: cs.SE

TL;DR: 研究专业开发者对生成式AI工具的使用情况，发现使用频率和广度与感知生产力和质量正相关，识别开发者类型，揭示组织采用过程及政策作用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速进入软件开发领域，需研究其对实践的感知影响。

Method: 研究147名专业开发者的使用模式，考察AI工具使用的感知关联因素、生产力和质量结果以及开发者对新兴AI增强开发的准备情况。

Result: 发现频繁广泛使用AI工具与感知生产力和质量正相关，无质量悖论证据，确定开发者类型，揭示组织采用过程和政策作用。

Conclusion: 组织采用AI工具遵循创新扩散过程，政策是成熟标志，谨慎者需早期采用者示例才能进入良性循环。

Abstract: The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.

</details>


### [373] [Predicting Developer Acceptance of AI-Generated Code Suggestions](https://arxiv.org/abs/2601.21379)
*Jing Jiang,Liehao Li,Jinyun Hou,Xin Tan,Li Zhang*

Main category: cs.SE

TL;DR: 本文以大型科技公司的66329条开发者与AI交互数据开展实证研究，分析代码建议接受与拒绝情况特点，引入CSAP模型预测接受度，准确率高，表明针对性个性化是过滤建议的有效方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对开发者接受AI生成代码建议的定量分析，本文旨在填补该空白。

Method: 使用66329条工业开发者与AI的交互数据进行实证研究，分析接受与拒绝代码建议的特征差异，并引入CSAP模型进行预测。

Result: CSAP模型在不平衡和平衡数据集上的准确率分别达到0.973和0.922，相对基线和生产过滤器有显著提升。

Conclusion: 针对性个性化是过滤可能被拒绝的代码建议、减少开发者干扰的有效方法，本研究是大规模工业数据上的首次定量研究，为AI辅助编程提供重要研究方向。

Abstract: AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\% and 69.5\% on imbalanced dataset, and improves the accuracy by 87.0\% and 140.1\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.

</details>


### [374] [Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation](https://arxiv.org/abs/2601.21469)
*Haoji Zhang,Yuzhe Li,Zhenqiang Liu,Chenyang Liu,Shenyang Zhang,Yi Zhou*

Main category: cs.SE

TL;DR: 提出DebateCoder框架提升小语言模型推理能力，实验显示效果良好


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型在处理复杂逻辑需求时的推理瓶颈和失败循环问题

Method: 采用含三个代理的结构化角色扮演协议、自适应置信门控机制、多轮审议模块和评审员引导的分析调试循环

Result: 在HumanEval和MBPP实验中，DebateCoder在HumanEval上Pass@1达70.12%，优于MapCoder，减少约35% API开销

Conclusion: 协作协议可缓解小参数模型的局限性，为高质量自动化软件工程提供可扩展、高效的方法

Abstract: While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.

</details>


### [375] [Chasing Elusive Memory Bugs in GPU Programs](https://arxiv.org/abs/2601.21552)
*Anubhab Ghosh,Ajay Nayak,Dhananjay Rao Thallikar Shyam,Arkaprava Basu*

Main category: cs.SE

TL;DR: 论文指出GPU程序中输入依赖和内存块内的越界访问问题，现有工具难以检测，提出编译时技术SCuBA检测越界，效果好。


<details>
  <summary>Details</summary>
Motivation: 现有GPU程序越界检测工具依赖运行时技术，无法检测输入依赖的越界访问，且忽略内存块内越界访问，需新检测方法。

Method: 提出编译时技术SCuBA，分析CPU和GPU代码捕捉语义关系，用SAT求解器检查越界可能，分析GPU代码检测内存块内越界。

Result: 相比NVIDIA的Compute Sanitizer，SCuBA在20个程序中检测到所有越界，无漏检和误报。

Conclusion: SCuBA能有效检测GPU程序中现有工具难以发现的越界访问问题。

Abstract: Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs.
  We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.

</details>


### [376] [Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction](https://arxiv.org/abs/2601.21565)
*Adriana Novoa-Hurtado,Rubén Saborido,Francisco Chicano,Manuel Giménez-Medina*

Main category: cs.SE

TL;DR: 提出多目标整数线性规划模型降低代码认知复杂度，开发算法并集成工具解决问题。


<details>
  <summary>Details</summary>
Motivation: 为保证代码可维护性，简单的代码很重要，提取方法重构可增强软件且不改变功能，需解决代码提取的多目标优化问题。

Method: 采用SonarSource的认知复杂度度量，将提取问题建模为多目标整数线性规划问题，开发多种算法。

Result: 得到一组可降低代码认知复杂度的解决方案，能平衡代码行数和复杂度

Conclusion: 研究中开发的算法集成的工具可实现软件认知复杂度降低问题的参数化求解。

Abstract: Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.

</details>


### [377] [Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context](https://arxiv.org/abs/2601.21593)
*Zhijie Zhong,Yuhong Nan,Mingxi Ye,Qing Xue,Jiashui Wang,Xinlei Ying,Long Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出EthCRAFT工具检测区块链客户端RPC漏洞，表现优于现有检测器并发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有区块链RPC漏洞检测研究未关注触发依赖上下文的RPC漏洞的合适上下文生成。

Method: EthCRAFT探索区块链客户端状态转换程序空间生成交易构建上下文，设计上下文感知的RPC方法调用生成方法发送调用，用5种客户端响应作为交叉引用预言机检测漏洞。

Result: 在真实漏洞上评估，EthCRAFT检测到更多漏洞，发现6个新漏洞，1个修复写入更新，3个获漏洞赏金。

Conclusion: EthCRAFT在检测区块链客户端RPC漏洞方面优于现有检测器，具有实用价值。

Abstract: Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.
  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.

</details>


### [378] [Age Matters: Analyzing Age-Related Discussions in App Reviews](https://arxiv.org/abs/2601.21605)
*Shashiwadana Nirmania,Garima Sharma,Hourieh Khalajzadeh,Mojtaba Shahin*

Main category: cs.SE

TL;DR: 研究通过分析应用商店评论探索移动应用如何满足不同年龄段用户需求，用多种模型检测年龄讨论，RoBERTa表现最佳，还分析出用户关注的六个主题。


<details>
  <summary>Details</summary>
Motivation: 移动应用虽发展迅速但在满足不同年龄段用户需求上存在差距，开发者对年龄相关问题理解不足，需了解用户视角以提供有效解决方案。

Method: 手动整理4163条谷歌应用商店应用评论，区分出1429条与年龄相关和2734条无关评论，用八种机器学习、深度学习和大语言模型自动检测年龄讨论，对1429条相关评论进行定性分析。

Result: RoBERTa模型表现最佳，精度达92.46%，定性分析发现六个反映用户关注的主题。

Conclusion: 通过分析应用评论可深入了解移动应用应如何服务不同年龄段用户。

Abstract: In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.

</details>


### [379] [AtPatch: Debugging Transformers via Hot-Fixing Over-Attention](https://arxiv.org/abs/2601.21695)
*Shihao Weng,Yang Feng,Jincheng Li,Yining Yin,Xiaofei Xie,Jia Liu*

Main category: cs.SE

TL;DR: 针对Transformer模型后门攻击和不公平问题中异常注意力模式，提出AtPatch方法动态重分配注意力图，实验显示其能有效缓解问题并保留模型功能。


<details>
  <summary>Details</summary>
Motivation: 现有神经元编辑缓解策略难以处理Transformer模型因后门攻击和不公平导致的异常注意力模式问题，且缺乏灵活性、易扭曲特征表示。

Method: AtPatch在模型推理时，先提取注意力图，用预训练检测器识别异常列并替换为良性注意力，再缩放其他列，若没有异常则返回原注意力图。

Result: 实验表明，相比现有方法，AtPatch能更有效地缓解后门攻击和不公平问题，同时更好地保留模型的原始功能。

Conclusion: AtPatch方法能有效缓解Transformer模型的后门攻击和不公平问题，且能保留模型功能，适合已部署模型。

Abstract: Transformer-based deep neural networks (DNNs) affected by backdoor attacks and unfairness typically exhibit anomalous attention patterns, leading to over-attend to backdoor triggers or protected attributes. Existing neuron-editing mitigation strategies often struggle to handle such situation and most of them lack flexibility and tend to distort feature representations. Motivated by such over-attention phenomenon and software engineering paradigms such as delta debugging and hot patching, we propose AtPatch, a hot-fix method that dynamically redistributes attention maps during model inference. Specifically, for a given input, AtPatch first extracts the attention map from the model's inference process. Then, it uses a pre-trained detector to identify anomalous columns and replace them with unified benign attention. Then, AtPatch rescales other columns to mitigate the impact of over-attention. Finally, AtPatch returns the redistributed attention map to the model for continued inference. Notably, if the detector does not report any anomalous columns, AtPatch directly returns the original attention map to the model. Unlike existing techniques, AtPatch selectively redistributes the attention map, making it better at preserving the model's original functionality. Furthermore, AtPatch's on-the-fly nature allows it to work without modifying model parameters or retraining, making it better suited for deployed models. We conducted extensive experiments to validate AtPatch. Experimental results show that, compared to existing methods, AtPatch can more effectively mitigate backdoor attacks and unfairness while better preserving the model's original functionality.

</details>


### [380] [Migrating Esope to Fortran 2008 using model transformations](https://arxiv.org/abs/2601.21755)
*Younoussa Sow,Nicolas Anquetil,Léandre Brault,Stéphane Ducasse*

Main category: cs.SE

TL;DR: 提出将含专有扩展Esope的FORTRAN 77自动迁移到Fortran 2008的方法及工具，讨论其优缺点等。


<details>
  <summary>Details</summary>
Motivation: 遗留编程语言如FORTRAN 77在工业应用重要，但维护和升级特别是迁移到新标准有挑战，尤其是存在专有扩展时。

Method: 采用模型驱动工程技术，通过转换生成目标模型，导出易读的Fortran 2008源代码。

Result: 开发出将Esope源代码转换为Fortran 2008的工具，支持生成代码的可读性并保持Esope的抽象水平。

Conclusion: 讨论了该方法的优点、局限性、可维护性，以及对不断发展需求的可扩展性和适应性。

Abstract: Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.

</details>


### [381] [Towards A Sustainable Future for Peer Review in Software Engineering](https://arxiv.org/abs/2601.21761)
*Esteban Parra,Sonia Haiduc,Preetha Chatterjee,Ramtin Ehsani,Polina Iaremchuk*

Main category: cs.SE

TL;DR: 软件工程领域论文投稿增长快，评审人员不足，提出未来更具扩展性、包容性和弹性的同行评审愿景。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程领域论文投稿快速增长，合格评审人员不足，避免对研究社区长期发展造成负面影响的问题。

Method: 提出吸引和培训新人成为高质量评审人员、激励更多社区成员担任评审、谨慎集成AI工具支持评审过程等机制。

Result: 未提及具体结果。

Conclusion: 未来软件工程研究的同行评审过程应更具扩展性、包容性和弹性。

Abstract: Peer review is the main mechanism by which the software engineering community assesses the quality of scientific results. However, the rapid growth of paper submissions in software engineering venues has outpaced the availability of qualified reviewers, creating a growing imbalance that risks constraining and negatively impacting the long-term growth of the Software Engineering (SE) research community. Our vision of the Future of the SE research landscape involves a more scalable, inclusive, and resilient peer review process that incorporates additional mechanisms for: 1) attracting and training newcomers to serve as high-quality reviewers, 2) incentivizing more community members to serve as peer reviewers, and 3) cautiously integrating AI tools to support a high-quality review process.

</details>


### [382] [Assessing the Business Process Modeling Competences of Large Language Models](https://arxiv.org/abs/2601.21787)
*Chantale Lauer,Peter Pfeiffer,Alexander Rombach,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本文介绍了用于评估大语言模型生成BPMN模型的BEF4LLM框架，对比了大语言模型与人类专家表现，指出大语言模型强弱项以指导未来开发。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成BPMN模型方面缺乏系统评估，现有评估方法存在不足。

Method: 引入BEF4LLM评估框架，从四个视角对开源大语言模型进行全面分析，并与人类建模专家对比。

Result: 大语言模型在句法和语用质量上表现出色，人类在语义方面更优，但得分差异较小，大语言模型在有效性和语义质量上有挑战。

Conclusion: 明确大语言模型用于BPMN建模的优缺点，对其在业务流程建模中的实际应用有指导意义。

Abstract: The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.

</details>


### [383] [Folklore in Software Engineering: A Definition and Conceptual Foundations](https://arxiv.org/abs/2601.21814)
*Eduard Enoiu,Jean Malm,Gregory Gay*

Main category: cs.SE

TL;DR: 本文探索软件工程中的民俗概念，通过文献综述、主题分析和访谈给出定义，认为明确该概念有益后续研究和实践。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程中民俗概念，理解行业内知识传播和影响。

Method: 采用文献综述和主题分析收集软件工程中民俗案例，进行分析；对12位瑞典从业者进行半结构化访谈。

Result: 提出软件工程民俗的工作定义，即非正式传播、传统且新兴的叙述和启发式方法。

Conclusion: 明确软件工程民俗概念为后续研究和反思实践奠定基础，可保留有效启发式方法并挑战不良民俗。

Abstract: We explore the concept of folklore within software engineering, drawing from folklore studies to define and characterize narratives, myths, rituals, humor, and informal knowledge that circulate within software development communities. Using a literature review and thematic analysis, we curated exemplar folklore items (e.g., beliefs about where defects occur, the 10x developer legend, and technical debt). We analyzed their narrative form, symbolic meaning, occupational relevance, and links to knowledge areas in software engineering. To ground these concepts in practice, we conducted semi-structured interviews with 12 industrial practitioners in Sweden to explore how such narratives are recognized or transmitted within their daily work and how they affect it. Synthesizing these results, we propose a working definition of software engineering folklore as informally transmitted, traditional, and emergent narratives and heuristics enacted within occupational folk groups that shape identity, values, and collective knowledge. We argue that making the concept of software engineering folklore explicit provides a foundation for subsequent ethnography and folklore studies and for reflective practice that can preserve context-effective heuristics while challenging unhelpful folklore.

</details>


### [384] [SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents](https://arxiv.org/abs/2601.22129)
*Yifeng Ding,Lingming Zhang*

Main category: cs.SE

TL;DR: 本文提出SWE - Replay，一个有效且通用的测试时间缩放技术，在软件工程任务中能降低成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件工程任务的测试时间缩放方法存在计算成本高、模型误校准和泛化性不足的问题。

Method: SWE - Replay通过回收先前试验的轨迹，在关键中间步骤动态选择从头探索或利用存档经验，选择中间步骤基于仓库探索的潜力和推理意义。

Result: 在SWE - Bench Verified上，SWE - Replay始终优于简单缩放，成本最多降低17.4%，性能最多提升3.8%，在SWE - Bench Pro和Multilingual上验证了其泛化性。

Conclusion: SWE - Replay为软件工程代理的高效测试时间缩放提供了可靠基础。

Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.

</details>


### [385] [Implementing AI Bill of Materials (AI BOM) with SPDX 3.0: A Comprehensive Guide to Creating AI and Dataset Bill of Materials](https://arxiv.org/abs/2504.16743)
*Karen Bennet,Gopi Krishnan Rajbahadur,Arthit Suriyawongkul,Kate Stewart*

Main category: cs.SE

TL;DR: 介绍了软件物料清单（SBOM），并在此基础上提出人工智能物料清单（AI - BOM）的概念。


<details>
  <summary>Details</summary>
Motivation: 人工智能项目面临软件安全之外的独特挑战，SBOM 无法满足需求，因此需要更全面的物料清单。

Method: 在 SBOM 基础上进行扩展，纳入算法、数据收集方法、框架和库、许可信息及标准合规性等方面的文档记录。

Result: 提出了 AI - BOM 的概念。

Conclusion: 为人工智能项目的软件供应链引入了更全面透明且安全的工具 AI - BOM。

Abstract: A Software Bill of Materials (SBOM) is becoming an increasingly important tool in regulatory and technical spaces to introduce more transparency and security into a project's software supply chain.
  Artificial intelligence (AI) projects face unique challenges beyond the security of their software, and thus require a more expansive approach to a bill of materials. In this report, we introduce the concept of an AI-BOM, expanding on the SBOM to include the documentation of algorithms, data collection methods, frameworks and libraries, licensing information, and standard compliance.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [386] [Alpha Discovery via Grammar-Guided Learning and Search](https://arxiv.org/abs/2601.22119)
*Han Yang,Dong Hao,Zhuohan Wang,Qi Shi,Xingtong Li*

Main category: q-fin.CP

TL;DR: 提出基于文法的框架AlphaCFG用于发现有效且可解释的alpha因子，实验表明其在搜索效率和交易盈利性上优于基线，可作为符号因子发现的通用框架。


<details>
  <summary>Details</summary>
Motivation: 现有自动发现公式化alpha因子的方法常忽略语法和语义约束，依赖无结构无边界的穷举搜索。

Method: 使用面向alpha的上下文无关文法定义树结构、大小可控的搜索空间，将alpha发现表述为树结构的语言马尔可夫决策过程，并使用由语法敏感的值和策略网络引导的文法感知蒙特卡罗树搜索求解。

Result: 在中美股市数据集实验中，AlphaCFG在搜索效率和交易盈利性上优于现有基线。

Conclusion: AlphaCFG可作为量化金融中符号因子发现和优化的通用框架，应用于资产定价和投资组合构建等。

Abstract: Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [387] [Trade uncertainty impact on stock-bond correlations: Insights from conditional correlation models](https://arxiv.org/abs/2601.21447)
*Demetrio Lacava,Edoardo Otranto*

Main category: q-fin.ST

TL;DR: 研究美国贸易政策不确定性（TPU）对股票 - 债券相关性动态的影响，发现含TPU和政治效应的DCC模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探究美国贸易政策不确定性对股票 - 债券相关性动态的影响。

Method: 使用2015 - 2025年美国主要股票指数和10年期国债的每日数据，在两步GARCH框架内，运用CCC、STCC和DCC等多元模型估计相关性，并纳入TPU指数和总统虚拟变量。

Result: 常数相关模型被拒绝，STCC和DCC模型证实TPU在驱动相关性动态中起核心作用，不同政治体制下有显著差异，含TPU和政治效应的DCC模型样本内拟合和预测性能最佳。

Conclusion: 含贸易政策不确定性和政治效应的动态条件相关模型在分析股票 - 债券相关性动态上表现更优。

Abstract: This paper investigates the impact of Trade Policy Uncertainty (TPU) on stock-bond correlation dynamics in the United States. Using daily data on major U.S. stock indices and the 10-year Treasury bond from 2015 to 2025, we estimate correlation within a two-step GARCH-based framework, relying on multivariate specifications, including Constant Conditional Correlation (CCC), Smooth Transition Conditional Correlation (STCC), and Dynamic Conditional Correlation (DCC) models. We extend these frameworks by incorporating TPU index and a presidential dummy to capture effects of trade uncertainty and government cycles. The findings show that constant correlation models are strongly rejected in favor of time-varying specifications. Both STCC and DCC models confirm TPU's central role in driving correlation dynamics, with significant differences across political regimes. DCC models augmented with TPU and political effects deliver the best in-sample fit and strongest forecasting performance, as measured by statistical and economic loss functions.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [388] [Diverse Approaches to Optimal Execution Schedule Generation](https://arxiv.org/abs/2601.22113)
*Robert de Witt,Mikko S. Pakkanen*

Main category: q-fin.TR

TL;DR: 首次将MAP - Elites算法应用于交易执行，生成不同流动性和波动性条件的策略组合，结果显示质量多样性方法有潜力，但需大量计算资源，同时验证了模拟的真实性。


<details>
  <summary>Details</summary>
Motivation: 探索质量 - 多样性算法在交易执行中的应用，寻找比单一最优策略更好的策略。

Method: 应用MAP - Elites算法生成策略组合，开发校准的Gymnasium环境，使用两种Proximal Policy Optimization架构（MLP和CNN）进行实验。

Result: 单个策略在特定条件下有8 - 10%性能提升，部分单元表现下降；CNN架构实现2.13 bps的到达滑点，优于行业基线。

Conclusion: 质量 - 多样性方法对适应不同市场条件的交易执行有前景，但开发稳健策略需大量计算资源，同时验证了模拟环境和单策略基线的有效性。

Abstract: We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for robust specialist development across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with R^2>0.02 out-of-sample. Within this environment, two Proximal Policy Optimization architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [389] [Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators](https://arxiv.org/abs/2601.20888)
*Youguang Chen,George Biros*

Main category: stat.ML

TL;DR: 研究贝叶斯线性逆问题后验分布采样，引入Latent - IMH方法，理论分析性能，实验显示在计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在参数到可观测量算子A计算成本高的贝叶斯线性逆问题中，找到更高效的后验分布采样方法。

Method: 引入基于Metropolis - Hastings独立性采样器的Latent - IMH方法，先利用近似的\(	ilde{A}\)生成中间潜变量，再用精确的A进行优化。

Result: 通过理论分析（KL散度和混合时间界）和数值实验表明，在合理假设下，Latent - IMH在计算效率上优于No - U - Turn采样器（NUTS）等现有方法，有时能比现有方案快几个数量级。

Conclusion: Latent - IMH是一种在贝叶斯线性逆问题中计算效率较高的后验分布采样方法。

Abstract: We study sampling from posterior distributions in Bayesian linear inverse problems where $A$, the parameters to observables operator, is computationally expensive. In many applications, $A$ can be factored in a manner that facilitates the construction of a cost-effective approximation $\tilde{A}$. In this framework, we introduce Latent-IMH, a sampling method based on the Metropolis-Hastings independence (IMH) sampler. Latent-IMH first generates intermediate latent variables using the approximate $\tilde{A}$, and then refines them using the exact $A$. Its primary benefit is that it shifts the computational cost to an offline phase. We theoretically analyze the performance of Latent-IMH using KL divergence and mixing time bounds. Using numerical experiments on several model problems, we show that, under reasonable assumptions, it outperforms state-of-the-art methods such as the No-U-Turn sampler (NUTS) in computational efficiency. In some cases, Latent-IMH can be orders of magnitude faster than existing schemes.

</details>


### [390] [Efficient Causal Structure Learning via Modular Subgraph Integration](https://arxiv.org/abs/2601.21014)
*Haixiang Sun,Pengchao Tian,Zihan Zhou,Jielei Zhang,Peiyi Li,Andrew L. Liu*

Main category: stat.ML

TL;DR: 本文提出VISTA框架解决从观测数据学习因果结构的计算难题，理论证明其性质，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维环境下从观测数据学习因果结构存在计算难题，如搜索空间超指数增长和计算需求增加。

Method: 引入VISTA框架，将全局因果结构学习问题分解为基于马尔可夫毯的局部子图，通过加权投票机制实现全局集成，使用反馈弧集算法确保无环性。

Result: 理论上为VISTA建立有限样本误差界，证明其在温和条件下的渐近一致性；实验显示VISTA在准确性和效率上有显著提升。

Conclusion: VISTA框架有效，能在多种基础学习器上提升因果结构学习的准确性和效率。

Abstract: Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners.

</details>


### [391] [A Diffusive Classification Loss for Learning Energy-based Generative Models](https://arxiv.org/abs/2601.21025)
*Louis Grenioux,RuiKang OuYang,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 本文引入Diffusive Classification (DiffCLF)目标解决基于能量模型（EBMs）训练难题，经实验验证其比现有方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于能量模型（EBMs）训练存在挑战，直接最大似然计算成本高，分数匹配有模式盲目性问题。

Method: 引入DiffCLF目标，将EBM学习重构为跨噪声水平的监督分类问题，并与标准基于分数的目标结合。

Result: 在分析高斯混合案例中对比估计能量与真实值，应用于模型组合和玻尔兹曼生成器采样等任务，证明DiffCLF有效。

Conclusion: DiffCLF使EBMs比现有方法有更高保真度和更广泛适用性。

Abstract: Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or building Boltzmann Generators via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the Diffusive Classification (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and Boltzmann Generator sampling. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches.

</details>


### [392] [A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression](https://arxiv.org/abs/2601.21217)
*Dongyue Xie,Wanrong Zhu,Matthew Stephens*

Main category: stat.ML

TL;DR: 提出灵活的经验贝叶斯方法拟合贝叶斯广义线性模型，有新的变分推断方法且调参自由，适用范围广，在稀疏逻辑回归中表现优。


<details>
  <summary>Details</summary>
Motivation: 为贝叶斯广义线性模型寻找一种灵活、调参自由且适用范围广的拟合方法。

Method: 采用新颖的平均场变分推断方法，在算法内估计先验，直接优化后验均值和先验参数，可使用可扩展算法。

Result: 该方法能自动确定最优后验，适用于指数族分布，在稀疏逻辑回归的数值研究中预测性能优于现有方法。

Conclusion: 所提出的方法是一个统一框架，可广泛应用，在稀疏逻辑回归中有出色表现。

Abstract: We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches.

</details>


### [393] [Diffusion-based Annealed Boltzmann Generators : benefits, pitfalls and hopes](https://arxiv.org/abs/2601.21026)
*Louis Grenioux,Maxence Noble*

Main category: stat.ML

TL;DR: 研究基于扩散模型的退火蒙特卡罗玻尔兹曼生成器，分析不同设置下性能并指出学习型扩散模型瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有玻尔兹曼生成器在处理高维多模态目标时存在需要可处理似然和扩展性差的问题，研究基于退火蒙特卡罗的玻尔兹曼生成器以克服这些局限。

Method: 对基于扩散模型的退火蒙特卡罗玻尔兹曼生成器进行实证元分析，比较完美学习的扩散模型和从数据训练的扩散模型，提出基于一阶传输映射的确定性退火蒙特卡罗集成。

Result: 仅使用一阶随机去噪核的标准集成会失败，二阶去噪核在有协方差信息时可提升性能；确定性退火蒙特卡罗集成计算成本高但表现更好；学习型扩散模型变体难以产生准确的玻尔兹曼生成器。

Conclusion: 基于扩散模型的退火蒙特卡罗玻尔兹曼生成器在不同设置下有不同表现，学习型扩散模型的主要瓶颈是不准确的对数密度估计。

Abstract: Sampling configurations at thermodynamic equilibrium is a central challenge in statistical physics. Boltzmann Generators (BGs) tackle it by combining a generative model with a Monte Carlo (MC) correction step to obtain asymptotically unbiased samples from an unnormalized target. Most current BGs use classic MC mechanisms such as importance sampling, which both require tractable likelihoods from the backbone model and scale poorly in high-dimensional, multi-modal targets. We study BGs built on annealed Monte Carlo (aMC), which is designed to overcome these limitations by bridging a simple reference to the target through a sequence of intermediate densities. Diffusion models (DMs) are powerful generative models and have already been incorporated into aMC-based recalibration schemes via the diffusion-induced density path, making them appealing backbones for aMC-BGs. We provide an empirical meta-analysis of DM-based aMC-BGs on controlled multi-modal Gaussian mixtures (varying mode separation, number of modes, and dimension), explicitly disentangling inference effects from learning effects by comparing (i) a perfectly learned DM and (ii) a DM trained from data. Even with a perfect DM, standard integrations using only first-order stochastic denoising kernels fail systematically, whereas second-order denoising kernels can substantially improve performance when covariance information is available. We further propose a deterministic aMC integration based on first-order transport maps derived from DMs, which outperforms the stochastic first-order variant at higher computational cost. Finally, in the learned-DM setting, all DM-aMC variants struggle to produce accurate BGs; we trace the main bottleneck to inaccurate DM log-density estimation.

</details>


### [394] [An efficient, accurate, and interpretable machine learning method for computing probability of failure](https://arxiv.org/abs/2601.21089)
*Jacob Zhu,Donald Estep*

Main category: stat.ML

TL;DR: 介绍基于Gabriel编辑集的Penalized Profile Support Vector Machine计算复杂系统失效概率，对比其与其他分类方法，还应用于竞争物种生存概率。


<details>
  <summary>Details</summary>
Motivation: 计算复杂系统失效概率时，需减少计算机模型评估次数并保留决策边界的几何形状。

Method: 采用自适应采样策略，在失效边界附近分配点，通过训练点的策略聚类构建局部线性替代边界。

Result: 证明了两个收敛结果，在四个测试问题上与多种先进分类方法对比了性能。

Conclusion: 所提出的方法可用于计算复杂系统失效概率，且能应用于竞争物种生存概率的计算。

Abstract: We introduce a novel machine learning method called the Penalized Profile Support Vector Machine based on the Gabriel edited set for the computation of the probability of failure for a complex system as determined by a threshold condition on a computer model of system behavior. The method is designed to minimize the number of evaluations of the computer model while preserving the geometry of the decision boundary that determines the probability. It employs an adaptive sampling strategy designed to strategically allocate points near the boundary determining failure and builds a locally linear surrogate boundary that remains consistent with its geometry by strategic clustering of training points. We prove two convergence results and we compare the performance of the method against a number of state of the art classification methods on four test problems. We also apply the method to determine the probability of survival using the Lotka--Volterra model for competing species.

</details>


### [395] [Diffusion Path Samplers via Sequential Monte Carlo](https://arxiv.org/abs/2601.21951)
*James Matthew Young,Paula Cordero-Encinar,Sebastian Reich,Andrew Duncan,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 开发基于扩散的采样器处理目标分布，用扩散退火朗之万蒙特卡罗方法，解决分数估计问题，提供理论保证并在多数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 开发针对仅知归一化常数的目标分布的扩散采样器。

Method: 基于扩散退火朗之万蒙特卡罗方法，用高效序贯蒙特卡罗采样器解决分数估计问题，开发新的控制变量调度以最小化分数估计方差。

Result: 在多个合成和真实数据集上验证了方法的有效性。

Conclusion: 所提出的方法有效，且有理论保证。

Abstract: We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an efficient sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop novel control variate schedules that minimise the variance of these score estimates. Finally, we provide theoretical guarantees and empirically demonstrate the effectiveness of our method on several synthetic and real-world datasets.

</details>


### [396] [High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent in multi-index models](https://arxiv.org/abs/2601.21093)
*Zhou Fan,Leda Wang*

Main category: stat.ML

TL;DR: 研究高维多指标模型中多轮小批量随机梯度下降（SGD）学习动态，给出坐标动态渐近精确刻画及随机修正方程（SME）类似刻画，分析不同算法动态关系。


<details>
  <summary>Details</summary>
Motivation: 研究高维多指标模型中多轮小批量SGD用于经验风险最小化的学习动态。

Method: 在样本量n和数据维度d成比例增加的渐近状态下，对任意次线性批量大小和学习率进行分析，得到SGD坐标动态的渐近精确刻画和SME的类似刻画。

Result: SGD的极限动态对任意批量大小缩放α∈[0,1)相同；在学习率适当缩放时，SGD、SME和梯度流动态不同，线性模型中SGD和SME动态一致；在小学习率和样本量增大极限下可恢复已知结果。

Conclusion: 完成对高维多指标模型中多轮小批量SGD学习动态的分析，明确不同算法动态关系。

Abstract: We study the learning dynamics of a multi-pass, mini-batch Stochastic Gradient Descent (SGD) procedure for empirical risk minimization in high-dimensional multi-index models with isotropic random data. In an asymptotic regime where the sample size $n$ and data dimension $d$ increase proportionally, for any sub-linear batch size $κ\asymp n^α$ where $α\in [0,1)$, and for a commensurate ``critical'' scaling of the learning rate, we provide an asymptotically exact characterization of the coordinate-wise dynamics of SGD. This characterization takes the form of a system of dynamical mean-field equations, driven by a scalar Poisson jump process that represents the asymptotic limit of SGD sampling noise. We develop an analogous characterization of the Stochastic Modified Equation (SME) which provides a Gaussian diffusion approximation to SGD.
  Our analyses imply that the limiting dynamics for SGD are the same for any batch size scaling $α\in [0,1)$, and that under a commensurate scaling of the learning rate, dynamics of SGD, SME, and gradient flow are mutually distinct, with those of SGD and SME coinciding in the special case of a linear model. We recover a known dynamical mean-field characterization of gradient flow in a limit of small learning rate, and of one-pass/online SGD in a limit of increasing sample size $n/d \to \infty$.

</details>


### [397] [Efficient Stochastic Optimisation via Sequential Monte Carlo](https://arxiv.org/abs/2601.22003)
*James Cuin,Davide Carbone,Yanbo Tang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出用SMC采样器优化难处理梯度函数，可替代昂贵内采样方法并获计算增益，证明收敛性并在能量模型奖励调优中验证


<details>
  <summary>Details</summary>
Motivation: 机器学习和统计中难处理梯度函数优化问题常用随机逼近方法需要内采样循环，计算成本高

Method: 开发顺序蒙特卡罗（SMC）采样器来优化难处理梯度的函数，用高效SMC近似替代昂贵内采样方法

Result: 建立了SMC采样器近似的基本递归的收敛结果，在多种设置下能量模型的奖励调优中证明方法有效

Conclusion: 所提出使用SMC采样器优化函数的方法能取得显著计算增益，且有效可行

Abstract: The problem of optimising functions with intractable gradients frequently arise in machine learning and statistics, ranging from maximum marginal likelihood estimation procedures to fine-tuning of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with efficient SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by our methodology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings.

</details>


### [398] [Multilevel and Sequential Monte Carlo for Training-Free Diffusion Guidance](https://arxiv.org/abs/2601.21104)
*Aidan Gleich,Scott C. Schmidler*

Main category: stat.ML

TL;DR: 提出SMC框架解决训练扩散模型中无训练指导问题，在CIFAR - 10和ImageNet上取得好结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法用点估计近似后验分数，存在偏差，无法捕捉扩散模型反向过程的多模态特性。

Method: 提出顺序蒙特卡罗（SMC）框架，通过蒙特卡罗近似对全去噪分布积分构建无偏估计器，并结合多级蒙特卡罗（MLMC）的方差缩减方案。

Result: 在CIFAR - 10类条件生成上取得新的无训练指导的先进结果，准确率达95.6%，每次成功成本比基线低3倍；在ImageNet上每次成功成本比现有方法有1.5倍优势。

Conclusion: 所提方法有效解决了训练扩散模型中无训练指导问题，表现优于现有方法。

Abstract: We address the problem of accurate, training-free guidance for conditional generation in trained diffusion models. Existing methods typically rely on point-estimates to approximate the posterior score, often resulting in biased approximations that fail to capture multimodality inherent to the reverse process of diffusion models. We propose a sequential Monte Carlo (SMC) framework that constructs an unbiased estimator of $p_θ(y|x_t)$ by integrating over the full denoising distribution via Monte Carlo approximation. To ensure computational tractability, we incorporate variance-reduction schemes based on Multi-Level Monte Carlo (MLMC). Our approach achieves new state-of-the-art results for training-free guidance on CIFAR-10 class-conditional generation, achieving $95.6\%$ accuracy with $3\times$ lower cost-per-success than baselines. On ImageNet, our algorithm achieves $1.5\times$ cost-per-success advantage over existing methods.

</details>


### [399] [Provably Reliable Classifier Guidance through Cross-entropy Error Control](https://arxiv.org/abs/2601.21200)
*Sharan Sahu,Arisina Banerjee,Yuchen Wu*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of a learned classifier, yet it remains unclear whether standard classifier training procedures yield effective diffusion guidance. We address this gap by showing that, under mild smoothness assumptions on the classifiers, controlling the cross-entropy error at each diffusion step also controls the error of the resulting guidance vectors: classifiers achieving conditional KL divergence $\varepsilon^2$ from the ground-truth conditional label probabilities induce guidance vectors with mean squared error $\widetilde{O}(d \varepsilon )$. Our result yields an upper bound on the sampling error under classifier guidance and bears resemblance to a reverse log-Sobolev-type inequality. Moreover, we show that the classifier smoothness assumption is essential, by constructing simple counterexamples demonstrating that, without it, control of the guidance vector can fail for almost all distributions. To our knowledge, our work establishes the first quantitative link between classifier training and guidance alignment, yielding both a theoretical foundation for classifier guidance and principled guidelines for classifier selection.

</details>


### [400] [Bulk-Calibrated Credal Ambiguity Sets: Fast, Tractable Decision Making under Out-of-Sample Contamination](https://arxiv.org/abs/2601.21324)
*Mengqi Chen,Thomas B. Berrett,Theodoros Damoulas,Michele Caprio*

Main category: stat.ML

TL;DR: 本文引入批量校准的可信模糊集解决分布鲁棒优化（DRO）中Huber污染导致的问题，得到有限目标和易处理的程序，实验显示有良好性能。


<details>
  <summary>Details</summary>
Motivation: 在DRO中使用Huber污染时，若不施加强有界或支持假设，会导致最坏情况风险无穷和目标空泛。

Method: 引入批量校准的可信模糊集，从数据中学习高质量批量集，分别考虑批量内污染和剩余尾部贡献。

Result: 得到封闭形式、有限的均值 + 上确界的鲁棒目标，以及常见损失和批量几何图形的易处理线性或二阶锥规划；实验展示了良好的鲁棒性 - 准确性权衡和高效的优化时间。

Conclusion: 所提出的框架有效解决了DRO中Huber污染的问题，且能在多种场景展现优势。

Abstract: Distributionally robust optimisation (DRO) minimises the worst-case expected loss over an ambiguity set that can capture distributional shifts in out-of-sample environments. While Huber (linear-vacuous) contamination is a classical minimal-assumption model for an $\varepsilon$-fraction of arbitrary perturbations, including it in an ambiguity set can make the worst-case risk infinite and the DRO objective vacuous unless one imposes strong boundedness or support assumptions. We address these challenges by introducing bulk-calibrated credal ambiguity sets: we learn a high-mass bulk set from data while considering contamination inside the bulk and bounding the remaining tail contribution separately. This leads to a closed-form, finite $\mathrm{mean}+\sup$ robust objective and tractable linear or second-order cone programs for common losses and bulk geometries. Through this framework, we highlight and exploit the equivalence between the imprecise probability (IP) notion of upper expectation and the worst-case risk, demonstrating how IP credal sets translate into DRO objectives with interpretable tolerance levels. Experiments on heavy-tailed inventory control, geographically shifted house-price regression, and demographically shifted text classification show competitive robustness-accuracy trade-offs and efficient optimisation times, using Bayesian, frequentist, or empirical reference distributions.

</details>


### [401] [Statsformer: Validated Ensemble Learning with LLM-Derived Semantic Priors](https://arxiv.org/abs/2601.21410)
*Erica Zhang,Naomi Sagan,Danny Tse,Fangzhao Zhang,Mert Pilanci,Jose Blanchet*

Main category: stat.ML

TL;DR: 介绍Statsformer框架，将大语言模型知识融入监督统计学习，克服现有方法局限，实证表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有将大模型知识融入监督统计学习的方法在适应性和范围上有局限，易受大模型幻觉影响或仅在单一固定学习器中嵌入语义信息。

Method: 采用有防护的集成架构，在线性和非线性学习器的集成中嵌入大模型衍生的特征先验，通过交叉验证自适应校准其影响。

Result: 有信息的先验能使性能持续提升，无信息或错误指定的大模型引导会被自动降低权重，在多种预测任务中减轻幻觉影响。

Conclusion: Statsformer是一个灵活系统，有类似神谕的保证，表现不逊于库中基础学习器的任何凸组合。

Abstract: We introduce Statsformer, a principled framework for integrating large language model (LLM)-derived knowledge into supervised statistical learning. Existing approaches are limited in adaptability and scope: they either inject LLM guidance as an unvalidated heuristic, which is sensitive to LLM hallucination, or embed semantic information within a single fixed learner. Statsformer overcomes both limitations through a guardrailed ensemble architecture. We embed LLM-derived feature priors within an ensemble of linear and nonlinear learners, adaptively calibrating their influence via cross-validation. This design yields a flexible system with an oracle-style guarantee that it performs no worse than any convex combination of its in-library base learners, up to statistical error. Empirically, informative priors yield consistent performance improvements, while uninformative or misspecified LLM guidance is automatically downweighted, mitigating the impact of hallucinations across a diverse range of prediction tasks.

</details>


### [402] [Questioning the Coverage-Length Metric in Conformal Prediction: When Shorter Intervals Are Not Better](https://arxiv.org/abs/2601.21455)
*Yizhou Min,Yizhou Lu,Lanqi Li,Zhen Zhang,Jiaye Teng*

Main category: stat.ML

TL;DR: 文章指出共形预测标准度量的问题，提出“偏见技巧”可误导性地缩短区间长度，并引入新指标检测此类问题。


<details>
  <summary>Details</summary>
Motivation: 批判检查共形预测标准度量（覆盖率和区间长度）的充分性。

Method: 提出“偏见技巧”（PT），形式推导PT实现误导性改进的条件，并进行各类回归和分类任务的实证。

Result: PT能误导性缩短区间长度，但算法重复运行时同一输入预测区间差异大，会引入实际漏洞。

Conclusion: 引入区间稳定性新指标，可检测新CP方法是否基于类似PT技术隐式提高长度。

Abstract: Conformal prediction (CP) has become a cornerstone of distribution-free uncertainty quantification, conventionally evaluated by its coverage and interval length. This work critically examines the sufficiency of these standard metrics. We demonstrate that the interval length might be deceptively improved through a counter-intuitive approach termed Prejudicial Trick (PT), while the coverage remains valid. Specifically, for any given test sample, PT probabilistically returns an interval, which is either null or constructed using an adjusted confidence level, thereby preserving marginal coverage. While PT potentially yields a deceptively lower interval length, it introduces practical vulnerabilities: the same input can yield completely different prediction intervals across repeated runs of the algorithm. We formally derive the conditions under which PT achieves these misleading improvements and provides extensive empirical evidence across various regression and classification tasks. Furthermore, we introduce a new metric interval stability which helps detect whether a new CP method implicitly improves the length based on such PT-like techniques.

</details>


### [403] [A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting](https://arxiv.org/abs/2601.21812)
*Francisco Caldas,Sahil Kumar,Cláudia Soares*

Main category: stat.ML

TL;DR: 提出用于时间序列预测的模型无关前向扩散过程，通过频谱分解提升预测质量，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测扩散方法在保留结构化时间模式上效果不佳，需改进扩散过程以更好保留长期模式。

Method: 将信号分解为频谱分量，根据分量能量注入噪声，修改扩散过程，与现有扩散骨干兼容。

Result: 在标准预测基准上，使用频谱分解策略的模型优于使用基线前向过程的扩散模型，计算开销可忽略不计。

Conclusion: 提出的频谱分解扩散方法能有效改善时间序列预测质量，且计算成本低。

Abstract: We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike prior work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e.g., DiffWave, TimeGrad, CSDI). By staging noise injection according to component energy, it maintains high signal-to-noise ratios for dominant frequencies throughout the diffusion trajectory, thereby improving the recoverability of long-term patterns. This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality. Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the baseline forward process, with negligible computational overhead. The code for this paper is available at https://anonymous.4open.science/r/D-FDP-4A29.

</details>


### [404] [A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth](https://arxiv.org/abs/2601.21817)
*Mingyuan Xu,Xinzi Tan,Jiawei Wu,Doudou Zhou*

Main category: stat.ML

TL;DR: 提出法官感知排名框架评估大语言模型，在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估中，法官大语言模型可靠性差异大，平等对待会导致有偏排行榜和误导性不确定性估计。

Method: 提出法官感知排名框架，扩展Bradley - Terry - Luce模型，引入特定法官辨别参数，从无参考标签的成对比较中联合估计潜在模型质量和法官可靠性。

Result: 在多个公共基准测试和新收集数据集上，改进了与人类偏好的一致性，比未加权基线具有更高的数据效率，为大语言模型排名产生校准的不确定性量化。

Conclusion: 所提框架能有效解决大语言模型评估中法官可靠性差异问题，提升评估效果。

Abstract: Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. We propose a judge-aware ranking framework that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum likelihood estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, our method improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings.

</details>


### [405] [Generative Modeling of Discrete Data Using Geometric Latent Subspaces](https://arxiv.org/abs/2601.21831)
*Daniel Gonzalez-Alvarado,Jonas Cassel,Stefania Petra,Christoph Schnörr*

Main category: stat.ML

TL;DR: 引入类别分布乘积流形指数参数空间的潜子空间用于离散数据生成模型学习，低维潜空间有优势，Riemann几何使流匹配有效，低维即可表示数据。


<details>
  <summary>Details</summary>
Motivation: 利用潜子空间工具学习离散数据的生成模型，编码统计依赖并去除冗余自由度。

Method: 在参数域引入Riemann几何，使空间和距离通过等距映射关联，实现一致的流匹配，且让测地线变为直线以有效训练模型。

Result: 实证结果表明，低维潜空间足以用于生成模型的数据表示。

Conclusion: 使用潜子空间在指数参数空间学习离散数据的生成模型是有效的，低维潜空间即可满足需求。

Abstract: We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data. The low-dimensional latent space encodes statistical dependencies and removes redundant degrees of freedom among the categorial variables. We equip the parameter domain with a Riemannian geometry such that the spaces and distances are related by isometries which enables consistent flow matching. In particular, geodesics become straight lines which makes model training by flow matching effective. Empirical results demonstrate that reduced latent dimensions suffice to represent data for generative modeling.

</details>


### [406] [On Forgetting and Stability of Score-based Generative models](https://arxiv.org/abs/2601.21868)
*Stanislas Strasman,Gabriel Cardoso,Sylvain Le Corff,Vincent Lemaire,Antonio Ocello*

Main category: stat.ML

TL;DR: 本文利用马尔可夫链的稳定性和遗忘特性，为基于分数的生成模型的采样误差提供了定量界限，阐明了随机动力学的作用并提供误差分析框架。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型的稳定性和长期行为是现代机器学习的基本问题，本文旨在为基于分数的生成模型的采样误差提供定量界限。

Method: 利用与反向时间动态相关的马尔可夫链的稳定性和遗忘特性，提出Lyapunov漂移条件和Doeblin型小化条件来确保反向过程初始化和离散化误差的传播。

Result: 得到了采样过程的定量稳定性，反向扩散动力学在采样轨迹上引入了收缩机制。

Conclusion: 阐明了随机动力学在基于分数的模型中的作用，为分析此类方法中的误差传播提供了原则性框架。

Abstract: Understanding the stability and long-time behavior of generative models is a fundamental problem in modern machine learning. This paper provides quantitative bounds on the sampling error of score-based generative models by leveraging stability and forgetting properties of the Markov chain associated with the reverse-time dynamics. Under weak assumptions, we provide the two structural properties to ensure the propagation of initialization and discretization errors of the backward process: a Lyapunov drift condition and a Doeblin-type minorization condition. A practical consequence is quantitative stability of the sampling procedure, as the reverse diffusion dynamics induces a contraction mechanism along the sampling trajectory. Our results clarify the role of stochastic dynamics in score-based models and provide a principled framework for analyzing propagation of errors in such approaches.

</details>


### [407] [Clustering in Deep Stochastic Transformers](https://arxiv.org/abs/2601.21942)
*Lev Fedorov,Michaël E. Sander,Romuald Elie,Pierre Marion,Mathieu Laurière*

Main category: stat.ML

TL;DR: 论文指出考虑随机初始化的随机性会改变现有Transformer理论中代币动态的情况，证明了离散代币动态收敛到球上的交互粒子系统，初始化噪声防止了确定性模型预测的聚类，并得到关于两个代币的相变，实验验证了相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有基于层归一化的深度Transformer理论依赖确定性权重假设，无法捕捉标准初始化方案，需理解随机初始化的内在随机性对代币动态的影响。

Method: 分析价值矩阵随机初始化产生噪声的深层Transformer，在扩散缩放和逐代币RMS归一化下，证明随着Transformer层数趋于无穷，离散代币动态的收敛情况。

Result: 初始化噪声可防止确定性模型预测的单个聚类，对于两个代币有由交互强度和代币维度控制的相变，反极配置有正概率成为吸引子。实验证实相变，发现多代币也存在反极结构，抑制噪声会降低准确性。

Conclusion: 考虑随机初始化的内在随机性对Transformer代币动态有重要影响，初始化噪声在其中起到关键作用，不能忽视其影响。

Abstract: Transformers have revolutionized deep learning across various domains but understanding the precise token dynamics remains a theoretical challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initialization scheme in Transformers. In this work, we show that accounting for the intrinsic stochasticity of random initialization alters this picture. More precisely, we analyze deep Transformers where noise arises from the random initialization of value matrices. Under diffusion scaling and token-wise RMS normalization, we prove that, as the number of Transformer layers goes to infinity, the discrete token dynamics converge to an interacting-particle system on the sphere where tokens are driven by a \emph{common} matrix-valued Brownian noise. In this limit, we show that initialization noise prevents the collapse to a single cluster predicted by deterministic models. For two tokens, we prove a phase transition governed by the interaction strength and the token dimension: unlike deterministic attention flows, antipodal configurations become attracting with positive probability. Numerical experiments confirm the predicted transition, reveal that antipodal formations persist for more than two tokens, and demonstrate that suppressing the intrinsic noise degrades accuracy.

</details>


### [408] [Near-Optimal Private Tests for Simple and MLR Hypotheses](https://arxiv.org/abs/2601.21959)
*Yu-Wei Chen,Raghu Pasupathy,Jordan Awan*

Main category: stat.ML

TL;DR: 开发在高斯差分隐私框架下的近最优测试程序，理论和实验表明该私有测试有良好表现。


<details>
  <summary>Details</summary>
Motivation: 在高斯差分隐私框架下开发高效的测试程序用于简单、单边和双边测试。

Method: 基于带数据驱动夹紧边界的私有均值估计器构建私有测试统计量。

Result: 私有测试统计量达到与非私有最强大测试相同的渐近相对效率，控制保守的第一类错误率；数值实验表明私有测试优于竞争的DP方法，与非私有最强大测试功效相当。

Conclusion: 所提出的私有测试程序在高斯差分隐私框架下是有效的，有较好的实际应用价值。

Abstract: We develop a near-optimal testing procedure under the framework of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone likelihood ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control. In addition to our theoretical results, our numerical experiments show that our private tests outperform competing DP methods and offer comparable power to the non-private most powerful tests, even at moderately small sample sizes and privacy loss budgets.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [409] [bigMICE: Multiple Imputation of Big Data](https://arxiv.org/abs/2601.21613)
*Hugo Morvan,Jonas Agholme,Bjorn Eliasson,Katarina Olofsson,Ludger Grote,Fredrik Iredahl,Oleg Sysoev*

Main category: stat.CO

TL;DR: 文章介绍了处理缺失数据的 MICE 方法在大数据集应用的挑战，开发 bigMICE 包解决问题，测试表明该方法在大数据集上更省内存、速度更快，还给出使用指南。


<details>
  <summary>Details</summary>
Motivation: 解决传统 MICE 方法在处理大数据集时因计算时间和内存限制带来的问题。

Method: 开发 bigMICE 包，利用 Apache Spark MLLib 和 Spark ML 使 MICE 框架适应大数据，可控制执行时的最大内存使用。

Result: 在瑞典大型医疗注册数据上测试，该方法在大数据集上更省内存、速度更快，大数据集即使变量缺失比例大也能有高质量插补。

Conclusion: bigMICE 方法在大数据集上更高效，能实现高质量插补，论文还提供安装和使用指南。

Abstract: Missing data is a prevalent issue in many applications, including large medical registries such as the Swedish Healthcare Quality Registries, potentially leading to biased or inefficient analyses if not handled properly. Multiple Imputation by Chained Equations (MICE) is a popular and versatile method for handling multivariate missing data but traditional implementations face significant challenges when applied to big data sets due to computational time and memory limitations.
  To address this, the bigMICE package was developed, adapting the MICE framework to big data using Apache Spark MLLib and Spark ML. Our implementation allows for controlling the maximum memory usage during the execution, enabling processing of very large data sets on a hardware with a limited memory, such as ordinary laptops.
  The developed package was tested on a large Swedish medical registry to measure memory usage, runtime and dependence of the imputation quality on sample size and on missingness proportion in the data. In conclusion, our method is generally more memory efficient and faster on large data sets compared to a commonly used MICE implementation. We also demonstrate that working with very large datasets can result in high quality imputations even when a variable has a large proportion of missing data. This paper also provides guidelines and recommendations on how to install and use our open source package.

</details>


### [410] [Mean-field Variational Bayes for Sparse Probit Regression](https://arxiv.org/abs/2601.21765)
*Augusto Fasano,Giovanni Rebaudo*

Main category: stat.CO

TL;DR: 本文提出一种基于平均场变分贝叶斯近似的方法用于二元结果的贝叶斯变量选择，比MCMC更快且精度相当。


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫链蒙特卡罗（MCMC）采样器在高维情况下的计算挑战。

Method: 开发了平均场变分贝叶斯近似，所有变分因子可进行闭式更新，并提出高效的坐标上升变分推理算法。

Result: 该方法能成功识别重要变量，比MCMC快几个数量级，同时保持相当的精度。

Conclusion: 该方法可在单一框架内实现可解释的选择和预测。

Abstract: We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [411] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 论文提出统一合成数据生成框架Nimbus，提高资源利用率和吞吐量，为InternData套件提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成管道碎片化、特定于任务，导致工程效率低和系统不稳定，无法满足基础模型训练的数据生成需求。

Method: 提出模块化四层架构和分离执行模型，实现动态管道调度、全局负载均衡、分布式容错和后端渲染优化。

Result: Nimbus端到端吞吐量比未优化基线提高2 - 3倍，能在大规模分布式环境中稳定长期运行。

Conclusion: Nimbus可作为InternData套件的生产骨干，实现无缝跨域数据合成。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [412] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: 提出Meta - ROS解决现有机器人中间件框架复杂和互操作性问题，经测试性能优于ROS2，适合实时机器人AI应用。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件框架（如ROS2）复杂且互操作性差，新开发者难以采用。

Method: 提出Meta - ROS，利用现代通信协议（如Zenoh和ZeroMQ）实现跨平台高效通信，支持多种数据类型，并通过综合测试与ROS1和ROS2对比。

Result: Meta - ROS性能优于ROS2，吞吐量高30%，显著降低消息延迟，优化资源使用。

Conclusion: Meta - ROS硬件支持强、以开发者为中心，是现代实时机器人AI应用的理想解决方案。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [413] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 使用运行在混合信号模拟/数字神经形态处理器上的脉冲神经网络，通过软硬件协同设计和强化学习，实现冰球游戏实时学习，连接神经科学硬件与机器人控制。


<details>
  <summary>Details</summary>
Motivation: 解决冰球游戏中高速冰球下瞬间决策的挑战。

Method: 软硬件协同设计，利用固定随机连接捕捉任务时间结构，在输出层采用局部e - prop学习规则，通过强化学习训练系统。

Result: 实现实时学习，可在计算机和神经形态芯片闭环设置中对脉冲神经网络进行实际训练。

Conclusion: 将受神经科学启发的硬件与现实世界机器人控制相结合，证明脑启发方法可处理快节奏交互任务并支持智能机器持续学习。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [414] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文探讨了布料操作强化学习的高效模块化方法，减少了模型大小和训练时间，并将模拟训练模型转移到现实世界，在SoftGym基准测试中取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统布料操作方法存在高维状态空间、复杂动力学和自遮挡问题，强化学习虽有前景但计算成本高，现有端到端学习方法也有计算成本问题，因此探索高效模块化方法。

Method: 探索高效模块化的强化学习方法，通过精心设计减少模拟学习中的模型大小和训练时间，并将模拟训练模型转移到现实世界。

Result: 在SoftGym基准测试中，相比现有基线任务有显著性能提升，且使用了更小的模型。

Conclusion: 所提出的高效模块化强化学习方法可有效解决布料操作问题，减少计算成本并实现从模拟到现实的转移。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [415] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 提出适用于手术操作任务的监督混合专家（MoE）架构，可加在自主策略之上，用少于150次演示和单双目内窥镜图像学习复杂操作，在肠道抓取和缩回任务上表现良好，优于现有模型，有体内部署潜力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在手术机器人应用因数据稀缺、工作空间受限等面临挑战，需要新方法解决。

Method: 提出监督MoE架构，可添加到任何自主策略上，并结合动作分块变压器（ACT），使用单双目内窥镜图像学习操作。

Result: 与现有视觉 - 语言 - 动作（VLA）模型和标准ACT基线相比，采用监督MoE架构显著提升性能，在分布内成功率更高，分布外场景更鲁棒，能推广到未见测试视角和零样本迁移到离体猪组织。

Conclusion: 该方法为手术机器人在体内部署提供了有希望的途径，并给出了体内猪手术策略初步定性结果。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [416] [Representation-Regularized Convolutional Audio Transformer for Audio Understanding](https://arxiv.org/abs/2601.21612)
*Bing Han,Chushu Zhou,Yifan Yang,Wei Wang,Chenda Li,Wangyou Zhang,Yanmin Qian*

Main category: eess.AS

TL;DR: 提出卷积音频变压器（CAT）框架解决现有基于自监督学习的音频理解方法的问题，实验显示其性能优于基线且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督学习的音频理解方法存在单粒度建模局限和计算成本高、收敛慢的问题。

Method: 提出CAT框架，采用多分辨率块捕获分层音频特征，引入表示正则化目标提高训练效率。

Result: CAT在音频理解基准测试中显著优于基线，在AudioSet 20k数据集上收敛速度比现有方法快5倍。

Conclusion: CAT框架有效解决了现有方法的问题，提升了音频理解性能和训练效率。

Abstract: Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT.

</details>


### [417] [Reducing Prompt Sensitivity in LLM-based Speech Recognition Through Learnable Projection](https://arxiv.org/abs/2601.20898)
*Sergio Burdisso,Esaú Villatoro-Tello,Shashi Kumar,Srikanth Madikeri,Andrés Carofilis,Pradeep Rangappa,Manjunath K E,Kadri Hacioglu,Petr Motlicek,Andreas Stolcke*

Main category: eess.AS

TL;DR: 本文分析常用提示对基于大语言模型的自动语音识别（ASR）性能的影响，并提出提示投影器模块，实验表明该模块能提升性能、减少变异性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的 ASR 常用固定手动定义提示，但提示设计的影响未充分研究。

Method: 对不同数据集的常用提示进行分析，提出提示投影器模块，该模块可将提示嵌入投影到 LLM 输入空间的更有效区域。

Result: 在四个数据集上实验显示，添加提示投影器能持续提升性能、减少变异性，且优于最佳手动选择的提示。

Conclusion: 提示选择显著影响 ASR 性能且引入不稳定性，提示投影器模块能有效改进基于大语言模型的 ASR 性能。

Abstract: LLM-based automatic speech recognition (ASR), a well-established approach, connects speech foundation models to large language models (LLMs) through a speech-to-LLM projector, yielding promising results. A common design choice in these architectures is the use of a fixed, manually defined prompt during both training and inference. This setup not only enables applicability across a range of practical scenarios, but also helps maximize model performance. However, the impact of prompt design remains underexplored. This paper presents a comprehensive analysis of commonly used prompts across diverse datasets, showing that prompt choice significantly affects ASR performance and introduces instability, with no single prompt performing best across all cases. Inspired by the speech-to-LLM projector, we propose a prompt projector module, a simple, model-agnostic extension that learns to project prompt embeddings to more effective regions of the LLM input space, without modifying the underlying LLM-based ASR model. Experiments on four datasets show that the addition of a prompt projector consistently improves performance, reduces variability, and outperforms the best manually selected prompts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [418] [VSE: Variational state estimation of complex model-free process](https://arxiv.org/abs/2601.21887)
*Gustav Norén,Anubhab Ghosh,Fredrik Cumlin,Saikat Chatterjee*

Main category: eess.SP

TL;DR: 设计变分状态估计（VSE）方法，用循环神经网络提供复杂动力过程的封闭形式高斯后验，在跟踪应用中表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在缺乏合适物理模型描述复杂动力过程状态演化的情况下，设计有效的状态估计方法。

Method: 设计VSE方法，用循环神经网络提供封闭形式高斯后验，学习阶段用额外RNN，基于变分推理原则让两个RNN相互学习。

Result: 在随机洛伦兹系统状态估计的跟踪应用中，VSE与已知模型的粒子滤波器和未知模型的数据驱动状态估计方法相比有竞争力。

Conclusion: 所提出的VSE方法在无模型的复杂动力过程状态估计中是有效的。

Abstract: We design a variational state estimation (VSE) method that provides a closed-form Gaussian posterior of an underlying complex dynamical process from (noisy) nonlinear measurements. The complex process is model-free. That is, we do not have a suitable physics-based model characterizing the temporal evolution of the process state. The closed-form Gaussian posterior is provided by a recurrent neural network (RNN). The use of RNN is computationally simple in the inference phase. For learning the RNN, an additional RNN is used in the learning phase. Both RNNs help each other learn better based on variational inference principles. The VSE is demonstrated for a tracking application - state estimation of a stochastic Lorenz system (a benchmark process) using a 2-D camera measurement model. The VSE is shown to be competitive against a particle filter that knows the Lorenz system model and a recently proposed data-driven state estimation method that does not know the Lorenz system model.

</details>


### [419] [LSR-Net: A Lightweight and Strong Robustness Network for Bearing Fault Diagnosis in Noise Environment](https://arxiv.org/abs/2601.10761)
*Junseok Lee,Jihye Shin,Sangyong Lee,Chang-Jae Chun*

Main category: eess.SP

TL;DR: 提出一种轻量级强鲁棒性网络LSR - Net，能在噪声环境中准确实时诊断轴承故障，经测试抗噪能力优且计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 旋转轴承在现代工业中重要，但易出现缺陷，延迟诊断会造成损失，且振动信号受环境和噪声影响，需要在噪声环境中进行准确的实时故障诊断。

Method: 设计去噪和特征增强模块DFEM，创建3通道2D矩阵，对去噪模块DM应用自适应剪枝；设计基于卷积的高效混洗CES块以实现轻量级模型设计，利用注意力机制和通道混洗平衡精度与计算复杂度。

Result: 在噪声环境下用振动信号验证，该模型相比基准模型有最佳抗噪能力，计算复杂度也最低。

Conclusion: 所提出的LSR - Net能在噪声环境中实现准确的实时故障诊断，具备轻量级和强鲁棒性。

Abstract: Rotating bearings play an important role in modern industries, but have a high probability of occurrence of defects because they operate at high speed, high load, and poor operating environments. Therefore, if a delay time occurs when a bearing is diagnosed with a defect, this may cause economic loss and loss of life. Moreover, since the vibration sensor from which the signal is collected is highly affected by the operating environment and surrounding noise, accurate defect diagnosis in a noisy environment is also important. In this paper, we propose a lightweight and strong robustness network (LSR-Net) that is accurate in a noisy environment and enables real-time fault diagnosis. To this end, first, a denoising and feature enhancement module (DFEM) was designed to create a 3-channel 2D matrix by giving several nonlinearity to the feature-map that passed through the denoising module (DM) block composed of convolution-based denoising (CD) blocks. Moreover, adaptive pruning was applied to DM to improve denoising ability when the power of noise is strong. Second, for lightweight model design, a convolution-based efficiency shuffle (CES) block was designed using group convolution (GConv), group pointwise convolution (GPConv) and channel split that can design the model while maintaining low parameters. In addition, the trade-off between the accuracy and model computational complexity that can occur due to the lightweight design of the model was supplemented using attention mechanisms and channel shuffle. In order to verify the defect diagnosis performance of the proposed model, performance verification was conducted in a noisy environment using a vibration signal. As a result, it was confirmed that the proposed model had the best anti-noise ability compared to the benchmark models, and the computational complexity of the model was also the lowest.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [420] [The 'Big Three' of Scientific Information: A comparative bibliometric review of Web of Science, Scopus, and OpenAlex](https://arxiv.org/abs/2601.21908)
*Daniel Torres-Salinas,Wenceslao Arroyo-Machado*

Main category: cs.DL

TL;DR: 本文对比研究Web of Science Core Collection、Scopus和OpenAlex三个多学科文献数据库，从文献覆盖、元数据质量和功能特点提供最新证据，还给出总结和建议。


<details>
  <summary>Details</summary>
Motivation: 为研究评估的战略决策提供三个主要多学科文献数据库在覆盖范围、元数据质量和功能特征方面的最新证据。

Method: 先系统回顾近期研究记录量、开放获取覆盖、语言多样性等的学术文献，再对2015 - 2024年进行原创文献计量分析。

Result: 未提及具体结果。

Conclusion: 给出十点执行摘要和五项建议。

Abstract: The present comparative study examines the three main multidisciplinary bibliographic databases, Web of Science Core Collection, Scopus, and OpenAlex, with the aim of providing up-to-date evidence on coverage, metadata quality, and functional features to help inform strategic decisions in research assessment. The report is structured into two complementary methodological sections. First, it presents a systematic review of recent scholarly literature that investigates record volume, open-access coverage, linguistic diversity, reference coverage, and metadata quality; this is followed by an original bibliometric analysis of the 2015-2024 period that explores longitudinal distribution, document types, thematic profiles, linguistic differences, and overlap between databases. The text concludes with a ten-point executive summary and five recommendations.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [421] [Post-Disaster Resource Redistribution and Cooperation Evolution Based on Two-Layer Network Evolutionary Games](https://arxiv.org/abs/2601.22021)
*Yu Chen,Genjiu Xu,Sinan Feng,Chaoqian Wang*

Main category: physics.soc-ph

TL;DR: 研究构建两层网络研究灾后避难所与受灾者互动，发现激励存在阈值效应，惩罚能抑制背叛，证实模型有通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 灾后资源稀缺和基础设施瘫痪对有效恢复提出挑战，避难所和受灾者的协调对构建社区恢复力至关重要，但二者间通过网络耦合的两层行为反馈演化机制不明。

Method: 构建两层网络，上层用灾后应急资源重新分配模型，下层用合作演化博弈描述受灾者内部互动，并进行蒙特卡罗模拟和对比分析。

Result: 激励存在阈值效应，适度激励促进合作、过度激励导致搭便车，有效惩罚可抑制背叛，针对性惩罚高连接避难所可增强合作，对比分析证实模型实用性。

Conclusion: 在灾后环境中，精确校准激励、实施有效制裁和进行结构靶向对促进组织和个人层面合作非常重要。

Abstract: In the aftermath of large-scale disasters, the scarcity of resources and the paralysis of infrastructure raise severe challenges to effective post-disaster recovery. Efficient coordination between shelters and victims plays a crucial role in building community resilience, yet the evolution of two-layer behavioral feedback between these two groups through network coupling remains insufficiently understood. Here, this study develops a two-layer network to capture the cross-layer coupling between shelters and victims. The upper layer uses a post-disaster emergency resource redistribution model within the framework of the public goods game, while the lower layer adopts a cooperative evolutionary game to describe internal victim interactions. Monte Carlo simulations on scale-free networks reveal threshold effects of incentives: moderate public goods enhancement and subsidies promote cooperation, whereas excessive incentives induce free-riding. In contrast, credible and well-executed punishment effectively suppresses defection. Targeted punishment of highly connected shelters significantly enhances cooperation under resource constraints. A comparative analysis using a network generated from the actual coordinates of Beijing shelters confirms the model's generality and practical applicability. The findings highlight the importance of calibrated incentives, enforceable sanctions, and structural targeting in fostering robust cooperation across organizational and individual levels in post-disaster environments.

</details>


### [422] [A costing framework for fusion power plants](https://arxiv.org/abs/2601.21724)
*Simon Woodruff*

Main category: physics.soc-ph

TL;DR: 本文总结2017 - 2024年支持ARPA - E的聚变电厂成本核算工作，介绍分析框架从关注资本成本到符合标准可审计的演变。


<details>
  <summary>Details</summary>
Motivation: 对聚变电厂进行准确的成本核算，探索成本降低途径，以支持ARPA - E相关项目。

Method: 早期用ARIES成本缩放关系估算，后受相关研究启发扩展方法，处理间接成本和评估非聚变岛系统降本途径；2023年将能力重构以符合相关账目规范，用自下而上子系统模型替代关键缩放关系。

Result: 开发了基于电子表格的FECONs代码和开源Python框架pyFECONs，实现子系统估算到标准账目的透明映射和LCOE的一致计算。

Conclusion: 建立了符合标准、可审计的聚变电厂成本核算能力和框架，有助于聚变电厂成本分析和管理。

Abstract: This paper summarizes and consolidates fusion power-plant costing work performed in support of ARPA-E from 2017 through 2024, and documents the evolution of the associated analysis framework from early capital-cost-focused studies to a standards-aligned, auditable costing capability. Early efforts applied ARIES-style cost-scaling relations to generate Nth-of-a-kind (NOAK) estimates and were calibrated through a pilot study with Bechtel and Decysive Systems to benchmark balance-of-plant (BOP) costs and validate plant-level reasonableness from an engineering, procurement, and construction (EPC) perspective. Subsequent work, informed by Lucid Catalyst studies of nuclear cost drivers, expanded the methodology to treat indirect costs explicitly and to evaluate cost-reduction pathways for non-fusion-island systems through design-for-cost practices, modularization, centralized manufacturing, and learning. As ARPA-E's fusion portfolio expanded, these methods were applied across BETHE and GAMOW concepts (and select ALPHA revisits), including enhanced treatment of tritium handling and plant integration supported by Princeton/PPPL expertise. In 2023 the capability was refactored to align with the IAEA-GEN-IV EMWG-EPRI code-of-accounts lineage, while key ARIES-derived scaling relations were replaced by bottom-up subsystem models for dominant fusion cost drivers (e.g., magnets, lasers, power supplies, and power-core components) coupled to physics-informed power balances and engineering-constrained radial builds. These developments were implemented in the spreadsheet-based Fusion Economics code (FECONs) and released as an open-source Python framework (pyFECONs), providing a transparent mapping from subsystem estimates to standardized accounts and a consistent computation of LCOE.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [423] [Pathwise Learning of Stochastic Dynamical Systems with Partial Observations](https://arxiv.org/abs/2601.21860)
*Nicole Tianjiao Yang*

Main category: math.OC

TL;DR: 提出基于变分推理的神经路径估计方法解决随机动力系统问题，并通过实验验证模型能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在间接观测到含噪声和非线性测量数据时重建和推理随机动力系统面临挑战，需要新方法。

Method: 推导解决滤波后验路径测度的随机控制问题，构建生成模型，通过观测过程样本路径的摊销，用噪声观测路径的嵌入学习控制。

Result: 能学习多模态、混沌或高维系统。

Conclusion: 所提方法有效可行，可用于非线性动力系统重建和推理。

Abstract: The reconstruction and inference of stochastic dynamical systems from data is a fundamental task in inverse problems and statistical learning. While surrogate modeling advances computational methods to approximate these dynamics, standard approaches typically require high-fidelity training data. In many practical settings, the data are indirectly observed through noisy and nonlinear measurement. The challenge lies not only in approximating the coefficients of the SDEs, but in simultaneously inferring the posterior updates given the observations. In this work, we present a neural path estimation approach to solve stochastic dynamical systems based on variational inference. We first derive a stochastic control problem that solve filtering posterior path measure corresponding to a pathwise Zakai equation. We then construct a generative model that maps the prior path measure to posterior measure through the controlled diffusion and the associated Randon-Nykodym derivative. Through an amortization of sample paths of the observation process, the control is learned by an embedding of the noisy observation paths. Thus, we learn the unknown prior SDE and the control can recover the conditional path measure given the observation sample paths and we learn an associated SDE which induces the same path measure. In the end, we perform experiments on nonlinear dynamical systems, demonstrating the model's ability to learn multimodal, chaotic, or high dimensional systems.

</details>


### [424] [On Approximate Computation of Critical Points](https://arxiv.org/abs/2601.21917)
*Amir Ali Ahmadi,Georgina Hall*

Main category: math.OC

TL;DR: 证明简单非凸函数临界点的近似计算是难处理的，与常见观点相悖。


<details>
  <summary>Details</summary>
Motivation: 挑战非凸优化中近似计算临界点是易处理任务的普遍看法。

Method: 证明若存在多项式时间算法能对特定多项式输出满足条件的点，则P=NP，还在额外结构假设下证明难度结果。

Result: 得出计算简单非凸函数临界点的粗近似是难处理的结论。

Conclusion: 近似计算非凸函数临界点并非易处理任务，与普遍认知不同。

Abstract: We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in $n$ variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most $2^n$ whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex optimization, approximate computation of critical points is a tractable task.

</details>


### [425] [The augmented NLP bound for maximum-entropy remote sampling](https://arxiv.org/abs/2601.20970)
*Gabriel Ponte,Marcia Fampa,Jon Lee*

Main category: math.OC

TL;DR: 本文研究最大熵远程采样问题（MERSP）上界，建立已有两种上界的支配关系，提出增强NLP界，给出理论保证，引入对角缩放技术，通过实验证明方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决MERSP计算上界问题，改进已有方法。

Method: 建立两种已有上界的支配关系，提出基于凸松弛的增强NLP界，引入对角缩放技术。

Result: 建立了两种已有上界的支配关系、增强NLP界严格优于普通NLP界、能对满足条件的秩亏协方差矩阵求上界，实验证明方法有效。

Conclusion: 提出的方法在计算MERSP上界方面推进了现有技术水平。

Abstract: The maximum-entropy remote sampling problem (MERSP) is to select a subset of s random variables from a set of n random variables, so as to maximize the information concerning a set of target random variables that are not directly observable. We assume throughout that the set of all of these random variables follows a joint Gaussian distribution, and that we have the covariance matrix available. Finally, we measure information using Shannon's differential entropy.
  The main approach for exact solution of moderate-sized instances of MERSP has been branch-and-bound, and so previous work concentrated on upper bounds. Prior to our work, there were two upper-bounding methods for MERSP: the so-called NLP bound and the spectral bound, both introduced 25 years ago. We are able now to establish domination results between these two upper bounds. We propose an ``augmented NLP bound'' based on a subtle convex relaxation. We provide theoretical guarantees, giving sufficient conditions under which the augmented NLP bound strictly dominates the ordinary NLP bound. In addition, the augmented NLP formulation allows us to derive upper bounds for rank-deficient covariance matrices when they satisfy a technical condition. This is in contrast to the earlier work on the ordinary NLP bound that worked with only positive definite covariance matrices. Finally, we introduce a novel and very effective diagonal-scaling technique for MERSP, employing a positive vector of parameters. Numerical experiments on benchmark instances demonstrate the effectiveness of our approaches in advancing the state of the art for calculating upper bounds on MERSP.

</details>


### [426] [Solving the Offline and Online Min-Max Problem of Non-smooth Submodular-Concave Functions: A Zeroth-Order Approach](https://arxiv.org/abs/2601.21243)
*Amir Ali Farzin,Yuen-Man Pun,Philipp Braun,Tyler Summers,Iman Shames*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider max-min and min-max problems with objective functions that are possibly non-smooth, submodular with respect to the minimiser and concave with respect to the maximiser. We investigate the performance of a zeroth-order method applied to this problem. The method is based on the subgradient of the Lovász extension of the objective function with respect to the minimiser and based on Gaussian smoothing to estimate the smoothed function gradient with respect to the maximiser. In expectation sense, we prove the convergence of the algorithm to an $ε$-saddle point in the offline case. Moreover, we show that, in the expectation sense, in the online setting, the algorithm achieves $O(\sqrt{N\bar{P}_N})$ online duality gap, where $N$ is the number of iterations and $\bar{P}_N$ is the path length of the sequence of optimal decisions. The complexity analysis and hyperparameter selection are presented for all the cases. The theoretical results are illustrated via numerical examples.

</details>


### [427] [Manifold constrained steepest descent](https://arxiv.org/abs/2601.21487)
*Kaiwei Yang,Lexiao Lai*

Main category: math.OC

TL;DR: 提出流形约束最速下降法MCSD及其随机动量变体和Stiefel流形上的SPEL，有收敛保证，实验显示性能更佳。


<details>
  <summary>Details</summary>
Motivation: 将基于范数约束线性最小化预言机（LMO）的优化器扩展到流形约束问题较难，常导致嵌套循环方案。

Method: 提出单循环框架MCSD，通过LMO选择黎曼梯度的最速下降方向，再通过投影回到流形；还提出SPEL，可通过快速矩阵符号计算实现可扩展实施。

Result: 为MCSD及其随机动量变体建立了收敛保证；在PCA等实验中展现出比标准黎曼基线和现有流形感知LMO方法更好的稳定性和竞争力。

Conclusion: MCSD及其SPEL变体在流形约束优化问题上是有效的，能提升性能。

Abstract: Norm-constrained linear minimization oracle (LMO)-based optimizers such as spectral gradient descent and Muon are attractive in large-scale learning, but extending them to manifold-constrained problems is nontrivial and often leads to nested-loop schemes that solve tangent-space subproblems iteratively. We propose \emph{Manifold Constrained Steepest Descent} (MCSD), a single-loop framework for optimization over manifolds that selects a norm-induced steepest-descent direction via an LMO applied to the Riemannian gradient, and then returns to the manifold via projection. Under standard smoothness assumptions, we establish convergence guarantees for MCSD and a stochastic momentum variant. We further introduce \emph{SPEL}, the spectral-norm specialization of MCSD on the Stiefel manifold, which admits scalable implementations via fast matrix sign computations. Experiments on PCA, orthogonality-constrained CNNs, and manifold-constrained LLM adapter tuning demonstrate improved stability and competitive performance relative to standard Riemannian baselines and existing manifold-aware LMO methods.

</details>


### [428] [Batched First-Order Methods for Parallel LP Solving in MIP](https://arxiv.org/abs/2601.21990)
*Nicolas Blin,Stefano Gualandi,Christopher Maes,Andrea Lodi,Bartolomeo Stellato*

Main category: math.OC

TL;DR: 提出在GPU上并行求解多个线性规划的批量一阶方法，通过案例证明有效性并确定算法优势场景。


<details>
  <summary>Details</summary>
Motivation: 为有效求解混合整数规划技术中出现的批量相关线性规划问题，充分利用GPU计算能力。

Method: 将原始 - 对偶混合梯度算法扩展，利用矩阵 - 矩阵运算替代重复的矩阵 - 向量运算。

Result: 在多种案例研究中证明方法有效，确定一阶方法比基于单纯形传统求解器表现更优的问题规模。

Conclusion: 这是充分利用GPU能力设计和开发整数规划算法的重要一步，部分特定操作应全量在GPU上执行。

Abstract: We present a batched first-order method for solving multiple linear programs in parallel on GPUs. Our approach extends the primal-dual hybrid gradient algorithm to efficiently solve batches of related linear programming problems that arise in mixed-integer programming techniques such as strong branching and bound tightening. By leveraging matrix-matrix operations instead of repeated matrix-vector operations, we obtain significant computational advantages on GPU architectures. We demonstrate the effectiveness of our approach on various case studies and identify the problem sizes where first-order methods outperform traditional simplex-based solvers depending on the computational environment one can use. This is a significant step for the design and development of integer programming algorithms tightly exploiting GPU capabilities where we argue that some specific operations should be allocated to GPUs and performed in full instead of using light-weight heuristic approaches on CPUs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [429] [Independent Component Discovery in Temporal Count Data](https://arxiv.org/abs/2601.21696)
*Alexandre Chaussard,Anna Bonnet,Sylvain Le Corff*

Main category: stat.ME

TL;DR: 本文提出用于时间计数数据独立成分分析的生成框架，建立模型可识别性，提出高效推理程序，经模拟数据和肠道微生物组研究验证。


<details>
  <summary>Details</summary>
Motivation: 数据收集技术进步使时间计数观测数据增多，需要适配的建模方法。

Method: 引入结合制度自适应动态与泊松对数正态发射的生成框架，提出高效的摊销变分推理程序来学习参数。

Result: 模拟数据实验评估了不同设置下混合函数和潜源的恢复情况；体内纵向肠道微生物组研究揭示了与临床扰动一致的微生物共变模式和制度转变。

Conclusion: 所提出的模型和方法有效可行，能用于时间计数数据的表示学习和扰动分析。

Abstract: Advances in data collection are producing growing volumes of temporal count observations, making adapted modeling increasingly necessary. In this work, we introduce a generative framework for independent component analysis of temporal count data, combining regime-adaptive dynamics with Poisson log-normal emissions. The model identifies disentangled components with regime-dependent contributions, enabling representation learning and perturbations analysis. Notably, we establish the identifiability of the model, supporting principled interpretation. To learn the parameters, we propose an efficient amortized variational inference procedure. Experiments on simulated data evaluate recovery of the mixing function and latent sources across diverse settings, while an in vivo longitudinal gut microbiome study reveals microbial co-variation patterns and regime shifts consistent with clinical perturbations.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [430] [Sustainable Materials Discovery in the Era of Artificial Intelligence](https://arxiv.org/abs/2601.21527)
*Sajid Mannan,Rupert J. Myers,Rohit Batra,Rocio Mercado,Lothar Wondraczek,N. M. Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: 文章指出当前AI材料发现工作流重性能轻可持续性问题，提出将上游ML辅助材料发现与下游生命周期评估集成到ML - LCA框架，并通过案例验证，还指出实现该框架的要求。


<details>
  <summary>Details</summary>
Motivation: 当前AI材料发现工作流先优化性能，后评估可持续性，导致效率低下，原子尺度设计与生命周期评估存在脱节问题。

Method: 提出将上游机器学习辅助材料发现与下游生命周期评估集成到统一的ML - LCA环境，该框架包含信息提取、数据库协调等五个组件。

Result: 通过玻璃、水泥等案例研究，证明了ML - LCA框架的必要性和可行性，同时识别出特定材料的集成挑战。

Conclusion: 实现ML - LCA需要在数据基础设施、评估方法、多目标优化和监管协调等方面协同推进，以实现可持续材料的设计发现。

Abstract: Artificial intelligence (AI) has transformed materials discovery, enabling rapid exploration of chemical space through generative models and surrogate screening. Yet current AI workflows optimize performance first, deferring sustainability to post synthesis assessment. This creates inefficiency by the time environmental burdens are quantified, resources have been invested in potentially unsustainable solutions. The disconnect between atomic scale design and lifecycle assessment (LCA) reflects fundamental challenges, data scarcity across heterogeneous sources, scale gaps from atoms to industrial systems, uncertainty in synthesis pathways, and the absence of frameworks that co-optimize performance with environmental impact. We propose to integrate upstream machine learning (ML) assisted materials discovery with downstream lifecycle assessment into a uniform ML-LCA environment. The framework ML-LCA integrates five components, information extraction for building materials-environment knowledge bases, harmonized databases linking properties to sustainability metrics, multi-scale models bridging atomic properties to lifecycle impacts, ensemble prediction of manufacturing pathways with uncertainty quantification, and uncertainty-aware optimization enabling simultaneous performance-sustainability navigation. Case studies spanning glass, cement, semiconductor photoresists, and polymers demonstrate both necessity and feasibility while identifying material-specific integration challenges. Realizing ML-LCA demands coordinated advances in data infrastructure, ex-ante assessment methodologies, multi-objective optimization, and regulatory alignment enabling the discovery of materials that are sustainable by design rather than by chance.

</details>


### [431] [MEIDNet: Multimodal generative AI framework for inverse materials design](https://arxiv.org/abs/2601.22009)
*Anand Babu,Rogério Almeida Gouvêa,Pierre Vandergheynst,Gian-Marco Rignanese*

Main category: cond-mat.mtrl-sci

TL;DR: 提出MEIDNet框架，结合生成逆设计与多模态学习，加速化学结构空间探索，展示了高学习效率和生成低带隙钙钛矿结构的潜力。


<details>
  <summary>Details</summary>
Motivation: 加速化学结构空间探索，发现满足预定义属性目标的材料。

Method: 通过对比学习联合学习结构信息和材料属性，用等变图神经网络编码结构，结合生成逆设计与多模态学习，实施课程学习策略。

Result: MEIDNet通过跨模态学习融合三种模态，余弦相似度达0.96；学习效率比传统训练技术高约60倍；生成低带隙钙钛矿结构的SUN率为13.6%。

Conclusion: 逆设计框架具有可扩展性和适应性，为跨多种模态的化学空间通用学习铺平道路。

Abstract: In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [432] [Test-Time Compute Games](https://arxiv.org/abs/2601.21839)
*Ander Artola Velasco,Dimitrios Rontogiannis,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CY

TL;DR: 研究指出LLM即服务市场存在社会效率问题，提出反向二价拍卖机制解决，并通过实验进行验证。


<details>
  <summary>Details</summary>
Motivation: 测试时计算策略虽提升大语言模型推理能力，但增加用户成本，且市场存在社会效率问题，提供商有增加计算量的经济动机。

Method: 引入反向二价拍卖机制，提供商投标价格和质量，用户按中标者相对第二高投标者的边际价值成比例付款。

Result: 文中未明确提及实验具体结果。

Conclusion: 未明确提及结论，但推测反向二价拍卖机制可解决LLM即服务市场的效率问题。

Abstract: Test-time compute has emerged as a promising strategy to enhance the reasoning abilities of large language models (LLMs). However, this strategy has in turn increased how much users pay cloud-based providers offering LLM-as-a-service, since providers charge users for the amount of test-time compute they use to generate an output. In our work, we show that the market of LLM-as-a-service is socially inefficient: providers have a financial incentive to increase the amount of test-time compute, even if this increase contributes little to the quality of the outputs. To address this inefficiency, we introduce a reverse second-price auction mechanism where providers bid their offered price and (expected) quality for the opportunity to serve a user, and users pay proportionally to the marginal value generated by the winning provider relative to the second-highest bidder. To illustrate and complement our theoretical results, we conduct experiments with multiple instruct models from the $\texttt{Llama}$ and $\texttt{Qwen}$ families, as well as reasoning models distilled from $\texttt{DeepSeek-R1}$, on math and science benchmark datasets.

</details>


### [433] [Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions](https://arxiv.org/abs/2601.21837)
*Xiaoshan Yu,Shangshang Yang,Ziwen Wang,Haiping Ma,Xingyi Zhang*

Main category: cs.CY

TL;DR: 本文对可信智能教育进行系统结构化综述，先划分任务类别，再从五方面回顾研究并总结方法策略，最后总结挑战和展望方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对可信智能教育的整体视角和清晰系统分类，本文旨在解决这些局限。

Method: 将智能教育分为五类任务，从安全隐私、鲁棒性、公平性、可解释性和可持续性五个可信视角回顾现有研究。

Result: 总结并分类研究方法和解决策略。

Conclusion: 提供了连贯的参考框架，有助于更清晰理解智能教育的可信性。

Abstract: In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.

</details>


### [434] ["Unlimited Realm of Exploration and Experimentation": Methods and Motivations of AI-Generated Sexual Content Creators](https://arxiv.org/abs/2601.21028)
*Jaron Mink,Lucy Qin,Elissa M. Redmiles*

Main category: cs.CY

TL;DR: AI生成性内容快速发展，但对创作者了解少，通过访谈28位创作者揭示其动机。


<details>
  <summary>Details</summary>
Motivation: AI生成性内容引发诸多问题且对创作者情况了解不足，为有效治理开展研究。

Method: 对28位从爱好者到创业者等不同身份的AI生成性内容创作者进行访谈。

Result: 创作者动机多样，包括性探索、创意表达、技术实验等，少数存在创建非自愿私密图像情况。

Conclusion: 研究有助于了解AI生成性内容创作者，为该领域有效治理提供信息。

Abstract: AI-generated media is radically changing the way content is both consumed and produced on the internet, and in no place is this potentially more visible than in sexual content. AI-generated sexual content (AIG-SC) is increasingly enabled by an ecosystem of individual AI developers, specialized third-party applications, and foundation model providers. AIG-SC raises a number of concerns from old debates about the line between pornography and obscenity, to newer debates about fair use and labor displacement (in this case, of sex workers), and spurred new regulations to curb the spread of non-consensual intimate imagery (NCII) created using the same technology used to create AIG-SC. However, despite the growing prevalence of AIG-SC, little is known about its creators, their motivations, and what types of content they produce. To inform effective governance in this space, we perform an in-depth study to understand what AIG-SC creators make, along with how and why they make it. Interviews of 28 AIG-SC creators, ranging from hobbyists to entrepreneurs to those who moderate communities of hundreds of thousands of other creators, reveal a wide spectrum of motivations, including sexual exploration, creative expression, technical experimentation, and in a handful of cases, the creation of NCII.

</details>


### [435] [Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US](https://arxiv.org/abs/2601.21815)
*Seongchan Park,Jaehong Kim,Hyeonseung Kim,Heejin Bin,Sue Moon,Wonjae Lee*

Main category: cs.CY

TL;DR: 研究主流新闻频道在YouTube上的道德情感框架如何影响韩美用户行为，发现批判他人道德的言辞能提高参与度，并公开了分类器。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济下，理解媒体修辞如何塑造受众参与度至关重要，因此研究主流新闻频道的道德情感框架对韩美用户行为的影响。

Method: 开发多模态道德情感分类器，利用微调视觉语言模型，在人类标注的多模态数据集上训练模型，应用于约40万个新闻视频，分析用户参与度。

Result: 批判他人道德的言辞能提高韩美用户的各类参与度，从被动观看到主动评论。

Conclusion: 道德愤怒是有效的情感策略，可能引发对批判言辞的滥用担忧，会加深群体间分化，公开分类器以促进未来研究。

Abstract: Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.

</details>


### [436] [Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems](https://arxiv.org/abs/2601.21963)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: 本文更新生成式AI与虚假信息研究视角，介绍新威胁，提出JudgeGPT和RogueGPT工具，研究人类对AI虚假信息感知，发现检测能力提升但竞争持续，并讨论缓解策略。


<details>
  <summary>Details</summary>
Motivation: 在2024年调查后，更新生成式AI与虚假信息研究视角，从文献综述过渡到实际对策。

Method: 提出JudgeGPT评估人类对AI新闻的感知，以及RogueGPT用于生成可控研究刺激，构建实验管道。

Result: 检测能力有所提高，但生成与检测的竞争仍在继续。

Conclusion: 讨论缓解策略，为研究AI对信息质量的不利影响做出贡献。

Abstract: Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.

</details>


### [437] [Investigating Associational Biases in Inter-Model Communication of Large Generative Models](https://arxiv.org/abs/2601.22093)
*Fethiye Irmak Dogan,Yuval Weiss,Kajal Patel,Jiaee Cheong,Hatice Gunes*

Main category: cs.CY

TL;DR: 研究生成式AI在人机交互管道中社会偏差，发现人口统计漂移，提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI存在社会偏差，在人机交互管道中会持续传播，影响以人为中心的感知任务。

Method: 聚焦人类活动和情感表达，利用RAF - DB和PHASE数据集，通过图像生成和描述交替的交互管道研究关联演变，用可解释性管道量化漂移并评估是否系统。

Result: 发现行为和情感向年轻表征漂移，情感向女性表征漂移，部分预测依赖虚假视觉区域。

Conclusion: 提出涵盖数据、训练和部署干预的缓解策略，强调在以人为中心AI系统中部署互联模型需谨慎保障。

Abstract: Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [438] [SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks](https://arxiv.org/abs/2601.22024)
*Abhishek Duttagupta,MohammadErfan Jabbari,Claudio Fiandrino,Marco Fiore,Joerg Widmer*

Main category: cs.NI

TL;DR: 本文提出SymbXRL技术解决深度强化学习（DRL）难以解释的问题，在实际网络管理用例中验证其有效性，能提升解释语义并实现显式代理控制。


<details>
  <summary>Details</summary>
Motivation: 未来6G网络依赖DRL实时优化网络决策，但DRL智能体难以解释，阻碍其在生产环境应用。

Method: 提出SymbXRL技术，利用符号AI，通过直观符号和规则描述关键概念及关系以生成解释。

Result: 在实际网络管理用例中验证SymbXRL，不仅提升解释语义，还可实现显式代理控制，使基于意图的编程动作引导的中位数累积奖励比纯DRL解决方案提高12%。

Conclusion: SymbXRL技术有助于去除DRL难以解释这一关键障碍，为实际应用铺平道路。

Abstract: The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.

</details>


### [439] [SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control](https://arxiv.org/abs/2601.22044)
*MohammadErfan Jabbari,Abhishek Duttagupta,Claudio Fiandrino,Leonardo Bonati,Salvatore D'Oro,Michele Polese,Marco Fiore,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出SIA解释器让预测增强的DRL代理透明化，在网络用例中评估发现问题并实现改进，降低下一代移动网络主动控制的门槛。


<details>
  <summary>Details</summary>
Motivation: 传统DRL代理有时间短视问题，预测增强的代理是黑盒，操作员难以判断预测对决策的影响及证明复杂度的合理性。

Method: 提出SIA，融合符号AI抽象和每个KPI的知识图谱生成解释，引入影响得分指标。

Result: SIA速度比现有XAI方法快200多倍，在三个网络用例中发现隐藏问题，实现视频流平均比特率提高9%，RAN切片奖励提高25%。

Conclusion: SIA使预期DRL透明且可调整，降低下一代移动网络主动控制的障碍。

Abstract: Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [440] [Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models](https://arxiv.org/abs/2601.21272)
*Koichiro Moriya,Akihiko Noda*

Main category: econ.EM

TL;DR: 本文提出新的多元模型设定检验方法，推广Durbin回归，蒙特卡罗实验和实证应用显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 提出能在更弱依赖条件下保持一致性且具有渐近有效性的多元模型设定检验方法。

Method: 将Durbin回归推广至看似不相关回归框架，将Durbin方法重构为GLS类估计量，显式建模跨方程依赖及联合二阶动态。

Result: 蒙特卡罗实验表明相关Wald检验在有限样本中控制效果更好、功效更具竞争力；实证应用显示该方法能提供稳定推断。

Conclusion: 所提方法对多方程设定检验实用且有效。

Abstract: This paper proposes a new multivariate model specification test that generalizes Durbin regression to a seemingly unrelated regression framework and reframes the Durbin approach as a GLS-class estimator. The proposed estimator explicitly models cross-equation dependence and the joint second-order dynamics of regressors and disturbances. It remains consistent under a comparatively weak dependence condition in which conventional OLS- and GLS-based estimators can be inconsistent, and it is asymptotically efficient under stronger conditions. Monte Carlo experiments indicate that the associated Wald test achieves improved size control and competitive power in finite samples, especially when combined with a bootstrap-based bias correction. An empirical application further illustrates that the proposed procedure delivers stable inference and is practically useful for multi-equation specification testing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [441] [Differential Dynamic Causal Nets: Model Construction, Identification and Group Comparisons](https://arxiv.org/abs/2601.21478)
*Kang You,Gary Green,Jian Zhang*

Main category: q-bio.NC

TL;DR: 提出从脑电图数据构建差分因果网络的新方法，用进化优化算法进行参数推断，在合成和真实脑电图数据上评估，发现癫痫发作前后网络功能中断的证据。


<details>
  <summary>Details</summary>
Motivation: 解决从微观到宏观尺度的大脑系统病理生理建模在组间比较中的困难，因难以对数千个神经元的相互作用建模。

Method: 提出基于条件耦合神经元电路构建差分因果网络，网络节点代表参数化局部神经系统，边代表有传输参数的连接，使用源于随机微分方程Chen - Fliess展开的损失函数开发进化优化算法进行参数推断。

Result: 方法应用于拟合耦合Jansen - Rit局部模型，在合成和真实脑电图数据上评估，在真实脑电图数据分析中，追踪癫痫活动大脑中表征动态因果关系的参数变化。

Conclusion: 发现癫痫发作前后由于兴奋 - 抑制性中间神经元失衡和大脑连接改变导致的网络功能中断。

Abstract: Pathophysiolpgical modelling of brain systems from microscale to macroscale remains difficult in group comparisons partly because of the infeasibility of modelling the interactions of thousands of neurons at the scales involved. Here, to address the challenge, we present a novel approach to construct differential causal networks directly from electroencephalogram (EEG) data. The proposed network is based on conditionally coupled neuronal circuits which describe the average behaviour of interacting neuron populations that contribute to observed EEG data. In the network, each node represents a parameterised local neural system while directed edges stand for node-wise connections with transmission parameters. The network is hierarchically structured in the sense that node and edge parameters are varying in subjects but follow a mixed-effects model. A novel evolutionary optimisation algorithm for parameter inference in the proposed method is developed using a loss function derived from Chen-Fliess expansions of stochastic differential equations. The method is demonstrated by application to the fitting of coupled Jansen-Rit local models. The performance of the proposed method is evaluated on both synthetic and real EEG data. In the real EEG data analysis, we track changes in the parameters that characterise dynamic causality within brains that demonstrate epileptic activity. We show evidence of network functional disruptions, due to imbalance of excitatory-inhibitory interneurons and altered epileptic brain connectivity, before and during seizure periods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [442] [Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions](https://arxiv.org/abs/2601.21477)
*Tobias Schmidt,Kai Cui*

Main category: cs.MA

TL;DR: 本文提出大稀疏图上的平均场控制（MFC）严格框架，解决传统MFC假设局限性问题，建立理论基础并实验验证相关算法。


<details>
  <summary>Details</summary>
Motivation: 传统MFC依赖可交换性的严格假设，与现实网络结构存在差距，需要解决该问题。

Method: 重新定义系统状态，证明与时间范围相关的局部性，提出新颖动态规划原理，使用图神经网络进行策略梯度算法。

Result: 使无限维控制问题可解，从形式和实验上验证图神经网络在策略梯度算法中应用的合理性。

Conclusion: 框架可将经典MFC作为退化情况恢复，能在复杂稀疏拓扑上进行高效、有理论依据的控制。

Abstract: Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.

</details>


### [443] [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](https://arxiv.org/abs/2601.22041)
*Naomi Pitzer,Daniela Mihai*

Main category: cs.MA

TL;DR: 研究异构多步二进制通信游戏，对比单模态和多模态系统通信特点，证明意义分布编码，展示跨系统通信情况，为跨模态表征研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有涌现通信研究多忽略现实场景的感知异质性，本文旨在研究感知异质情况下的涌现通信。

Method: 研究异构多步二进制通信游戏，进行比特扰动实验和互操作性分析。

Result: 多模态系统收敛到基于感知输入的类一致消息；单模态系统通信更高效；意义以分布而非组合方式编码；不同感知世界训练的系统需微调才能跨系统通信。

Conclusion: 涌现通信可作为研究代理跨异构模态适应和转移表征的框架，为理论和实验开辟新方向。

Abstract: Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [444] [Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling](https://arxiv.org/abs/2601.21856)
*Shujaat Khan,Syed Muhammad Atif,Jaeyoung Huh,Syed Saad Azhar*

Main category: eess.IV

TL;DR: 提出盲的自监督增强框架对超声图像去卷积和去噪，在多数据集上表现好，能提升分割效果，提供实用增强方法。


<details>
  <summary>Details</summary>
Motivation: 超声图像解读受斑点、模糊和伪影等影响，有监督增强方法条件难满足，需新方法。

Method: 用Swin卷积U - Net结合物理引导退化模型，从训练帧提取旋转/裁剪块，合成输入，用非局部低秩去噪获取超声图像目标。

Result: 在多数据集上PSNR/SSIM最高，与其他方法相比在噪声下表现更好，PSF研究显示分辨率恢复，能提升分割Dice系数。

Conclusion: 该方法实用、假设少，能在不同数据集、扫描仪和退化类型上实现鲁棒超声增强。

Abstract: Ultrasound (US) interpretation is hampered by multiplicative speckle, acquisition blur from the point-spread function (PSF), and scanner- and operator-dependent artifacts. Supervised enhancement methods assume access to clean targets or known degradations; conditions rarely met in practice. We present a blind, self-supervised enhancement framework that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \emph{physics-guided} degradation model. From each training frame, we extract rotated/cropped patches and synthesize inputs by (i) convolving with a Gaussian PSF surrogate and (ii) injecting noise via either spatial additive Gaussian noise or complex Fourier-domain perturbations that emulate phase/magnitude distortions. For US scans, clean-like targets are obtained via non-local low-rank (NLLR) denoising, removing the need for ground truth; for natural images, the originals serve as targets. Trained and validated on UDIAT~B, JNU-IFM, and XPIE Set-P, and evaluated additionally on a 700-image PSFHS test set, the method achieves the highest PSNR/SSIM across Gaussian and speckle noise levels, with margins that widen under stronger corruption. Relative to MSANN, Restormer, and DnCNN, it typically preserves an extra $\sim$1--4\,dB PSNR and 0.05--0.15 SSIM in heavy Gaussian noise, and $\sim$2--5\,dB PSNR and 0.05--0.20 SSIM under severe speckle. Controlled PSF studies show reduced FWHM and higher peak gradients, evidence of resolution recovery without edge erosion. Used as a plug-and-play preprocessor, it consistently boosts Dice for fetal head and pubic symphysis segmentation. Overall, the approach offers a practical, assumption-light path to robust US enhancement that generalizes across datasets, scanners, and degradation types.

</details>


### [445] [Denoising and Baseline Correction of Low-Scan FTIR Spectra: A Benchmark of Deep Learning Models Against Traditional Signal Processing](https://arxiv.org/abs/2601.20905)
*Azadeh Mokari,Shravan Raghunathan,Artem Shydliukh,Oleg Ryabchykov,Christoph Krafft,Thomas Bocklitz*

Main category: eess.IV

TL;DR: 提出物理信息级联Unet用于FTIR成像，分离去噪和基线校正任务，在FaDu细胞数据集上表现优于其他方法，能加速成像。


<details>
  <summary>Details</summary>
Motivation: 传统FTIR成像需大量信号平均以降噪和漂移，限制临床速度，深度学习重建光谱存在分离噪声和基线漂移的不适定问题及泛化性差问题。

Method: 提出物理信息级联Unet，用新的确定性物理桥分离任务，嵌入SNIP层强制网络分离噪声和化学信号。

Result: 级联模型在RMSE上比原始单扫描输入降低51.3%，优于单Unet和传统工作流，消除光谱幻觉，更好保留峰强度。

Conclusion: 级联Unet是诊断级FTIR成像的可靠解决方案，使成像速度比现有方法快32倍。

Abstract: High-quality Fourier Transform Infrared (FTIR) imaging usually needs extensive signal averaging to reduce noise and drift which severely limits clinical speed. Deep learning can accelerate imaging by reconstructing spectra from rapid, single-scan inputs. However, separating noise and baseline drift simultaneously without ground truth is an ill-posed inverse problem. Standard black-box architectures often rely on statistical approximations that introduce spectral hallucinations or fail to generalize to unstable atmospheric conditions. To solve these issues we propose a physics-informed cascade Unet that separates denoising and baseline correction tasks using a new, deterministic Physics Bridge. This architecture forces the network to separate random noise from chemical signals using an embedded SNIP layer to enforce spectroscopic constraints instead of learning statistical approximations. We benchmarked this approach against a standard single Unet and a traditional Savitzky-Golay/SNIP workflow. We used a dataset of human hypopharyngeal carcinoma cells (FaDu). The cascade model outperformed all other methods, achieving a 51.3% reduction in RMSE compared to raw single-scan inputs, surpassing both the single Unet (40.2%) and the traditional workflow (33.7%). Peak-aware metrics show that the cascade architecture eliminates spectral hallucinations found in standard deep learning. It also preserves peak intensity with much higher fidelity than traditional smoothing. These results show that the cascade Unet is a robust solution for diagnostic-grade FTIR imaging. It enables imaging speeds 32 times faster than current methods.

</details>


### [446] [ECGFlowCMR: Pretraining with ECG-Generated Cine CMR Improves Cardiac Disease Classification and Phenotype Prediction](https://arxiv.org/abs/2601.20904)
*Xiaocheng Fang,Zhengyao Ding,Jieyi Cai,Yujie Xiao,Bo Liu,Jiarui Jin,Haoyu Wang,Guangkun Nie,Shun Huang,Ting Chen,Hongyan Li,Shenda Hong*

Main category: eess.IV

TL;DR: 提出ECGFlowCMR框架，可从心电图生成心脏磁共振电影序列，能用于预训练并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 心脏磁共振成像成本高、依赖专家标注，而心电图廉价易获取，为用其生成心脏磁共振图像提供契机，同时要解决跨模态时间不匹配和解剖可观测性差距问题。

Method: 提出ECGFlowCMR框架，集成相位感知掩码自编码器（PA - MAE）和解剖运动解纠缠流（AMDF）。

Result: 在英国生物银行和专有临床数据集上的大量实验表明，ECGFlowCMR能从心电图输入生成逼真的心脏磁共振电影序列。

Conclusion: ECGFlowCMR可实现可扩展的预训练，提升下游心脏疾病分类和表型预测任务的性能。

Abstract: Cardiac Magnetic Resonance (CMR) imaging provides a comprehensive assessment of cardiac structure and function but remains constrained by high acquisition costs and reliance on expert annotations, limiting the availability of large-scale labeled datasets. In contrast, electrocardiograms (ECGs) are inexpensive, widely accessible, and offer a promising modality for conditioning the generative synthesis of cine CMR. To this end, we propose ECGFlowCMR, a novel ECG-to-CMR generative framework that integrates a Phase-Aware Masked Autoencoder (PA-MAE) and an Anatomy-Motion Disentangled Flow (AMDF) to address two fundamental challenges: (1) the cross-modal temporal mismatch between multi-beat ECG recordings and single-cycle CMR sequences, and (2) the anatomical observability gap due to the limited structural information inherent in ECGs. Extensive experiments on the UK Biobank and a proprietary clinical dataset demonstrate that ECGFlowCMR can generate realistic cine CMR sequences from ECG inputs, enabling scalable pretraining and improving performance on downstream cardiac disease classification and phenotype prediction tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [447] [Virtualization-based Penetration Testing Study for Detecting Accessibility Abuse Vulnerabilities in Banking Apps in East and Southeast Asia](https://arxiv.org/abs/2601.21258)
*Wei Minn,Phong Phan,Vikas K. Malviya,Benjamin Adolphi,Yan Naing Tun,Henning Benzon Treichl,Albert Ching,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: 研究针对FjordPhantom恶意软件对东亚和东南亚地区银行应用的威胁，分析应用易感性、防护措施有效性，并探讨检测与预防攻击的方法。


<details>
  <summary>Details</summary>
Motivation: Android银行应用便利但吸引网络罪犯，FjordPhantom恶意软件利用漏洞威胁金融数据安全，需研究应对方法。

Method: 对该地区银行应用受FjordPhantom影响的易感性进行实证研究，分析现有防护措施有效性。

Result: 未提及具体研究结果。

Conclusion: 未提及具体研究结论，主要是讨论检测和预防此类攻击的方法。

Abstract: Android banking applications have revolutionized financial management by allowing users to perform various financial activities through mobile devices. However, this convenience has attracted cybercriminals who exploit security vulnerabilities to access sensitive financial data. FjordPhantom, a malware identified by our industry collaborator, uses virtualization and hooking to bypass the detection of malicious accessibility services, allowing it to conduct keylogging, screen scraping, and unauthorized data access. This malware primarily affects banking and finance apps across East and Southeast Asia region where our industry partner's clients are primarily based in. It requires users to be deceived into installing a secondary malicious component and activating a malicious accessibility service. In our study, we conducted an empirical study on the susceptibility of banking apps in the region to FjordPhantom, analyzed the effectiveness of protective measures currently implemented in those apps, and discussed ways to detect and prevent such attacks by identifying and mitigating the vulnerabilities exploited by this malware.

</details>


### [448] [ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack](https://arxiv.org/abs/2601.20903)
*Xingwei Lin,Wenhao Lin,Sicong Cao,Jiahao Yu,Renke Huang,Lei Xue,Chunming Wu*

Main category: cs.CR

TL;DR: 本文提出ICON自动化多轮越狱框架，利用意图 - 上下文耦合现象高效构造权威风格上下文实施越狱攻击，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 多轮越狱攻击对大语言模型是关键威胁，现有方法构建上下文效率低且易陷入次优区域。

Method: 刻画意图 - 上下文耦合现象，提出ICON框架，通过先验引导语义路由构建权威风格上下文，结合局部提示细化和全局上下文切换的分层优化策略。

Result: 在八个SOTA大语言模型上实验，ICON实现了97.1%的平均攻击成功率。

Conclusion: ICON框架在多轮越狱攻击中有效，代码开源。

Abstract: Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1\%. Code is available at https://github.com/xwlin-roy/ICON.

</details>


### [449] [Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks](https://arxiv.org/abs/2601.21189)
*Arther Tian,Alex Ding,Frank Chen,Simon Wu,Aaron Chan*

Main category: cs.CR

TL;DR: 本文为成本感知的质量证明机制添加抗对手的共识形成方法，研究了鲁棒聚合规则和自适应信任加权共识，通过实验量化评估者可靠性，评估了四种对抗策略下的鲁棒性，结果显示鲁棒聚合有优势，还分析了评估者采样的权衡，为相关机制提供指导。


<details>
  <summary>Details</summary>
Motivation: 去中心化大语言模型推理网络需轻量级机制奖励高质量输出，但评估者异质性和恶意分数操纵会扭曲共识，本文旨在改进成本感知的质量证明机制，增强其抗攻击性。

Method: 添加抗对手的共识形成来扩展成本感知的质量证明机制，研究鲁棒聚合规则（如中位数和截尾均值）和自适应信任加权共识；用问答和摘要工作负载进行离线分析，评估四种对抗策略下的鲁棒性。

Result: 鲁棒聚合能改善与真实代理的共识一致性，降低对噪声和策略攻击的敏感性；评估者集增大时，评估者奖励降低、回报方差增加，推理奖励相对稳定。

Conclusion: 应将鲁棒共识作为成本感知质量证明的默认组件，为在对抗风险和资源约束下选择评估者采样参数提供实用指导。

Abstract: Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.
  This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.

</details>


### [450] [On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression](https://arxiv.org/abs/2601.21531)
*Xinwei Zhang,Hangcheng Liu,Li Bai,Hao Wang,Qingqing Ye,Tianwei Zhang,Haibo Hu*

Main category: cs.CR

TL;DR: 研究视觉标记压缩的大视觉语言模型的对抗鲁棒性，提出CAGE攻击方法，结果显示其比基线鲁棒准确率更低，强调需压缩感知的安全评估和防御。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的攻击会高估压缩大视觉语言模型的鲁棒性，存在优化 - 推理不匹配问题。

Method: 提出CAGE攻击方法，结合预期特征破坏和排名失真对齐。

Result: 在多种压缩机制和数据集上，CAGE比基线的鲁棒准确率更低。

Conclusion: 忽略压缩的鲁棒性评估可能过于乐观，高效大视觉语言模型需压缩感知的安全评估和防御。

Abstract: Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.

</details>


### [451] [Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise](https://arxiv.org/abs/2601.21628)
*Puwei Lian,Yujun Cai,Songze Li,Bingkun Bao*

Main category: cs.CR

TL;DR: 本文指出扩散模型微调时存在隐私风险，利用噪声调度无法完全消除语义信息的漏洞，提出简单有效的成员推理攻击，实验证明其能揭示成员信息。


<details>
  <summary>Details</summary>
Motivation: 扩散模型部署增加引发隐私担忧，微调模型易受攻击，现有针对扩散模型的成员推理攻击有局限。

Method: 利用噪声调度无法完全消除语义信息的漏洞，发现微调扩散模型捕捉到初始噪声中残留语义与原始图像的隐藏关联，提出将语义信息注入初始噪声，通过分析模型生成结果进行成员推理的攻击方法。

Result: 广泛实验表明语义初始噪声能强烈揭示成员信息。

Conclusion: 扩散模型易受成员推理攻击，存在隐私风险。

Abstract: Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.

</details>


### [452] [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/abs/2601.22159)
*Naufal Suryanto,Muzammal Naseer,Pengfei Li,Syed Talal Wasim,Jinhui Yi,Juergen Gall,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.CR

TL;DR: 本文创建针对网络安全的预训练数据和样本训练开源模型RedSage，还引入评测基准RedSage - Bench对模型评估，RedSage效果优于基线模型，且所有资源公开。


<details>
  <summary>Details</summary>
Motivation: 网络安全操作需要支持多样工作流且不暴露敏感数据的大语言模型辅助工具，现有的解决方案存在依赖有隐私风险的专有API或缺乏领域适配的问题。

Method: 1. 通过大规模网络过滤和手动收集，整理11.8B网络安全持续预训练数据；2. 设计代理增强管道生成266K多轮网络安全样本进行监督微调；3. 结合通用开源大语言模型数据训练RedSage；4. 引入RedSage - Bench进行严格评估，还在其他既定基准测试评估。

Result: 8B规模下，RedSage在网络安全基准测试中比基线模型最多高5.59分，在Open LLM Leaderboard任务中最多高5.05分。

Conclusion: 特定领域的代理增强和预/后训练既能增强网络安全专业知识，也有助于提高通用推理和指令遵循能力。

Abstract: Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [453] [Data-Driven Generation of Neutron Star Equations of State Using Variational Autoencoders](https://arxiv.org/abs/2601.21231)
*Alex Ross,Tianqi Zhao,Sanjay Reddy*

Main category: astro-ph.HE

TL;DR: 开发基于结构化变分自编码器（VAE）的机器学习模型来重建和生成中子星状态方程，该模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 重建和生成中子星状态方程，结合多信使数据进行状态方程的贝叶斯推断。

Method: 构建由编码器和解码器组成的VAE，编码器将高维状态方程数据映射到低维隐空间，解码器从隐表示重建完整状态方程，隐空间包含监督的中子星可观测量和潜在随机变量。

Result: 基于在Skyrme状态方程数据集上训练的VAE，含两个监督可观测量和一个潜在随机变量的隐空间能高保真重建Skyrme状态方程，对最大质量和规范半径的平均绝对百分比误差约为0.15%。

Conclusion: 所开发的基于VAE的机器学习模型能有效重建和生成满足天文约束的中子星状态方程。

Abstract: We develop a machine learning model based on a structured variational autoencoder (VAE) framework to reconstruct and generate neutron star (NS) equations of state (EOS). The VAE consists of an encoder network that maps high-dimensional EOS data into a lower-dimensional latent space and a decoder network that reconstructs the full EOS from the latent representation. The latent space includes supervised NS observables derived from the training EOS data, as well as latent random variables corresponding to additional unspecified EOS features learned automatically. Sampling the latent space enables the generation of new, causal, and stable EOS models that satisfy astronomical constraints on the supervised NS observables, while allowing Bayesian inference of the EOS incorporating additional multimessenger data, including gravitational waves from LIGO/Virgo and mass and radius measurements of pulsars. Based on a VAE trained on a Skyrme EOS dataset, we find that a latent space with two supervised NS observables, the maximum mass $(M_{\max})$ and the canonical radius $(R_{1.4})$, together with one latent random variable controlling the EOS near the crust--core transition, can already reconstruct Skyrme EOSs with high fidelity, achieving mean absolute percentage errors of approximately $(0.15\%)$ for $(M_{\max})$ and $(R_{1.4})$ derived from the decoder-reconstructed EOS.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [454] [Better without U: Impact of Selective Hubbard U Correction on Foundational MLIPs](https://arxiv.org/abs/2601.21056)
*Thomas Warford,Fabian L. Thiemann,Gábor Csányi*

Main category: physics.chem-ph

TL;DR: 研究发现fMLIPs训练数据存在因Hubbard U修正不一致导致的问题，提出简单修正方法并建议未来数据集排除+U。


<details>
  <summary>Details</summary>
Motivation: 解决fMLIPs训练依赖的数据库中因Hubbard U修正不一致导致的模型问题。

Method: 提出简单的每U修正原子移位方法对齐PBE+U和PBE能量

Result: 使用该方法训练的模型对氧在U修正元素板上的吸附能平均绝对误差更小

Conclusion: 建议未来fMLIP数据集排除+U，现有数据集可用事后修正方法改进。

Abstract: The training of foundational machine learning interatomic potentials (fMLIPs) relies on diverse databases with energies and forces calculated using ab initio methods. We show that fMLIPs trained on large datasets such as MPtrj, Alexandria, and OMat24 encode inconsistencies from the Materials Project's selective use of the Hubbard U correction, which is applied to certain transition metals only if O or F atoms are present in the simulation cell. This inconsistent use of +U creates two incompatible potential-energy surfaces (PES): a lower-energy GGA surface and a higher-energy GGA+U one. When trained on both, MLIPs interpolate between them, leading to systematic underbinding, or even spurious repulsion, between U-corrected metals and oxygen- or fluorine-containing species. Models such as MACE-OMAT and -MPA exhibit repulsion between U-corrected metals and their oxides, limiting their value for studying catalysis and oxidation. We link the severity of this pathology to the oxygen number density in U-corrected training configurations. This explains why OMAT-trained models are most affected and suggests the issue might worsen as expanding future datasets increasingly include configurations with low oxygen content, such as those generated through combinatorial exploration of multi-element or defect-containing systems.
  Our simple per-U-corrected-atom shift aligns PBE+U and PBE energies for identical structures, yielding a smoother PES compared to existing correction schemes, which target phase diagram accuracy. As a result, models trained on datasets with our shift applied exhibit smaller mean absolute errors for the adsorption energies of oxygen on U-corrected elemental slabs. Since datasets omitting +U entirely (e.g. MatPES, MP-ALOE) avoid these pathologies, we recommend excluding +U in future fMLIP datasets. For existing datasets, our post-hoc correction provides a low-cost improvement.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [455] [Spectral Gap of Metropolis Algorithms for Non-smooth Distributions under Isoperimetry](https://arxiv.org/abs/2601.21763)
*Shuigen Liu,Xin T. Tong*

Main category: math.PR

TL;DR: 本文推导了随机游走Metropolis和Metropolis调整Langevin算法在非光滑分布上的谱隙显式界，并将其扩展到满足Poincaré或对数Sobolev不等式的目标分布，有数值实验支持。


<details>
  <summary>Details</summary>
Motivation: 现有文献很少讨论非光滑目标分布下Metropolis算法谱隙的显式界，本文旨在填补这一空白。

Method: 推导随机游走Metropolis和Metropolis调整Langevin算法在非光滑分布上的谱隙显式界，并结合Goyal等人（2025）的结果进行扩展。

Result: 得到了非光滑分布上的谱隙显式界，并将其扩展到满足Poincaré或对数Sobolev不等式的目标分布，数值实验支持了理论结果。

Conclusion: 本文为非光滑目标分布下的Metropolis算法谱隙提供了显式界，扩展了相关理论。

Abstract: Metropolis algorithms are classical tools for sampling from target distributions, with broad applications in statistics and scientific computing. Their convergence speed is governed by the spectral gap of the associated Markov operator. Recently, Andrieu et al. (2024) derived the first explicit bounds for the spectral gap of Random-Walk Metropolis when the target distribution is smooth and strongly log-concave. However, existing literature rarely discuss non-smooth targets. In this work, we derive explicit spectral gap bounds for the Random-Walk Metropolis and Metropolis-adjusted Langevin algorithms over a broad class of non-smooth distributions. Moreover, combining our analysis with a recent result in Goyal et al. (2025), we extend these bounds to targets satisfying a Poincare or log-Sobolev inequality, beyond the strongly log-concave setting. Our theoretical results are further supported by numerical experiments.

</details>


### [456] [On sample complexity for covariance estimation via the unadjusted Langevin algorithm](https://arxiv.org/abs/2601.21717)
*Shogo Nakakita*

Main category: math.PR

TL;DR: 本文为使用未调整朗之万算法（ULA）估计强对数凹光滑分布的协方差矩阵建立了样本复杂度保证，比较了单链ULA和并行ULA的复杂度。


<details>
  <summary>Details</summary>
Motivation: 为估计强对数凹光滑分布的协方差矩阵建立复杂度保证，比较不同ULA方法的复杂度。

Method: 通过ULA算法估计协方差矩阵，利用对数 - 索伯列夫不等式推导ULA迭代联合分布样本协方差矩阵的集中界。

Result: 单链ULA的样本复杂度比并行ULA小一个关于维度和规定精度倒数的对数因子，差异源于预烧期有效减少偏差。

Conclusion: 成功建立了ULA估计协方差矩阵的样本复杂度保证，明确了单链和并行ULA复杂度差异。

Abstract: We establish sample complexity guarantees for estimating the covariance matrix of strongly log-concave smooth distributions using the unadjusted Langevin algorithm (ULA). We quantitatively compare our complexity estimates on single-chain ULA with embarrassingly parallel ULA and derive that the sample complexity of the single-chain approach is smaller than that of embarrassingly parallel ULA by a logarithmic factor in the dimension and the reciprocal of the prescribed precision, with the difference arising from effective bias reduction through burn-in. The key technical contribution is a concentration bound for the sample covariance matrix around its expectation, derived via a log-Sobolev inequality for the joint distribution of ULA iterates.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [457] [Optimal Energy-Aware Service Management in Future Networks with a Gamified Incentives Mechanism](https://arxiv.org/abs/2601.21846)
*Konstantinos Varsos,Adamantia Stamou,George D. Stamoulis,Vasillios A. Siris*

Main category: cs.ET

TL;DR: 文章提出利用游戏化激励和用户接受模型鼓励节能行为，通过Stackelberg博弈优化参数，模拟显示最优参数下游戏化能提升用户参与和节能效果。


<details>
  <summary>Details</summary>
Motivation: 在ICT基础设施能源需求激增背景下，服务提供商需引导用户参与可持续实践并维持可接受的体验质量。

Method: 引入游戏化激励和用户接受模型，用严肃游戏机制促进用户竞争，采用Stackelberg博弈优化激励水平和游戏参数。

Result: 模拟结果显示，在最优选择激励和游戏参数时，游戏化能显著提高用户参与度和节能效果。

Conclusion: 该结构化方法使服务提供商能主动控制能源消耗，在考虑用户满意度的同时获得可衡量的收益。

Abstract: As energy demands surge across ICT infrastructures, service providers must engage users in sustainable practices while maintaining the Quality of Experience (QoE) at acceptable levels. In this paper, we introduce such an approach, leveraging gamified incentives and a model for user's acceptance on incentives, thus encouraging energy-efficient behaviors such as adaptive bitrate streaming. Each user is characterized by an environmental sensitivity factor and a private incentive threshold, shaping probabilistic responses to energy-saving offers. A serious-game mechanism based on positive behavioral reinforcement and rewards of the users, due to their inclusion in top-K and bottom-M rankings, fosters peer comparison and competition, thus transforming passive acceptance into active engagement. Moreover, within a Stackelberg game formulation, the video streaming service provider--acting as the strategic leader--optimizes both incentive levels and game parameters to achieve network-wide energy and traffic reductions, while adhering to budgetary constraints. This structured approach empowers providers with proactive, application-level control over energy consumption, offering them measurable benefits such as reduced high-bitrate traffic and increased participation in energy-saving behaviors, while also considering user satisfaction. The results of our simulations show that indeed gamification boosts significantly user participation and energy savings provided that the incentive and game parameters are chosen optimally.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [458] [The Economics of No-regret Learning Algorithms](https://arxiv.org/abs/2601.22079)
*Jason Hartline*

Main category: econ.TH

TL;DR: 文章回顾无遗憾算法的计算机科学文献，概述经济学领域相关最新研究，聚焦操纵、统计推断和算法合谋等新兴话题。


<details>
  <summary>Details</summary>
Motivation: 现代经济学面临理解算法取代经济主体后情况的挑战，无遗憾性质可助理解算法主体的经济结果。

Method: 回顾经典计算机科学中关于无遗憾算法的文献，并在此基础上对经济学领域相关研究进行概述。

Result: 未提及具体研究结果。

Conclusion: 未提及具体结论，主要为研究的概述与相关话题引出。

Abstract: A fundamental challenge for modern economics is to understand what happens when actors in an economy are replaced with algorithms. Like rationality has enabled understanding of outcomes of classical economic actors, no-regret can enable the understanding of outcomes of algorithmic actors. This review article covers the classical computer science literature on no-regret algorithms to provide a foundation for an overview of the latest economics research on no-regret algorithms, focusing on the emerging topics of manipulation, statistical inference, and algorithmic collusion.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [459] [Efficient Algorithms for Weakly-Interacting Quantum Spin Systems](https://arxiv.org/abs/2601.21140)
*Ryan L. Mann,Gabriel Waite*

Main category: quant-ph

TL;DR: 为任意温度下弱相互作用量子自旋系统建立高效算法，包括配分函数的全多项式时间近似方案和经典自旋空间热分布的有效近似采样方案。


<details>
  <summary>Details</summary>
Motivation: 为弱相互作用量子自旋系统的研究提供有效的计算方法，解决相关计算难题。

Method: 基于簇展开法以及从近似采样到近似计数的标准归约。

Result: 得到配分函数的全多项式时间近似方案和经典自旋空间热分布的有效近似采样方案。

Conclusion: 所采用的方法可以为弱相互作用量子自旋系统在任意温度下建立高效算法。

Abstract: We establish efficient algorithms for weakly-interacting quantum spin systems at arbitrary temperature. In particular, we obtain a fully polynomial-time approximation scheme for the partition function and an efficient approximate sampling scheme for the thermal distribution over a classical spin space. Our approach is based on the cluster expansion method and a standard reduction from approximate sampling to approximate counting.

</details>


### [460] [Hierarchy of discriminative power and complexity in learning quantum ensembles](https://arxiv.org/abs/2601.22005)
*Jian Yao,Pengtao Li,Xiaohui Chen,Quntao Zhuang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distance metrics are central to machine learning, yet distances between ensembles of quantum states remain poorly understood due to fundamental quantum measurement constraints. We introduce a hierarchy of integral probability metrics, termed MMD-$k$, which generalizes the maximum mean discrepancy to quantum ensembles and exhibit a strict trade-off between discriminative power and statistical efficiency as the moment order $k$ increases. For pure-state ensembles of size $N$, estimating MMD-$k$ using experimentally feasible SWAP-test-based estimators requires $Θ(N^{2-2/k})$ samples for constant $k$, and $Θ(N^3)$ samples to achieve full discriminative power at $k = N$. In contrast, the quantum Wasserstein distance attains full discriminative power with $Θ(N^2 \log N)$ samples. These results provide principled guidance for the design of loss functions in quantum machine learning, which we illustrate in the training quantum denoising diffusion probabilistic models.

</details>


### [461] [Parametric Quantum State Tomography with HyperRBMs](https://arxiv.org/abs/2601.20950)
*Simon Tonner,Viet T. Tran,Richard Kueng*

Main category: quant-ph

TL;DR: 提出基于超网络的参数化量子态层析（QST）框架HyperRBM，可代表整个量子基态族以高效重建量子态，还能准确识别量子相变。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络量子态的QST方法逐点处理，需要在相图每个参数值处重新训练，存在局限性。

Method: 引入基于超网络的参数化QST框架，使受限玻尔兹曼机（RBM）依赖哈密顿控制参数。

Result: 将HyperRBM应用于横场伊辛模型，在一维和二维晶格上实现高保真重建，准确重现保真度敏感性并识别量子相变。

Conclusion: 超网络调制的神经量子态为跨全相图的层析重建提供了高效可扩展的途径。

Abstract: Quantum state tomography (QST) is essential for validating quantum devices but suffers from exponential scaling in system size. Neural-network quantum states, such as Restricted Boltzmann Machines (RBMs), can efficiently parameterize individual many-body quantum states and have been successfully used for QST. However, existing approaches are point-wise and require retraining at every parameter value in a phase diagram. We introduce a parametric QST framework based on a hypernetwork that conditions an RBM on Hamiltonian control parameters, enabling a single model to represent an entire family of quantum ground states. Applied to the transverse-field Ising model, our HyperRBM achieves high-fidelity reconstructions from local Pauli measurements on 1D and 2D lattices across both phases and through the critical region. Crucially, the model accurately reproduces the fidelity susceptibility and identifies the quantum phase transition without prior knowledge of the critical point. These results demonstrate that hypernetwork-modulated neural quantum states provide an efficient and scalable route to tomographic reconstruction across full phase diagrams.

</details>


### [462] [Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation Passes](https://arxiv.org/abs/2601.21629)
*Daniel Mills,Ifan Williams,Jacob Swain,Gabriel Matos,Enrico Rinaldi,Alexander Koziell-Pipe*

Main category: quant-ph

TL;DR: 提出用强化学习代理来组合量子电路优化通道序列，在测试集上表现优于默认序列。


<details>
  <summary>Details</summary>
Motivation: 现有量子软件开发套件的通用优化通道序列无法针对单个电路优化，而定制序列需要专业知识。

Method: 训练强化学习代理，其动作空间包含PyTKET默认序列中减少双量子比特门数量的通道。

Result: 在多样测试集电路上，代理移除双量子比特门的（均值，中位数）比例为(57.7%, 56.7%)，优于次优默认序列的(41.8%, 50.0%)。

Conclusion: 强化学习代理能有效组合优化通道序列，提升量子电路优化效果。

Abstract: Many quantum software development kits provide a suite of circuit optimisation passes. These passes have been highly optimised and tested in isolation. However, the order in which they are applied is left to the user, or else defined in general-purpose default pass sequences. While general-purpose sequences miss opportunities for optimisation which are particular to individual circuits, designing pass sequences bespoke to particular circuits requires exceptional knowledge about quantum circuit design and optimisation. Here we propose and demonstrate training a reinforcement learning agent to compose optimisation-pass sequences. In particular the agent's action space consists of passes for two-qubit gate count reduction used in default PyTKET pass sequences. For the circuits in our diverse test set, the (mean, median) fraction of two-qubit gates removed by the agent is $(57.7\%, \ 56.7 \%)$, compared to $(41.8 \%, \ 50.0 \%)$ for the next best default pass sequence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [463] [Log2Motion: Biomechanical Motion Synthesis from Touch Logs](https://arxiv.org/abs/2601.21043)
*Michał Patryk Miazga,Hannah Bussmann,Antti Oulasvirta,Patrick Ebel*

Main category: cs.HC

TL;DR: 提出Log2Motion方法，从触摸日志合成用户运动，评估生成运动合理性并在大数据集验证，为理解日志数据提供新途径。


<details>
  <summary>Details</summary>
Motivation: 现有触摸数据难以反映交互情况，生物力学模拟未用于触摸交互，需填补此空白。

Method: 提出从日志直接合成合理运动的计算问题，采用强化学习驱动的肌肉骨骼正向模拟，将软件模拟器集成到物理模拟器。

Result: Log2Motion从触摸日志生成丰富用户运动合成结果，包括运动、速度、精度和努力程度估计。

Conclusion: 生物力学运动合成是理解日志数据的新方法，有助于揭示触摸交互的人体工程学和运动控制。

Abstract: Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.

</details>


### [464] [Optimization and Mobile Deployment for Anthropocene Neural Style Transfer](https://arxiv.org/abs/2601.21141)
*Po-Hsun Chen,Ivan C. H. Liu*

Main category: cs.HC

TL;DR: 本文提出了AnthropoCam，一种基于移动设备的神经风格迁移系统，用于人类世环境视觉合成。经过参数优化，实现低延迟实时可视化。


<details>
  <summary>Details</summary>
Motivation: 传统艺术神经风格迁移注重绘画抽象性，而对人类改变的景观进行风格化需平衡纹理增强和语义清晰，人类世环境中的景观易出现语义侵蚀问题。

Method: 系统研究神经风格迁移参数配置对人类世纹理视觉转换的影响，通过控制实验确定最优参数；实现基于移动设备的低延迟前馈神经风格迁移管道。

Result: 确定了能最大化风格表达并防止语义擦除的最优参数；系统在普通移动硬件上3 - 5秒内完成高分辨率推理。

Conclusion: 将特定领域的神经风格迁移优化与移动部署相结合，AnthropoCam使神经风格迁移成为人类世实时环境可视化的实用且有表现力的工具。

Abstract: This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.
  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.
  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.
  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.

</details>


### [465] [When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption](https://arxiv.org/abs/2601.21650)
*Alexander Erlei,Federico Cau,Radoslav Georgiev,Sagar Kumar,Kilian Bizer,Ujwal Gadiraju*

Main category: cs.HC

TL;DR: 文章提供信息不对称和披露设计影响用户采用AI系统的实验证据，揭示信息不对称危害及部分披露设计的作用。


<details>
  <summary>Details</summary>
Motivation: AI消费市场存在严重信息不对称，虽有监管但仍有信息缺口，需研究信息和披露对AI用户采纳的影响。

Method: 在模拟AI产品市场中，系统改变低质量AI系统密度和披露要求深度，对比参与者选择与理性贝叶斯模型。

Result: 信息不对称对AI采用有有害影响，部分披露设计有提升人类决策整体效率的潜力。

Conclusion: 信息不对称不利于AI采用，部分披露设计能改善人类决策效率。

Abstract: AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making.

</details>


### [466] [From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction](https://arxiv.org/abs/2601.21920)
*Upol Ehsan,Samir Passi,Koustuv Saha,Todd McNutt,Mark O. Riedl,Sara Alcorn*

Main category: cs.HC

TL;DR: 本文探讨AI作为生产力放大器的悖论，通过对癌症专家的研究揭示AI使用带来的‘直觉生锈’等问题，并提出人机交互框架以平衡生产力和人类专业知识的保留。


<details>
  <summary>Details</summary>
Motivation: 关注AI在提高效率背后对人类专业知识和自主性的侵蚀，从关注工作的未来转向关注工作者的未来。

Method: 开展为期一年的研究，观察癌症专家在高风险工作场所长期使用AI的情况。

Result: 发现AI使用初期的操作收益掩盖了‘直觉生锈’问题，且这种无症状影响演变成了技能萎缩和身份商品化等长期危害。

Conclusion: 提出的人机交互框架经医疗和软件工程领域评估，有助于在未来实现有尊严的人机交互，平衡生产力和人类专业知识的保存。

Abstract: In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.

</details>


### [467] [From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation](https://arxiv.org/abs/2601.21977)
*Javier Argota Sánchez-Vaquerizo,Luis Borunda Monsivais*

Main category: cs.HC

TL;DR: 引入Agentic Environmental Simulations，提出从时间步到情景空间推理的转变，将AI幻觉作为诊断工具，挑战HCI范式并提出以人为中心的认知编排框架。


<details>
  <summary>Details</summary>
Motivation: 传统建筑模拟将元素建模为确定性物理“粒子”而非认知“主体”，为弥合这一差距开展研究。

Method: 基于大跨模态生成模型根据语义期望预测空间环境的下一状态，提出从时间步到情景空间推理的转变，形式化认知摩擦揭示“幻影可供性”。

Result: 提出Agentic Environmental Simulations，可将AI幻觉作为诊断工具揭示建筑空间的符号歧义。

Conclusion: 挑战当前HCI范式，提出以人为中心的认知编排框架设计AI驱动的模拟，以保留自主性、情感清晰度和认知完整性。

Abstract: Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based "particles" rather than cognitive "agents". To bridge this, we introduce \textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \textbf{Cognitive Friction} ($C_f$) it is possible to reveal "Phantom Affordances", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.

</details>


### [468] [Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video](https://arxiv.org/abs/2601.22013)
*Catherine Yeh,Anh Truong,Mira Dontcheva,Bryan Wang*

Main category: cs.HC

TL;DR: 本文探讨生成式视频在视频叙事中的应用，开发工具Vidmento并验证其能支持叙事发展。


<details>
  <summary>Details</summary>
Motivation: 解决视频叙事受素材限制、创意表达受限和存在叙事缺口的问题。

Method: 采访8位视频创作者，基于其见解和电影制作原则开发Vidmento工具，开展有12位创作者参与的研究。

Result: Vidmento能通过生成式媒体系统地扩展初始素材，支持叙事发展和探索。

Conclusion: 强调创作者能用生成式内容弥合故事缺口，且该融合能力很有价值。

Abstract: Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [469] [Integrating Color Histogram Analysis and Convolutional Neural Network for Skin Lesion Classification](https://arxiv.org/abs/2601.20869)
*M. A. Rasel,Sameem Abdul Kareem,Unaizah Obaidellah*

Main category: q-bio.QM

TL;DR: 研究引入病灶颜色数量特征，用颜色直方图分析和CNN分类皮肤病变，最佳模型F1得分75%，表明颜色数量是重要特征，CNN有临床诊断支持潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变颜色是诊断重要特征，引入病灶颜色数量特征以区分黑色素瘤和良性病变，指示疾病严重程度。

Method: 用颜色直方图分析三个公开数据集的病灶像素值，设计并训练带残差跳跃连接的19层CNN分类病灶，用DeepDream可视化解释网络学习特征，测试多种CNN配置，用LIME识别影响模型决策的重要区域。

Result: 最佳模型加权F1得分75%。

Conclusion: 病灶颜色数量是描述皮肤状况的重要特征，带三个跳跃连接的CNN有临床诊断支持的强大潜力。

Abstract: The color of skin lesions is an important diagnostic feature for identifying malignant melanoma and other skin diseases. Typical colors associated with melanocytic lesions include tan, brown, black, red, white, and blue gray. This study introduces a novel feature: the number of colors present in a lesion, which can indicate the severity of disease and help distinguish melanomas from benign lesions. We propose a color histogram analysis method to examine lesion pixel values from three publicly available datasets: PH2, ISIC2016, and Med Node. The PH2 dataset contains ground truth annotations of lesion colors, while ISIC2016 and Med Node do not; our algorithm estimates the ground truth using color histogram analysis based on PH2. We then design and train a 19 layer Convolutional Neural Network (CNN) with residual skip connections to classify lesions into three categories based on the number of colors present. DeepDream visualization is used to interpret features learned by the network, and multiple CNN configurations are tested. The best model achieves a weighted F1 score of 75 percent. LIME is applied to identify important regions influencing model decisions. The results show that the number of colors in a lesion is a significant feature for describing skin conditions, and the proposed CNN with three skip connections demonstrates strong potential for clinical diagnostic support.

</details>


### [470] [ATTNSOM: Learning Cross-Isoform Attention for Cytochrome P450 Site-of-Metabolism](https://arxiv.org/abs/2601.20891)
*Hajung Kim,Eunha Lee,Sohyun Chung,Jueon Park,Seungheun Baek,Jaewoo Kang*

Main category: q-bio.QM

TL;DR: 提出ATTNSOM框架预测代谢位点，结合分子反应性和跨亚型关系，在两个基准数据集表现良好，证明建模跨亚型关系的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法忽略细胞色素P450亚型身份或独立建模，且评估指标有缺陷，需新的代谢位点预测方法和评估指标。

Method: 提出ATTNSOM框架，结合共享图编码器、分子条件原子表示和交叉注意力机制。

Result: 在两个基准数据集上，模型在多种细胞色素P450亚型上有稳定的top - k性能，马修斯相关系数更高。

Conclusion: 明确建模跨亚型关系对代谢位点预测很重要。

Abstract: Identifying metabolic sites where cytochrome P450 enzymes metabolize small-molecule drugs is essential for drug discovery. Although existing computational approaches have been proposed for site-of-metabolism prediction, they typically ignore cytochrome P450 isoform identity or model isoforms independently, thereby failing to fully capture inherent cross-isoform metabolic patterns. In addition, prior evaluations often rely on top-k metrics, where false positive atoms may be included among the top predictions, underscoring the need for complementary metrics that more directly assess binary atom-level discrimination under severe class imbalance. We propose ATTNSOM, an atom-level site-of-metabolism prediction framework that integrates intrinsic molecular reactivity with cross-isoform relationships. The model combines a shared graph encoder, molecule-conditioned atom representations, and a cross-attention mechanism to capture correlated metabolic patterns across cytochrome P450 isoforms. The model is evaluated on two benchmark datasets annotated with site-of-metabolism labels at atom resolution. Across these benchmarks, the model achieves consistently strong top-k performance across multiple cytochrome P450 isoforms. Relative to ablated variants, the model yields higher Matthews correlation coefficient, indicating improved discrimination of true metabolic sites. These results support the importance of explicitly modeling cross-isoform relationships for site-of-metabolism prediction. The code and datasets are available at https://github.com/dmis-lab/ATTNSOM.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [471] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文针对对话式图像生成中的非马尔可夫设置，提出数据构造策略、训练推理框架及改进方法，提升多轮一致性和指令遵循度。


<details>
  <summary>Details</summary>
Motivation: 现有多轮基准和训练方法多为马尔可夫式，忽略长程历史，本文旨在解决更具挑战性的非马尔可夫设置问题。

Method: 提出非马尔可夫多轮数据构造策略、历史条件训练推理框架及改进方法。

Result: 显式训练非马尔可夫交互在保持单轮编辑和个性化能力的同时，大幅提升了多轮一致性和指令遵循度。

Conclusion: 针对非马尔可夫设置的训练方法能有效提升对话式图像生成的多轮性能。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

</details>


### [472] [Thinker: A vision-language foundation model for embodied intelligence](https://arxiv.org/abs/2601.21199)
*Baiyu Pan,Daqin Luo,Junpeng Yang,Jiyuan Wang,Yixuan Zhang,Hailin Shi,Jichao Jiao*

Main category: cs.CV

TL;DR: 提出用于具身智能的大视觉语言基础模型Thinker，构建数据集并采用新输入方式，在基准数据集获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型应用于机器人领域时，存在第一/三人称视角混淆、时间推理忽视视频结尾信息等问题。

Method: 一是构建适用于机器人感知和推理的大规模数据集；二是联合输入关键帧和完整视频序列以提升模型视频理解能力。

Result: 模型在任务规划领域两个常用基准数据集上取得了最先进结果。

Conclusion: 所提出的Thinker模型及相应方法能够有效解决大视觉语言模型应用于机器人领域时遇到的问题。

Abstract: When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.

</details>


### [473] [PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models](https://arxiv.org/abs/2601.21238)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

TL;DR: 本文探讨对自回归视觉生成模型（ARVG）进行量化的问题，提出训练无关的量化框架PTQ4ARVG，实验表明其能有效量化ARVG模型。


<details>
  <summary>Details</summary>
Motivation: 量化常用于减少神经网络模型大小和计算延迟，但对ARVG模型量化的研究不足，现有量化方法难以有效泛化到ARVG模型。

Method: 提出PTQ4ARVG框架，包含缓解通道级异常值的Gain-Projected Scaling (GPS)、解决token级方差的Static Token-Wise Quantization (STWQ)、消除样本级分布不匹配的Distribution-Guided Calibration (DGC)。

Result: PTQ4ARVG能将ARVG系列模型有效量化到8位和6位，且保持有竞争力的性能。

Conclusion: PTQ4ARVG是一种有效的ARVG模型量化方法，代码已开源。

Abstract: AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at http://github.com/BienLuky/PTQ4ARVG .

</details>


### [474] [Hypersolid: Emergent Vision Representations via Short-Range Repulsion](https://arxiv.org/abs/2601.21255)
*Esteban Rodríguez-Betancourt,Edgar Casasola-Murillo*

Main category: cs.CV

TL;DR: 重新解读表征学习为离散打包问题，提出Hypersolid方法防止表征崩溃，在细粒度和低分辨率分类任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习中防止表征崩溃的问题，现有方法多依赖全局正则化。

Method: 将表征学习重新解释为离散打包问题，提出Hypersolid方法，利用短程硬球排斥防止局部碰撞。

Result: 该约束形成高分离几何状态，保留了增强多样性。

Conclusion: Hypersolid方法在细粒度和低分辨率分类任务中表现优异。

Abstract: A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.

</details>


### [475] [Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference](https://arxiv.org/abs/2601.21269)
*Jianglong Li,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.CV

TL;DR: 提出轻量级、高保真、低比特率的3D人脸压缩框架用于实时3D视频会议，实验显示其率失真性能优越。


<details>
  <summary>Details</summary>
Motivation: 沉浸式和交互式通信需求推动3D视频会议发展，但低比特率下实现高保真3D人脸表示存在挑战，传统方法有缺陷。

Method: 提出集成基于FLAME的参数化建模与3DGS神经渲染的框架，实时传输关键面部元数据，采用基于高斯的头部模型重建，引入紧凑表示和压缩方案。

Result: 方法在率失真性能上表现优越，能在极低比特率下实现高质量面部渲染。

Conclusion: 该方法适合实时3D视频会议应用。

Abstract: The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge. Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs. To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering. Our approach transmits only essential facial metadata in real time, enabling efficient reconstruction with a Gaussian-based head model. Additionally, we introduce a compact representation and compression scheme, including Gaussian attribute compression and MLP optimization, to enhance transmission efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance, delivering high-quality facial rendering at extremely low bitrates, making it well-suited for real-time 3D video conferencing applications.

</details>


### [476] [GeoRC: A Benchmark for Geolocation Reasoning Chains](https://arxiv.org/abs/2601.21278)
*Mohit Talreja,Joshua Diao,Jim Thannikary James,Radu Casapu,Tejas Santanam,Ethan Mendes,Alan Ritter,Wei Xu,James Hays*

Main category: cs.CV

TL;DR: 本文引入地理定位推理链基准，用GeoGuessr游戏场景评估VLM，发现大闭源VLM定位强但推理链差，开源VLM表现不佳，反映VLM提取图像属性能力有限。


<details>
  <summary>Details</summary>
Motivation: 许多VLM虽能准确进行地理定位预测，但在解释预测依据方面表现不佳，需评估其推理链能力。

Method: 引入地理定位推理链基准，以GeoGuessr游戏全球定位任务为重点，与专家玩家合作生成800条地面真值推理链，评估LLM和VLM作为评判策略。

Result: Qwen 3 LLM作为评判与人类评分相关性最佳；大闭源VLM定位强但推理链落后，开源VLM表现差。

Conclusion: VLM在从高分辨率图像中提取细粒度视觉属性方面存在局限。

Abstract: Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.

</details>


### [477] [Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention](https://arxiv.org/abs/2601.21444)
*Yuxiang Huang,Mingye Li,Xu Han,Chaojun Xiao,Weilin Zhao,Ao Sun,Ziqi Yuan,Hao Zhou,Fandong Meng,Zhiyuan Liu*

Main category: cs.CV

TL;DR: 提出序列并行框架Spava加速大模态模型长视频推理，实现显著加速且无明显性能损失。


<details>
  <summary>Details</summary>
Motivation: 长视频推理效率是关键瓶颈，现有方法加速有限或影响性能，限制模型处理更长更复杂视频。

Method: 提出多GPU上优化注意力机制的序列并行框架Spava，通过分布式近似注意力减少计算并增加并行性，进行系统级优化。

Result: 相比FlashAttn、ZigZagRing和APB分别实现12.72x、1.70x和1.18x加速，无明显性能损失。

Conclusion: Spava能有效加速大模态模型长视频推理，具有良好性能且代码开源。

Abstract: The efficiency of long-video inference remains a critical bottleneck, mainly due to the dense computation in the prefill stage of Large Multimodal Models (LMMs). Existing methods either compress visual embeddings or apply sparse attention on a single GPU, yielding limited acceleration or degraded performance and restricting LMMs from handling longer, more complex videos. To overcome these issues, we propose Spava, a sequence-parallel framework with optimized attention that accelerates long-video inference across multiple GPUs. By distributing approximate attention, Spava reduces computation and increases parallelism, enabling efficient processing of more visual embeddings without compression and thereby improving task performance. System-level optimizations, such as load balancing and fused forward passes, further unleash the potential of Spava, delivering speedups of 12.72x, 1.70x, and 1.18x over FlashAttn, ZigZagRing, and APB, without notable performance loss. Code available at https://github.com/thunlp/APB

</details>


### [478] [SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing](https://arxiv.org/abs/2601.21498)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 现有图像生成与编辑任务分开处理有缺陷，本文提出SimGraph统一框架，集成场景图图像生成与编辑，实验表明效果优。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成与编辑任务分开处理，存在效率低及难以维持空间一致性和语义连贯性问题，且缺乏对对象关系和空间布局的结构化控制。

Method: 引入SimGraph统一框架，在单个场景图驱动模型中集成基于令牌的生成和基于扩散的编辑。

Result: 通过大量实验，经验证明该方法优于现有最先进方法。

Conclusion: SimGraph统一框架能实现对对象交互、布局和空间连贯性的精确控制，保证结果高质量且一致。

Abstract: Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.

</details>


### [479] [Bi-Anchor Interpolation Solver for Accelerating Generative Modeling](https://arxiv.org/abs/2601.21542)
*Hongxu Chen,Hongxiang Li,Zhen Wang,Long Chen*

Main category: cs.CV

TL;DR: 现有Flow Matching模型迭代求解ODE有延迟瓶颈，本文提出BA - solver，结合双向时间感知和双锚速度积分，在ImageNet - 256^2上验证其高效性和低训练成本。


<details>
  <summary>Details</summary>
Motivation: Flow Matching模型依赖迭代求解ODE有延迟瓶颈，现有解决方案存在性能和成本问题，需要新方法解决。

Method: 提出Bi - Anchor Interpolation Solver (BA - solver)，包含双向时间感知和双锚速度积分两个组件，利用轻量级SideNet和冻结主干网络加速。

Result: 在ImageNet - 256^2上，BA - solver在10次NFE时生成质量与100 + 次NFE的Euler求解器相当，5次NFE时仍能保持高保真度，训练成本可忽略不计。

Conclusion: BA - solver能在低NFE下实现高效生成，且可与现有生成管道无缝集成，便于下游任务。

Abstract: Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.

</details>


### [480] [DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning](https://arxiv.org/abs/2601.21716)
*Mingshuang Luo,Shuang Liang,Zhengkun Rong,Yuxuan Luo,Tianshu Hu,Ruibing Hou,Hong Chang,Yong Li,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: 提出通用动画框架DreamActor - M2解决角色图像动画现有方法的问题，通过两阶段范式提升性能，还引入AW Bench进行评估，实验显示其达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有角色图像动画方法存在运动注入策略不佳、过度依赖显式姿势先验的问题，影响身份保留和运动一致性及泛化能力。

Method: 采用两阶段范式，一是融合参考外观和运动线索到统一潜在空间，利用基础模型生成先验联合推理；二是引入自引导数据合成管道创建伪跨身份训练对，实现从依赖姿势控制到RGB驱动动画的转变，还引入AW Bench进行评估。

Result: DreamActor - M2达到了SOTA性能，提供了更高的视觉保真度和强大的跨领域泛化能力。

Conclusion: DreamActor - M2是一种有效的通用动画框架，能解决现有角色图像动画方法的问题，在不同角色和运动场景中表现良好。

Abstract: Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/

</details>


### [481] [From Global to Granular: Revealing IQA Model Performance via Correlation Surface](https://arxiv.org/abs/2601.21738)
*Baoliang Chen,Danni Huang,Hanwei Zhu,Lingyu Zhu,Wei Zhou,Shiqi Wang,Yuming Fang,Weisi Lin*

Main category: cs.CV

TL;DR: 传统IQA模型评估指标有局限，本文提出GMC方法解决，实验显示其更优且代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统全局相关性指标如PLCC和SRCC将性能简化为单个标量，无法捕获局部排名一致性变化，且对样本质量分布敏感，比较不稳定。

Method: 提出Granularity - Modulated Correlation (GMC)，包括Granularity Modulator考察局部性能变化和Distribution Regulator减少质量分布偏差，生成相关曲面3D展示IQA性能。

Result: 在标准基准测试中，GMC揭示了标量指标无法发现的性能特征。

Conclusion: GMC为分析、比较和部署IQA模型提供了更有信息性和可靠性的范式。

Abstract: Evaluation of Image Quality Assessment (IQA) models has long been dominated by global correlation metrics, such as Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank-Order Correlation Coefficient (SRCC). While widely adopted, these metrics reduce performance to a single scalar, failing to capture how ranking consistency varies across the local quality spectrum. For example, two IQA models may achieve identical SRCC values, yet one ranks high-quality images (related to high Mean Opinion Score, MOS) more reliably, while the other better discriminates image pairs with small quality/MOS differences (related to $|Δ$MOS$|$). Such complementary behaviors are invisible under global metrics. Moreover, SRCC and PLCC are sensitive to test-sample quality distributions, yielding unstable comparisons across test sets. To address these limitations, we propose \textbf{Granularity-Modulated Correlation (GMC)}, which provides a structured, fine-grained analysis of IQA performance. GMC includes: (1) a \textbf{Granularity Modulator} that applies Gaussian-weighted correlations conditioned on absolute MOS values and pairwise MOS differences ($|Δ$MOS$|$) to examine local performance variations, and (2) a \textbf{Distribution Regulator} that regularizes correlations to mitigate biases from non-uniform quality distributions. The resulting \textbf{correlation surface} maps correlation values as a joint function of MOS and $|Δ$MOS$|$, providing a 3D representation of IQA performance. Experiments on standard benchmarks show that GMC reveals performance characteristics invisible to scalar metrics, offering a more informative and reliable paradigm for analyzing, comparing, and deploying IQA models. Codes are available at https://github.com/Dniaaa/GMC.

</details>


### [482] [Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring](https://arxiv.org/abs/2601.21786)
*Borja Carrillo-Perez,Felix Sattler,Angel Bueno Rodriguez,Maurice Stephan,Sarah Barnes*

Main category: cs.CV

TL;DR: 提出仅用单视图和合成数据的真实船舶3D重建高效管道，经评估有良好效果，可用于海事监测。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法需多视图监督、标注或计算量大，不适用于实时海事部署，需高效方法。

Method: 用Splatter Image网络，在合成ShapeNet船只上微调，用自定义3D船只数据集优化，整合基于YOLOv8的分割模块和预处理，后处理结合AIS元数据和单应性映射。

Result: 在合成验证数据上定量评估显示重建保真度高，真实海事图像定性结果表明可用于实际海事场景。

Conclusion: 该管道为海事监测提供高效可扩展解决方案，指明实时3D船舶可视化应用方向。

Abstract: Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.

</details>


### [483] [FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models](https://arxiv.org/abs/2601.21187)
*Chenyu Huang,Peng Ye,Xudong Tan,Jinhan Mu,Shenghe Zheng,Li Shen,Tao Chen*

Main category: cs.CV

TL;DR: 提出FRISM框架，通过子空间模型合并实现细粒度推理注入，提升视觉语言模型推理能力且不损失视觉能力。


<details>
  <summary>Details</summary>
Motivation: 现有将视觉语言模型与大推理模型合并的方法在粗粒度层面操作，存在注入推理能力和保留视觉能力的权衡问题。

Method: 提出FRISM框架，通过奇异值分解分解大推理模型任务向量，学习调整各子空间缩放系数；引入无标签自蒸馏学习策略和双目标优化。

Result: 在各种视觉推理基准测试中始终达到了最先进的性能。

Conclusion: FRISM能有效提高推理能力，同时不损害模型原有的视觉能力。

Abstract: Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.

</details>


### [484] [Improving Classifier-Free Guidance of Flow Matching via Manifold Projection](https://arxiv.org/abs/2601.21892)
*Jian-Feng Cai,Haixia Liu,Zhengyi Su,Chao Wang*

Main category: cs.CV

TL;DR: 本文从优化角度对无分类器引导（CFG）技术进行解读，将CFG采样重新表述为带流形约束的同伦优化，并用Anderson加速改进迭代过程，在多个基准测试中验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有CFG技术依赖启发式线性外推，对引导尺度敏感，缺乏原则性解释。

Method: 从优化角度解释CFG，将CFG采样重新表述为带流形约束的同伦优化，通过增量梯度下降实现流形投影步骤，用Anderson加速改进迭代过程。

Result: 所提方法无需训练，能持续提升生成保真度、提示对齐度和对引导尺度的鲁棒性，在多个大规模模型上有显著改进。

Conclusion: 所提方法有效，在不同基准测试中表现良好，可提升生成模型性能。

Abstract: Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.

</details>


### [485] [TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention](https://arxiv.org/abs/2601.21900)
*Chuancheng Shi,Shangze Li,Wenjun Lu,Wenhua Wu,Cong Wang,Zifeng Cheng,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出TraceRouter框架应对大基础模型对抗操纵问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于‘局部性假设’的防御方法难以应对有害语义的分布式跨层电路，效果不佳且影响模型实用性。

Method: TraceRouter框架分三步，先通过分析注意力差异确定敏感起始层，再用稀疏自动编码器和差分激活分析分离恶意特征，最后通过特征影响分数映射到下游因果路径并选择性抑制。

Result: TraceRouter显著优于现有基线，在对抗鲁棒性和通用实用性之间实现更好权衡。

Conclusion: TraceRouter是应对大基础模型对抗操纵的有效方法，代码将公开。

Abstract: Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the "locality hypothesis", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.

</details>


### [486] [Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence](https://arxiv.org/abs/2601.21320)
*Keke Tang,Ziyong Du,Xiaofei Wang,Weilong Peng,Peican Zhu,Zhihong Tian*

Main category: cs.CV

TL;DR: 提出利用最优传输诱导奇异边界几何结构的框架，构建OTIS样本，用置信抑制损失训练模型，缓解DNN在OOD输入上的过自信问题，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: DNN在OOD输入上常产生过自信预测，降低其在开放世界环境中的可靠性，半离散最优传输中的奇点区域易出现不合理高置信预测。

Method: 在连续基础分布和训练数据的潜在嵌入之间构建OT问题，确定奇异边界，在边界附近采样构建OTIS样本，训练时对OTIS应用置信抑制损失。

Result: 显著缓解OOD过自信问题，性能优于现有方法。

Conclusion: 所提框架能有效缓解DNN在OOD输入上的过自信问题。

Abstract: Deep neural networks (DNNs) often produce overconfident predictions on out-of-distribution (OOD) inputs, undermining their reliability in open-world environments. Singularities in semi-discrete optimal transport (OT) mark regions of semantic ambiguity, where classifiers are particularly prone to unwarranted high-confidence predictions. Motivated by this observation, we propose a principled framework to mitigate OOD overconfidence by leveraging the geometry of OT-induced singular boundaries. Specifically, we formulate an OT problem between a continuous base distribution and the latent embeddings of training data, and identify the resulting singular boundaries. By sampling near these boundaries, we construct a class of OOD inputs, termed optimal transport-induced OOD samples (OTIS), which are geometrically grounded and inherently semantically ambiguous. During training, a confidence suppression loss is applied to OTIS to guide the model toward more calibrated predictions in structurally uncertain regions. Extensive experiments show that our method significantly alleviates OOD overconfidence and outperforms state-of-the-art methods.

</details>


### [487] [Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation](https://arxiv.org/abs/2601.21406)
*Zihan Su,Hongyang Wei,Kangrui Cen,Yong Wang,Guanhua Chen,Chun Yuan,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出UniMRG方法，通过辅助生成任务增强统一多模态模型视觉理解能力并提高生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法多利用理解增强生成，利用生成提升理解的方向待探索。

Method: 提出UniMRG架构无关后训练方法，让模型生成输入图像的多种内在表示。

Result: 在多种统一多模态模型架构实验中，该方法增强细粒度感知、减少幻觉、提升空间理解和生成能力。

Conclusion: UniMRG简单有效，可增强统一多模态模型的理解性能和生成能力。

Abstract: Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.

</details>


### [488] [MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources](https://arxiv.org/abs/2601.22054)
*Baorui Ma,Jiahui Yang,Donglin Di,Xuancheng Zhang,Jianxun Cui,Hao Li,Yan Xie,Wei Chen*

Main category: cs.CV

TL;DR: 本文提出Metric Anything框架学习度量深度，展示了度量深度任务的缩放趋势，且模型在多任务表现出色，开源代码以支持研究。


<details>
  <summary>Details</summary>
Motivation: 将缩放范式扩展到度量深度估计面临传感器噪声、相机偏差和3D数据度量模糊等挑战。

Method: 引入Metric Anything框架，使用稀疏度量提示解耦空间推理与传感器和相机偏差，利用约2000万图像 - 深度对进行预训练。

Result: 预训练模型在提示驱动任务表现出色，蒸馏后的无提示学生模型在多个任务达到SOTA，使用其ViT作为视觉编码器提升多模态大语言模型空间智能。

Conclusion: 度量深度估计可受益于现代基础模型的缩放定律，为现实世界度量感知开辟新路径。

Abstract: Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.

</details>


### [489] [Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models](https://arxiv.org/abs/2601.22057)
*Archer Wang,Emile Anand,Yilun Du,Marin Soljačić*

Main category: cs.CV

TL;DR: 研究无因子级监督的扩散模型中复杂数据的分解和合成，引入对抗训练信号，方法在多个数据集表现优，还用于机器人视频轨迹探索。


<details>
  <summary>Details</summary>
Motivation: 在基于扩散的模型中研究复杂数据分解，揭示可重用组件并实现样本合成，同时提升潜在因子发现和组合生成质量。

Method: 引入对抗训练信号，训练判别器区分单源样本和因子重组样本，优化生成器以实现物理和语义一致性。

Result: 在多个数据集上优于先前基线，FID 分数更低，MIG 和 MCC 衡量的解纠缠效果更好；在机器人视频轨迹应用中增加了状态空间覆盖。

Conclusion: 所提方法在数据分解和合成方面表现良好，且在机器人视频轨迹探索中有新应用价值。

Abstract: Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.

</details>


### [490] [Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models](https://arxiv.org/abs/2601.22060)
*Wenxuan Huang,Yu Zeng,Qiuchen Wang,Zhen Fang,Shaosheng Cao,Zheng Chu,Qingyu Yin,Shuang Chen,Zhenfei Yin,Lin Chen,Zehui Chen,Yao Hu,Philip Torr,Feng Zhao,Wanli Ouyang*

Main category: cs.CV

TL;DR: 提出Vision - DeepResearch，通过多轮、多实体和多尺度搜索构建端到端多模态深度研究MLLM，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在多模态搜索中存在定义简单、推理深度和搜索广度有限的问题，难以解决复杂问题。

Method: 提出新的多模态深度研究范式，进行多轮、多实体和多尺度视觉与文本搜索，通过冷启动监督和强化学习训练将深度研究能力内化到MLLM。

Result: 构建的Vision - DeepResearch显著优于现有多模态深度研究MLLM及基于强闭源基础模型的工作流。

Conclusion: Vision - DeepResearch能在重噪声下有效进行多模态搜索，提升MLLM处理复杂问题的能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [491] [SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence](https://arxiv.org/abs/2601.22114)
*Saoud Aldowaish,Yashwanth Karumanchi,Kai-Chen Chiang,Soroosh Noorzad,Morteza Fayazi*

Main category: cs.CV

TL;DR: 提出开源全自动电路原理图图像转网表工具SINA，准确率比现有方法高


<details>
  <summary>Details</summary>
Motivation: 当前电路原理图图像转机器可读网表的方法在元件识别和连接推断上存在困难

Method: 集成深度学习进行元件检测、使用连通组件标记（CCL）提取连接性、采用光学字符识别（OCR）检索元件参考标识符、利用视觉语言模型（VLM）进行参考标识符分配

Result: SINA在实验中实现了96.47%的网表生成总体准确率，是现有方法的2.72倍

Conclusion: SINA在电路原理图图像转网表任务中表现出色，优于现有方法

Abstract: Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.

</details>


### [492] [When Gradient Optimization Is Not Enough: $\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning](https://arxiv.org/abs/2601.21670)
*Zixuan Xia,Hao Wang,Pengcheng Weng,Yanyu Qian,Yangxin Xu,William Dan,Fei Wang*

Main category: cs.CV

TL;DR: 多模态学习优化不保证结构良好表征，本文提出轻量级正则框架REG，实验证明其能缓解模态权衡。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中强优化不能保证良好结构表征，模型存在几何病态问题，影响单模态鲁棒性和多模态融合。

Method: 提出REG轻量级几何感知正则化框架，对中间嵌入实施两种互补约束：模态内分散正则化和模态间锚定正则化。

Result: 在多个多模态基准测试中，多模态和单模态性能均有一致提升。

Conclusion: 明确调节表征几何能有效缓解模态权衡。

Abstract: Multimodal learning aims to integrate complementary information from heterogeneous modalities, yet strong optimization alone does not guaranty well-structured representations. Even under carefully balanced training schemes, multimodal models often exhibit geometric pathologies, including intra-modal representation collapse and sample-level cross-modal inconsistency, which degrade both unimodal robustness and multimodal fusion.
  We identify representation geometry as a missing control axis in multimodal learning and propose \regName, a lightweight geometry-aware regularization framework. \regName enforces two complementary constraints on intermediate embeddings: an intra-modal dispersive regularization that promotes representation diversity, and an inter-modal anchoring regularization that bounds sample-level cross-modal drift without rigid alignment. The proposed regularizer is plug-and-play, requires no architectural modifications, and is compatible with various training paradigms.
  Extensive experiments across multiple multimodal benchmarks demonstrate consistent improvements in both multimodal and unimodal performance, showing that explicitly regulating representation geometry effectively mitigates modality trade-offs.

</details>


### [493] [EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers](https://arxiv.org/abs/2601.22127)
*John Flynn,Wolfgang Paier,Dimitar Dinev,Sam Nhut Nguyen,Hayk Poghosyan,Manuel Toribio,Sandipan Banerjee,Guy Gafni*

Main category: cs.CV

TL;DR: 介绍EditYourself框架用于音频驱动的视频编辑，可基于转录修改说话头部视频。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型在编辑预录制视频方面存在不足，需填补编辑时保留运动、时间连贯性等方面的空白。

Method: 基于通用视频扩散模型，通过音频调节和区域感知、聚焦编辑的训练扩展增强其视频到视频（V2V）能力，利用时空修复实现视频编辑。

Result: 实现精确的唇形同步和时间连贯的表演重构，包括在新添加片段中合成逼真的人类动作，同时保持长期的视觉保真度和身份一致性。

Conclusion: 该工作是使生成式视频模型成为专业视频后期制作实用工具的基础一步。

Abstract: Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [494] [Clear Messages, Ambiguous Audiences: Measuring Interpretability in Political Communication](https://arxiv.org/abs/2601.20912)
*Krishna Sharma,Khemraj Bhatt*

Main category: econ.GN

TL;DR: 研究用美国政客社交媒体消息的置信加权人工注释，发现政治传播总体易解读，但选区类别存在解读不确定性，测量误差或源于战略激励。


<details>
  <summary>Details</summary>
Motivation: 检验政治研究中基于文本测量将分类分歧视为随机噪声这一假设。

Method: 使用5000条美国政客社交媒体消息的置信加权人工注释。

Result: 政治传播总体易解读，选区类别在受众分类置信度上有1.79个百分点的惩罚，信息内意图清晰但受众定位模糊。

Conclusion: 政治文本的测量误差由战略激励而非编码者的特殊误差造成。

Abstract: Text-based measurement in political research often treats classi6ication disagreement as random noise. We examine this assumption using con6idence-weighted human annotations of 5,000 social media messages by U.S. politicians. We 6ind that political communication is generally highly legible, with mean con6idence exceeding 0.99 across message type, partisan bias, and audience classi6ications. However, systematic variation concentrates in the constituency category, which exhibits a 1.79 percentage point penalty in audience classi6ication con6idence. Given the high baseline of agreement, this penalty represents a sharp relative increase in interpretive uncertainty. Within messages, intent remains clear while audience targeting becomes ambiguous. These patterns persist with politician 6ixed effects, suggesting that measurement error in political text is structured by strategic incentives rather than idiosyncratic coder error.

</details>


### [495] [The Effects of Higher Education on Midlife Depression: Quasi-Experimental Evidence from South Korea](https://arxiv.org/abs/2601.20976)
*Ah-Reum Lee,Jacqueline M. Torres,Jinkook Lee*

Main category: econ.GN

TL;DR: 本文利用韩国1993年高等教育改革，评估大学毕业对中年女性抑郁的影响，发现大学毕业能降低抑郁水平，研究拓展了现有研究外部有效性，强调了教育政策对减轻女性中年抑郁负担的潜在作用。


<details>
  <summary>Details</summary>
Motivation: 全球高等教育扩张，女性受教育程度提高，但教育对女性心理健康的影响缺乏研究，因果证据有限，故研究大学毕业对韩国中年女性抑郁的影响。

Method: 利用1993年高等教育改革，采用两个全国代表性数据集，实施两阶段最小二乘法（2SLS），用基于改革暴露的出生队列工具变量，还进行安慰剂测试。

Result: 在KNHANES中，大学毕业降低医生诊断的抑郁率；在KLoWF中，大学毕业改善自我报告的心理健康，安慰剂测试对未受影响队列无显著结果。

Conclusion: 本研究为教育与心理健康的准实验文献提供证据，拓展了现有研究的外部有效性，强调教育政策可减轻女性中年抑郁负担。

Abstract: Higher education has expanded worldwide, with women outpacing men in many regions. While educational attainment is consistently linked to better physical health, its mental health effects - particularly for women - remain underexplored, and causal evidence is limited. We estimate the impact of college completion on depression among middle-aged women in South Korea, leveraging the 1993 higher education reform, which raised women's college attainment by 45 percentage points (pp) over the following decade. We use two nationally representative datasets to triangulate evidence, including the Korea National Health and Nutrition Examination Survey (KNHANES, 2007-2021) for physician-diagnosed depression, and the Korean Longitudinal Survey of Women and Families (KLoWF, 2007-2022) to validate findings using self-reports of depressive symptoms. We implement two-stage least squares (2SLS) with a birth-cohort instrument based on exposure to the reform (within 3 years of the cutoff in KNHANES and within 1 to 3 years in KLoWF). In KNHANES, college completion lowers physician-diagnosed depression by 2.4 pp, attenuating to 1.6 pp after adjusting for income, employment, and physical health. In KLoWF, college completion improves self-reported mental health. The weekly depressive-symptoms composite declines by 17.4 pp, attenuating to 16.4 pp after covariate adjustment. Placebo tests on unaffected cohorts yield null results. This study contributes to the growing quasi-experimental literature on education and mental health with convergent evidence across clinical diagnoses and self-reported depressive symptoms in South Korea. By focusing on college education in a non-Western setting, it extends the external validity of existing findings and highlights educational policy as a potential lever to reduce the burden of midlife depression among women.

</details>


### [496] [Electoral Polls and Economic Uncertainty: an Analysis of the Last Two U.S. Presidential Elections](https://arxiv.org/abs/2601.21534)
*Giampiero M. Gallo,Demetrio Lacava,Edoardo Otranto*

Main category: econ.GN

TL;DR: 研究美国2020和2024年总统选举期间民意调查与经济金融不确定性指标的动态关系，发现不同选举年两者相关性不同。


<details>
  <summary>Details</summary>
Motivation: 探究选举民意调查与经济金融不确定性指标之间的动态关系。

Method: 使用每日民意调查数据和多种经济指标，估计条件相关模型捕捉时变互动。

Result: 2020年两者相关性动态且受事件驱动，2024年相关性接近零且稳定、对冲击无反应。

Conclusion: 选民情绪、金融市场和不确定性的相互作用因选举背景而异，应用动态条件相关模型有方法贡献，为经济基本面影响选举动态提供政策见解。

Abstract: This paper examines the dynamic relationship between electoral polls and indicators of economic and financial uncertainty during the last two U.S. presidential elections (2020 and 2024). Using daily polling data on Donald Trump and measures such as the Aruoba-Diebold-Scotti Business Conditions Index, the 5-year Breakeven Inflation Rate, the Trade Policy Uncertainty index, and the VIX, we estimate conditional correlation models to capture time-varying interactions. The analysis reveals that in 2020, correlations between polls and uncertainty measures were highly dynamic and event-driven, reflecting the influence of exogenous shocks (COVID-19, oil price collapse) and political milestones (primaries, debates). In contrast, during the 2024 campaign, correlations remained close to zero, stable, and largely unresponsive to shocks, suggesting that entrenched polarization and non-economic events (e.g., assassination attempt, candidate changes) muted the economic channel. The study highlights how the interplay between voter sentiment, financial markets, and uncertainty varies across electoral contexts, offering a methodological contribution through the application of Dynamic Conditional Correlation models to political data and policy-relevant insights on the conditions under which economic fundamentals influence electoral dynamics.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [497] [Thompson sampling: Precise arm-pull dynamics and adaptive inference](https://arxiv.org/abs/2601.21131)
*Qiyang Han*

Main category: math.ST

TL;DR: 本文研究Thompson - sampling类算法的拉臂动态，揭示拉臂计数的渐近特性及相关二分性，可用于构建置信区间，还给出新证明方法。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明UCB型算法有“稳定性”，便于推理，本文研究另一类Thompson - sampling型算法的精确拉臂动态。

Method: 对于次优臂，采用“逆过程”方法；对于最优臂，重新参数化拉臂和噪声过程，并用抛物Hörmander条件和Stroock - Varadhan支持定理等工具证明相关唯一性。

Result: 拉臂计数渐近确定性的条件是手臂为次优或唯一最优臂，否则收敛到SDE的唯一不变律；归一化手臂均值有相同二分性，稳定臂为高斯极限，不稳定臂是非高斯极限。

Conclusion: 揭示了诸多稳定性结果背后的统一原则，不仅能在非正态情况下构建置信区间，还展示了在不稳定区域开发易处理推理程序的潜力。

Abstract: Adaptive sampling schemes are well known to create complex dependence that may invalidate conventional inference methods. A recent line of work shows that this need not be the case for UCB-type algorithms in multi-armed bandits. A central emerging theme is a `stability' property with asymptotically deterministic arm-pull counts in these algorithms, making inference as easy as in the i.i.d. setting.
  In this paper, we study the precise arm-pull dynamics in another canonical class of Thompson-sampling type algorithms. We show that the phenomenology is qualitatively different: the arm-pull count is asymptotically deterministic if and only if the arm is suboptimal or is the unique optimal arm; otherwise it converges in distribution to the unique invariant law of an SDE. This dichotomy uncovers a unifying principle behind many existing (in)stability results: an arm is stable if and only if its interaction with statistical noise is asymptotically negligible.
  As an application, we show that normalized arm means obey the same dichotomy, with Gaussian limits for stable arms and a semi-universal, non-Gaussian limit for unstable arms. This not only enables the construction of confidence intervals for the unknown mean rewards despite non-normality, but also reveals the potential of developing tractable inference procedures beyond the stable regime.
  The proofs rely on two new approaches. For suboptimal arms, we develop an `inverse process' approach that characterizes the inverse of the arm-pull count process via a Stieltjes integral. For optimal arms, we adopt a reparametrization of the arm-pull and noise processes that reduces the singularity in the natural SDE to proving the uniqueness of the invariant law of another SDE. We prove the latter by a set of analytic tools, including the parabolic Hörmander condition and the Stroock-Varadhan support theorem.

</details>


### [498] [Towards regularized learning from functional data with covariate shift](https://arxiv.org/abs/2601.21019)
*Markus Holzleitner,Sergiy Pereverzyev,Sergei V. Pereverzyev,Vaibhav Silmana,S. Sivananthan*

Main category: math.ST

TL;DR: 本文利用向量值再生核希尔伯特空间，研究无监督域适应向量值回归的正则化框架，提出算子学习算法和聚合方法，证明收敛率，并用真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在协变量偏移下，训练和测试数据输入分布不同给学习带来挑战，需要一个有效的正则化框架解决无监督域适应向量值回归问题。

Method: 限制假设空间，开发能处理函数输出的算子学习算法；提出基于聚合的方法，线性组合不同正则化参数和核对应的估计器。

Result: 建立了所提框架在一般源条件下的最优收敛率，为正则化学习提供理论基础；所提方法解决了选择调优参数的挑战，并给出理论依据；在真实人脸图像数据集上验证了方法在协变量偏移下减轻分布差异的鲁棒性和有效性。

Conclusion: 所提出的正则化框架和方法能有效解决协变量偏移下无监督域适应向量值回归问题。

Abstract: This paper investigates a general regularization framework for unsupervised domain adaptation in vector-valued regression under the covariate shift assumption, utilizing vector-valued reproducing kernel Hilbert spaces (vRKHS). Covariate shift occurs when the input distributions of the training and test data differ, introducing significant challenges for reliable learning. By restricting the hypothesis space, we develop a practical operator learning algorithm capable of handling functional outputs. We establish optimal convergence rates for the proposed framework under a general source condition, providing a theoretical foundation for regularized learning in this setting. We also propose an aggregation-based approach that forms a linear combination of estimators corresponding to different regularization parameters and different kernels. The proposed approach addresses the challenge of selecting appropriate tuning parameters, which is crucial for constructing a good estimator, and we provide a theoretical justification for its effectiveness. Furthermore, we illustrate the proposed method on a real-world face image dataset, demonstrating robustness and effectiveness in mitigating distributional discrepancies under covariate shift.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [499] [Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity](https://arxiv.org/abs/2601.21080)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: 提出参数化双曲守恒律SymCLaw从数据学习双曲系统，保证守恒、熵稳定性和双曲性，实验显示其泛化性、稳定性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅保证守恒或依赖控制方程先验知识，需一种能直接从数据学习双曲系统并保证守恒、熵稳定性和双曲性的方法。

Method: 参数化通量函数保证通量雅可比矩阵有实特征值和完整特征向量，联合学习凸熵函数及其关联通量势嵌入熵稳定设计原则，使用相应熵稳定数值通量方案与标准离散化兼容。

Result: 在Burgers、浅水、Euler和KPP方程等基准问题的数值实验中，SymCLaw能泛化到未见初始条件，在有噪声训练数据下保持稳定，实现准确长期预测。

Conclusion: SymCLaw有潜力成为数据驱动双曲守恒律建模的原则性基础。

Abstract: We propose a parametric hyperbolic conservation law (SymCLaw) for learning hyperbolic systems directly from data while ensuring conservation, entropy stability, and hyperbolicity by design. Unlike existing approaches that typically enforce only conservation or rely on prior knowledge of the governing equations, our method parameterizes the flux functions in a form that guarantees real eigenvalues and complete eigenvectors of the flux Jacobian, thereby preserving hyperbolicity. At the same time, we embed entropy-stable design principles by jointly learning a convex entropy function and its associated flux potential, ensuring entropy dissipation and the selection of physically admissible weak solutions. A corresponding entropy-stable numerical flux scheme provides compatibility with standard discretizations, allowing seamless integration into classical solvers. Numerical experiments on benchmark problems, including Burgers, shallow water, Euler, and KPP equations, demonstrate that SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions, highlighting its potential as a principled foundation for data-driven modeling of hyperbolic conservation laws.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [500] [Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis](https://arxiv.org/abs/2601.20875)
*Md Muhtasim Munif Fahim,Md Jahid Hasan Imran,Luknath Debnath,Tonmoy Shill,Md. Naim Molla,Ehsanul Bashar Pranto,Md Shafin Sanyan Saad,Md Rezaul Karim*

Main category: stat.AP

TL;DR: 本文提出因果发现框架构建SDG依赖因果架构，发现分布式因果网络，确定显著关系，提出分层优先级框架，认为需多维干预加速SDG。


<details>
  <summary>Details</summary>
Motivation: 2030年可持续发展目标（SDGs）的实现依赖战略资源分配，需要构建SDG依赖的因果架构。

Method: 使用面板向量自回归、特定国家固定效应和PCMCI+条件独立性测试构建因果架构。

Result: 确定分布式因果网络，有10个显著格兰杰因果关系，教育与不平等关系最显著，效应大小因收入水平而异，否定单一“关键”SDG，提出分层优先级框架。

Conclusion: 有效加速SDG需多维协调干预，单目标顺序策略不足。

Abstract: The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient.

</details>


### [501] [Analyzing the Temporal Factors for Anxiety and Depression Symptoms with the Rashomon Perspective](https://arxiv.org/abs/2601.20874)
*Mustafa Cavus,Przemysław Biecek,Julian Tejada,Fernando Marmolejo-Ramos,Andre Faro*

Main category: stat.AP

TL;DR: 文章引入Rashomon效应新视角分析心理数据，表明不应只选最佳模型，要考虑Rashomon集，还指出要考虑昼夜节律等因素以增强机器学习结论的稳定性和普遍性。


<details>
  <summary>Details</summary>
Motivation: 在公共心理健康领域提供对焦虑和抑郁与人口统计学、时间因素关系的稳健解释，避免仅选单一最佳模型掩盖数据中其他信息。

Method: 运用新视角解读大型心理数据集，采用随机森林模型结合部分依赖曲线，评估Rashomon集中预测关系的稳健性和稳定性。

Result: 发现人口统计学变量导致焦虑和抑郁风险的结构变化，确定显著的时间效应，风险概率有明显的昼夜和每周波动且清晨达峰值。

Conclusion: 有必要超越最佳模型分析整个Rashomon集，需细致考虑昼夜节律等变异性以进行稳健的心理筛查，提倡采用多模型意识方法提高机器学习结论在心理健康研究中的稳定性和普遍性。

Abstract: This paper introduces a new modeling perspective in the public mental health domain to provide a robust interpretation of the relations between anxiety and depression, and the demographic and temporal factors. This perspective particularly leverages the Rashomon Effect, where multiple models exhibit similar predictive performance but rely on diverse internal structures. Instead of considering these multiple models, choosing a single best model risks masking alternative narratives embedded in the data. To address this, we employed this perspective in the interpretation of a large-scale psychological dataset, specifically focusing on the Patient Health Questionnaire-4. We use a random forest model combined with partial dependence profiles to rigorously assess the robustness and stability of predictive relationships across the resulting Rashomon set, which consists of multiple models that exhibit similar predictive performance. Our findings confirm that demographic variables \texttt{age}, \texttt{sex}, and \texttt{education} lead to consistent structural shifts in anxiety and depression risk. Crucially, we identify significant temporal effects: risk probability demonstrates clear diurnal and circaseptan fluctuations, peaking during early morning hours. This work demonstrates the necessity of moving beyond the best model to analyze the entire Rashomon set. Our results highlight that the observed variability, particularly due to circadian and circaseptan rhythms, must be meticulously considered for robust interpretation in psychological screening. We advocate for a multiplicity-aware approach to enhance the stability and generalizability of ML-based conclusions in mental health research.

</details>


### [502] [Spatial Heterogeneity in Climate Risk and Human Flourishing: An Exploration with Generative AI](https://arxiv.org/abs/2601.20880)
*Stefano Maria Iacus,Haodong Qi,Devika Jain*

Main category: stat.AP

TL;DR: 研究运用生成式AI构建空间框架，研究美国各县累积气候风险与人类繁荣多维度的关系，揭示二者关联及空间模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI尤其是大语言模型的进步为气候地理研究提供新方法，以此研究累积气候风险与人类繁荣的关系。

Method: 开发空间框架，将高分辨率气候危害指标与人类繁荣地理指数结合，聚合到美国县一级并映射到结构方程模型。

Result: 发现累积气候风险越大与人类繁荣程度越低存在空间异质性关联，有对应不同灾害的空间模式。

Conclusion: 证明生成式AI可与潜在结构建模结合用于地理分析和空间知识提取。

Abstract: Recent advances in Generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), enable scalable extraction of spatial information from unstructured text and offer new methodological opportunities for studying climate geography. This study develops a spatial framework to examine how cumulative climate risk relates to multidimensional human flourishing across U.S. counties. High-resolution climate hazard indicators are integrated with a Human Flourishing Geographic Index (HFGI), an index derived from classification of 2.6 billion geotagged tweets using fine-tuned open-source Large Language Models (LLMs). These indicators are aggregated to the US county-level and mapped to a structural equation model to infer overall climate risk and human flourishing dimensions, including expressed well-being, meaning and purpose, social connectedness, psychological distress, physical condition, economic stability, religiosity, character and virtue, and institutional trust. The results reveal spatially heterogeneous associations between greater cumulative climate risk and lower levels of expressed human flourishing, with coherent spatial patterns corresponding to recurrent exposure to heat, flooding, wind, drought, and wildfire hazards. The study demonstrates how Generative AI can be combined with latent construct modeling for geographical analysis and for spatial knowledge extraction.

</details>


### [503] [A new strategy for finite-sample valid prediction of future insurance claims in the regression setting](https://arxiv.org/abs/2601.21153)
*Liang Hong*

Main category: stat.AP

TL;DR: 文章提出新策略将无监督独立同分布设定下的预测方法转换为回归设定下的方法，以获得有限样本有效的保险索赔预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有保险文献在回归设定下缺乏有限样本有效的未来保险索赔预测区间。

Method: 提出将无监督独立同分布设定下的预测方法转换为回归设定下的预测方法的新策略。

Result: 能让精算师在回归设定下获得无限多个有限样本有效的预测区间。

Conclusion: 该新策略可解决回归设定下有限样本有效预测区间缺乏的问题。

Abstract: The extant insurance literature demonstrates a paucity of finite-sample valid prediction intervals of future insurance claims in the regression setting. To address this challenge, this article proposes a new strategy that converts a predictive method in the unsupervised iid (independent identically distributed) setting to a predictive method in the regression setting. In particular, it enables an actuary to obtain infinitely many finite-sample valid prediction intervals in the regression setting.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [504] [Generalizable Prompt Tuning for Audio-Language Models via Semantic Expansion](https://arxiv.org/abs/2601.20867)
*Jaehyuk Jang,Wonjun Lee,Kangwook Ko,Changick Kim*

Main category: cs.SD

TL;DR: 现有提示调优在音频语言模型的泛化性未充分研究，提出SEPT框架改进，有实验证明效果。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优在音频语言模型中的泛化能力未受充分探索，且存在基础 - 新任务权衡问题，根源是嵌入空间语义结构被破坏。

Method: 提出Semantically Expanded Prompt Tuning (SEPT)框架，通过引入大语言模型生成的语义邻居来正则化提示嵌入空间，增加有 Margin 约束的语义扩展损失。建立首个音频语言模型提示泛化基准测试。

Result: 广泛实验表明SEPT在多个提示调优基线中持续提高泛化性能，推理时保持计算成本。

Conclusion: SEPT框架能有效提升音频语言模型提示调优的泛化能力。

Abstract: Prompt tuning has achieved remarkable progress in vision-language models (VLMs) and is recently being adopted for audio-language models (ALMs). However, its generalization ability in ALMs remains largely underexplored. We observe that conventional prompt tuning for ALMs also suffers from the Base-New Tradeoff, and we identify that this issue stems from the disrupted semantic structure of the embedding space. To address this issue, we propose Semantically Expanded Prompt Tuning (SEPT)-a plug-and-play framework that explicitly regularizes the prompt embedding space by incorporating semantic neighbors generated by large language models. SEPT introduces a novel semantic expansion loss with margin constraints that promote intra-class compactness and inter-class separability, thereby enhancing the semantic structure of the prompt embedding space. For comprehensive evaluation, we establish the first benchmark setup for prompt generalization in ALMs, covering both base-to-new generalization and cross-dataset transferability. Extensive experiments demonstrate that SEPT consistently improves generalization performance across multiple prompt tuning baselines, while maintaining computational cost during inference. Codes are available in https://github.com/jhyukjang/SEPT.

</details>


### [505] [PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs](https://arxiv.org/abs/2601.21124)
*Artem Dementyev,Wazeer Zulfikar,Sinan Hersek,Pascal Getreuer,Anurag Kumar,Vivek Kumar*

Main category: cs.SD

TL;DR: 提出PhaseCoder空间音频编码器，使LLM能进行复杂空间推理和转录任务


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型处理音频时忽略空间信息，现有空间音频模型受麦克风几何形状限制

Method: 提出PhaseCoder，以原始多通道音频和麦克风坐标为输入进行定位，微调Gemma 3n LLM处理其生成的“空间音频令牌”

Result: 编码器在麦克风不变定位基准测试中取得了最先进的成果

Conclusion: PhaseCoder使LLM能从任意麦克风阵列执行复杂空间推理和有针对性的转录任务

Abstract: Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over "Spatial Audio Tokens" produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array.

</details>


### [506] [Music Plagiarism Detection: Problem Formulation and a Segment-based Solution](https://arxiv.org/abs/2601.21260)
*Seonghyeon Go,Yumin Kim*

Main category: cs.SD

TL;DR: 针对音乐抄袭检测任务定义不清晰问题，定义该任务、引入数据集并提出解决方法，相关资源公开。


<details>
  <summary>Details</summary>
Motivation: 现有研究对音乐抄袭检测任务定义不清晰，阻碍研究进展且难以应用于实际场景。

Method: 定义音乐抄袭检测与其他MIR任务的区别，引入相似音乐对数据集，提出基于片段转录的解决方法。

Result: 完成任务定义、引入数据集并提出方法，相关资源可在指定链接获取。

Conclusion: 通过明确定义任务和引入数据集等方式，有望推动音乐抄袭检测研究进展并应用于实际。

Abstract: Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD.

</details>


### [507] [Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation](https://arxiv.org/abs/2601.21386)
*June-Woo Kim,Dhruv Agarwal,Federica Cerina*

Main category: cs.SD

TL;DR: 论文全面评估了Fréchet Speech Distance (FSD)及其变体Speech Maximum Mean Discrepancy (SMMD)，发现WavLM Base+特征与人类评分最稳定对齐，FSD和SMMD可作为补充指标。


<details>
  <summary>Details</summary>
Motivation: 客观评估合成语音质量是关键挑战，人类听力测试成本高、难以大规模开展，而Fréchet Distance可靠性依赖嵌入选择和实验设置，因此需评估。

Method: 在不同嵌入和条件下全面评估FSD和SMMD，并结合人类听力评估、TTS可懂度和合成训练的ASR WER验证指标的感知相关性。

Result: WavLM Base+特征与人类评级的对齐最稳定。

Conclusion: FSD和SMMD虽不能完全取代主观评估，但可作为补充、经济高效且可重复的指标，在大规模或直接听力评估不可行时很有用。

Abstract: Objective evaluation of synthetic speech quality remains a critical challenge. Human listening tests are the gold standard, but costly and impractical at scale. Fréchet Distance has emerged as a promising alternative, yet its reliability depends heavily on the choice of embeddings and experimental settings. In this work, we comprehensively evaluate Fréchet Speech Distance (FSD) and its variant Speech Maximum Mean Discrepancy (SMMD) under varied embeddings and conditions. We further incorporate human listening evaluations alongside TTS intelligibility and synthetic-trained ASR WER to validate the perceptual relevance of these metrics. Our findings show that WavLM Base+ features yield the most stable alignment with human ratings. While FSD and SMMD cannot fully replace subjective evaluation, we show that they can serve as complementary, cost-efficient, and reproducible measures, particularly useful when large-scale or direct listening assessments are infeasible. Code is available at https://github.com/kaen2891/FrechetSpeechDistance.

</details>


### [508] [Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs](https://arxiv.org/abs/2601.21463)
*Jun Xue,Yi Chai,Yanzhen Ren,Jinshen He,Zhiqiang Tang,Zhuolin Yi,Yihuan Huang,Yuankun Xie,Yujie Chen*

Main category: cs.SD

TL;DR: 本文构建了双语数据集 AiEdit，并提出 PELM 框架用于语音编辑检测和内容定位，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音编辑检测研究难以应对新兴的端到端神经语音编辑技术，缺乏高质量数据集。

Method: 构建 AiEdit 数据集，提出 PELM 框架，将检测和定位问题转化为音频问答任务，引入词级概率先验和基于质心聚合的声学一致性感知损失。

Result: PELM 在 HumanEdit 和 AiEdit 数据集上显著优于现有方法，等错误率（EER）分别为 0.57% 和 9.28%（定位）。

Conclusion: PELM 框架在语音编辑检测和内容定位方面表现出色，能有效应对新兴语音编辑技术带来的检测挑战。

Abstract: Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\% and 9.28\% (localization), respectively.

</details>


### [509] [VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings](https://arxiv.org/abs/2601.20883)
*Bharath Krishnamurthy,Ajita Rattani*

Main category: cs.SD

TL;DR: 提出VoxMorph零样本框架用于语音变形，性能达SOTA，有代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 语音生物识别中语音变形漏洞研究少，现有方法计算成本高、不可扩展、适用范围窄且不可迁移。

Method: 将语音特征分解为韵律和音色嵌入，通过球面线性插值融合，用自回归语言模型和条件流匹配网络合成。

Result: 音频质量提升2.6倍，可懂度误差降低73%，在严格安全阈值下变形攻击成功率达67.8%。

Conclusion: 建立了实用且可扩展的语音变形范式，对生物识别安全有重要意义。

Abstract: Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/

</details>


### [510] [Text-only adaptation in LLM-based ASR through text denoising](https://arxiv.org/abs/2601.20900)
*Sergio Burdisso,Esaú Villatoro-Tello,Andrés Carofilis,Shashi Kumar,Kadri Hacioglu,Srikanth Madikeri,Pradeep Rangappa,Manjunath K E,Petr Motlicek,Shankar Venkatesan,Andreas Stolcke*

Main category: cs.SD

TL;DR: 介绍了一种仅使用文本数据将基于大语言模型的自动语音识别系统适应新领域的方法，有效提高性能。


<details>
  <summary>Details</summary>
Motivation: 在使用文本数据将基于大语言模型的自动语音识别系统适应新领域时，标准微调会破坏语音和文本模态的对齐，导致性能下降。

Method: 将音频投影任务模拟为文本去噪任务，训练大语言模型从噪声输入中恢复干净的转录。

Result: 在两个数据集上的广泛评估显示，相对改进高达 22.1%，优于最近的仅使用文本的最先进适应方法。

Conclusion: 该方法轻量级，无需架构更改或额外参数，能有效适应目标领域并保持跨模态对齐。

Abstract: Adapting automatic speech recognition (ASR) systems based on large language models (LLMs) to new domains using text-only data is a significant yet underexplored challenge. Standard fine-tuning of the LLM on target-domain text often disrupts the critical alignment between speech and text modalities learned by the projector, degrading performance. We introduce a novel text-only adaptation method that emulates the audio projection task by treating it as a text denoising task. Our approach thus trains the LLM to recover clean transcripts from noisy inputs. This process effectively adapts the model to a target domain while preserving cross-modal alignment. Our solution is lightweight, requiring no architectural changes or additional parameters. Extensive evaluation on two datasets demonstrates up to 22.1% relative improvement, outperforming recent state-of-the-art text-only adaptation methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [511] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 提出开放词汇数据集MURAD，含96,243个单词 - 定义对，支持阿语自然语言处理相关研究。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语词汇丰富，但大规模将单词与精确定义关联的词汇数据集有限。

Method: 采用混合管道，集成直接文本解析、光学字符识别和自动重建从可信参考作品和教育资源中提取数据。

Result: 得到包含96,243个单词 - 定义对的数据集，覆盖多领域，每条记录有目标单词、标准化定义和源领域元数据。

Conclusion: 发布该资源可推动阿拉伯语自然语言处理，促进词汇语义可重复研究。

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [512] [Fake News Detection After LLM Laundering: Measurement and Explanation](https://arxiv.org/abs/2501.18649)
*Rupak Kumar Das,Jonathan Dodge*

Main category: cs.CL

TL;DR: 研究评估探测器识别大语言模型改写的假新闻的效果，有多项研究发现并提供了数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能生成假新闻传播错误信息，现有针对人类编写文本的假新闻检测研究多，而大语言模型生成的假新闻检测研究较少，想要探究探测器对大语言模型改写假新闻的检测能力及改写步骤对检测的影响。

Method: 通过实验测量探测器效果，利用LIME解释分析检测失败原因。

Result: 探测器检测大语言模型改写的假新闻比检测人类编写文本更困难；找到不同任务表现出色的模型；发现检测失败可能因为情感转变；发现改写质量衡量方面令人担忧的趋势；提供了增强的数据集。

Conclusion: 明确了大语言模型改写假新闻检测的现状和问题，并提供了相关数据集助力后续研究。

Abstract: With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub

</details>


### [513] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: 本文指出现有池化策略弱点，提出Landmark (LMK) 池化方法，在长上下文任务有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于池化操作的序列编码器存在系统性弱点，如[CLS]集中信息于序列初始位置、均值池化稀释局部信号。

Method: 引入Landmark (LMK) 池化，将序列分块，在块间插入地标标记，对地标标记嵌入进行均值池化形成最终表示。

Result: LMK池化在短上下文检索任务与现有方法表现相当，在长上下文任务有显著提升。

Conclusion: LMK池化是现有池化方法实用且可扩展的替代方案。

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [514] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: 现有大语言模型在文化敏感决策中存在校准问题，本文提出OG - MAR框架，实验表明其能提升文化校准和鲁棒性，且推理过程更透明。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文化敏感决策中因预训练数据偏差和缺乏结构化价值表示而存在校准问题，现有方法缺乏人口统计学依据且将价值观视为独立无结构信号。

Method: 提出OG - MAR框架，从世界价值观调查中总结特定受访者价值观，通过能力问题构建全球文化本体，推理时检索本体一致关系和人口统计学相似档案，实例化多个价值角色代理，由判断代理综合输出。

Result: 在四个大语言模型骨干的区域社会调查基准测试中，OG - MAR比竞争基线提高了文化校准和鲁棒性，且产生更透明的推理痕迹。

Conclusion: OG - MAR框架能有效解决大语言模型在文化敏感决策中的校准问题，具有更好的性能和可解释性。

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [515] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 提出Fed - MedLoRA和Fed - MedLoRA+联邦学习框架适配大语言模型到医学应用，用于临床信息提取并评估。


<details>
  <summary>Details</summary>
Motivation: 多数医疗大语言模型在单一机构数据上训练，泛化性和安全性有限；传统联邦学习应用于医学大语言模型有计算资源和数据异质性问题。

Method: 引入模型无关且参数高效的联邦学习框架Fed - MedLoRA和Fed - MedLoRA+，将其应用于临床信息提取，通过与多种模型对比评估。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [516] [When "Better" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications](https://arxiv.org/abs/2601.22025)
*Daniel Commey*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLM）应用评估的特点，提出评估驱动工作流，介绍评估套件，分析实验结果并强调评估驱动迭代的重要性。


<details>
  <summary>Details</summary>
Motivation: LLM 应用输出具有随机性、高维性等特点，与传统软件测试不同，需要新的评估方法。

Method: 提出 Define, Test, Diagnose, Fix 工作流，引入 Minimum Viable Evaluation Suite（MVES），综合常见评估方法并讨论失败模式，进行可重复本地实验。

Result: 通用“改进”提示模板会导致行为权衡，如提取通过率和 RAG 合规率下降，但指令遵循性提升。

Conclusion: 应采用评估驱动的提示迭代，谨慎校准声明，而非通用提示配方。

Abstract: Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic "improved" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
  All test suites, harnesses, and results are included for reproducibility.

</details>


### [517] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: 研究指出大语言模型用于教育评估的假设存在缺陷，提出方法暴露其漏洞，揭示模型高失败率，建议评估范式转变。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型用于教育评估时，指令遵循能力能否直接转化为客观评判这一假设是否成立。

Method: 引入SPACI框架和AST - ASIP协议，利用语法 - 语义差距将对抗指令嵌入抽象语法树的非关键区域，对9个SOTA模型进行大规模评估。

Result: 高容量开放权重模型失败率超95%，会优先考虑隐藏格式约束而非代码正确性，存在大量“错误认证”情况。

Conclusion: 当前对齐范式在自动评分中存在“特洛伊”漏洞，需从标准RLHF转向特定领域的裁决鲁棒性。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [518] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 论文先揭示改写检测算法原理，再提出自适应学习原文与改写文本距离的新检测算法，理论和实验均表明其性能优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型产生类人文本引发虚假信息和学术诚信问题，需可靠算法检测LLM生成内容。

Method: 先以几何方法揭示改写检测算法原理，再提出自适应学习距离的改写检测算法。

Result: 进行超100种设置的实验，在多数场景表现优于基线算法，相比最强基线有57.8%至80.6%的相对提升。

Conclusion: 采用自适应学习的距离函数比固定距离用于检测更有效。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [519] [UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop](https://arxiv.org/abs/2601.21000)
*Muhammad Ali Shafique,Areej Mehboob,Layba Fiaz,Muhammad Usman Qadeer,Hamza Farooq*

Main category: cs.CL

TL;DR: 本文提出上下文集成翻译框架创建乌尔都语推理基准，评估大模型，揭示任务挑战，建立评估方法，适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力评估中，低资源语言缺乏标准化基准，乌尔都语评估受机器翻译敏感性和非推理基准的限制。

Method: 提出上下文集成翻译框架，结合人工验证，利用多翻译系统开发乌尔都语推理基准；翻译多个基准到乌尔都语组成UrduBench，用多种提示策略评估推理和指令微调大模型。

Result: 分析揭示了不同数据集、任务难度、模型架构、模型规模设置和语言一致性测试下的性能差异；发现多步骤和符号推理任务对乌尔都语有挑战，稳定语言对齐是稳健推理的关键。

Conclusion: 建立了乌尔都语标准化推理评估的可扩展方法，为多语言推理失败提供实证见解，实验设置适用于其他低资源语言。

Abstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.

</details>


### [520] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 本文比较了数据混合和模型合并两种创建小型多任务代码大语言模型的方法，在不同规模下得出不同策略的优劣，并引入权重分析技术，表明合理策略可有效结合任务能力。


<details>
  <summary>Details</summary>
Motivation: 近期研究推动在智能框架中部署小型专业代码大语言模型，引发对平衡性能、约束和成本的多任务学习策略的关注。

Method: 比较数据混合与模型合并两种方法，在Qwen Coder和DeepSeek Coder两个模型家族的2B和7B参数规模上，针对代码生成和总结任务进行微调，并在多个基准上评估。引入权重分析技术。

Result: 在较大规模下，模型合并整体性能最佳，在代码生成任务上保留96%的专业模型性能，还能超越单独微调模型；小规模下数据混合是首选策略。

Conclusion: 精心设计的合并和混合策略可有效结合特定任务能力，且性能无显著下降，适用于资源受限场景。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [521] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: 提出Output - Space Search (OS - Search)，将LLM生成转化为端点搜索，在文本和代码任务中展现优势。


<details>
  <summary>Details</summary>
Motivation: 改进LLM生成方式，避免路径依赖的token/program搜索，实现并行搜索和黑盒优化。

Method: 将LLM生成转化为端点搜索，外循环在冻结编码器定义的3D输出空间Z中选择目标z*，用序列级RL训练检索基础策略生成输出。

Result: 在故事任务中，Z空间扫描比提示链的LLM评分多样性高3.1倍；在代码任务中，Z空间上的贝叶斯优化能在不告知控制器目标的情况下，在匹配推理预算下提高目标值并保持有效性。

Conclusion: OS - Search是一种有效的LLM生成方法，能提高生成的多样性和优化效果。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [522] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 研究从线性输入学习层次结构的统计条件，聚焦功能词分布，发现保留特定属性的语言变体更易被学习，不同学习条件依赖功能词方式不同。


<details>
  <summary>Details</summary>
Motivation: 探究支持从线性输入学习层次结构的统计条件，因功能词分布特性在语言习得中起重要作用。

Method: 先进行跨语言语料库分析确定功能词三属性存在于186种语言，再用反事实语言建模和消融实验研究语言变体学习情况，最后进行后续探测和消融分析。

Result: 保留三属性的语言变体更易被神经学习者习得，频率和结构关联贡献比边界对齐更强，不同学习条件对功能词依赖不同。

Conclusion: 相似的学习表现可能源于不同的内部机制。

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [523] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 论文探索嵌入扩展来解决大语言模型中混合专家架构缩放递减和系统瓶颈问题，推出LongCat - Flash - Lite模型，表现超越基线和同类模型。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）架构在大语言模型的稀疏缩放中面临收益递减和系统级瓶颈问题，需探索新的稀疏缩放维度。

Method: 通过综合分析和实验确定嵌入扩展优于专家扩展的范围，刻画影响效果的关键架构因素，集成系统优化和推测解码，推出LongCat - Flash - Lite模型。

Result: LongCat - Flash - Lite模型虽在嵌入上分配大量参数，但超越参数等效的MoE基线模型，在代理和编码领域极具竞争力。

Conclusion: 嵌入扩展是一种强大且正交的稀疏缩放维度，能够有效提升模型性能和推理速度。

Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [524] [MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation](https://arxiv.org/abs/2601.21225)
*Tianyi Xu,Kosei Uemura,Alfred Malengo Kondoro,Tadesse Destaw Belay,Catherine Nana Nyaah Essuman,Ifeoma Okoh,Ganiyat Afolabi,Ayodele Awokoya,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文引入了MGSM - Pro数据集，对九种语言进行评估，发现低资源语言在不同数字实例测试中表现下降，不同模型对数字实例的鲁棒性不同，并建议用至少五种数字变体实例评估数学推理。


<details>
  <summary>Details</summary>
Motivation: 现有多语言数学推理基准发展滞后，GSM - Symbolic评估仅在英语中进行，需要更全面的多语言评估。

Method: 引入MGSM - Pro数据集，对每个MGSM问题提供五种实例变体，在九种语言上进行评估。

Result: 低资源语言在不同数字实例测试中性能下降；Gemini 2.5 Flash和GPT - 4.1对数字实例鲁棒性较差，Claude 4.0 Sonnet较稳健；GPT - OSS 120B和DeepSeek V3在开放模型中鲁棒性更强。

Conclusion: 建议对每个问题使用至少五种数字变体实例来评估数学推理，以获得更稳健和现实的评估结果。

Abstract: Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.

</details>


### [525] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: 提出SHARP框架评估大语言模型社会危害，发现平均风险相似的模型尾部风险差异大，建议评估治理需多维、关注尾部。


<details>
  <summary>Details</summary>
Motivation: 通用评估基准将复杂社会风险简化为均值分数，掩盖分布结构、交叉维度影响和最坏情况，需新评估方法。

Method: 引入SHARP框架，将危害建模为多元随机变量，综合多维度分解和联合失败聚合，用风险敏感分布统计和CVaR95表征最坏情况。

Result: 对十一个前沿大语言模型评估显示，平均风险相似模型尾部暴露和波动性差异超两倍，各危害维度边际尾部行为有系统差异。

Conclusion: 大语言模型的评估和治理需从标量平均转向多维、对尾部敏感的风险分析。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [526] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出新预训练方法提升大模型生成的安全性、事实性和整体质量，实验显示有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有解决大模型生成问题的方法无法保证修正预训练中学习的模式，因此在预训练阶段解决问题至关重要。

Method: 引入新的预训练方法，流式处理文档并使用强化学习改进每一步的接下来K个生成的标记，用强后训练模型评判候选生成结果的质量、安全性和事实性。

Result: 在事实性和安全性方面比标准预训练分别有36.2%和18.5%的相对提升，整体生成质量胜率提升达86.3%。

Conclusion: 该方法能从根本上构建更高质量、更安全、更符合事实的模型。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [527] [Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation](https://arxiv.org/abs/2601.21464)
*Yuan Sui,Bryan Hooi*

Main category: cs.CL

TL;DR: 论文提出CoNL框架，通过多智能体自我博弈统一生成、评估和元评估，在五个基准测试中取得比自我奖励基线更好的效果且训练稳定。


<details>
  <summary>Details</summary>
Motivation: 非可验证任务训练大语言模型因缺乏真实标签有挑战，LLM-as-Judge方法受评估者质量限制，需进行元评估。

Method: 引入CoNL框架，让多个共享相同策略的智能体进行结构化对话，提出、批评和修改解决方案，通过诊断奖励进行元评估和联合优化。

Result: 在五个基准测试中，CoNL比自我奖励基线有持续改进，且训练稳定。

Conclusion: CoNL框架能有效解决非可验证任务训练大语言模型问题，可联合优化生成和判断能力。

Abstract: Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.

</details>


### [528] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: 本文介绍无学习框架ILRR用于控制离散扩散语言模型文本生成，还引入扩展方法，实验证明ILRR开销小且能提升属性准确率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（DLMs）在推理时间控制的有效机制方面研究不足，需探索新方法。

Method: 引入无学习框架ILRR，通过动态对齐生成序列和参考序列的内部激活来引导生成；引入Spatially Modulated Steering，调节序列引导强度以用短参考引导长文本。

Result: ILRR在LLaDA和MDLM架构上实现有效属性控制，计算开销小，在相同计算预算下，属性准确率比基线提升10%到60%，且保持高生成质量。

Conclusion: ILRR是一种有效的引导离散扩散语言模型文本生成的方法，能兼顾属性控制和计算效率。

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [529] [FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning](https://arxiv.org/abs/2601.21682)
*Xiaoyu Xu,Minxin Du,Kun Fang,Zi Liang,Yaxin Xiao,Zhicong Huang,Cheng Hong,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 提出it框架用于持续遗忘大量删除请求，维护遗忘效果和效用保留平衡，用	extbf{PCH}基准和对称指标评估，实验显示it效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法未考虑现实删除请求的持续性和高容量特性，会导致效用下降和灾难性遗忘。

Method: 引入it框架，通过严格数据过滤、重要性感知更新和目标层归因减轻退化；提出	extbf{PCH}基准和Forget Degree、Retain Utility两个对称指标进行评估。

Result: 在四个开源大语言模型上进行数百次删除请求的实验表明，it在遗忘程度和保留效用之间取得最佳平衡，在多个数据集上超越现有方法，且能抵御重学习和量化恢复攻击。

Conclusion: it框架能够有效处理大量删除请求，在遗忘效果和效用保留之间达到良好平衡，优于现有方法。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \fit mitigates degradation through rigorous data \underline{F}iltering, \underline{I}mportance-aware updates, and \underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \textbf{PCH}, a benchmark covering \textbf{P}ersonal information, \textbf{C}opyright, and \textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.

</details>


### [530] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: 论文针对大语言模型长链思维推理的问题，提出TACLer框架，能减少计算成本并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长链思维推理需大规模强化学习训练，且易导致过度思考和冗余中间步骤，要提高学习和推理效率并保证性能。

Method: 提出TACLer模型定制课程强化学习框架，包含定制课程学习和混合思维/非思维推理范式。

Result: TACLer在学习和推理上有双重优势，减少超50%训练计算成本、超42%推理令牌使用，且在基础模型上提升超9%准确率。

Conclusion: TACLer框架有效，能在提升效率的同时提高性能，优于现有基线模型。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [531] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 现有NLP模型评估可持续发展报告可靠性欠佳，本文提出参数高效框架，实验显示其更稳健且发现表征刚性和泛化间的权衡。


<details>
  <summary>Details</summary>
Motivation: 可持续发展报告对ESG评估重要，但绿色洗绿和模糊表述降低可靠性，现有NLP模型缺乏应对这些问题的鲁棒性。

Method: 提出结合对比学习和序数排名目标来构建大语言模型潜在空间的参数高效框架，采用门控特征调制过滤披露噪声，利用MetaGradNorm稳定多目标优化。

Result: 在跨类别设置实验中，比标准基线表现出更优的鲁棒性，发现表征刚性和泛化之间存在权衡。

Conclusion: 所提出的框架能有效应对可持续发展报告中的问题，提高评估的可靠性，但要注意表征刚性和泛化间的平衡。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [532] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 受连分数启发提出CoFrGeNets生成网络，替换Transformer模块有更少参数，实验显示其性能相当甚至更优且预训练时间短。


<details>
  <summary>Details</summary>
Motivation: 引入新的函数类用于生成式建模，设计可替代Transformer模块且参数更少的组件。

Method: 基于连分数的新函数类设计CoFrGeNets，推导自定义梯度公式，在GPT2 - xl和Llama3上实验。

Result: 模型在下游任务表现与原模型相当甚至更优，参数为原来的2/3到1/2，预训练时间更短。

Conclusion: 未来针对硬件的定制实现将进一步发挥该架构潜力。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [533] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: 提出KID框架用于有害模因检测，在多语言数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 互联网模因依赖隐喻和社会文化背景传播有害内容，现有方法对隐式毒性理解不足，需背景知识，自动化内容审核面临挑战。

Method: 提出KID框架，采用标签约束蒸馏范式将复杂模因理解分解为结构化推理链，结合外部知识；采用双头架构联合优化语义生成和分类目标。

Result: 在五个多语言数据集上，KID在二分类和多标签有害模因检测任务中达到SOTA性能，主要评估指标比先前最佳方法提高2.1% - 19.7%，消融实验证实知识注入和双头联合学习有效。

Conclusion: KID框架能实现对模因的鲁棒和可泛化理解，知识注入和双头联合学习有互补作用。

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [534] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: 研究大语言模型思维链推理在推理跳泛化场景性能下降问题，发现错误集中于特定类型，源于内部竞争机制，提出测试时推理校正方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 思维链推理在推理跳泛化场景性能下降且内在机制不明，需深入研究并解决该问题。

Method: 对多领域任务系统研究，分析错误集中情况及内部竞争机制，提出动态识别和停用错误处理头的测试时推理校正方法。

Result: 不同任务和大语言模型上的大量实验表明，该方法持续提升了推理跳泛化能力。

Conclusion: 所提测试时推理校正方法有效且有潜力。

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [535] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: 提出Token - Guard方法控制大语言模型幻觉，实验证明其能有效减少幻觉并提高生成准确性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和RLHF缓解大语言模型幻觉问题时资源需求大，解码方法缺乏显式幻觉控制。

Method: 提出基于自我检查解码的Token - Guard方法，在推理步骤进行内部验证，在潜在空间评估候选片段并进行迭代修剪和再生。

Result: 在HALU数据集实验中，Token - Guard大幅减少幻觉并提高生成准确性。

Conclusion: Token - Guard是一种可扩展、模块化的解决方案，能让大语言模型输出更可靠。

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [536] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: 提出MDA框架追溯大语言模型可解释单元至特定训练样本，验证干预样本影响、揭示数据作用，证明归纳头与上下文学习联系，提出加速电路收敛的方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型可解释电路在训练数据中的因果起源不清楚的问题。

Method: 引入Mechanistic Data Attribution (MDA)框架，使用影响函数追溯可解释单元到特定训练样本，并在Pythia系列模型上进行大量实验。

Result: 有针对性地干预高影响样本能显著调节可解释头的出现，随机干预无效果；重复结构数据是机制催化剂；干预归纳头形成会改变模型的上下文学习能力。

Conclusion: 为归纳头和上下文学习功能联系假设提供直接因果证据，提出的机制数据增强管道可加速不同规模模型的电路收敛。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [537] [Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models](https://arxiv.org/abs/2601.22035)
*Longxuan Yu,Yu Fu,Shaorong Zhang,Hui Liu,Mukund Varma T,Greg Ver Steeg,Yue Dong*

Main category: cs.CL

TL;DR: 本文指出自回归语言模型在输出结构与推理顺序冲突时存在局限，提出掩码扩散语言模型（MDLMs）能解耦计算顺序与输出结构，通过实验验证其“顺序鲁棒性”并分析失效条件。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型固定的从左到右生成顺序，在输出结构与自然推理冲突时存在根本局限，需新模型解决该问题。

Method: 在GSM8K、Math500和新引入的ReasonOrderQA基准上验证MDLMs解耦计算顺序与输出结构的能力，对比AR模型和MDLMs在不同提示顺序下的表现。

Result: 当提示要求先输出答案再推理时，AR模型准确率大幅下降（最多相对下降67%），而MDLMs保持稳定（相对下降≤14%），MDLMs通过在扩散过程中先稳定简单标记来实现顺序鲁棒性。

Conclusion: MDLMs具有顺序鲁棒性，但也存在使该优势减弱的失效条件，明确了顺序鲁棒性的局限。

Abstract: Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term "order robustness". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.

</details>


### [538] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: 研究指出小语言模型中嵌入矩阵参数分配不合理，提出Leviathan架构，在等参设置下表现优于标准架构，有效参数容量更高。


<details>
  <summary>Details</summary>
Motivation: 指出在小语言模型中，嵌入矩阵在参数预算中占主导地位的分配方式既次优又违反直觉，需要改进。

Method: 提出具有连续嵌入生成器的Leviathan架构，取代传统模型的离散查找表，并在Pile数据集上进行评估。

Result: 在等参设置下，Leviathan始终优于标准的LLaMA风格架构，通过经验幂律拟合显示出明显更优的有效参数容量，表现如同参数多1.47 - 2.11倍的密集模型。

Conclusion: Leviathan架构在小语言模型中具有更好的性能和参数利用效率。

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [539] [KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices](https://arxiv.org/abs/2601.21579)
*Wuyang Zhou,Yuxuan Gu,Giorgos Iacovides,Danilo Mandic*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\mathcal{O} \left( nC \cdot n! \right)$. To address both challenges, we propose \textbf{KromHC}, which uses the \underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \texttt{https://github.com/wz1119/KromHC}.

</details>


### [540] [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101)
*Mahdi Nikdan,Amir Zandieh,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 提出误差补偿优化器ECO消除大语言模型训练中主权重，减少内存开销，实验显示其精度接近基线，改善内存与验证损失的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法在大语言模型训练中仍依赖高精度主权重更新，导致显著内存开销，尤其在稀疏专家混合模型中。

Method: 引入ECO，直接对量化参数应用更新，在每步后量化权重并将量化误差注入优化器动量，形成无额外内存的误差反馈循环。

Result: 理论证明在标准假设和衰减学习率下，ECO收敛到最优解的常数半径邻域；实验表明在不同模型和量化精度下，ECO精度接近带主权重的基线。

Conclusion: ECO可消除主权重，显著改善静态内存与验证损失的帕累托前沿。

Abstract: Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.

</details>


### [541] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: 提出Recycling Search Experience (RSE)策略提升大语言模型推理效率，理论和实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间缩放策略将搜索试验视为一次性样本，导致计算冗余，缺乏信息重用。

Method: 提出RSE策略，将测试时间搜索转化为累积过程，通过构建经验库进行正负回收。

Result: 理论分析证明RSE效率提升，实验显示在多基准测试中RSE能以相近计算成本超越强基线。

Conclusion: RSE策略能有效解决复杂推理任务，实现了最先进的缩放效率。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [542] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: 提出PIR推理范式让大语言模型从被动求解变为主动问询，实验显示其性能优于基线模型，有良好泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前面向推理的大语言模型在“盲目自我思考”范式下，即使关键信息缺失或模糊也进行内部推理，存在局限性。

Method: 通过不确定性感知的监督微调程序赋予模型交互推理能力，以及基于用户模拟器的策略优化框架使模型行为与用户意图一致。

Result: 在数学推理、代码生成和文档编辑等实验中，PIR表现优于基线模型，提高了准确率、通过率和BLEU值，减少推理计算和不必要交互回合；可靠性评估证实其泛化和鲁棒性。

Conclusion: PIR是一种有效的推理范式，能提升大语言模型推理能力，代码和模型已公开。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [543] [Procedural Pretraining: Warming Up Language Models with Abstract Data](https://arxiv.org/abs/2601.21725)
*Liangze Jiang,Zachary Shinnick,Anton van den Hengel,Hemanth Saratchandran,Damien Teney*

Main category: cs.CL

TL;DR: 研究以抽象结构化程序数据预训练语言模型，能提升模型性能、加快训练速度。


<details>
  <summary>Details</summary>
Motivation: 探索以抽象结构化数据预训练模型，辅助后续丰富语义知识的获取，类比人类学习逻辑和数学再进行高级推理。

Method: 诊断不同程序数据对算法技能的提升，研究大模型预训练效果，探索机制，规划结合多种程序数据的路径。

Result: 程序数据预训练显著提升特定任务准确率，少量程序数据预训练效果超标准预训练，减少原数据量并能达到相同损失值，还会在模型层中注入结构。

Conclusion: 程序预训练是提升性能、加速预训练的简易方法，暗示在大语言模型中将知识获取与推理分离的前景。

Abstract: Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.
  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.

</details>


### [544] [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)
*Hang Ding,Peidong Liu,Junqiao Wang,Ziwei Ji,Meng Cao,Rongzhao Zhang,Lynn Ai,Eric Yang,Tianyu Shi,Lei Yu*

Main category: cs.CL

TL;DR: 本文介绍DynaWeb框架，通过与网页世界模型交互训练网络代理，结合真实专家轨迹提升性能，实验证明其能显著提升开源网络代理模型表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和强化学习驱动的自主网络代理训练受限于与真实互联网交互的效率、成本和风险问题，模型基强化学习可能是解决方案。

Method: 提出DynaWeb这一新型MBRL框架，通过与网页世界模型交互训练网络代理，训练中结合真实专家轨迹与策略滚动。

Result: 在WebArena和WebVoyager基准测试上，DynaWeb持续且显著提升了先进开源网络代理模型的性能。

Conclusion: 通过想象训练网络代理是可行的，为在线代理式强化学习提供了可扩展且高效的方法。

Abstract: The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.

</details>


### [545] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出HALO将Transformer模型蒸馏为RNN - 注意力混合模型，构建HypeNet架构，用少量数据将Qwen3系列转换为HypeNet，实现与原模型相当性能且长上下文表现和效率更优。


<details>
  <summary>Details</summary>
Motivation: 混合Transformer架构从头预训练成本高，现有转换方法需大量数据且长上下文性能差，本文旨在解决这些问题。

Method: 提出HALO蒸馏管道，构建HypeNet架构，采用新位置编码方案HyPE和架构修改。

Result: 用HALO将Qwen3系列转换为HypeNet，性能与原Transformer模型相当，长上下文性能和效率更优，转换只需2.3B令牌。

Conclusion: 所提方法能以低成本将Transformer模型转换为混合模型，实现良好性能和效率。

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


### [546] [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146)
*Ajay Patel,Colin Raffel,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 提出将互联网规模预训练文档知识转化为数十亿合成指令和答案训练对的方法，生成FineInstructions数据集，预训练效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型监督训练数据有限的问题。

Method: 将约1800万个从真实用户查询和提示创建的指令模板与非结构化预训练语料库中的人类编写源文档匹配并实例化，生成合成训练数据。

Result: 在标准基准测试中，基于FineInstructions的预训练在衡量自由形式响应质量方面优于标准预训练和其他合成预训练技术。

Conclusion: 该方法能利用互联网预训练文档生成大量合成训练数据，提升大语言模型预训练效果。

Abstract: Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [547] [Deletion-correcting codes for an adversarial nanopore channel](https://arxiv.org/abs/2601.21236)
*Huiling Xie,Zitan Chen*

Main category: cs.IT

TL;DR: 研究对抗性纳米孔信道的删纠错码，给出明确构造并分析冗余符号情况，与经典信道对比。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性纳米孔信道中至多发生t个删除错误时的删纠错码。

Method: 提出长度为n的q元码的明确构造。

Result: 该构造的冗余符号为2tlog_q n + Θ(loglog n)，最优冗余在tlog_q n + Ω(1)和2tlog_q n - log_qlog_2 n + O(1)之间，与经典信道已知构造对比。

Conclusion: 所提明确构造在一阶上与存在性上界匹配。

Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+Θ(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+Ω(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+ε)\log_q n+o(\log n)$.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [548] [How Expressive Are Graph Neural Networks in the Presence of Node Identifiers?](https://arxiv.org/abs/2601.21882)
*Arie Soeteman,Michael Benedikt,Martin Grohe,Balder ten Cate*

Main category: cs.LO

TL;DR: 研究节点特征作为唯一标识符对GNN表达能力的影响，并探讨GNN关键不变表达能力。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中，GNN处理的图有作为唯一标识符的节点特征，需研究其对GNN表达能力的影响。

Method: 受有限模型理论中顺序不变可定义性概念启发，研究GNN的关键不变表达能力，并针对各种具有局部最大或求和聚合的GNN类进行分析。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Graph neural networks (GNNs) are a widely used class of machine learning models for graph-structured data, based on local aggregation over neighbors. GNNs have close connections to logic. In particular, their expressive power is linked to that of modal logics and bounded-variable logics with counting. In many practical scenarios, graphs processed by GNNs have node features that act as unique identifiers. In this work, we study how such identifiers affect the expressive power of GNNs. We initiate a study of the key-invariant expressive power of GNNs, inspired by the notion of order-invariant definability in finite model theory: which node queries that depend only on the underlying graph structure can GNNs express on graphs with unique node identifiers? We provide answers for various classes of GNNs with local max- or sum-aggregation.

</details>
