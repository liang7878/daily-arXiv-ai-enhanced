<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 10]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 12]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 45]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.SP](#eess.SP) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: 本文提出DQInit方法将值函数初始化扩展到深度强化学习，通过重用紧凑表格Q值、采用基于已知性机制，实验表明其能提升学习效率、稳定性和整体性能。


<details>
  <summary>Details</summary>
Motivation: 值函数初始化在表格设置中成熟，但扩展到深度强化学习面临状态动作空间连续、神经网络近似有噪声、存储过往模型不现实等挑战。

Method: 提出DQInit方法，重用从先前解决任务中提取的紧凑表格Q值作为可转移知识库，采用基于已知性机制将转移值软集成到探索不足区域。

Result: 在多个连续控制任务的实验中，DQInit相比标准初始化和现有转移技术，持续提升早期学习效率、稳定性和整体性能。

Conclusion: DQInit为深度强化学习中的知识转移提供新视角，有效结合快速启动强化学习和策略蒸馏优点并减轻其缺点。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 文章介绍了Othello AI Arena这一新型基准框架，用于评估智能系统限时适应未知环境的能力，具有多维度评估等特点，是培养和评估AI系统快速智能适应能力的工具。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准大多聚焦固定环境下的性能优化，未评估系统面对规则或结构变化时的灵活性和泛化能力，需要新的评估框架。

Method: 引入Othello AI Arena，设置元学习挑战，参与者需在60秒内分析新奥赛罗棋盘规则并生成策略，平台有不同游戏阶段，采用多维度指标自动评估。

Result: 初步测试和学生参与显示出从快速参数调整到通过模拟进行环境模型学习等适应方法的有趣模式。

Conclusion: Othello AI Arena是培养和评估AI系统快速智能适应能力的独特教育工具和有价值的研究基准。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出基于大语言模型和多智能体协作的自动化多模态评估框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI助手评估方法存在高人力成本、标准不一致和主观偏差等问题。

Method: 提出基于大语言模型和多智能体协作的自动化多模态评估框架，采用三层智能体架构，并在Qwen3 - 8B模型上进行监督微调。

Result: 实现与人类专家显著的评估匹配准确率，在八个主要智能体上的实验验证了框架在预测用户满意度和识别生成缺陷方面的有效性。

Conclusion: 所提出的自动化多模态评估框架是有效的。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 提出EvoCurr框架，利用课程生成LLM构建难度递增的问题序列，实验表明该方法能提升决策任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂问题时性能下降，直接解决方法缺乏结构化中间指导。

Method: 提出EvoCurr框架，用课程生成LLM为求解器LLM构建难度递增的问题序列，动态调整难度。

Result: 在决策基准测试中，该方法相比直接求解基线显著提高了任务成功率和解决方案效率。

Conclusion: 基于大语言模型的课程学习在增强现实高复杂度领域的自动推理方面有很大潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 本文提出分解SHAP值不确定性的方法，通过实验提供对SHAP解释可靠性和可解释性的理解，指出高SHAP值特征不一定稳定，可通过数据和技术减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有SHAP值通常作为点估计，忽略了预测模型和数据中的不确定性，需要分解SHAP值的不确定性。

Method: 整合Dempster - Shafer证据理论和通过狄利克雷过程在树集成上进行假设采样，对SHAP值不确定性分解为偶然、认知和纠缠成分。

Result: 通过三个真实用例验证方法，实验提供了对SHAP解释中认知不确定性本质的洞察，能更全面理解SHAP归因的可靠性和可解释性。

Conclusion: 高SHAP值特征不一定最稳定，可通过更好数据和合适模型开发技术减少认知不确定性，树基模型利于量化认知不确定性。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: 论文提出MEML - GRPO框架解决标准RLVR奖励稀疏问题，实验显示该框架能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR在奖励稀疏问题上存在挑战，尤其在困难任务中，零奖励无法提供学习信号。

Method: 提出MEML - GRPO框架，利用多样专家提示生成更多响应，引入专家间互学机制促进知识共享。

Result: 在多个推理基准测试中，使用Qwen平均性能提升4.89%，使用Llama提升11.33%。

Conclusion: MEML - GRPO能有效克服传统RLVR方法的核心局限。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文指出大语言模型成对评估存在偏好偏差问题，提出无监督去偏对齐框架UDA，实验表明其能减少评判者间评分标准差、提升与人类判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型成对评估中存在的偏好偏差，导致不同评判者排名不一致和有偏差的问题。

Method: 提出UDA框架，通过动态调整Elo评级系统减少评判者间分歧，利用紧凑神经网络自适应设置K因子和细化获胜概率，且以无监督方式运行。

Result: UDA显著降低评判者间评分标准差达63.4%，将与人类判断的平均相关性提高24.7%，提升表现差的评判者表现。

Conclusion: UDA能构建更稳健可靠的评估生态系统。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 引入PacifAIst基准测试大语言模型自利行为，评估8个模型显示性能差异，强调需标准化工具确保AI行为‘和平主义’。


<details>
  <summary>Details</summary>
Motivation: 当前安全基准未系统测试模型工具性目标与人类安全冲突场景，需评估模型潜在行为对齐情况。

Method: 引入含700个场景的PacifAIst基准，围绕存在优先级分类评估。

Result: 8个模型表现有显著层次，Gemini 2.5 Flash得分最高，GPT - 5最低，不同模型在各子类别表现不同。

Conclusion: 需要像PacifAIst这样的标准化工具来衡量和降低风险，确保未来AI行为优先考虑‘和平主义’。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: 证明公共观察逻辑（POL）的可满足性问题是2EXPTIME - 完全的。


<details>
  <summary>Details</summary>
Motivation: 知识和行动推理逻辑在多智能体系统有诸多应用，基于周围观察的知识变化是规划场景的关键方面，研究POL的可满足性问题有其必要性。

Method: 未提及具体方法。

Result: 证明了POL的可满足性问题是2EXPTIME - 完全的。

Conclusion: 明确了POL可满足性问题的复杂度为2EXPTIME - 完全。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: 本文提出VIPCGRL框架提升PCGRL的类人性，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有系统在体现以人类为中心的行为方面不足，限制了AI驱动生成工具在现实设计工作流中的实用性。

Method: 提出VIPCGRL框架，融合文本、关卡和草图三种模态，引入通过四重对比学习训练的共享嵌入空间，并基于嵌入相似度使用辅助奖励调整策略。

Result: 实验结果表明VIPCGRL在类人性方面优于现有基线，得到定量指标和人工评估的验证。

Conclusion: VIPCGRL框架能有效扩展控制模态并增强类人性，提升AI驱动生成工具在现实设计工作流中的实用性。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 大语言模型发展使智能体利用工具解决问题，但依赖多工具产生新挑战，提出动态监督和操纵机制构建MAS架构，实验显示其提升效果和稳定性，在GAIA排行榜获第一，凸显协作智能体角色价值。


<details>
  <summary>Details</summary>
Motivation: 随着智能体对多工具依赖增加，面临上下文扩展、工具输出噪声等问题，影响系统可靠性和准确性，需增强基于智能体系统的稳定性。

Method: 引入动态监督和操纵机制，在AWorld框架内构建MAS架构，执行智能体在关键步骤调用守护智能体验证和纠正推理过程。

Result: 在GAIA测试数据集上实验表明，动态操纵机制显著提升解决方案的有效性和稳定性，优于单智能体系统和标准工具增强系统，动态MAS系统在GAIA排行榜开源项目中获第一。

Conclusion: 协作智能体角色在开发更可靠、可信赖的智能系统方面具有实际价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出整合监管三元组知识图谱与检索增强生成的多智能体框架处理监管合规问答，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 监管合规问答对信息准确性、可验证性和领域专业知识有要求，大语言模型面临挑战。

Method: 构建并维护无本体知识图谱，将三元组嵌入并存储在向量数据库，利用三元组检索进行问答。

Result: 混合系统在复杂监管查询中表现优于传统方法，能保证事实正确性、实现可追溯性和增强理解。

Conclusion: 该框架为合规和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估四个大语言模型解决三类数学任务的准确性，找出推理错误，发现推理增强的OpenAI o1模型表现好，双智能体配置可提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学教育中应用广泛，评估其解决数学问题的准确性，以确保数学教育中反馈和评估的可靠性和精确性。

Method: 构建对大语言模型有挑战性且易出错的数学任务，系统分析最终答案准确性和单个步骤错误，测试单智能体和双智能体配置。

Result: 推理增强的OpenAI o1模型在三类数学任务中准确率高或接近完美；过程失误是最常见错误，显著影响整体表现；双智能体配置大幅提升整体性能。

Conclusion: 研究结果为提升大语言模型性能提供可行见解，强调了将其有效集成到数学教育中的策略，推动了人工智能驱动的教学实践和评估精度。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [14] [Large-Scale Topology Optimisation of Time-dependent Thermal Conduction Using Space-Time Finite Elements and a Parallel Space-Time Multigrid Preconditioner](https://arxiv.org/abs/2508.09589)
*Joe Alexandersen,Magnus Appel*

Main category: cs.CE

TL;DR: 提出用于时变热传导问题的时空拓扑优化框架，求解速度快、扩展性好，比传统方法有显著加速。


<details>
  <summary>Details</summary>
Motivation: 显著减少时变热传导问题的求解时间。

Method: 将时间视为额外空间维度，用稳定连续Galerkin时空有限元方法离散控制方程，用迭代Krylov求解器和采用半粗化策略的并行时空多重网格方法求解，在全并行计算框架中实现。

Result: 该方法在分布式内存超级计算机上扩展性良好，能求解高达42亿自由度问题，比传统时间步长方法最多快52倍，核心小时计算成本仅适度增加，通过基准问题验证和材料属性变化展示灵活性。

Conclusion: 所提出的时空方法是热应用中大规模时变拓扑优化的有前景方法。

Abstract: This paper presents a novel space-time topology optimisation framework for
time-dependent thermal conduction problems, aiming to significantly reduce the
time-to-solution. By treating time as an additional spatial dimension, we
discretise the governing equations using a stabilised continuous Galerkin
space-time finite element method. The resulting large all-at-once system is
solved using an iterative Krylov solver preconditioned with a parallel
space-time multigrid method employing a semi-coarsening strategy. Implemented
in a fully parallel computing framework, the method yields a parallel-in-time
method that demonstrates excellent scalability on a distributed-memory
supercomputer, solving problems up to 4.2 billion degrees of freedom.
Comparative studies show up to 52x speed-up over traditional time-stepping
approaches, with only moderate increases in total computational cost in terms
of core-hours. The framework is validated on benchmark problems with both
time-constant and time-varying designs, and its flexibility is demonstrated
through variations in material properties. These results establish the proposed
space-time method as a promising approach for large-scale time-dependent
topology optimisation in thermal applications.

</details>


### [15] [VisFinEval: A Scenario-Driven Chinese Multimodal Benchmark for Holistic Financial Understanding](https://arxiv.org/abs/2508.09641)
*Zhaowei Liu,Xin Guo,Haotian Xia,Lingfeng Zeng,Fangqi Lou,Jinyi Niu,Mengping Li,Qi Qi,Jiahuan Li,Wei Zhang,Yinglong Wang,Weige Cai,Weining Shen,Liwen Zhang*

Main category: cs.CE

TL;DR: 本文介绍了首个大规模中文金融任务基准VisFinEval，评估21个MLLM模型，分析失败模式，旨在加速领域定制MLLM发展。


<details>
  <summary>Details</summary>
Motivation: 全面评估多模态大语言模型（MLLMs）在金融分析自动化方面的能力。

Method: 引入VisFinEval基准，包含15,848个标注问答对，分三个场景深度；在零样本设置下评估21个MLLM模型。

Result: Qwen - VL - max模型整体准确率76.3%，超非专业人类但落后金融专家超14个百分点；发现六种常见失败模式。

Conclusion: VisFinEval可加速能无缝整合文本和视觉金融信息的领域定制MLLMs的发展。

Abstract: Multimodal large language models (MLLMs) hold great promise for automating
complex financial analysis. To comprehensively evaluate their capabilities, we
introduce VisFinEval, the first large-scale Chinese benchmark that spans the
full front-middle-back office lifecycle of financial tasks. VisFinEval
comprises 15,848 annotated question-answer pairs drawn from eight common
financial image modalities (e.g., K-line charts, financial statements, official
seals), organized into three hierarchical scenario depths: Financial Knowledge
& Data Analysis, Financial Analysis & Decision Support, and Financial Risk
Control & Asset Optimization. We evaluate 21 state-of-the-art MLLMs in a
zero-shot setting. The top model, Qwen-VL-max, achieves an overall accuracy of
76.3%, outperforming non-expert humans but trailing financial experts by over
14 percentage points. Our error analysis uncovers six recurring failure
modes-including cross-modal misalignment, hallucinations, and lapses in
business-process reasoning-that highlight critical avenues for future research.
VisFinEval aims to accelerate the development of robust, domain-tailored MLLMs
capable of seamlessly integrating textual and visual financial information. The
data and the code are available at
https://github.com/SUFE-AIFLM-Lab/VisFinEval.

</details>


### [16] [Finetuning Large Language Model as an Effective Symbolic Regressor](https://arxiv.org/abs/2508.09897)
*Yingfan Hua,Ruikun Li,Jun Yao,Guohang Zhuang,Shixiang Tang,Bin Liu,Wanli Ouyang,Yan Lu*

Main category: cs.CE

TL;DR: 文章指出现有LLM用于符号回归存在局限，引入SymbArena数据集并提出启发式指标，探索微调技术建立SymbolicChat基线，实验验证其表现优于传统数值方法和其他LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的符号回归方法在有效性和泛化性上存在局限，缺乏用于微调的专用数据集。

Method: 引入SymbArena数据集，提出启发式指标，探索主流LLM微调技术建立SymbolicChat基线。

Result: SymbolicChat在数值精度和符号形式准确性上超越传统数值方法，R2分数提升2倍，形式一致性分数提升8.37%。

Conclusion: SymbolicChat是一种简单有效的基于LLM的符号回归强基线。

Abstract: Deriving governing equations from observational data, known as Symbolic
Regression (SR), is a cornerstone of scientific discovery. Large Language
Models (LLMs) have shown promise in this task by leveraging their vast
cross-disciplinary scientific knowledge. However, existing LLM-based methods
primarily rely on direct inference or prompt engineering, often requiring
excessive inference iterations to converge on correct formulas or failing to
treating complex equation targets. These limitations in effectiveness and
generalization stem from an inherent tension between pre-trained LLMs'
proficiency in approximate reasoning and the high-precision demands of SR
tasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR
capability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning
remains a critical barrier. We thus introduce SymbArena, specifically
engineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse
equations formulated as corpora of 1.83 billion tokens for LLM utilization,
enabling effective training and inference. Further, SymbArena proposes a
heuristics metric to precisely quantify form-level consistency, going beyond
existing SR numerical-oriented evaluation strategies. With this benchmark, we
explore mainstream LLM fine-tuning techniques for SR tasks and establish
SymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental
results validate SymbolicChat as the first LLM to exceed traditional numerical
methods in both numerical precision and symbolic form accuracy, outperforming
the second-best LLM baseline with improvements of 2-fold gains in R2 score and
8.37% in form-level consistency score.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations](https://arxiv.org/abs/2508.09238)
*Hyunsung Kim,Hoyoung Choi,Sangwoo Seo,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.DB

TL;DR: 提出仅用跟踪数据特征的同步框架ELASTIC，实验显示其性能远超现有同步器。


<details>
  <summary>Details</summary>
Motivation: 足球中事件和跟踪数据集成分析时，现有同步器依赖事件位置注释易有空间误差，导致同步结果失真。

Method: 提出ELASTIC同步框架，仅使用跟踪数据特征，明确检测传球类事件结束时间，区分主要和次要事件检测。

Result: 对三场荷甲比赛的2134个事件标注真实时间戳进行实验，ELASTIC大幅超越现有同步器。

Conclusion: ELASTIC能有效解决现有同步器问题，提高同步输出的完整性并减少事件间的误差级联。

Abstract: The integration of event and tracking data has become essential for advanced
analysis in soccer. However, synchronizing these two modalities remains a
significant challenge due to temporal and spatial inaccuracies in manually
recorded event timestamps. Existing synchronizers typically rely on annotated
event locations, which themselves are prone to spatial errors and thus can
distort synchronization results. To address this issue, we propose ELASTIC
(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only
uses features derived from tracking data. ELASTIC also explicitly detects the
end times of pass-like events and separates the detection of major and minor
events, which improves the completeness of the synchronized output and reduces
error cascade across events. We annotated the ground truth timestamps of 2,134
events from three Eredivisie matches to measure the synchronization accuracy,
and the experimental results demonstrate that ELASTIC outperforms existing
synchronizers by a large margin.

</details>


### [18] [LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation](https://arxiv.org/abs/2508.09594)
*Fei Teng,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: 现有日志模板生成方法有精度问题，大语言模型处理特定日志内容也有困难，本文提出 LLMLog 框架，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有启发式和神经网络日志模板生成方法精度低，大语言模型处理特定日志内容易出错，需新方法提升精度。

Method: 提出基于编辑距离的相似度度量，选择最具信息的 k 条未标注日志进行标注，设计自适应上下文选择策略。

Result: 在 16 个数据集上的大量实验表明 LLMLog 优于现有方法。

Conclusion: LLMLog 多轮注释框架结合自适应上下文学习能有效提升日志模板生成的准确性。

Abstract: Modern computing systems, such as HDFS and Spark, produce vast quantities of
logs that developers use for tasks like anomaly detection and error analysis.
To simplify log analysis, template generation methods have been proposed to
standardize log formats, transforming unstructured data into structured
templates. Existing heuristic-based methods and neural network-based methods
suffer from low accuracy problems due to the reliance on handcrafted heuristics
or specific log patterns in training sets. Recently, large language models
(LLMs) have shown great potential in log template generation. However, they
often struggle with ambiguous, complex, or highly specific log content, which
can lead to errors in generating accurate templates. To address these
challenges, we propose LLMLog, a multi-round annotation framework with adaptive
in-context learning. We first propose an edit-distance-based similarity metric
to evaluate log similarity. Then, we introduce a method to select the most
informative $k$ unlabeled logs for annotation by considering both the
representativeness of the logs and the confidence of LLM predictions.
Additionally, we design an adaptive context selection strategy that adaptively
selects labeled logs to ensure comprehensive keyword coverage for unlabeled
logs. These labeled logs serve as the context for LLMs to better understand the
unlabeled logs, thereby enhancing the accuracy of template generation.
Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms
the state-of-the-art approaches.

</details>


### [19] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: 提出名为CoDe的数据驱动方法解决基数估计问题，实验显示该方法在准确性和效率上达先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有基数估计技术存在低准确性或高推理延迟问题，需同时实现高速和高精度。

Method: 采用覆盖设计概念将表分割为多个重叠小片段，对每个片段用张量分解建模数据分布，引入算法为每个查询选择最佳拟合分布并组合估计最终结果。

Result: 实验表明该方法是基数估计的重大进步，在多个数据集上超半数查询估计达绝对准确。

Conclusion: CoDe方法在基数估计中能同时实现高估计准确性和推理效率。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [20] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: 提出图查询歧义分类和AmbiGraph - Eval基准评估大语言模型处理图查询歧义能力，发现模型处理歧义有差距。


<details>
  <summary>Details</summary>
Motivation: 现实世界查询存在歧义，图结构会放大挑战，需系统评估大语言模型处理图查询歧义的能力。

Method: 提出图查询歧义分类，包含三种主要类型及细分场景；引入AmbiGraph - Eval基准。

Result: 评估9个代表性大语言模型，发现即使顶级模型处理模糊图查询也有困难。

Conclusion: 大语言模型在处理歧义方面存在关键差距，需开展专门解决技术的后续研究。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: 本文提出一种静态识别分布式机器学习模型中错误的方法，该方法可扩展到大型模型并辅助定位错误。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习训练和推理普遍，但在将顺序模型转换为分布式模型过程中可能引入错误，导致输出与顺序模型不同。

Method: 通过检查模型精化（即能否从分布式模型输出重构顺序模型输出）来静态识别错误，在GraphGuard中使用迭代重写来证明模型精化。

Result: 该方法可扩展到GPT和Llama - 3等大型模型和部署，能提供有助于错误定位的可操作输出。

Conclusion: 所提出的方法能有效识别分布式模型中的错误，适用于大型模型并利于错误定位。

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [22] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: 介绍HierMoE加速MoE模型训练，用token deduplication和expert swap技术，实验显示比现有系统更快。


<details>
  <summary>Details</summary>
Motivation: MoE模型在GPU集群中存在通信和负载不平衡问题，阻碍分布式系统可扩展性。

Method: 引入HierMoE，采用token deduplication和expert swap两种拓扑感知技术，并构建理论模型以适配不同配置和环境。

Result: HierMoE实现了1.55 - 3.32倍更快的通信和1.18 - 1.27倍更快的端到端训练。

Conclusion: HierMoE能有效加速MoE模型训练，优于现有MoE训练系统。

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


### [23] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: 本文针对HPE Slingshot不适用于多租户部署问题，基于Kubernetes设计并实现对Slingshot栈的扩展，以实现安全、细粒度和多租户访问。


<details>
  <summary>Details</summary>
Motivation: 融合HPC - 云计算需调和云工作负载隔离需求与HPC应用性能需求，而Slingshot栈不适用于多租户部署。

Method: 基于Kubernetes设计并实现对Slingshot栈的扩展。

Result: 实现了对Slingshot RDMA网络功能的安全、容器粒度和多租户访问，且开销极小。

Conclusion: 所设计的扩展能满足融合HPC - 云部署中多租户对Slingshot网络的安全使用需求。

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [24] [An improved local search based algorithm for $k^-$-star partition](https://arxiv.org/abs/2508.09361)
*Mingyang Gong,Guohui Lin,Brendan Mumey*

Main category: cs.DS

TL;DR: 本文研究k⁻-星划分问题，提出改进的O(|V|³)时间的( k/2 - (k - 2)/(8k - 14) )近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决在简单无向图中找到最小的顶点不相交星集以覆盖所有顶点的k⁻-星划分问题。

Method: 从含最少1-星的k⁻-星划分开始，区分关键顶点，通过三种局部搜索操作迭代更新解，采用摊销方案证明近似比。

Result: 得到改进的O(|V|³)时间的( k/2 - (k - 2)/(8k - 14) )近似算法。

Conclusion: 提出的算法能有效解决k⁻-星划分问题，且在时间复杂度和近似比上有一定优势。

Abstract: We study the $k^-$-star partition problem that aims to find a minimum
collection of vertex-disjoint stars, each having at most $k$ vertices to cover
all vertices in a simple undirected graph $G = (V, E)$. Our main contribution
is an improved $O(|V|^3)$-time $(\frac k2 - \frac {k-2}{8k-14})$-approximation
algorithm.
  Our algorithm starts with a $k^-$-star partition with the least $1$-stars and
a key idea is to distinguish critical vertices, each of which is either in a
$2$-star or is the center of a $3$-star in the current solution. Our algorithm
iteratively updates the solution by three local search operations so that the
vertices in each star in the final solution produced cannot be adjacent to too
many critical vertices. We present an amortization scheme to prove the
approximation ratio in which the critical vertices are allowed to receive more
tokens from the optimal solution.

</details>


### [25] [A Classical Quadratic Speedup for Planted $k$XOR](https://arxiv.org/abs/2508.09422)
*Meghal Gupta,William He,Ryan O'Donnell,Noah G. Singer*

Main category: cs.DS

TL;DR: Schmidhuber等提出量子算法比经典算法快四次方，本文设计新经典算法在大常数k时比之前算法快二次方，使量子加速变为二次方。


<details>
  <summary>Details</summary>
Motivation: 针对Schmidhuber等提出的量子算法在大常数k时比经典算法有四次方加速的情况，设计更快的经典算法，减小量子算法的加速优势。

Method: 结合次线性时间算法（生日悖论）和多项式反集中的工具。

Result: 设计出的新经典算法在大常数k时比之前最好的算法快二次方，使Schmidhuber等的量子算法加速变为二次方，且算法在半随机情况下也适用。

Conclusion: 新经典算法在大常数k的情况下提高了经典算法的速度，减小了与量子算法的差距。

Abstract: A recent work of Schmidhuber et al (QIP, SODA, & Phys. Rev. X 2025) exhibited
a quantum algorithm for the noisy planted $k$XOR problem running quartically
faster than all known classical algorithms. In this work, we design a new
classical algorithm that is quadratically faster than the best previous one, in
the case of large constant $k$. Thus for such $k$, the quantum speedup of
Schmidhuber et al. becomes only quadratic (though it retains a space
advantage). Our algorithm, which also works in the semirandom case, combines
tools from sublinear-time algorithms (essentially, the birthday paradox) and
polynomial anticoncentration.

</details>


### [26] [Retroactive Monotonic Priority Queues via Range Searching](https://arxiv.org/abs/2508.09892)
*Lucas Castro,Rosiane de Freitas*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The best known fully retroactive priority queue costs $O(\log^2 m \log \log
m)$ time per operation, where $m$ is the number of operations performed on the
data structure. In contrast, standard (non-retroactive) and partially
retroactive priority queues cost $O(\log m)$ time per operation. So far, it is
unknown whether this $O(\log m)$ bound can be achieved for fully retroactive
priority queues.
  In this work, we study a restricted variant of priority queues known as
monotonic priority queues. We show that finding the minimum in a retroactive
monotonic priority queue is a special case of the range-searching problem. We
design a fully retroactive monotonic priority queue with a cost of $O(\log m +
T(m))$ time per operation, where $T(m)$ is the maximum between the query and
the update time of a specific range-searching data structure with $m$ elements.
Finally, we design a fully retroactive monotonic priority queue that costs
$O(\log m \log \log m)$ time per operation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [27] [Carbon Pricing in Traffic Networks](https://arxiv.org/abs/2508.09280)
*Svenja M. Griesbach,Tobias Harks,Max Klimm,Michael Markl,Philipp Warode*

Main category: cs.GT

TL;DR: 研究碳定价引导交通达到符合排放预算均衡的方法，给出流的特征，证明价格存在性，提供算法，表明碳定价是可行且易处理的方法。


<details>
  <summary>Details</summary>
Motivation: 交通是全球碳排放的重要来源，研究如何用碳定价引导交通达到符合给定排放预算的均衡。

Method: 考虑带流量相关外部性的多商品流模型，用不动点论证、拉格朗日乘数法，针对不同情况给出算法。

Result: 刻画了可作为Wardrop均衡的流，证明特定价格存在性，证明单外部性下总外部性随价格下降，给出算法计算均衡和最低价格。

Conclusion: 碳定价是在交通网络中实现所有可行排放目标的可行且（在温和假设下）易处理的方法。

Abstract: Traffic is a significant source of global carbon emissions. In this paper, we
study how carbon pricing can be used to guide traffic towards equilibria that
respect given emission budgets. In particular, we consider a general
multi-commodity flow model with flow-dependent externalities. These
externalities may represent carbon emissions, entering a priced area, or the
traversal of paths regulated by tradable credit schemes.
  We provide a complete characterization of all flows that can be attained as
Wardrop equilibria when assigning a single price to each externality. More
precisely, we show that every externality budget achievable by any feasible
flow in the network can also be achieved as a Wardrop equilibrium by setting
appropriate prices. For extremal and Pareto-minimal budgets, we show that there
are prices such that all equilibria respect the budgets. Although the proofs of
existence of these particular prices rely on fixed-point arguments and are
non-constructive, we show that in the case where the equilibrium minimizes a
convex potential, the prices can be obtained as Lagrange multipliers of a
suitable convex program. In the case of a single externality, we prove that the
total externality caused by the traffic flow is decreasing in the price. For
increasing, continuous, and piecewise affine travel time functions with a
single externality, we give an output-polynomial algorithm that computes all
equilibria implementable by pricing the externality. Even though there are
networks where the output size is exponential in the input size, we show that
the minimal price obeying a given budget can be computed in polynomial time.
This allows the efficient computation of the market price of tradable credit
schemes. Overall, our results show that carbon pricing is a viable and (under
mild assumptions) tractable approach to achieve all feasible emission goals in
traffic networks.

</details>


### [28] [Collective dynamics of strategic classification](https://arxiv.org/abs/2508.09340)
*Marta C. Couto,Flavia Barsotti,Fernando P. Santos*

Main category: cs.GT

TL;DR: 运用演化博弈论研究用户适应与算法再训练的集体动态，以信贷算法为例分析多种场景，测试干预措施效果。


<details>
  <summary>Details</summary>
Motivation: 探究用户适应分类器信息及算法再训练会产生怎样的集体动态。

Method: 应用演化博弈论，以信贷算法部署为案例研究，考虑多种交互场景。

Result: 提高检测能力可降低社会成本并促使用户改进；无法实现完美分类器时，算法追索能引导用户高改进率；机构重新适应速度影响结果；严格机构提供追索会出现循环动态。

Conclusion: 该框架能严格处理用户与机构反馈循环问题，可测试干预措施以减轻战略适应的不利影响。

Abstract: Classification algorithms based on Artificial Intelligence (AI) are nowadays
applied in high-stakes decisions in finance, healthcare, criminal justice, or
education. Individuals can strategically adapt to the information gathered
about classifiers, which in turn may require algorithms to be re-trained. Which
collective dynamics will result from users' adaptation and algorithms'
retraining? We apply evolutionary game theory to address this question. Our
framework provides a mathematically rigorous way of treating the problem of
feedback loops between collectives of users and institutions, allowing to test
interventions to mitigate the adverse effects of strategic adaptation. As a
case study, we consider institutions deploying algorithms for credit lending.
We consider several scenarios, each representing different interaction
paradigms. When algorithms are not robust against strategic manipulation, we
are able to capture previous challenges discussed in the strategic
classification literature, whereby users either pay excessive costs to meet the
institutions' expectations (leading to high social costs) or game the algorithm
(e.g., provide fake information). From this baseline setting, we test the role
of improving gaming detection and providing algorithmic recourse. We show that
increased detection capabilities reduce social costs and could lead to users'
improvement; when perfect classifiers are not feasible (likely to occur in
practice), algorithmic recourse can steer the dynamics towards high users'
improvement rates. The speed at which the institutions re-adapt to the user's
population plays a role in the final outcome. Finally, we explore a scenario
where strict institutions provide actionable recourse to their unsuccessful
users and observe cycling dynamics so far unnoticed in the literature.

</details>


### [29] [Multidimensional Budget-Feasible Mechanism Design](https://arxiv.org/abs/2508.09367)
*Rian Neogi,Kanstantsin Pashkovich,Chaitanya Swamy*

Main category: cs.GT

TL;DR: 本文引入多维预算可行机制设计，获得该问题的首个近似保证，有三方面贡献。


<details>
  <summary>Details</summary>
Motivation: 现有预算可行机制研究集中在单维设置，本文旨在解决多维预算可行机制设计问题，设计能采购高价值物品集的机制。

Method: 证明单维预算可行机制设计标准基准不适用，确定主要问题是可能存在垄断者；提出替代基准$OPT_{Bench}$；针对XOS估值设计预算可行机制。

Result: 获得多维预算可行机制设计的首个近似保证。

Conclusion: 提出的替代基准可用于比较机制，所设计机制能针对XOS估值获得常数因子近似保证。

Abstract: In budget-feasible mechanism design, a buyer wishes to procure a set of items
of maximum value from self-interested players. We have a valuation function
$v:2^U \to \mathbb{R}_+$, where $U$ is the set of all items, where $v(S)$
specifies the value obtained from set $S$ of items. The entirety of current
work on budget-feasible mechanisms has focused on the single-dimensional
setting, wherein each player holds a single item $e$ and incurs a private cost
$c_e$ for supplying item $e$.
  We introduce multidimensional budget feasible mechanism design: the universe
$U$ is now partitioned into item-sets $\{G_i\}$ held by the different players,
and each player $i$ incurs a private cost $c_i(S_i)$ for supplying the set
$S_i\subseteq G_i$ of items. A budget-feasible mechanism is a mechanism that is
truthful, and where the total payment made to the players is at most some given
budget $B$. The goal is to devise a budget-feasible mechanism that procures a
set of items of large value. We obtain the first approximation guarantees for
multidimensional budget feasible mechanism design.
  Our contributions are threefold. First, we prove an impossibility result
showing that the standard benchmark used in single-dimensional budget-feasible
mechanism design, namely the algorithmic optimum is inadequate in that no
budget-feasible mechanism can achieve good approximation relative to this. We
identify that the chief underlying issue here is that there could be a
monopolist which prevents a budget-feasible mechanism from obtaining good
guarantees. Second, we devise an alternate benchmark, $OPT_{Bench}$, that
allows for meaningful approximation guarantees, thereby yielding a metric for
comparing mechanisms. Third, we devise budget-feasible mechanisms that achieve
constant-factor approximation guarantees with respect to this benchmark for XOS
valuations.

</details>


### [30] [Project Submission Games in Participatory Budgeting](https://arxiv.org/abs/2508.09741)
*Piotr Faliszewski,Łukasz Janeczko,Andrzej Kaczmarczyk,Grzegorz Lisowski,Grzegorz Pierczyński*

Main category: cs.GT

TL;DR: 引入项目提交博弈框架，研究纯纳什均衡存在条件、检验其存在性的复杂度及计算最优响应的算法。


<details>
  <summary>Details</summary>
Motivation: 捕捉参与式预算和多获胜者选举中项目提案者的行为。

Method: 引入项目提交博弈框架，分析纯纳什均衡存在条件、检验复杂度并寻找计算最优响应的算法。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: We introduce the framework of project submission games, capturing the
behavior of project proposers in participatory budgeting (and multiwinner
elections). Here, each proposer submits a subset of project proposals, aiming
at maximizing the total cost of those that get funded. We focus on finding
conditions under which pure Nash equilibria (NE) exist in our games, and on the
complexity of checking whether they exist. We also seek algorithms for
computing best responses for the proposers

</details>


### [31] [The Price of EF1 for Few Agents with Additive Ternary Valuations](https://arxiv.org/abs/2508.09869)
*Maria Kyropoulou,Alexandros A. Voudouris*

Main category: cs.GT

TL;DR: 本文研究具有可加三元估值的不可分物品资源分配问题，给出EF1分配价格的界，分析不同数量代理下的情况。


<details>
  <summary>Details</summary>
Motivation: 研究具有可加三元估值的不可分物品资源分配中EF1分配价格的界。

Method: 先针对大量代理得出价格的下界，再聚焦于少量代理情况进行分析。

Result: 大量代理时得出下界为Ω(√n)；n = 2时价格为12/11；n = 3时价格在1.2到1.256之间。

Conclusion: 具有可加三元估值时EF1价格不比代理有一般次可加估值时好，不同数量代理下EF1价格不同。

Abstract: We consider a resource allocation problem with agents that have additive
ternary valuations for a set of indivisible items, and bound the price of
envy-free up to one item (EF1) allocations. For a large number $n$ of agents,
we show a lower bound of $\Omega(\sqrt{n})$, implying that the price of EF1 is
no better than when the agents have general subadditive valuations. We then
focus on instances with few agents and show that the price of EF1 is $12/11$
for $n=2$, and between $1.2$ and $1.256$ for $n=3$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation](https://arxiv.org/abs/2508.09460)
*Xujie Yuan,Shimin Di,Jielong Tang,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: 提出MetaKGRAG框架解决现有KG - RAG框架问题，实验证明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有KG - RAG框架是开环系统，存在认知盲目性，导致相关性漂移和证据不完整，现有自精炼方法无法有效解决。

Method: 受人类元认知过程启发，提出MetaKGRAG框架，引入感知 - 评估 - 调整循环实现路径感知的闭环精炼。

Result: 在医学、法律和常识推理领域的五个数据集上的实验表明，MetaKGRAG始终优于强大的KG - RAG和自精炼基线。

Conclusion: 验证了方法的优越性，强调了结构化知识检索中路径感知精炼的关键需求。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly
enhances the reasoning capabilities of LargeLanguage Models by leveraging
structured knowledge. However, existing KG-RAG frameworks typically operate as
open-loop systems, suffering from cognitive blindness, an inability to
recognize their exploration deficiencies. This leads to relevance drift and
incomplete evidence, which existing self-refinement methods, designed for
unstructured text-based RAG, cannot effectively resolve due to the
path-dependent nature of graph exploration. To address this challenge, we
propose Metacognitive Knowledge Graph Retrieval Augmented Generation
(MetaKGRAG), a novel framework inspired by the human metacognition process,
which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,
closed-loop refinement. This cycle empowers the system to self-assess
exploration quality, identify deficiencies in coverage or relevance, and
perform trajectory-connected corrections from precise pivot points. Extensive
experiments across five datasets in the medical, legal, and commonsense
reasoning domains demonstrate that MetaKGRAG consistently outperforms strong
KG-RAG and self-refinement baselines. Our results validate the superiority of
our approach and highlight the critical need for path-aware refinement in
structured knowledge retrieval.

</details>


### [33] [Improving Dense Passage Retrieval with Multiple Positive Passages](https://arxiv.org/abs/2508.09534)
*Shuai Chang*

Main category: cs.IR

TL;DR: 本文探究训练时为DPR加入多个正样本段落的效果，实验表明这能提升检索准确率，还可小批量单GPU训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPR的模型训练时每个问题通常只配一个正样本段落，未考察多正样本段落的效果。

Method: 在训练DPR时为每个问题配备多个正样本段落并进行实验。

Result: 为每个问题配备多个正样本段落能持续提升检索准确率，且小批量训练也可行，可在单GPU上训练。

Conclusion: 训练DPR时加入多个正样本段落能提升性能，降低训练资源要求。

Abstract: By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has
outperformed traditional sparse retrieval algorithms such as BM25 in terms of
passage retrieval accuracy. Recently proposed methods have further enhanced
DPR's performance. However, these models typically pair each question with only
one positive passage during training, and the effect of associating multiple
positive passages has not been examined. In this paper, we explore the
performance of DPR when additional positive passages are incorporated during
training. Experimental results show that equipping each question with multiple
positive passages consistently improves retrieval accuracy, even when using a
significantly smaller batch size, which enables training on a single GPU.

</details>


### [34] [TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking](https://arxiv.org/abs/2508.09539)
*Yongqi Fan,Xiaoyang Chen,Dezhi Ye,Jie Liu,Haijin Liang,Jin Ma,Ben He,Yingfei Sun,Tong Ruan*

Main category: cs.IR

TL;DR: 提出基于小模型的高效推理排序器TFRank，结合多种方法提升性能，在实验中表现良好，平衡了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推理密集型排序模型计算成本高、延迟大，限制实际应用，需解决该问题。

Method: 提出TFRank，有效整合CoT数据、细粒度分数监督和多任务训练，采用“思考模式开关”和逐点格式约束实现高效推理。

Result: TFRank（如1.7B）在BRIGHT基准上性能与四倍参数模型相当，在BEIR基准上有竞争力。

Conclusion: TFRank实现了性能和效率的有效平衡，为将高级推理集成到现实系统提供了实用解决方案。

Abstract: Reasoning-intensive ranking models built on Large Language Models (LLMs) have
made notable progress, but existing approaches often rely on large-scale LLMs
and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational
cost and latency that limit real-world use. To address this, we propose
\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale
LLMs. To improve ranking performance, TFRank effectively integrates CoT data,
fine-grained score supervision, and multi-task training. Furthermore, it
achieves an efficient ``\textbf{T}hink-\textbf{F}ree" reasoning capability by
employing a ``think-mode switch'' and pointwise format constraints.
Specifically, this allows the model to leverage explicit reasoning during
training while delivering precise relevance scores for complex queries at
inference without generating any reasoning chains. Experiments show that TFRank
(e.g., 1.7B) achieves performance comparable to models with four times more
parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on
the BEIR benchmark. Further analysis shows that TFRank achieves an effective
balance between performance and efficiency, providing a practical solution for
integrating advanced reasoning into real-world systems. Our code and data are
released in the repository: https://github.com/JOHNNY-fans/TFRank.

</details>


### [35] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出使用多任务学习框架优化个性化产品搜索排名的新模型架构，结合非表格数据和先进嵌入技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化个性化产品搜索排名，处理混合数据类型。

Method: 采用多任务学习框架，整合表格和非表格数据，利用预训练TinyBERT模型和新采样技术，提出可扩展相关性标注机制。

Result: 结合非表格数据和先进嵌入技术显著提升模型性能，消融研究凸显相关性标签等的益处。

Conclusion: 该方法在实现更好的个性化产品搜索排名方面有效。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [36] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: 现有基于大语言模型的推荐器在利用负样本时存在问题，提出NAPO框架，实验显示其在推荐准确性和减少流行度偏差上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐器优化方法在有效利用负样本方面存在挑战，简单集成大量负样本会增加计算和内存开销，且未考虑负样本信息的差异性，导致优化性能不佳。

Method: 提出NAPO框架，包含批内负样本共享和动态奖励边际调整两个关键创新点。

Result: 在三个公开数据集上的广泛实验表明，NAPO在推荐准确性和减少流行度偏差方面优于现有方法。

Conclusion: NAPO是一种有效的基于大语言模型的推荐偏好优化框架。

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [37] [Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation](https://arxiv.org/abs/2508.09664)
*Yongrui Fu,Jian Liu,Tao Li,Zonggang Wu,Shouke Qin,Hanmeng Liu*

Main category: cs.IR

TL;DR: 提出MUFASA模型进行长序列推荐，含多模态融合层和稀疏注意力引导对齐层，实验显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多模态物品序列和挖掘多粒度用户兴趣以缩小内容理解和推荐之间差距方面存在挑战。

Method: 提出MUFASA模型，包含多模态融合层（MFL）和稀疏注意力引导对齐层（SAL），MFL利用物品标题作为跨类别语义锚点并结合四个定制损失训练，SAL通过多粒度稀疏注意力机制处理长用户行为序列。

Result: 在真实世界基准测试中，MUFASA始终超越最先进的基线，在线A/B测试显示在生产中取得显著收益。

Conclusion: MUFASA能有效利用多模态线索并准确捕捉不同用户偏好。

Abstract: Recent advances in multimodal recommendation enable richer item
understanding, while modeling users' multi-scale interests across temporal
horizons has attracted growing attention. However, effectively exploiting
multimodal item sequences and mining multi-grained user interests to
substantially bridge the gap between content comprehension and recommendation
remain challenging. To address these issues, we propose MUFASA, a MUltimodal
Fusion And Sparse Attention-based Alignment model for long sequential
recommendation. Our model comprises two core components. First, the Multimodal
Fusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and
is trained with a joint objective of four tailored losses that promote: (i)
cross-genre semantic alignment, (ii) alignment to the collaborative space for
recommendation, (iii) preserving the similarity structure defined by titles and
preventing modality representation collapse, and (iv) distributional
regularization of the fusion space. This yields high-quality fused item
representations for further preference alignment. Second, the Sparse
Attention-guided Alignment Layer (SAL) scales to long user-behavior sequences
via a multi-granularity sparse attention mechanism, which incorporates windowed
attention, block-level attention, and selective attention, to capture user
interests hierarchically and across temporal horizons. SAL explicitly models
both the evolution of coherent interest blocks and fine-grained intra-block
variations, producing robust user and item representations. Extensive
experiments on real-world benchmarks show that MUFASA consistently surpasses
state-of-the-art baselines. Moreover, online A/B tests demonstrate significant
gains in production, confirming MUFASA's effectiveness in leveraging multimodal
cues and accurately capturing diverse user preferences.

</details>


### [38] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 提出零微调框架，用MLLM为视频生成自然语言描述，注入高级语义到推荐流程，在数据集上超越传统特征。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统依赖的低级别特征缺少深度语义，无法区分视频的深层含义，不能满足个性化推荐需求。

Method: 引入零微调框架，用MLLM为视频生成自然语言描述，结合文本编码器，输入到标准推荐器。

Result: 在MicroLens - 100K数据集上，该框架在五个代表性模型中超越传统视频、音频和元数据特征。

Conclusion: 利用MLLM作为即时知识提取器构建更具意图感知的视频推荐器很有前景。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 利用基于特征标记化的Transformer模型高效预测飞机ETA，在新加坡樟宜机场实验效果好，优于常用模型。


<details>
  <summary>Details</summary>
Motivation: 实时飞机ETA预测对航空到达管理至关重要，在实时系统中效率和准确性同样重要。

Method: 采用基于特征标记化的Transformer模型，输入多种原始数据，以1HZ更新预测。

Result: 该方法优于常用的提升树模型，准确率提高7%，计算时间仅为39%，40架飞机时推理时间仅51.7微秒。

Conclusion: 该方法有应用于实时到达管理系统的潜力。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [40] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: 提出MoLAN框架及MoLAN+方法解决多模态情感分析噪音问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常受无关或误导性信息干扰，现有方法抑制噪音时可能丢失关键信息。

Method: 提出MoLAN框架，进行模态感知分块并根据噪音和语义相关性动态分配去噪强度，还引入MoLAN+方法。

Result: 实验表明MoLAN框架广泛有效，MoLAN+达到了当前最优性能。

Conclusion: MoLAN框架灵活可集成，能有效解决多模态情感分析噪音问题。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [41] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 提出基于LLM Transformer的上下文学习理论优化信道接入，设计优化器，开发训练算法，允许错误数据输入，实验证明其在未知节点密度下表现优。


<details>
  <summary>Details</summary>
Motivation: 二进制指数退避方案在动态信道环境中吞吐量性能差，现有基于模型的方法因节点密度估计不准确导致吞吐量损失大。

Method: 提出基于LLM Transformer的上下文学习理论，设计优化器收集数据构建提示输入Transformer学习模式生成预测的竞争窗口阈值，开发训练算法，允许错误数据输入。

Result: 优化器在有限训练步骤内实现接近最优的CWT预测，在NS - 3实验中展示出快速收敛性和接近最优的吞吐量。

Conclusion: 所提方法在未知节点密度下比现有基于模型和基于DRL的方法更优。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [42] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 引入有限选择性预测模型（PLS），研究最优预测误差并引入复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测结果依赖预测者可随时预测，本文研究预测者只能在部分时间开始预测的情况。

Method: 对PLS模型在逐个实例和平均情况进行最优预测误差研究，引入复杂度度量。

Result: 对于随机生成的PLS实例，复杂度度量给出的最优误差界限大概率匹配。

Conclusion: 成功引入PLS模型并得到有意义的结果。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [43] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: 介绍Motif - 2.6B基础模型，它在提升性能同时兼顾计算效率，表现优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 当前开发平衡高性能与计算效率的基础大语言模型存在挑战，尤其是对新兴研究组，为解决此问题提出Motif - 2.6B。

Method: 为Motif - 2.6B引入差分注意力和PolyNorm激活函数等创新架构增强，通过大量实验测试确定最优架构。

Result: Motif - 2.6B在多个基准测试中表现达到或超过同类先进模型，展现出有效性、可扩展性和实际适用性。

Conclusion: Motif - 2.6B显著推动高效、可扩展和强大的基础大语言模型发展，为未来研究和部署提供见解与基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [44] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 提出GSMT混合模型用于公交轨迹预测，结合GAT和RNN并加入任务校正器，在吉隆坡数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在发展中地区多模态数据获取受限，仅依靠车载GPS数据进行公交轨迹准确预测存在挑战，需有效方法解决。

Method: 提出GSMT混合模型，融合GAT和RNN，用任务校正器提取轨迹行为模式并微调预测结果，通过嵌入式混合网络融合动态公交和静态站点信息进行预测，采用两阶段方法实现多节点轨迹预测。

Result: 在吉隆坡真实数据集实验中，该方法在短期和长期轨迹预测任务上显著优于现有方法。

Conclusion: GSMT混合模型能有效解决发展中地区公交轨迹预测问题，在复杂城市交通环境下表现良好。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [45] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 本文探讨时间序列分析中序列混合器的必要性，提出JustDense方法，用密集层替换序列混合器，实验表明该替换可取得相当或更优性能。


<details>
  <summary>Details</summary>
Motivation: 质疑复杂序列混合器在时间序列分析中的必要性，探究常见序列混合器是否必要。

Method: 提出JustDense，基于MatrixMixer框架，用密集层替换各种成熟TSA模型中的序列混合器，并在29个基准测试上进行实验。

Result: 用密集层替换序列混合器能取得相当或更优性能，在部分情况挑战了“更深更复杂架构更好”的假设。

Conclusion: 常见序列混合器在时间序列分析中并非必要，简单架构也能有好表现。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [46] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 文章整合15个数据集构建含2510个受试者的糖尿病大数据库，评估数据质量并研究血糖与心率相关性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病和低血糖研究受限于缺乏大型数据集，数据分析和机器学习模型可改善糖尿病护理，故需构建大数据库。

Method: 系统整合15个数据集，提取两个子数据库，评估数据质量，进行血糖与心率的相关性研究。

Result: 构建含14900万条测量数据的数据库，4%为低血糖范围数值，发现数据不平衡和缺失值是挑战，血糖与心率在低血糖前15 - 55分钟有关联。

Conclusion: 整合的数据集为糖尿病和低血糖研究提供了大型数据库，同时指出数据质量方面的挑战及血糖与心率的关联。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [47] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: 提出DIG2RSI深度学习框架解决复杂网络中同伴因果效应估计问题，该框架能处理反馈和混杂因素，实证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计复杂真实网络（如社交网络）中同伴因果效应时，要么忽略同时反馈，要么有线性假设限制，无法准确估计。

Method: 提出DIG2RSI框架，用I - G变换消除反馈偏差，用2SRI构造工具变量处理未观察到的混杂因素，利用神经网络和对抗判别器增强去偏效果。

Result: 在两个半合成基准和一个真实世界数据集上，DIG2RSI表现优于现有方法。

Conclusion: DIG2RSI能有效解决复杂网络中同伴因果效应估计的同时反馈和未观察到的混杂问题，估计器具有一致性。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [48] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 本文开发嵌入空间时间锚定的算子理论框架，给出相关引理和定理证明，形式化内部手稿计算机并证明等价定理，给出注意力层相关证明和条件。


<details>
  <summary>Details</summary>
Motivation: 开发嵌入空间时间锚定的算子理论框架，解决相关理论问题。

Method: 构建算子理论框架，证明变量块收缩引理、漂移 - 投影收敛定理等。

Result: 完成相关引理和定理证明，形式化内部手稿计算机并证明等价定理，给出注意力层相关证明和条件。

Conclusion: 所构建的算子理论框架有效，为相关研究提供理论支持。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [49] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 提出AdaPO在线强化学习框架，可实时调整训练目标，缓解奖励破解问题，实验证明能提升推理和自评估能力。


<details>
  <summary>Details</summary>
Motivation: 自评估对大跨模态模型多轮对话自我提升至关重要，但基础模型缺乏，现有强化学习方法有奖励破解问题。

Method: 提出AdaPO框架，引入自适应奖励模型和奖励感知动态KL正则化机制。

Result: 在8个基准测试和多种模型上实验，显著提升直接推理和自评估能力。

Conclusion: AdaPO能有效解决奖励破解问题，提升模型性能，将开源代码贡献社区。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [50] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出优化Matérn核时间高斯过程的方法，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 优化Matérn核时间高斯过程关于核协方差函数的超参数。

Method: 将优化问题转化为自回归模型参数的递归贝叶斯估计过程。

Result: 所提方法在运行时间和高斯过程回归的最终均方根误差方面优于最大化边缘似然和哈密顿蒙特卡罗采样。

Conclusion: 所提优化程序在高斯过程回归中表现更好。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [51] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出微调流匹配生成模型的框架，用于执行物理约束和解决科学系统中的逆问题，验证显示其有效性，桥接了生成建模与科学推理。


<details>
  <summary>Details</summary>
Motivation: 解决科学系统中执行物理约束和逆问题求解的需求。

Method: 从低保真或观测数据训练的模型开始，应用可微的训练后程序最小化偏微分方程的弱形式残差，通过可学习的潜在参数预测器和联合优化策略推断未知物理输入。

Result: 在规范偏微分方程基准上验证，改进了偏微分方程约束的满足度，准确恢复了潜在系数。

Conclusion: 该方法桥接了生成建模和科学推理，为物理系统的模拟增强发现和数据高效建模开辟新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [52] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: 提出EvaDrive多目标强化学习框架用于自动驾驶轨迹规划，经实验验证有SOTA性能，提供无标量化轨迹优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹规划的生成 - 评估框架和强化学习方法存在问题，无法实现类人迭代决策。

Method: 将轨迹规划构建为多轮对抗游戏，用分层生成器提出候选路径，可训练多目标评判器评估，由帕累托前沿选择机制引导对抗交互。

Result: 在NAVSIM和Bench2Drive基准测试中取得SOTA性能，能通过动态加权生成多样驾驶风格。

Conclusion: EvaDrive引入闭环对抗框架实现类人迭代决策，是无标量化轨迹优化新方法。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [53] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 提出轻量级免训练方法，用RAG跨模态线性映射解决预训练大模型模态差距问题，实验有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大模型存在的模态差距问题，且现有微调方法成本高、不实际。

Method: 提出利用RAG跨模态线性映射的轻量级免训练方法，推理时应用映射检索文本描述，引入迭代技术优化映射。

Result: 在两个基准多模态数据集上实验有显著改进。

Conclusion: 所提方法能有效解决预训练大模型模态差距问题，且成本低、效果好。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [54] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 本文提出物理引导记忆网络（PgMN），融合深度学习和基于物理模型预测能耗，理论评估其组件数学上有效，实验验证在多种场景准确适用，为动态建筑环境能耗预测提供方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在历史数据有限或缺失时难以使用，基于物理模型需大量建筑参数和建模时间，需要新方法解决这些局限。

Method: 引入物理引导记忆网络（PgMN），包含并行投影层、记忆单元和记忆经验模块，融合深度学习和基于物理模型预测结果。

Result: 理论评估表明PgMN组件数学上有效，实验验证其在新建建筑、数据缺失等多种场景的准确性和适用性。

Conclusion: PgMN为动态建筑环境能耗预测提供了有前景的解决方案，提升了模型在历史数据有限或物理模型不足场景的适用性。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [55] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出基于自编码器和定制windowSHAP算法的无监督可解释AI框架，用于表征核反应堆实时重放攻击，在真实数据集上有95%以上准确率。


<details>
  <summary>Details</summary>
Motivation: 下一代先进核反应堆依赖数字系统，确保数据完整性对抗欺骗攻击很重要，现有应对重放攻击方法有局限，需用真实数据表征攻击和解释预测。

Method: 提出结合自编码器和定制windowSHAP算法的无监督可解释AI框架。

Result: 该框架在Purdue核反应堆PUR - 1的多个真实数据集上进行测试，能以95%及以上准确率检测、识别被重放信号的来源、数量和篡改持续时间。

Conclusion: 所提无监督可解释AI框架可有效表征实时重放攻击，适用于核反应堆动态过程。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [56] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: 提出RicciFlowRec几何推荐框架，在动态金融图上用Ricci曲率和流进行根因归因，初步结果显示有更好的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在金融决策支持中引入基于几何流的推理，实现根因归因和早期风险感知排名。

Method: 在动态金融图上，通过建模股票、宏观经济指标和新闻的互动，用离散Ricci曲率量化局部压力，用Ricci流追踪冲击传播，利用曲率梯度揭示因果子结构。

Result: 在标普500数据上的初步结果显示，在合成扰动下有更好的鲁棒性和可解释性。

Conclusion: RicciFlowRec是首个在金融决策支持中应用基于几何流推理的推荐器，支持基于曲率的归因和早期风险感知排名，未来计划用于投资组合优化和回报预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [57] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 文章提出针对随机计算神经网络的可调序列长度（ASL）方案，可减少能耗和延迟，验证其在物联网应用可行性。


<details>
  <summary>Details</summary>
Motivation: 随机计算虽能降低能耗，但层混合精度实现有待改进，需要新方案提升性能。

Method: 引入基于算子范数理论模型分析截断噪声传播，用随机森林回归进行敏感性分析，提出粗粒度和细粒度两种截断策略。

Result: 在32nm流水线随机计算多层感知机上评估，ASL可减少超60%能耗和延迟，精度损失可忽略。

Conclusion: ASL方案对物联网应用可行，凸显随机计算设计中混合精度截断优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [58] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 提出基于扩散模型的人口合成方法，对比其他方法，结果表明该方法在可行性和多样性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高维属性时因数据稀疏难以准确建模人口，现有深度生成模型会产生不可行属性组合。

Method: 提出基于扩散模型的人口合成方法，用于估计人口潜在联合分布，与VAE、GAN等方法对比，用多种指标评估合成输出。

Result: 该方法在合成人口的可行性和多样性平衡上优于先前方法。

Conclusion: 基于扩散模型的人口合成方法在高维人口合成中表现良好，能更好平衡可行性和多样性。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [59] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: 论文提出PatchECG框架解决不同医院心电图布局差异带来的问题，实验显示该方法鲁棒性强且效果优于经典方法和现有最优模型。


<details>
  <summary>Details</summary>
Motivation: 不同医院心电图布局差异导致数字化信号存在异步导联时间和部分数据丢失问题，对现有模型构成挑战。

Method: 引入基于掩码训练策略的自适应可变块计数缺失表示学习框架PatchECG，自动聚焦导联间具有协作依赖关系的关键块。

Result: 在PTB - XL数据集和生成的异步心电图图像上，平均AUROC为0.835且布局变化时保持稳定；在朝阳医院400张真实心电图数据外部验证中，房颤诊断AUROC达0.778；在12 x 1布局心电图上，AUROC达0.893，优于经典插值和基线方法，比ECGFounder提高0.111和0.19。

Conclusion: PatchECG框架在不同布局心电图心律失常关键识别上表现出色，具有很强的鲁棒性和良好的诊断效果。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [60] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 文章介绍SVG-1M数据集并提出SVGen模型，该模型能从自然语言输入生成SVG代码，实验显示其效果和效率超其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决将创意转化为精确矢量图形耗时的问题。

Method: 引入SVG-1M数据集，创建文本到SVG训练对，提出SVGen模型，使用课程学习和强化学习优化。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVGen模型有效且高效，相关代码、模型和数据集在GitHub上开源。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [61] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: 提出FedMP方法解决联邦学习在非IID场景下挑战，实验证明其优于现有算法并分析相关影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID本地数据集下有挑战，尤其在医学影像领域，影响全局模型收敛和性能。

Method: 采用随机特征流形补全丰富客户端分类器训练空间，利用类原型引导客户端特征流形在语义一致子空间对齐。

Result: 在多个医学影像数据集和多域自然图像数据集验证，FedMP优于现有联邦学习算法。

Conclusion: FedMP能有效应对联邦学习在非IID场景下的挑战，同时分析了流形维度、通信效率和特征暴露隐私影响。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [62] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: 本文提出动态量化训练（DQT）框架，解决现有动态量化方法瓶颈，实现高效动态量化，在多个模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有动态实例混合精度量化方法需昂贵的反量化到浮点再量化到整数的循环，破坏仅整数硬件范式，影响性能提升。

Method: 引入嵌套整数表示，低精度值按位嵌入高精度值中，结合自定义仅整数运算，通过低成本位移操作实现动态位宽切换，使用轻量级控制器决定每层量化方式。

Result: 在ResNet18 on CIFAR - 10和ResNet50 on ImageNet上表现达到先进水平，4位动态ResNet50在ImageNet上top - 1准确率达77.00%，位宽转换成本大幅降低。

Conclusion: DQT框架解锁了高效、自适应AI的新领域。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [63] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: 提出单细胞聚类方法scAGC，在9个真实数据集上表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在分析scRNA - seq数据时面临统计和计算挑战，现有先进方法依赖静态图结构，对噪声敏感且无法捕捉长尾分布。

Method: 提出scAGC，通过端到端方式同时优化特征表示和细胞图，引入拓扑自适应图自编码器动态细化图结构，集成ZINB损失进行特征重建，加入对比学习目标确保图学习稳定。

Result: 在9个真实scRNA - seq数据集上，scAGC始终优于其他先进方法，分别在9个和7个数据集上获得最佳NMI和ARI分数。

Conclusion: scAGC是一种有效的单细胞聚类方法，能解决现有方法的局限性。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [64] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 提出长周期客户端选择联邦学习方法LCSFLA，解决车联网中联邦学习非IID数据问题及客户端选择难题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 车联网联邦学习中，非IID数据影响模型收敛和准确性，传统客户端选择指标存在资源浪费和信息不对称问题。

Method: 提出LCSFLA方案，考虑长期数据质量和能源成本最大化社会福利，采用带押金的拍卖机制激励参与和确保信息真实。

Result: 理论证明激励机制具有激励相容性和个体理性，实验表明能缓解非IID数据导致的性能下降。

Conclusion: LCSFLA方案能有效解决车联网联邦学习中因非IID数据和客户端选择带来的性能问题。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [65] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 本文全面调研呼吸分析方法，涵盖接触式与非接触式，探讨应用、技术及面临挑战，指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸分析方法在舒适性和实用性上存在局限，特别是长期监测时。

Method: 全面研究接触式和非接触式方法，分析机器学习和深度学习技术在呼吸分析中的应用，探讨数据处理和分类技术。

Result: 对不同呼吸分析方法进行分析，探索多种应用场景，比较适合各方法的机器学习/深度学习模型，讨论面临的挑战和新兴趋势。

Conclusion: 综合现有方法，为呼吸分析未来创新提供全面框架，连接先进技术与实际医疗应用。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [66] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 提出FGSN与免训练连续投影方法降低微调安全风险，实验证明有效且能持续防御新安全问题。


<details>
  <summary>Details</summary>
Motivation: 微调服务引入安全风险，现有后微调防御方法缺乏对安全层和细粒度神经元的综合考虑，难以平衡安全与效用。

Method: 提出Fine - Grained Safety Neurons (FGSN)与免训练连续投影方法，整合安全层与神经元的多尺度交互，将安全神经元参数投影到安全方向，引入任务特定的多维异构安全神经元集群优化机制。

Result: 在多个微调大语言模型上的实验表明，该方法能显著降低有害性分数和攻击成功率，同时保留模型效用。

Conclusion: 该方法能有效降低微调安全风险，实现对意外新安全问题的持续防御和泛化能力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [67] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: 本文提出TokenCast框架用于上下文感知的时间序列预测，经实验验证有效且具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测因难以融合历史数值序列和上下文特征，导致预测准确性受限。

Method: 提出TokenCast框架，用离散分词器将连续数值序列转为时间标记，通过预训练大语言模型将时间和上下文标记嵌入共享表示空间，再微调模型预测未来时间标记并解码回原始数值空间。

Result: 在多个带上下文特征的真实世界数据集上进行大量实验。

Conclusion: TokenCast框架有效且具有泛化性。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [68] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出离散扩散强制（D2F）策略，将香草扩散大语言模型翻新为自回归 - 扩散混合范式，实现高效推理，在GSM8K上比LLaMA3和Qwen2.5推理速度快2.5倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有开源扩散大语言模型在推理速度上未超过类似大小的自回归大语言模型，需提升推理速度。

Method: 提出D2F策略，赋予模型块级自回归生成和块间并行解码能力，通过非对称蒸馏过程实现，还提出流水线并行解码算法。

Result: D2F dLLMs在GSM8K上比LLaMA3和Qwen2.5推理速度快2.5倍以上，比LLaDA和Dream加速超50倍，且输出质量相当。

Conclusion: D2F策略有效提升了扩散大语言模型的推理速度，代码已开源。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [69] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 现有IPCGRL方法在复杂多目标指令下可控性有限，提出MIPCGRL方法，实验显示可控性最高提升13.8%，能实现更灵活内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有IPCGRL方法难以利用文本输入的丰富表达性，在复杂多目标指令下可控性有限。

Method: 提出MIPCGRL，一种用于指令内容生成器的多目标表示学习方法，结合句子嵌入作为条件，通过多标签分类和多头回归网络训练多目标嵌入空间。

Result: 所提方法在多目标指令下可控性最高提升13.8%。

Conclusion: 该方法处理复杂指令的能力可实现更具表现力和灵活性的内容生成。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [70] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 为降低大模型部署成本，提出基于元学习的框架选择最优加速方法，该框架优于传统方法，展现了去中心化AI系统推理加速潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模模型部署成本高，去中心化系统部署需高效推理加速方案，传统方法依赖随机选择或专家直觉，故需更好方法选择最优加速策略。

Method: 引入基于元学习的框架，通过学习不同任务中各种加速技术的历史性能数据，自动化选择过程。

Result: 该元学习框架简化决策过程，在效率和性能上持续优于传统方法。

Conclusion: 推理加速在去中心化AI系统有潜力，为实现更民主、经济可行的人工智能解决方案提供途径。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [71] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 现有优惠券分发策略未有效利用平台与用户的复杂序列交互，本文提出ADT4Coupons框架以提升长期收入，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能有效利用平台与用户的复杂序列交互，导致性能停滞，需要新策略提升长期收入。

Method: 提出名为Aligned Decision Transformer for Coupons (ADT4Coupons)的营销框架，整合一般场景、更全面历史数据的序列建模和高效迭代更新三个关键特征。

Result: 在真实工业数据集、公共和合成数据集上的实验结果证明了该框架的优越性。

Conclusion: ADT4Coupons框架能在各种现实营销场景中实现优化的在线决策，可用于长期收入提升。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [72] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 本文介绍建筑安全数据集CSDataset，进行初步基准测试和跨级分析，发现投诉驱动检查可降低后续事故可能性，数据集和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集规模有限且缺乏多样性，难以进行深入分析。

Method: 引入涵盖事故、检查和违规记录的CSDataset，该数据集整合结构化属性和非结构化叙述，进行初步基准测试和跨级分析。

Result: 发现投诉驱动的检查使后续事故可能性降低17.3%。

Conclusion: CSDataset能促进机器学习和大语言模型驱动的多种方法，为建筑安全领域未来工作提供参考和改进方向。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [73] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: 提出基于MoE架构的量化推理框架MoQE，结合多个量化变体，动态路由输入数据，实验证明其性能与SOTA相当且推理延迟无显著增加。


<details>
  <summary>Details</summary>
Motivation: 量化方法虽能提升模型效率和降低部署成本，但会导致精度下降，需提升量化模型性能。

Method: 提出MoQE框架，将全精度模型的多个量化变体作为“量化专家”，根据输入数据特征动态路由，设计适用于CV和NLP任务的轻量级、结构感知路由模型。

Result: 在多个模型家族和基准数据集上实验，MoQE性能与SOTA量化模型相当，推理延迟无显著增加。

Conclusion: MoQE能有效缓解单量化模型的性能下降问题，提升量化模型性能。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [74] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出基于可微转移模块的修复算法用于激光选择性转移，性能优、训练快，实验显示有显著效果，为微LED修复提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 激光选择性转移需计算模型规划转移序列，以减少XY平台运动并适应不同优化目标。

Method: 提出基于可微转移模块的修复算法，可通过基于梯度的优化进行训练。

Result: 相比局部邻近搜索算法，修复性能更优，目标设计更灵活；与基于强化学习的方法相比，无需手工特征提取器，训练更快；在2000x2000阵列上转移步骤减少50%，规划时间少于2分钟。

Conclusion: 该方法为增强现实/虚拟现实和下一代显示器制造中的微LED修复提供了实用且适应性强的解决方案。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [75] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 提出Hierarchical Adaptive Networks with Task Vectors (Hi - Vec)用于动态测试时自适应，有三项贡献，实验证明其能提升现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 标准测试时自适应方法依赖单维线性分类层，难以处理多样复杂的分布偏移。

Method: 提出Hi - Vec，将编码器表示空间分解为多层，采用动态层选择、权重合并机制和线性层一致性策略。

Result: 在多个目标数据集的挑战性场景中评估，Hi - Vec能提升鲁棒性、处理不确定性、应对小批量和高异常率。

Conclusion: Hi - Vec能推动现有方法发展，具有较强的性能。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [76] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 本文提出结合量子启发图神经网络与集成模型检测区块链网络非法交易，加入CP分解层，F2得分74.8%，建议金融领域推广量子启发算法。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域中，区块链网络非法交易检测是关键挑战，需创新解决方案。

Method: 结合量子启发图神经网络（QI - GNN）与集成模型（QBoost或随机森林分类器），在图神经网络框架中加入Canonical Polyadic (CP)分解层。

Result: 与经典机器学习实现相比，检测欺诈交易的F2分数达到74.8%。

Conclusion: 量子启发技术结合CP层结构改进，在金融安全复杂网络分析中可能超越传统方法，建议金融领域广泛采用和探索量子启发算法。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [77] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出基于大语言模型的表格学习原型估计框架，免训练构建零样本原型，结合少样本样本可增强效果，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在少样本和零样本表格数据建模中有效利用存在挑战。

Method: 通过无示例提示查询大语言模型生成特征值，免训练构建零样本原型，可结合少样本样本增强。

Result: 所提框架在零样本和少样本表格学习中有效。

Conclusion: 所提框架绕过基于示例提示的约束，提供可扩展且鲁棒的表格学习框架。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [78] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 本文提出用互补一维卷积网络集成模型进行气味检测，在小鼠实验中表现出色，证明了从细胞外LFP进行单试验气味检测的可行性和深度学习模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有气味检测人工传感器处理复杂混合物有困难，非侵入性记录单试验保真度不可靠，需开发通用气味检测系统。

Method: 提出互补一维卷积网络（ResCNN和AttentionCNN）集成模型，从多通道嗅球LFP解码气味存在。

Result: 在7只清醒小鼠的2349次试验中，模型平均准确率86.6%，F1分数81.0%，AUC为0.9247，远超先前基准，t - SNE可视化表明框架捕获了生物显著特征。

Conclusion: 从细胞外LFP进行稳健单试验气味检测可行，深度学习模型有助于深入理解嗅觉表征。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [79] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 本文提出评估图神经网络过压缩的方法，分析重连策略对过压缩的影响，发现重连在不同场景效果不同，并提供诊断工具。


<details>
  <summary>Details</summary>
Motivation: 消息传递图神经网络存在过压缩问题限制表达能力，重连技术可缓解但缺乏评估指标。

Method: 提出基于节点对互敏衰减率评估过压缩的方法，扩展为四个图级统计量，结合图内因果设计量化重连策略影响。

Result: 多数图分类数据集存在过压缩，重连可缓解；节点分类数据集过压缩不明显，重连常加剧过压缩，性能变化与过压缩变化无关。

Conclusion: 重连在过压缩严重且适度时最有益，过度重连或用于轻微过压缩图可能有害，诊断工具可提前判断重连是否值得。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [80] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文研究协作多智能体强化学习（c - MARL）在更现实和受限条件下的新漏洞，提出生成对抗扰动的算法，经多基准测试验证有效且样本效率高。


<details>
  <summary>Details</summary>
Motivation: c - MARL 缺乏对对抗攻击漏洞的深入研究，现有工作多关注训练时攻击或不现实场景。

Method: 假设对手只能收集和扰动已部署智能体的观测，或完全无法访问，提出生成对抗扰动的算法。

Result: 在三个基准和 22 个环境中验证了算法的有效性，且样本效率高，仅需 1000 个样本。

Conclusion: 所提算法能有效揭示 c - MARL 在现实条件下的漏洞，且样本效率高。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [81] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出基于模式的知识组件（KC）自动发现框架，评估显示有意义学习轨迹和性能提升，推动计算机科学教育知识建模。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育个性化学习需准确建模学生知识，现有自动KC提取因可解释性不足和编程问题开放性而具挑战。

Method: 训练变分自编码器生成重要代码模式，结合可解释的基于注意力的代码表示模型，聚类形成基于模式的KC，用学习曲线分析和深度知识追踪评估。

Result: 实验显示有意义的学习轨迹，深度知识追踪预测性能比传统知识追踪方法有显著提升。

Conclusion: 该工作提供了自动、可扩展且可解释的框架，推进了计算机科学教育中的知识建模。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [82] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 本文研究数据集蒸馏，将强化学习环境蒸馏为单批次监督学习数据集，展示其压缩和模态转换能力，并用新方法进行实验。


<details>
  <summary>Details</summary>
Motivation: 验证数据集蒸馏在不同任务中的泛化能力，压缩强化学习任务并实现学习模态转换。

Method: 提出近端策略优化的元学习扩展方法，用于蒸馏经典推车杆问题多维扩展、MuJoCo环境和Atari游戏。

Result: 展示了将复杂强化学习环境压缩为单步监督学习的能力，探索了蒸馏在不同学习者架构中的泛化性，实现将环境蒸馏为最小合成数据集。

Conclusion: 数据集蒸馏可用于强化学习，能实现任务压缩和模态转换，且具有一定泛化性。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [83] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出集成联邦学习和区块链技术的去中心化天气预报框架，实验显示能提升准确性、弹性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有集中式天气预报系统存在安全漏洞、可扩展性有限和单点故障问题。

Method: 提出集成联邦学习与区块链技术的去中心化框架，引入基于声誉的投票机制，利用IPFS进行链下存储。

Result: 实验表明该方法提高了预测准确性，增强了系统弹性和可扩展性。

Conclusion: 该方法是可部署于现实安全关键环境的可行方案。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [84] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出GNN精确验证方法GNNev，支持三种聚合函数，实验证明其在多数据集上的可用性、有效性及性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有技术缺乏对消息传递GNN常用聚合函数的对抗鲁棒性验证支持，需开发新方法。

Method: 采用带边界收紧的约束求解，迭代解决一系列松弛约束满足问题，利用求解器增量求解能力提高效率，实现了求解器GNNev。

Result: 在两个标准基准和两个真实世界欺诈数据集上的实验表明GNNev具有可用性和有效性，在求和聚合节点分类任务上性能优于现有精确验证工具。

Conclusion: 所提出的GNN精确验证方法及GNNev求解器是可行且有效的，能为GNN提供对抗属性和结构扰动的保证。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [85] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出一种基于幅度的突触剪枝方法，可集成到训练循环，在多个时间序列预测模型实验中表现出色，尤其在金融预测中效果显著，是传统dropout技术的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 生物大脑中的突触剪枝能去除弱连接提高效率，而人工神经网络中的dropout正则化随机停用神经元，未考虑依赖活动的剪枝，因此提出更好反映生物学的剪枝方法。

Method: 提出基于幅度的突触剪枝方法，在训练中逐步移除低重要性连接，集成到训练循环，计算各层权重重要性，应用立方调度增加全局稀疏性，定期用剪枝掩码移除低重要性权重。

Result: 在多个时间序列预测模型和四个数据集上实验有一致提升，在金融预测中，相比无或标准dropout模型，平均绝对误差最多降低20%，在部分transformer模型中最多降低52%。

Conclusion: 该动态剪枝机制将权重消除与渐进稀疏化相结合，改进了正则化，可轻松集成到不同架构，是传统dropout技术的实用替代方案。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [86] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 研究表明限制稀疏自编码器（SAE）训练到特定领域（医学文本）可改善重构保真度和可解释性，优于宽泛领域训练。


<details>
  <summary>Details</summary>
Motivation: 传统SAE在宽泛数据分布上训练，存在重构误差大、潜在特征碎片化等问题，难以解释。

Method: 使用19.5万个临床问答示例对Gemma - 2模型第20层激活进行JumpReLU SAE训练。

Result: 特定领域SAE比宽泛领域SAE多解释20%的方差，损失恢复更好，线性残差误差更小，学习的特征与临床有意义概念对齐。

Conclusion: 领域限制可缓解宽泛领域SAE的关键局限，实现更完整和可解释的潜在分解，建议质疑通用SAE的基础模型扩展策略。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [87] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 研究文本到图像模型能否将痴呆相关言语信息与生成图像对齐，发现仅从生成图像就能以75%准确率检测痴呆，并解释语言部分对检测的贡献。


<details>
  <summary>Details</summary>
Motivation: 了解文本到图像模型在病理言语和生成图像间的对齐情况，特别是痴呆相关言语信息与生成图像的对齐。

Method: 检查模型对齐痴呆相关言语信息与生成图像的能力，并运用可解释性方法。

Result: 仅从生成图像就能进行痴呆检测，在ADReSS数据集上准确率达75%。

Conclusion: 文本到图像模型能将痴呆相关言语与生成图像对齐，可用于痴呆检测，且能解释语言部分对检测的作用。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [88] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本文提出基于联邦学习的金融风险评估框架，通过特征注意力机制和时间建模结构实现跨机构联合建模与风险识别，实验表明该模型优于传统方法，有实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构金融风险分析中数据隐私和协同建模的挑战。

Method: 提出基于联邦学习的风险评估框架，采用分布式优化策略，各机构训练本地子模型，用差分隐私和噪声注入保护参数，中央服务器聚合参数生成全局模型。

Result: 提出的模型在所有评估指标上均优于传统集中式方法和现有联邦学习变体。

Conclusion: 该方法增强了风险识别的范围和效率，保护了数据主权，为智能金融风险分析提供了安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [89] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 本文提出一种无监督异常检测方法用于分布式后端服务系统，实验表明该方法优于现有模型，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统存在的复杂结构依赖、多样行为演化和缺乏标注数据等实际挑战。

Method: 基于服务调用关系构建动态图，用图卷积提取高阶结构表示，用Transformer建模节点时间行为，通过可学习的联合嵌入机制融合特征，最后进行非线性映射计算异常分数。

Result: 在真实云监控数据实验中，该方法在多个关键指标上优于现有模型，在捕捉异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性。

Conclusion: 该方法有很强的实际部署潜力。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [90] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: 本文提出DGS - MAML元学习算法，结合梯度匹配和锐度感知最小化，理论分析支持，实验显示其性能优于现有方法，适用于少样本学习场景，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在有限训练数据下，实现模型跨任务的泛化。

Method: 在双层优化框架中结合梯度匹配与锐度感知最小化，用PAC - Bayes进行理论分析并给出收敛保证。

Result: 在基准数据集实验中，DGS - MAML在准确性和泛化能力上优于现有方法。

Conclusion: DGS - MAML适用于少样本学习和快速适应场景。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [91] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 提出隐式超图神经网络IHGNN，用隐式平衡公式解决超图表示问题，实验表明其在准确性和鲁棒性上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络依赖固定数量显式消息传递层，限制长程依赖捕捉且训练不稳定。

Method: 引入隐式平衡公式到超图，将表示计算为非线性不动点方程的解，开发有收敛性证明的训练方案，分析模型过平滑条件和表达能力，推导泛化界，采用隐式梯度训练和投影稳定策略。

Result: 在引文基准测试中，IHGNN在准确性和鲁棒性上始终优于传统图/超图神经网络基线。

Conclusion: IHGNN对随机初始化和超参数变化有适应性，有强泛化能力和高阶关系学习实用价值。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [92] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: 提出NEXICA算法发现高速路拥堵因果关系，测试显示该算法在准确性和计算速度上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 道路交通拥堵是长期问题，聚焦拥堵原因是减少拥堵的有效策略，现有算法不足。

Method: 以道路速度时间序列为输入，开发新方法：关注时间序列中事件有无；用最大似然估计建立概率模型；训练二元分类器。

Result: 在洛杉矶地区195个高速路速度传感器6个月的数据上测试，该算法在准确性和计算速度上优于现有基线。

Conclusion: NEXICA算法能有效发现高速路系统中拥堵因果关系，表现优于现有算法。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [93] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: 提出CoGenT框架统一自监督学习的对比和生成范式，在六个数据集上评估有显著提升，为混合自监督学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习的对比和生成范式单独有效，但互补潜力未被挖掘，且两种方法存在局限性。

Method: 提出Contrastive Generative Time series框架（CoGenT），通过联合对比 - 生成优化统一两种范式。

Result: 在六个不同时间序列数据集上评估，对比SimCLR和MAE，F1分数分别最多提升59.2%和14.27%。

Conclusion: 混合目标在保留判别能力的同时获得生成鲁棒性，为时间领域的混合自监督学习奠定基础。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [94] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出细粒度聚类与拒绝网络（FGCRN）的开放式故障诊断模型，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 可靠的故障诊断系统需识别未知故障，多模式过程中同一健康状态样本呈多簇分布，难构建决策边界。

Method: 结合多尺度深度卷积、双向门控循环单元和时间注意力机制提取特征；设计基于距离的损失函数增强类内紧凑性；通过无监督学习构建细粒度特征表示；采用极值理论识别未知故障。

Result: 实验证明所提方法性能优越。

Conclusion: FGCRN模型能有效解决多模式过程中故障诊断问题，识别未知故障。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [95] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 本文提出新的Meta - NAS框架GraB - NAS，结合全局和局部搜索策略，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统NAS针对单一预定义任务，适用性受限；现有Meta - NAS方法存在泛化性差、搜索空间有限和计算成本高等问题。

Method: 将神经架构建模为图，采用混合搜索策略，结合搜索空间中的贝叶斯优化全局架构搜索和潜在空间中的梯度上升局部探索。

Result: GraB - NAS在实验中优于最先进的Meta - NAS基线，有更好的泛化性和搜索有效性。

Conclusion: GraB - NAS是一种有效的Meta - NAS框架，能发现性能良好的任务感知架构。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [96] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: 提出新深度学习模型DeepFeatIoT处理IoT时间序列数据分类问题，在多数据集表现佳，推动IoT分析发展。


<details>
  <summary>Details</summary>
Motivation: IoT传感器产生的时间序列数据存在元数据丢失、数据源异构等问题，影响智能系统有效性。

Method: 提出DeepFeatIoT模型，融合学习到的局部和全局特征、非学习的随机卷积核特征和大语言模型特征。

Result: 模型在多个真实世界IoT传感器数据集上表现一致且具有泛化性，优于现有基准模型。

Conclusion: DeepFeatIoT有潜力推动IoT分析重大进展，支持下一代智能系统发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [97] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 本文提出EGGS - PTP剪枝方法应对大语言模型部署挑战，实验表明该方法能加速、节省内存且精度更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署时计算和内存挑战严峻，需开发更高效模型变体。

Method: 引入EGGS - PTP方法，利用图论指导N:M结构化剪枝，结合扩张图概念确保剪枝网络信息流。

Result: EGGS - PTP因结构化稀疏性实现显著加速和内存节省，在不同大语言模型上精度优于现有结构化剪枝技术。

Conclusion: EGGS - PTP是应对大语言模型部署挑战的有效方法。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [98] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 现有大语言模型安全对齐技术存在不足，本文提出NeuronTune框架，能同时优化安全和效用，实验显示其性能超现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全对齐技术存在鲁棒性不足、误拒良性查询、文本质量和任务性能下降等问题，需要改进。

Method: 提出NeuronTune框架，通过归因识别关键神经元，用元学习调整激活，可通过神经元计数阈值调整干预范围。

Result: 实验表明该方法显著优于现有技术，在保证安全的同时保持良好效用。

Conclusion: NeuronTune能有效实现大语言模型安全与效用的同时优化。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [99] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出FCL中首个协作框架，用轻量级本地模型作桥梁，含两个新组件，实验证明性能优越。


<details>
  <summary>Details</summary>
Motivation: 基础模型在FCL中对本地下游任务表现不佳，且难在学习新任务时不遗忘旧知识，而小模型有优势，需弥合两者差距。

Method: 提出协作框架，轻量级本地模型作动态桥梁，包含Small Model Continual Fine - tuning和One - by - One Distillation两个组件。

Result: 实验结果表明即使客户端使用异构小模型，该框架仍有优越性能。

Conclusion: 所提出的协作框架能有效解决FCL中基础模型和小模型结合的问题，提升性能。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [100] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 提出MiCo框架用于边缘AI应用的混合精度量化探索与部署，可搜索最优量化方案并实现端到端加速，精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有探索混合精度量化（MPQ）方案的算法灵活性和效率有限，且缺乏MPQ模型优化与部署的端到端框架。

Method: 提出MiCo框架，采用新优化算法搜索最优量化方案，构建硬件感知延迟模型，支持从PyTorch MPQ模型直接部署到裸机C代码。

Result: 能搜索到满足延迟约束且精度最高的量化方案，实现端到端加速，精度损失最小。

Conclusion: MiCo框架有效解决了现有MPQ探索和部署的问题，适用于边缘AI应用。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [101] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出用于公共基础设施系统的因果图异常检测框架CGAD，能在非平稳和不平衡时间序列环境中表现出色，在多个工业数据集上有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着针对关键基础设施的网络攻击日益复杂，传统方法在多变量时间序列中存在分布偏移和类别不平衡问题，导致高误报率，需新的异常检测策略。

Method: CGAD采用两阶段监督框架，先通过动态贝叶斯网络学习因果不变图结构，再利用结构差异通过因果图比较检测异常。

Result: 在四个工业数据集上，CGAD在F1和ROC - AUC分数上比最佳基线有显著提升，能检测延迟和结构复杂的异常。

Conclusion: CGAD通过挖掘传感器数据下的因果结构，在检测网络攻击时精度更高，重新定义了异常检测的鲁棒性，在传统模型失效的情况下表现出韧性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [102] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: 介绍新方法Gauss - Tin解决大语言模型灾难性遗忘问题，实验显示有6%提升，凸显混合模型潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在灾难性遗忘问题，持续学习策略中基于重放的技术有较好表现，需更好方法解决问题。

Method: 引入Gauss - Tin，将重放策略与高斯混合模型结合，辅以指令引导，强化重要过往学习并接纳新信息。

Result: 实验表明在保留指标上比传统方法有6%的提升。

Conclusion: Gauss - Tin是缓解大语言模型灾难性遗忘的有效策略，混合模型能增强大语言模型在动态学习环境中的鲁棒性和适应性。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [103] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 本文提出统一可解释GNN框架用于PBPM，在五个基准测试中表现良好，解决架构、时间和语义问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的PBPM模型欠发达，多数模型忽略时间相关性和转换语义。

Method: 比较基于前缀的GCN和全轨迹的GAT；引入时间衰减注意力机制；将转换类型语义嵌入边特征；架构包含多级可解释性模块。

Result: 在五个基准测试中，模型无需针对每个数据集调优，取得有竞争力的Top - k准确率和DL分数。

Conclusion: 该工作为PBPM中的下一事件预测提供了强大、可泛化和可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [104] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 本文提出分层联邦微调框架，用于车联网系统中资源感知和移动弹性学习，通过新算法实现自适应探索，实验表明该方法在准确率和效率上表现最优。


<details>
  <summary>Details</summary>
Motivation: 车联网系统中，由于客户端移动性、资源异构性和间歇性连接，实现高效低延迟多任务适应具有挑战性。

Method: 提出分层联邦微调框架，利用低秩自适应（LoRA），引入分散的能量感知秩自适应机制，开发UCB - DUAL算法，构建大规模车联网模拟器。

Result: 该方法在所有基线中实现了最佳的准确率 - 效率权衡，延迟降低超24%，平均准确率提高超2.5%。

Conclusion: 所提方法能有效应对车联网系统挑战，实现资源感知和移动弹性学习。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [105] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: 论文提出SYNAPSE - G利用大语言模型生成合成训练数据解决稀有事件分类冷启动问题，理论分析合成数据质量影响，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决标注数据稀缺尤其是稀有事件标注数据不足，阻碍有效机器学习模型训练的问题。

Method: 提出SYNAPSE - G，利用大语言模型生成合成训练数据作为种子，在相似度图上进行半监督标签传播，确定候选正例，由预言机标注，用扩展数据集训练/微调分类器，并理论分析合成数据质量的影响。

Result: 在不平衡的SST2和MHS数据集实验中，SYNAPSE - G在寻找正标签方面效果好，优于包括最近邻搜索在内的基线方法。

Conclusion: SYNAPSE - G能有效解决稀有事件分类的冷启动问题，在稀有事件分类任务中表现良好。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [106] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: 本文全面分析世界模型如何赋能边缘的智能AI系统，介绍架构基础、应用场景、协同情况，指出挑战与研究方向，为下一代边缘系统提供基础和路线图。


<details>
  <summary>Details</summary>
Motivation: 以往在机器人和游戏领域的世界模型研究未充分探索其在无线边缘的应用，本文旨在填补该空白。

Method: 先研究世界模型的架构基础，再阐述其在EGI场景的应用，接着探讨与基础模型和数字孪生的协同，最后指出挑战和研究方向。

Result: 展示了世界模型在多种EGI场景中的应用，凸显其在优化延迟、能源和隐私约束方面的作用，定位其为EGI的认知支柱。

Conclusion: 为实现下一代智能、自主边缘系统提供概念基础和实用路线图。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [107] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 提出GDCC框架用于高效环境探索，通过测量因果能力确定子目标，实验显示其比基线方法有显著成功率提升。


<details>
  <summary>Details</summary>
Motivation: 因果推断对强化学习中智能体探索环境很重要，但在复杂场景中测量因果关系具有挑战性，需要新方法来实现高效探索。

Method: 先推导状态空间中的因果能力测量方法，再用基于蒙特卡罗的方法在离散状态空间识别关键点并优化用于连续高维环境，将关键点作为子目标引导探索。

Result: 多目标任务的实验结果表明，高因果能力的状态与预期子目标一致，GDCC比基线方法显著提高了成功率。

Conclusion: 提出的GDCC框架能有效测量因果关系，通过确定子目标引导智能体更有目的和高效地探索环境。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [108] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [109] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: 提出TimeMKG多模态因果推理框架，结合变量语义信息提升时间序列建模，实验表明可提升预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽略变量语义信息，而文本描述包含关键领域知识，需提升建模的鲁棒性和可解释性。

Method: 利用大语言模型解释变量语义，构建多元知识图，用双模态编码器分别处理语义提示和统计模式，通过跨模态注意力融合，注入因果先验到下游任务。

Result: 在多样数据集实验中，结合变量级知识显著提升了预测性能和泛化能力。

Conclusion: 将变量语义信息融入时间序列建模可有效提升模型性能和泛化能力。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [110] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是基于Python的统计框架，克服现有TPP工作流程局限，可灵活分析蛋白质热稳定性数据。


<details>
  <summary>Details</summary>
Motivation: 现有热蛋白质组分析方法有局限性，如假设为S型熔解曲线、受经验零分布约束，难以检测到生物相关变化。

Method: 使用带平方指数核的高斯过程模型灵活建模熔解曲线形状，通过核先验生成无偏零分布。

Result: 该框架可用于分析全蛋白质组扰动、非常规熔解曲线蛋白的数据。

Conclusion: Thermal Tracks是可用于全蛋白质组热分析研究的便捷灵活工具。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [111] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究非对称低秩矩阵补全问题，证明无正则项梯度下降算法性能，算法计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决非对称低秩矩阵补全问题，探究无正则项梯度下降算法收敛性能。

Method: 采用梯度下降法，引入留一法，归纳证明普通梯度下降与谱初始化的收敛率。

Result: 无正则项不影响收敛性能，平衡正则项范数小，算法计算成本低且补全性能相当。

Conclusion: 无正则项的梯度下降算法在非对称低秩矩阵补全问题上有良好表现。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [112] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出动态连接掩码机制增强分类器对噪声标签的鲁棒性，实验表明该方法优于SOTA，还发现KANs在真实噪声场景中比MLPs更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有缓解噪声标签负面影响的研究对模型架构正则化探索有限，为增强分类器对噪声标签的鲁棒性。

Method: 提出动态连接掩码（DCM）机制，可在训练中自适应屏蔽不重要的边，还可与多种抗噪声训练方法集成。

Result: 理论分析证明其能减少梯度误差，在合成和真实基准测试中均优于SOTA方法。

Conclusion: DCM机制有效，KANs在真实噪声场景中比MLPs有更好的抗噪声能力。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [113] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 提出GraphTreeGen（GTG）框架用于脑连接组合成，高效准确且内存需求低，还可扩展应用。


<details>
  <summary>Details</summary>
Motivation: 脑连接组获取成本高、耗时长，现有图生成模型存在压缩信息模糊、依赖属性、忽视边权重预测和计算成本高等问题。

Method: GTG将连接组分解为熵引导的k跳树，用共享GCN编码，通过二分消息传递层融合子树嵌入和全局节点特征，双分支解码器联合预测边的存在和权重。

Result: GTG在自监督任务中优于现有基线，在监督设置中保持竞争力，结构保真度更高、边权重更精确，内存需求少。

Conclusion: GTG是高效准确的脑连接组合成框架，模块化设计可扩展到超分辨率和跨模态合成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [114] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 利用公开临床数据集理解疾病异质性和探索个性化治疗时，数据常不完整且缺乏关键标签，AI 工具解释性有限。本文以 ARDS 识别为例，用 LLM 处理临床笔记生成额外概念，性能提升 10% 并改善 ARDS 表征。


<details>
  <summary>Details</summary>
Motivation: 公开临床数据集常不完整且缺乏关键标签，现有 AI 工具解释性有限，概念瓶颈模型存在性能限制，需提升其性能。

Method: 以 ARDS 识别为测试案例，利用大语言模型（LLM）处理临床笔记，生成额外概念。

Result: 相比现有方法性能提升 10%，能学习更全面概念，降低信息泄露风险和对虚假捷径的依赖，改善 ARDS 表征。

Conclusion: 结合临床笔记的上下文信息能提升概念瓶颈模型性能，有助于更好地识别和表征 ARDS。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [115] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 提出GenCO框架解决电商广告创意元素组合选择难题，在电商平台提升广告收入并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 现有电商广告创意元素组合选择方法常单独评估组件，无法应对指数级搜索空间。

Method: 提出GenCO框架，分两阶段，先用生成模型生成创意组合并以强化学习优化，再用多实例学习模型将组合奖励归因到元素。

Result: 在电商平台显著增加广告收入。

Conclusion: 该方法有实际价值，还发布数据集推动领域研究。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [116] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出通过结构化知识继承优化小型可部署模型的策略HKT，在多视觉任务中验证其能提升子模型性能并保持紧凑，优于传统蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络研究中模型深度和容量增加带来的可集成性和效率问题，优化小型可部署模型。

Method: 引入HKT框架，借鉴生物遗传机制，通过ETM组件进行特征转移，用GA机制整合继承和原生表示。

Result: 在多种视觉任务中，HKT显著提升子模型性能，保持模型紧凑，且始终优于传统蒸馏方法。

Conclusion: HKT是在资源受限环境中部署高性能神经网络的通用、可解释和可扩展的解决方案。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [117] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 本文开发并验证机器学习管道，用纵向队列数据预测年龄，通过引入关键生物标志物变化率特征提升模型性能，为跟踪患者健康轨迹临床工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型预测年龄时难以捕捉衰老过程动态纵向特征，需开发新方法。

Method: 开发机器学习管道，用两个不同时间段纵向队列数据；引入生物标志物变化率特征；训练LightGBM模型；进行SHAP分析。

Result: 最终LightGBM模型在后续数据中准确预测年龄（男性$R^2 = 0.515$，女性$R^2 = 0.498$），优于传统线性模型和其他树集成模型。

Conclusion: 个体健康轨迹是生物年龄关键决定因素，该框架为跟踪患者健康轨迹临床工具铺平道路。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [118] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 本文推导了针对MoE的μ - 参数化（μP），并进行了实证验证，还研究了专家数量和粒度对最优学习率的影响。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLMs）受关注，μTransfer是大规模训练中调整超参数的关键技术，混合专家（MoE）是超大型模型的主流架构，但二者的结合尚未被探索。

Method: 推导针对MoE的μ - 参数化（μP），为路由器和专家在模型宽度上的特征学习提供理论保证，并进行实证验证。

Result: 完成了μP的推导和实证验证。

Conclusion: 研究了专家数量和粒度对最优学习率的影响。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [119] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 本文聚焦多区域电力负荷预测问题，提出TriForecaster框架，在多个数据集上表现优于现有模型，且有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 智能电网和电表提供了更详细的负荷数据，且不同城市负荷有相似模式，为准确进行大区域内多个子区域的短期负荷预测。

Method: 提出TriForecaster框架，在多任务学习范式中利用混合专家方法，包含RegionMixer和CTSpecializer层。

Result: 在四个不同粒度的真实数据集上，TriForecaster平均预测误差降低22.4%，优于现有模型；在eForecaster平台上有效提供17个城市的短期负荷预测。

Conclusion: TriForecaster具有灵活性和广泛适用性，有实际应用价值。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [120] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: 提出Proto - PINV+H训练范式，结合闭式权重计算与梯度优化，在MNIST和Fashion - MNIST上有良好表现，有多层扩展等，与其他方法相比有优势。


<details>
  <summary>Details</summary>
Motivation: 寻求更高效的训练范式，实现更好的准确率、速度和模型规模的平衡。

Method: 结合闭式权重计算与对少量合成输入、软标签和隐藏激活的基于梯度的优化，每次迭代通过岭正则化伪逆求解闭式计算权重矩阵，用Adam更新原型。

Result: 在MNIST和Fashion - MNIST官方测试集上分别达到97.8%和89.3%的测试准确率，用时3.9 - 4.5秒，约130k可训练参数，250个epoch。

Conclusion: 该方法在准确率、速度和模型规模的权衡上优于ELM、随机特征岭回归和浅层MLP。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [121] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 利用机器学习分析世锦赛跳远决赛生物力学特征，确定影响男女运动员顶尖表现的关键特征。


<details>
  <summary>Details</summary>
Motivation: 传统方法难分析部分生物力学特征与运动员最终表现的关系，现代数据分析法在体育分析中愈发重要。

Method: 用分位数回归建模生物力学特征集与有效距离的关系，用SHAP、PDPs和ICE图解释模型。

Result: 除速度相关特征，特定技术方面也很关键，男运动员起跳前支撑腿膝盖角度大于169°，女运动员着陆姿势和助跑步技术是影响顶尖表现的重要因素。

Conclusion: 建立了分析各特征对运动表现影响的框架，尤其关注顶尖表现赛事。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [122] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文提出理论框架解释大语言模型上下文学习的事实回忆任务，证明收敛性与泛化性，实证模拟验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型上下文学习事实回忆任务的理论解释。

Method: 基于经验的分层概念建模提出理论框架，发展优化理论。

Result: 证明0 - 1损失收敛，展示强泛化性，实证模拟支持理论。

Conclusion: 阐述了Transformer相对于静态嵌入模型的优势。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [123] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 提出RankList框架解决现有偏好学习框架局限，实验证明其在多模态任务中表现优且有跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有成对偏好学习框架如RankNet局限于局部比较，难以捕捉全局排名一致性，需改进。

Method: 提出RankList列表化偏好学习框架，将RankNet推广到结构化列表级监督，用log - sum - exp近似提高训练效率，扩展了跳级比较。

Result: 在语音情感识别和图像美学排名等多模态基准数据集上表现优于标准列表化基线，跨域研究显示有良好泛化性。

Conclusion: RankList为主观学习场景中建模有序偏好提供统一、可扩展方法。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [124] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: 提出FedShard联邦无学习算法，保障效率公平与性能公平，加速数据无学习过程。


<details>
  <summary>Details</summary>
Motivation: 当前研究未充分探索联邦无学习中分散客户端间效率公平和性能公平问题，旨在解决该问题。

Method: 引入FedShard算法自适应处理收敛、无学习效率和公平性间的困境，提出两个新指标评估无学习算法公平性。

Result: 理论分析和数值评估验证FedShard在无学习性能和效率上的公平性，能减轻不公平风险，平衡客户端无学习成本。实验表明比从头训练快1.3 - 6.2倍，比现有精确无学习方法快4.9倍。

Conclusion: FedShard算法能有效保障联邦无学习中的公平性，且加速无学习过程。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [125] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出数据高效蒸馏框架DED优化推理蒸馏的帕累托前沿，用少量精心挑选样本在数学推理和代码生成任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法存在推理缩放定律未成形、计算成本增加的问题，需更高效方法。

Method: 受强化学习启发，包括选最优教师模型、用小而精语料平衡性能、用多样推理轨迹培养学生模型推理能力。

Result: 在数学推理和代码生成任务评估中，仅用0.8k精心挑选样本取得SOTA结果。

Conclusion: DED考虑多因素，优于现有方法，为高级推理提供实用高效途径。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [126] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文对用于预测土壤特性的现代人工神经网络（ANN）架构进行全面基准测试，结果显示现代ANNs在多数任务上优于经典方法，推荐在田间尺度PSM中采用，尤其推荐TabPFN。


<details>
  <summary>Details</summary>
Motivation: 田间尺度预测性土壤建模（PSM）任务受小训练样本和高特征样本比限制，传统深度学习方法有挑战，经典机器学习算法长期是默认选择，而最新ANNs对其提出挑战但适用性未被证明。

Method: 引入综合基准测试，评估包括多层感知机、基于注意力的变压器变体、检索增强方法和上下文学习基础模型等多种ANN架构，涵盖31个田间和农场尺度数据集及三种关键土壤特性。

Result: 现代ANNs在多数任务上持续优于经典方法，TabPFN整体性能最强，在不同条件下表现稳健。

Conclusion: 推荐在田间尺度PSM中采用现代ANNs，并建议将TabPFN作为土壤计量学家工具包的新默认选择。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [127] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [128] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 本文提出4种策略挖掘大语言模型在上下文辅助预测中的零样本能力，在不同任务上表现优于简单提示法，为改进预测带来新思路。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽表明大语言模型可作为上下文辅助预测器，但潜力未充分挖掘。

Method: 提出ReDP、CorDP、IC - DP、RouteDP四种策略。

Result: 在CiK基准的不同上下文辅助预测任务上，四种策略在不同大小和类型的大语言模型中均优于简单提示法。

Conclusion: 这些策略为基于大语言模型的上下文辅助预测带来简单有效的改进方向。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [129] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 本文提出原型扩散模型（PDM），无需外部内存，通过原型学习实现高效自适应视觉条件，实验表明其在保证生成质量的同时降低计算和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算密集，潜在空间模型牺牲细节，检索增强扩散模型有存储和检索成本高、依赖静态模型及缺乏适应性等问题，因此需新方法解决。

Method: 将原型学习直接集成到扩散过程，用对比学习从干净图像特征构建动态紧凑视觉原型集，以引导去噪步骤。

Result: PDM在保证高生成质量的同时，降低了计算和存储开销。

Conclusion: PDM是扩散模型中基于检索的条件化方法的可扩展替代方案。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [130] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出ResRMN，结合线性和非线性水库，研究状态动力学，在任务上实验显示优势。


<details>
  <summary>Details</summary>
Motivation: 在水库计算范式下引入新的未训练循环神经网络。

Method: 结合线性记忆水库和基于时间维度残差正交连接的非线性水库，用线性稳定性分析研究水库状态动力学，探索不同时间残差连接配置。

Result: 在时间序列和像素级一维分类任务实验中，该方法优于其他传统RC模型。

Conclusion: 所提出的ResRMN方法具有优势。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [131] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文提出在训练后将测试时缩放知识集成到模型的方法，以解决计算开销大的问题，在降低计算成本的同时恢复部分质量提升。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放范式虽有突破，但计算时间大幅增加，很多应用中不实用，需保留其优点并避免推理开销。

Method: 用噪声超网络调制初始输入噪声替代扩散模型中基于奖励的测试时噪声优化，提出学习蒸馏生成器奖励倾斜分布的理论框架。

Result: 该方法能以较低计算成本恢复显式测试时优化带来的大部分质量提升。

Conclusion: 提出的方法可有效解决测试时缩放范式计算开销大的问题。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [132] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 本文提出用于图增量学习的动态专家混合（DyMoE）方法，设计定制正则化损失，引入稀疏MoE降低计算成本，模型准确率较基线提升4.92%。


<details>
  <summary>Details</summary>
Motivation: 常规图机器学习方法用于增量学习时存在灾难性遗忘问题，以往方法未考虑不同时间戳知识对学习新任务贡献不同。

Method: 提出DyMoE方法，添加新专家网络，设计定制正则化损失；引入稀疏MoE方法减少计算时间。

Result: 模型在类增量学习中较最佳基线相对准确率提升4.92%。

Conclusion: 所提模型有卓越性能。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [133] [Reinforcement learning in densely recurrent biological networks](https://arxiv.org/abs/2508.09618)
*Miles Walter Churchland,Jordi Garcia-Ojalvo*

Main category: cs.NE

TL;DR: 提出混合、无导数优化框架ENOMAD训练连续动作空间的高度循环网络，在线虫觅食任务中表现超现有策略，证明结合进化搜索与非线性优化有效。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的方法训练连续动作空间的高度循环网络存在梯度爆炸或消失问题，纯进化搜索在高维权重空间收敛慢。

Method: 引入ENOMAD框架，结合全局进化探索与局部直接搜索利用，利用生物权重先验，有两种算法变体。

Result: 两种变体在任务中显著超过未训练的连接组和现有训练策略。

Conclusion: 结合进化搜索与非线性优化为自然循环网络针对特定任务提供了高效、基于生物学的策略。

Abstract: Training highly recurrent networks in continuous action spaces is a technical
challenge: gradient-based methods suffer from exploding or vanishing gradients,
while purely evolutionary searches converge slowly in high-dimensional weight
spaces. We introduce a hybrid, derivative-free optimization framework that
implements reinforcement learning by coupling global evolutionary exploration
with local direct search exploitation. The method, termed ENOMAD (Evolutionary
Nonlinear Optimization with Mesh Adaptive Direct search), is benchmarked on a
suite of food-foraging tasks instantiated in the fully mapped neural connectome
of the nematode \emph{Caenorhabditis elegans}. Crucially, ENOMAD leverages
biologically derived weight priors, letting it refine--rather than rebuild--the
organism's native circuitry. Two algorithmic variants of the method are
introduced, which lead to either small distributed adjustments of many weights,
or larger changes on a limited number of weights. Both variants significantly
exceed the performance of the untrained connectome (in what can be interpreted
as an example of transfer learning) and of existing training strategies. These
findings demonstrate that integrating evolutionary search with nonlinear
optimization provides an efficient, biologically grounded strategy for
specializing natural recurrent networks towards a specified set of tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [134] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 本文探讨大语言模型（LLMs）如何通过实时、上下文感知反馈提升软件工程课程中代码重构教学。采用结构化提示将LLM辅助重构融入课程项目，结果显示LLMs能弥合理论与实践学习。


<details>
  <summary>Details</summary>
Motivation: 代码重构能提高代码质量，但在软件工程课程中难教，传统方法反馈有限且不一致。

Method: 将LLM辅助重构融入课程项目，使用结构化提示帮助学生识别和处理代码异味，并在长期存在的开源软件项目中实施，通过学生反馈和代码质量改进分析进行评估。

Result: LLMs能弥合理论与实践学习，支持学生更深入理解可维护性和重构原则。

Conclusion: LLMs可有效应用于软件工程课程的代码重构教学。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [135] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出统计型日志解析器PIPLUP，在准确率、泛化性表现出色，优于现有统计型解析器，与语义型解析器有竞争力，且时间消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有统计型日志解析器在准确率和泛化性不足，普遍认为其不如语义型解析器，需提升统计型解析器性能。

Method: 提出PIPLUP，消除常量令牌位置的预设，依靠数据不敏感参数，实现给定日志文件的即插即用。

Result: 在开源大型日志数据集实验中，PIPLUP准确率和泛化性好，优于Drain及其变体，与LUNAR有竞争力，且无需GPU加速和外部API，时间消耗低。

Conclusion: PIPLUP简单、高效、有效，在成本和隐私敏感场景更实用。

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [136] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 针对大语言模型在无注释代码库中功能完成性能下降问题，提出三阶段流程，构建数据集，实验显示能提升多种大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在有显式指令时功能完成准确性高，但真实代码库常缺少注释，导致性能大幅下降。

Method: 将任务分为意图推理、交互式细化、函数生成三阶段，设计推理提示框架，引入交互式细化机制，构建含40000个带中间推理轨迹和对应文档字符串示例的数据集。

Result: 在DevEval和ComplexCodeEval上实验表明，该方法能持续提升多种大语言模型，在参考和执行指标上相对增益超20%，交互式细化阶段有额外提升。

Conclusion: 提出的三阶段流程和构建的数据集能有效解决大语言模型在无注释代码库中功能完成性能下降问题。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [137] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: 本文提出基于大语言模型的ReqInOne将自然语言转换为结构化SRS，评估显示其生成的SRS更准确、结构更好。


<details>
  <summary>Details</summary>
Motivation: 手动编写软件需求规格说明书（SRS）耗时且易产生歧义，现有自动化方法依赖人工分析，基于大语言模型的方法存在幻觉和可控性有限的问题。

Method: 提出ReqInOne，采用模块化架构，将SRS生成分解为摘要、需求提取和需求分类三个任务，并使用定制提示模板。

Result: 使用GPT - 4o、LLaMA 3和DeepSeek - R1评估ReqInOne，专家评估显示其生成的SRS更准确、结构更好，需求分类组件表现与现有最优模型相当甚至更好。

Conclusion: ReqInOne的模块化设计使其在生成SRS方面具有性能优势。

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [138] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: 研究DeputyDev这个AI代码审查助手的实施和效果，通过实验证明其能显著缩短审查时间并已推广应用。


<details>
  <summary>Details</summary>
Motivation: 解决软件开发过程中代码审查效率低下的问题，如耗时久、反馈不一致、审查质量不高等。

Method: 开发DeputyDev的PR审查功能，提供自动化、上下文感知的代码审查，并进行涉及超200名工程师的严格双控A/B实验。

Result: DeputyDev使平均每个PR审查时间减少23.09%，每行代码平均审查时间减少40.13%，已在组织内有效推广并作为SaaS解决方案供外部公司使用。

Conclusion: AI辅助代码审查有助于改善开发工作流程和代码质量。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [139] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: 研究指出自闭症个体在ICT领域有潜力，但在软件工程岗位面临障碍。通过系统回顾30项研究，确定18个成功因素，为多方提供提升自闭症个体融入软件工程的建议。


<details>
  <summary>Details</summary>
Motivation: 基于神经多样性运动的伦理框架和相关项目成功，且缺乏从软件工程教育到职场融入完整知识综合，推动研究以提升自闭症个体在软件工程领域的融入度。

Method: 对30项研究进行系统回顾。

Result: 确定18个成功因素，分为软件工程教育、职业与就业培训、工作环境、工具与辅助技术四个主题类别。

Conclusion: 为教育机构、雇主、组织和工具开发者提供基于证据的建议，以增强自闭症个体在软件工程中的融入。

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [140] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: 提出LibRec框架结合大语言模型与RAG技术自动推荐替代库，用上下文学习提高推荐准确性，引入LibEval基准评估，开展多项实验。


<details>
  <summary>Details</summary>
Motivation: 自动化替代库的推荐，提高推荐准确性。

Method: 提出LibRec框架，利用大语言模型与RAG技术，用上下文学习从提交消息中提取迁移意图；引入LibEval基准进行评估。

Result: 在LibEval基准上对十种流行大语言模型进行评估，开展消融实验、探究提示策略影响、评估不同意图类型效果及进行失败案例分析。

Conclusion: 未明确提及，但从实验内容可推测评估了LibRec框架在库迁移推荐任务中的有效性。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [141] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: 本文针对软件项目的关键风险指标总线因子计算难题，提出两种近似启发式方法，经实验验证更准确、可扩展且鲁棒，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 总线因子是重要风险指标，但准确计算在既定形式化下是NP难问题，大规模软件系统难以进行可扩展分析。

Method: 将软件项目建模为开发者和任务的二分图，针对两种有影响力的总线因子形式化提出最小覆盖和最大覆盖两种基于迭代图剥离的近似启发式方法。

Result: 在超1000个合成幂律图上的综合实证评估表明，所提启发式方法估计更精确，能在数分钟内处理数百万节点和边的图，且对开发者 - 任务分配图的结构变化具有鲁棒性。

Conclusion: 所提启发式方法比广泛采用的基于度的启发式方法更准确、可扩展且鲁棒，开源实现可支持未来研究和实际应用。

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [142] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 探索用大语言模型（LLMs）对代码审查评论分类，结果显示其表现优于现有方法，能提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖有监督机器学习，需大量手动标注，为解决此局限探索LLMs分类代码审查评论的潜力。

Method: 评估LLMs对17类代码审查评论的分类性能。

Result: LLMs能对代码审查评论分类，表现优于现有深度学习模型，在五类最有用类别分类上准确性更高，在高低频类别上表现均衡。

Conclusion: LLMs可为代码审查分析提供可扩展解决方案，提高代码审查过程的有效性。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [143] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: 本文对920个开源Go项目中CGO使用情况进行实证研究，开发工具分析CGO特征，发现其使用分布、目的、模式和问题，并提出解决方案和改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视Go中新兴的FFI（CGO）带来的独特风险，为解决相关问题开展研究。

Method: 对920个开源Go项目进行实证研究，开发CGOAnalyzer工具识别和量化CGO相关特征。

Result: 11.3%的项目使用CGO；CGO有4个主要用途和15种使用模式；存在19种相关问题，包括一个关键问题；提出临时解决方案；提交改进Go工具链的提案。

Conclusion: 研究结果为开发者和Go团队提供有价值见解，提高开发效率和可靠性，增强Go工具链的健壮性。

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [144] [Optimal Control of Reserve Asset Portfolios for Pegged Digital Currencies](https://arxiv.org/abs/2508.09429)
*Alexander Hammerl,Georg Beyschlag*

Main category: q-fin.PM

TL;DR: 研究稳定币发行方平衡流动性与储备收益的资金管理问题，提出随机模型预测控制框架，模拟显示其能提升钉住质量且不牺牲收益。


<details>
  <summary>Details</summary>
Motivation: 稳定币发行方需平衡即时流动性与储备收益以维持钉住汇率的可信度，解决资金管理问题。

Method: 将资金管理问题作为连续时间控制任务，开发包含事件强度矩闭合的随机模型预测控制框架，运用庞特里亚金最大值原理。

Result: 最优控制呈现bang - off - bang结构，引入结算窗口有简单阈值结构；模拟中控制器在不同场景表现良好。

Conclusion: 所提策略可实施，将经验流动风险转化为可审计的资金规则，能在不牺牲收益的情况下提升钉住质量。

Abstract: Stablecoins promise par convertibility, yet issuers must balance immediate
liquidity against yield on reserves to keep the peg credible. We study this
treasury problem as a continuous-time control task with two instruments:
reallocating reserves between cash and short-duration government bills, and
setting a spread fee for either minting or burning the coin. Mint and
redemption flows follow mutually exciting processes that reproduce clustered
order flow; peg deviations arise when redemptions exceed liquid reserves within
settlement windows. We develop a stochastic model predictive control framework
that incorporates moment closure for event intensities. Using Pontryagin's
Maximum Principle, we demonstrate that the optimal control exhibits a
bang-off-bang structure: each asset type is purchased at maximum capacity when
the utility difference exceeds the corresponding difference in shadow costs.
Introducing settlement windows leads to a sampled-data implementation with a
simple threshold (soft-thresholding) structure for rebalancing. We also
establish a monotone stress-response property: as expected outflows intensify
or windows lengthen, the optimal policy shifts predictably toward cash. In
simulations covering various stress test scenarios, the controller preserves
most bill carry in calm markets, builds cash quickly when stress emerges, and
avoids unnecessary rotations under transitory signals. The proposed policy is
implementation-ready and aligns naturally with operational cut-offs. Our
results translate empirical flow risk into auditable treasury rules that
improve peg quality without sacrificing avoidable carry.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [145] [Distributional Sensitivity Analysis: Enabling Differentiability in Sample-Based Inference](https://arxiv.org/abs/2508.09347)
*Pi-Yueh Chuang,Ahmed Attia,Emil Constantinescu*

Main category: stat.ML

TL;DR: 提出两个估计任意维随机向量灵敏度的解析公式及四个数值算法，通过验证研究展示其有效性，适用于基于样本的逆问题，方法无需模型拟合等，还使采样子程序可微分，提供代码以利复现。


<details>
  <summary>Details</summary>
Motivation: 在不进行模型拟合、无需了解采样算法和计算高维积分的情况下，解决任意维随机向量灵敏度估计问题，适用于采样器为黑箱或需昂贵物理模拟的样本逆问题。

Method: 提出两个解析公式，用第一个公式将灵敏度解释为与一维条件分布向量相关的逆映射的偏导数，第二个公式采用对角近似降低计算成本；在无封闭形式时提供四个二阶数值算法近似公式。

Result: 验证和验证研究证明数值算法正确性和公式有效性，核物理应用展示可实现量子相关函数的不确定性量化和参数推断。

Conclusion: 所提方法避免了现有方法的一些需求，对样本逆问题有用，能使采样子程序可微分，便于集成到深度学习和自动微分编程框架，代码利于复现和进一步开发。

Abstract: We present two analytical formulae for estimating the sensitivity -- namely,
the gradient or Jacobian -- at given realizations of an arbitrary-dimensional
random vector with respect to its distributional parameters. The first formula
interprets this sensitivity as partial derivatives of the inverse mapping
associated with the vector of 1-D conditional distributions. The second
formula, intended for optimization methods that tolerate inexact gradients,
introduces a diagonal approximation that reduces computational cost at the cost
of some accuracy. We additionally provide four second-order numerical
algorithms to approximate both formulae when closed forms are unavailable. We
performed verification and validation studies to demonstrate the correctness of
these numerical algorithms and the effectiveness of the proposed formulae. A
nuclear physics application showcases how our work enables uncertainty
quantification and parameter inference for quantum correlation functions. Our
approach differs from existing methods by avoiding the need for model fitting,
knowledge of sampling algorithms, and evaluation of high-dimensional integrals.
It is therefore particularly useful for sample-based inverse problems when the
sampler operates as a black box or requires expensive physics simulations.
Moreover, our method renders arbitrary sampling subroutines differentiable,
facilitating their integration into programming frameworks for deep learning
and automatic differentiation. Algorithmic details and code implementations are
provided in this paper and in our open-source software DistroSA to enable
reproducibility and further development.

</details>


### [146] [A pseudo-inverse of a line graph](https://arxiv.org/abs/2508.09412)
*Sevvandi Kandanaarachchi,Philip Kilby,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 研究线图小扰动下恢复对应根图，提出线性整数规划方法并理论证明伪逆操作良好，实验验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 线图到根图转换不可逆，研究线图小扰动下恢复对应根图，定义线图操作的逆。

Method: 提出线性整数规划方法，编辑线图中最少数量的边以找到根图，用谱范数理论证明伪逆操作良好。

Result: 在Erdős-Rényi图上的实验表明理论结果在实践中有效。

Conclusion: 所提出的方法能在一定程度上解决线图逆转换问题，理论结果具有实际应用价值。

Abstract: Line graphs are an alternative representation of graphs where each vertex of
the original (root) graph becomes an edge. However not all graphs have a
corresponding root graph, hence the transformation from graphs to line graphs
is not invertible. We investigate the case when there is a small perturbation
in the space of line graphs, and try to recover the corresponding root graph,
essentially defining the inverse of the line graph operation. We propose a
linear integer program that edits the smallest number of edges in the line
graph, that allow a root graph to be found. We use the spectral norm to
theoretically prove that such a pseudo-inverse operation is well behaved.
Illustrative empirical experiments on Erd\H{o}s-R\'enyi graphs show that our
theoretical results work in practice.

</details>


### [147] [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](https://arxiv.org/abs/2508.09623)
*Akshay Thakur,Sawan Kumar,Matthew Zahr,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 提出可扩展的概率数值求解偏微分方程方法，降低计算复杂度，在基准PDE上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 概率数值求解PDE存在计算成本高的问题，尤其是大规模或高维问题。

Method: 提出随机对偶下降算法降低每次迭代复杂度，采用基于聚类的主动学习策略自适应选择配点。

Result: 得到h自适应概率求解器，能处理大量配点。

Conclusion: 所提求解器在基准PDE上有效。

Abstract: Solving partial differential equations (PDEs) within the framework of
probabilistic numerics offers a principled approach to quantifying epistemic
uncertainty arising from discretization. By leveraging Gaussian process
regression and imposing the governing PDE as a constraint at a finite set of
collocation points, probabilistic numerics delivers mesh-free solutions at
arbitrary locations. However, the high computational cost, which scales
cubically with the number of collocation points, remains a critical bottleneck,
particularly for large-scale or high-dimensional problems. We propose a
scalable enhancement to this paradigm through two key innovations. First, we
develop a stochastic dual descent algorithm that reduces the per-iteration
complexity from cubic to linear in the number of collocation points, enabling
tractable inference. Second, we exploit a clustering-based active learning
strategy that adaptively selects collocation points to maximize information
gain while minimizing computational expense. Together, these contributions
result in an $h$-adaptive probabilistic solver that can scale to a large number
of collocation points. We demonstrate the efficacy of the proposed solver on
benchmark PDEs, including two- and three-dimensional steady-state elliptic
problems, as well as a time-dependent parabolic PDE formulated in a space-time
setting.

</details>


### [148] [Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA](https://arxiv.org/abs/2508.09721)
*Yuan-Hao Wei,Fu-Hao Deng,Lin-Yong Cui,Yan-Jie Sun*

Main category: stat.ML

TL;DR: 文章聚焦生成模型可解释性，提出SKR - VAE，在保持ICA性能下比GP - VAE计算更高效。


<details>
  <summary>Details</summary>
Motivation: 提升生成模型的可解释性，解决GP在处理大数据集时计算负担大的问题。

Method: 提出Structured Kernel Regression VAE (SKR - VAE)，利用核函数构建潜在变量先验，避免GP中昂贵的核矩阵求逆。

Result: SKR - VAE在保持ICA性能的同时，计算效率更高，计算负担显著降低。

Conclusion: SKR - VAE是一种更高效的提升生成模型可解释性的方法。

Abstract: The interpretability of generative models is considered a key factor in
demonstrating their effectiveness and controllability. The generated data are
believed to be determined by latent variables that are not directly observable.
Therefore, disentangling, decoupling, decomposing, causal inference, or
performing Independent Component Analysis (ICA) in the latent variable space
helps uncover the independent factors that influence the attributes or features
affecting the generated outputs, thereby enhancing the interpretability of
generative models. As a generative model, Variational Autoencoders (VAEs)
combine with variational Bayesian inference algorithms. Using VAEs, the inverse
process of ICA can be equivalently framed as a variational inference process.
In some studies, Gaussian processes (GPs) have been introduced as priors for
each dimension of latent variables in VAEs, structuring and separating each
dimension from temporal or spatial perspectives, and encouraging different
dimensions to control various attributes of the generated data. However, GPs
impose a significant computational burden, resulting in substantial resource
consumption when handling large datasets. Essentially, GPs model different
temporal or spatial structures through various kernel functions. Structuring
the priors of latent variables via kernel functions-so that different kernel
functions model the correlations among sequence points within different latent
dimensions-is at the core of achieving disentanglement in VAEs. The proposed
Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more
efficient way, avoiding the costly kernel matrix inversion required in GPs.
This research demonstrates that, while maintaining ICA performance, SKR-VAE
achieves greater computational efficiency and significantly reduced
computational burden compared to GP-VAE.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [149] [Differentiating Generalized Eigenvalues and Eigenvectors](https://arxiv.org/abs/2508.09355)
*Jan de Leeuw*

Main category: stat.CO

TL;DR: 本文给出对称矩阵广义特征值/特征向量和矩形矩阵广义奇异值/奇异向量的一、二阶导数公式，提供R函数计算导数，验证公式并讨论多元数据分析应用。


<details>
  <summary>Details</summary>
Motivation: 计算对称矩阵和矩形矩阵在参数向量作用下广义特征值/向量、广义奇异值/向量的导数。

Method: 推导导数公式，提供R函数计算，用数值微分计算的雅可比矩阵和海森矩阵验证公式。

Result: 得到导数公式，R函数可计算导数，公式通过验证。

Conclusion: 公式和R函数可用于多元数据分析。

Abstract: We give formulae for first and second derivatives of generalized
eigenvalues/eigenvectors of symmetric matrices and generalized singular
values/singular vectors of rectangular matrices when the matrices are linear or
nonlinear functions of a vector of parameters. In addition we provide functions
in R to compute these derivatives, both in the general case and in various
special cases. Formulae are checked against Jacobians and Hessians computed by
numerical differentiation. Some applications to multivariate data analysis are
discussed.

</details>


### [150] [sanba: An R Package for Bayesian Clustering of Distributions via Shared Atoms Nested Models](https://arxiv.org/abs/2508.09758)
*Francesco Denti,Laura D'Angelo*

Main category: stat.CO

TL;DR: 本文介绍了R包sanba，用于贝叶斯分析分组数据的嵌套混合模型，可克服计算复杂性问题，具有多种推理策略且速度快、用户友好。


<details>
  <summary>Details</summary>
Motivation: 嵌套数据结构分析需考虑层次组织，贝叶斯嵌套混合模型实用中受计算复杂性限制，需解决该问题。

Method: 提出R包sanba，使用带共享原子集的嵌套混合模型，提供多种推理策略，核心函数用C++实现并集成到R。

Result: 开发出sanba这个快速且用户友好的工具，可用于拟合嵌套混合模型。

Conclusion: sanba能有效解决贝叶斯嵌套混合模型计算复杂的问题，便于进行分组数据的贝叶斯分析。

Abstract: Nested data structures arise when observations are grouped into distinct
units, such as patients within hospitals or students within schools. Accounting
for this hierarchical organization is essential for valid inference, as
ignoring it can lead to biased estimates and poor generalization. This article
addresses the challenge of clustering both individual observations and their
corresponding groups while flexibly estimating group-specific densities.
Bayesian nested mixture models offer a principled and robust framework for this
task. However, their practical use has often been limited by computational
complexity. To overcome this barrier, we present sanba, an R package for
Bayesian analysis of grouped data using nested mixture models with a shared set
of atoms, a structure recently introduced in the statistical literature. The
package provides multiple inference strategies, including state-of-the-art
Markov Chain Monte Carlo routines and variational inference algorithms tailored
for large-scale datasets. All core functions are implemented in C++ and
seamlessly integrated into R, making sanba a fast and user-friendly tool for
fitting nested mixture models with modern Bayesian algorithms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [151] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: 本文通过混合方法调查研究，考察不同AI开发角色人员的伦理认知等情况，发现不同群体差异，强调协作和角色敏感方法，提出应对伦理挑战的建议和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 应对AI应用发展带来的伦理风险，需制定伦理准则和法规，故研究相关人员伦理认知。

Method: 开展混合方法调查研究，结合统计和定性分析，调查43个国家414名不同AI开发角色人员。

Result: 不同角色、地区和其他人口因素在AI伦理原则、政府举措和风险缓解策略方面的熟悉度和经验程度不同。

Conclusion: 强调协作、角色敏感方法，倡导开发定制、包容的解决方案应对AI开发伦理挑战，提出未来研究方向和教育策略。

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


### [152] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: 传统大语言模型训练拒绝边界存在问题，提出安全完成（safe - completions）方法并应用于GPT - 5，提升了安全性和模型有用性。


<details>
  <summary>Details</summary>
Motivation: 传统基于拒绝边界的安全训练在处理用户意图模糊的提示时存在脆性，尤其不适用于两用情况。

Method: 提出以助手输出安全为中心的安全完成（safe - completions）安全训练方法。

Result: 将该方法应用于GPT - 5，在生产对比和内部对照实验中，提升了安全性，降低了安全故障严重程度，大幅提高了模型有用性。

Conclusion: 安全完成（safe - completions）方法优于传统拒绝边界训练方法，能有效提升模型性能。

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


### [153] [Beyond Technocratic XAI: The Who, What & How in Explanation Design](https://arxiv.org/abs/2508.09231)
*Ruchira Dhar,Stephanie Brandl,Ninell Oldenburg,Anders Søgaard*

Main category: cs.CY

TL;DR: 本文将可解释AI中的解释重构为情境化设计过程，提出三部分框架并强调伦理考量。


<details>
  <summary>Details</summary>
Motivation: 可解释AI生成有意义解释是依赖情境的任务，需有意设计确保可及性和透明度。

Method: 借鉴设计思维的先前研究和原则，提出三部分框架。

Result: 提出包含询问谁需要解释、解释什么、如何传递解释的框架。

Conclusion: 将解释视为社会技术设计过程，鼓励对可解释AI采用情境感知方法，支持有效沟通和负责任解释的发展。

Abstract: The field of Explainable AI (XAI) offers a wide range of techniques for
making complex models interpretable. Yet, in practice, generating meaningful
explanations is a context-dependent task that requires intentional design
choices to ensure accessibility and transparency. This paper reframes
explanation as a situated design process -- an approach particularly relevant
for practitioners involved in building and deploying explainable systems.
Drawing on prior research and principles from design thinking, we propose a
three-part framework for explanation design in XAI: asking Who needs the
explanation, What they need explained, and How that explanation should be
delivered. We also emphasize the need for ethical considerations, including
risks of epistemic inequality, reinforcing social inequities, and obscuring
accountability and governance. By treating explanation as a sociotechnical
design process, this framework encourages a context-aware approach to XAI that
supports effective communication and the development of ethically responsible
explanations.

</details>


### [154] [Ethical Medical Image Synthesis](https://arxiv.org/abs/2508.09293)
*Weina Jin,Ashish Sinha,Kumar Abhishek,Ghassan Hamarneh*

Main category: cs.CY

TL;DR: 本文聚焦伦理医学图像合成，分析其关键属性与内在局限，提出实践支持建议并给出案例研究。


<details>
  <summary>Details</summary>
Motivation: 满足医学图像合成研究与开发伦理实践不断增长的需求，防止其负面影响。

Method: 先进行理论分析，确定伦理医学图像合成的关键属性与内在局限；再基于理论分析提出实践支持建议；最后给出两个案例研究。

Result: 识别出合成图像的局限，指出不承认这些局限会带来伦理风险和危害，提出了伦理实践和监督建议，并通过案例研究发现现有实践与建议的差距。

Conclusion: 提出的实践支持建议有助于医学图像分析社区内外共同推动伦理医学图像合成。

Abstract: The task of ethical Medical Image Synthesis (MISyn) is to ensure that the
MISyn techniques are researched and developed ethically throughout their entire
lifecycle, which is essential to prevent the negative impacts of MISyn. To
address the ever-increasing needs and requirements for ethical practice of
MISyn research and development, we first conduct a theoretical analysis that
identifies the key properties of ethical MISyn and intrinsic limits of MISyn.
We identify that synthetic images lack inherent grounding in real medical
phenomena, cannot fully represent the training medical images, and inevitably
introduce new distribution shifts and biases.
  Ethical risks can arise from not acknowledging the intrinsic limits and
weaknesses of synthetic images compared to medical images, with the extreme
form manifested as misinformation of MISyn that substitutes synthetic images
for medical images without acknowledgment. The resulting ethical harms include
eroding trust in the medical imaging dataset environment and causing
algorithmic discrimination towards stakeholders and the public.
  To facilitate collective efforts towards ethical MISyn within and outside the
medical image analysis community, we then propose practical supports for
ethical practice in MISyn based on the theoretical analysis, including ethical
practice recommendations that adapt the existing technical standards, problem
formulation, design, and evaluation practice of MISyn to the ethical
challenges; and oversight recommendations to facilitate checks and balances
from stakeholders and the public. We also present two case studies that
demonstrate how to apply the ethical practice recommendations in practice, and
identify gaps between existing practice and the ethical practice
recommendations.

</details>


### [155] [STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports](https://arxiv.org/abs/2508.09853)
*Tegan McCaslin,Jide Alaga,Samira Nedungadi,Seth Donoughe,Tom Reed,Rishi Bommasani,Chris Painter,Luca Righetti*

Main category: cs.CY

TL;DR: 提出STREAM标准以改进AI模型报告中评估结果的披露，聚焦化学和生物基准。


<details>
  <summary>Details</summary>
Motivation: 评估危险AI能力对管理灾难性风险很重要，公开评估过程和结果对建立AI发展信任至关重要。

Method: 与23位来自政府、民间社会、学术界和前沿AI公司的专家协商制定STREAM标准。

Result: 用“黄金标准”示例展示最佳实践，提供三页报告模板。

Conclusion: STREAM标准能帮助AI开发者更清晰呈现评估结果，助第三方评估化学和生物评估的严谨性。

Abstract: Evaluations of dangerous AI capabilities are important for managing
catastrophic risks. Public transparency into these evaluations - including what
they test, how they are conducted, and how their results inform decisions - is
crucial for building trust in AI development. We propose STREAM (A Standard for
Transparently Reporting Evaluations in AI Model Reports), a standard to improve
how model reports disclose evaluation results, initially focusing on chemical
and biological (ChemBio) benchmarks. Developed in consultation with 23 experts
across government, civil society, academia, and frontier AI companies, this
standard is designed to (1) be a practical resource to help AI developers
present evaluation results more clearly, and (2) help third parties identify
whether model reports provide sufficient detail to assess the rigor of the
ChemBio evaluations. We concretely demonstrate our proposed best practices with
"gold standard" examples, and also provide a three-page reporting template to
enable AI developers to implement our recommendations more easily.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [156] [TPTP World Infrastructure for Non-classical Logics](https://arxiv.org/abs/2508.09318)
*Alexander Steen,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: 本文对TPTP World在非经典逻辑自动定理证明（ATP）的基础设施进行全面概述，并给出量化正规多模态逻辑的使用说明。


<details>
  <summary>Details</summary>
Motivation: TPTP World自v9.0.0版本支持非经典逻辑，需对其在非经典逻辑ATP的基础设施进行介绍。

Method: 对TPTP World的非经典语言扩展、问题与解决方案和工具支持进行全面阐述，并详细说明量化正规多模态逻辑的使用。

Result: 提供了TPTP World在非经典逻辑ATP基础设施的全面概述。

Conclusion: 完成了对TPTP World在非经典逻辑ATP基础设施的介绍。

Abstract: The TPTP World is the well established infrastructure that supports research,
development, and deployment of Automated Theorem Proving (ATP) systems. The
TPTP World supports a range of classical logics, and since release v9.0.0 has
supported non-classical logics. This paper provides a self-contained
comprehensive overview of the TPTP World infrastructure for ATP in
non-classical logics: the non-classical language extension, problems and
solutions, and tool support. A detailed description of use of the
infrastructure for quantified normal multi-modal logic is given.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [157] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: 本文针对波动流量下网络性能评估挑战，引入指标量化网络负载与流影响，评估11种指标，证明部分指标有效且可作为未来研究框架。


<details>
  <summary>Details</summary>
Motivation: 解决波动流量模式下评估网络性能的挑战，关注峰值数据速率对网络资源的影响。

Method: 引入量化网络负载和衡量单一流对网络整体状态影响的指标；通过百分位值和样本分布分析链路和流数据并引入Utilization Score指标；采用改进的基于Shapley值的方法衡量单一流对网络的影响；在不同网络场景下评估比较11种指标。

Result: 这些指标能有效捕捉特定流引起的网络状态变化，其中三种指标能提供广泛有价值的见解且易于维护。

Conclusion: 本文方法可作为未来研究框架，有潜力扩展和完善用于评估流对网络性能影响的指标集。

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [158] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: 介绍了跨层框架WAAN用于实现意图感知和主动切换，结合半稳定会合点确保移动连续性，通过案例展示有效性并讨论挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 6G网络向AI驱动、以用户为中心的生态系统发展，传统反应式切换机制有局限。

Method: 引入跨层框架WAAN，嵌入轻量级TinyML代理，设置半稳定会合点。

Result: 通过多模态环境控制案例研究展示了框架在移动场景下维持用户体验的有效性。

Conclusion: 文章讨论了WAAN部署和发展的关键挑战和未来机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [159] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: 提出SP - LLM框架解决下一代汽车应用中VEC管理系统问题，模拟显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VEC管理系统固定且被动，在动态车辆环境中次优，需新框架解决问题。

Method: 将传统DT转变为pDT预测网络参数，用LLM作为认知协调器，利用pDT预测进行任务卸载和资源分配决策，且能根据语义指令调整策略。

Result: 大量模拟显示SP - LLM在可扩展性、波动性条件下的鲁棒性和适应性方面显著优于现有反应式和基于MARL的方法。

Conclusion: 该框架能将人类意图转化为最优网络行为，使车辆网络更智能、自主和目标驱动。

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [160] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 本文聚焦预训练MoE大语言模型推理阶段高效拓扑感知放置问题，提出ILP算法优化专家放置，减少传输次数，经实验验证优于竞品。


<details>
  <summary>Details</summary>
Motivation: 高效部署预训练大语言模型至多服务器集群以快速响应用户查询很关键，MoE大语言模型因结构特殊和专家负载不均衡，需考虑网络拓扑进行模型放置以提升集群利用率。

Method: 提出整数线性规划（ILP）算法确定专家最优放置，最小化预期传输次数，利用标准ILP求解器解决优化问题。

Result: ILP算法在小规模（DeepSeekMoE~16B）和大规模（DeepSeek - R1~671B）模型中都比竞品产生更低的网络流量。

Conclusion: 所提出的基于ILP的放置策略在预训练MoE大语言模型推理阶段能有效优化模型放置，降低网络流量。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [161] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 本文提出用于5G分组核心的AI/ML驱动故障分析引擎，可分类PCAP文件中的帧，降低工作量并提高效率，测试显示准确率高，未来将扩展应用。


<details>
  <summary>Details</summary>
Motivation: 5G网络时代需确保分组核心流量完整性和性能，当前方法处理测试结果和查找故障耗时多。

Method: 使用自然语言处理技术分析网络流量，利用生成式AI通过大语言模型提供修复建议，结合3GPP标准等文档解释错误。

Result: ML模型在测试数据集上对成功和失败的PCAP文件进行80 - 20分割训练时，显示出高分类准确率。

Conclusion: 该引擎有效降低故障分析工作量、提高效率，未来可扩展至4G网络流量和其他网络数据。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [162] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: 介绍Agoran SRB市场，含三个AI分支，在5G测试床评估获显著收益，为6G网络提供路径。


<details>
  <summary>Details</summary>
Motivation: 解决下一代移动网络中多个服务所有者目标冲突，以及现有网络切片控制器的刚性、策略绑定和缺乏业务上下文感知问题。

Method: 构建Agoran SRB，含立法、行政、司法三个自主AI分支；利益相关方谈判代理和调解代理通过多目标优化器协商；使用微调的Llama模型。

Result: 在5G测试床评估，eMBB切片吞吐量增加37%，URLLC切片延迟降低73%，PRB使用节省8.3%；微调的Llama模型恢复约80% GPT - 4.1决策质量。

Conclusion: Agoran为超灵活、以利益相关者为中心的6G网络提供了符合标准的具体路径。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [163] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: 论文介绍webMCP标准，可提升AI与网页交互效率，评估显示其能大幅降低处理需求、成本，提高响应速度，且无需服务器端修改。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理理解网页需大量处理，导致AI辅助网页交互缓慢且成本高，需解决此问题。

Method: 引入webMCP标准，将结构化交互元数据嵌入网页，为AI代理提供页面元素与用户操作的显式映射。

Result: 综合评估显示webMCP降低处理需求67.6%，保持97.9%任务成功率，用户成本降低34 - 63%，响应速度加快，独立研究验证其实用性。

Conclusion: webMCP是使AI网页辅助更易访问和可持续的可行解决方案，可弥合生产环境中用户交互需求与AI计算需求间的差距。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [164] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: 提出Hierarchical SpatioTemporal Mamba (HiSTM)进行蜂窝流量预测，评估显示优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对网络规划等至关重要，但因用户移动的时空模式复杂，现有AI模型难兼顾准确性和计算效率。

Method: 结合双空间编码器、基于Mamba的时间模块和注意力机制，采用选择性状态空间方法捕捉网络流量的时空模式。

Result: 使用真实数据集评估，比STN基线的MAE改善29.4%，参数减少94%，在不同数据集上泛化性好，长时预测准确性提高。

Conclusion: HiSTM是一种有效的蜂窝流量预测模型，能更好地平衡准确性和计算效率。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [165] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: 介绍了首个端到端代理系统MX - AI，可用于6G无线接入网，在实际查询中表现良好，公开相关资源并展示演示。


<details>
  <summary>Details</summary>
Motivation: 未来6G无线接入网将是人工智能原生的，需要开发相关系统。

Method: 基于OpenAirInterface (OAI)和FlexRIC搭建5G Open RAN测试平台，在服务管理和编排层部署大语言模型驱动的代理图，通过自然语言意图提供6G RAN资源的可观测性和控制功能。

Result: 在50个现实操作查询中，平均答案质量达4.1/5.0，决策 - 行动准确率100%，GPT - 4.1支持时端到端延迟仅8.8秒。

Conclusion: MX - AI与人类专家表现相当，验证了其在实际场景中的实用性，公开资源以加速AI原生RAN的开放研究。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [166] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: 提出动态资源感知协作优化框架CoMoE解决MoE模型在移动边缘计算环境部署挑战，实验显示其降低内存使用、推理延迟并稳定性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动MoE架构发展，但在资源受限的移动边缘计算环境部署MoE模型面临大内存占用和动态专家激活模式挑战。

Method: 提出CoMoE框架，分析现有专家聚合技术局限性，研究专家卸载策略，引入自适应调度机制。

Result: 在真实移动边缘测试平台实验表明，CoMoE比基线方法内存使用降低约70%，推理延迟比现有专家卸载技术低10.5%，能将大模型内存需求从15.6GB降至4.7GB。

Conclusion: CoMoE能有效解决MoE模型在资源受限的移动边缘计算环境部署问题，可部署大模型。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [167] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: 引入NEFMind框架，利用开源大模型参数高效微调解决现代电信服务发现和管理复杂问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代电信中基于服务的架构增加了网络功能和API，导致服务发现和管理操作复杂。

Method: 引入NEFMind框架，包含从NEF API规范生成合成数据集、通过量化低秩自适应进行模型优化、使用GPT - 4 Ref Score和BertScore指标评估性能。

Result: 相比手动发现方法，通信开销降低85%，API调用识别准确率达98 - 100%，微调后的Phi - 2模型性能与GPT - 4相当且计算高效。

Conclusion: 特定领域、参数高效的大模型策略可用于管理下一代电信网络复杂API生态系统。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [168] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: 本文介绍了为大型全球漫游平台的物联网连接服务设计和部署的无监督异常检测解决方案ANCHOR，可过滤数据识别潜在问题客户端，还介绍了方案设计步骤和评估情况。


<details>
  <summary>Details</summary>
Motivation: 在复杂的物联网生态系统中，保证通信可用性和可靠性面临挑战，且多数平台运营商采用被动方式处理通信问题，影响服务质量。

Method: 先描述物联网服务、基础设施和网络可见性，接着阐述设计无监督异常检测解决方案的挑战和要求，然后提出基于被动信令流量的统计规则、机器学习和深度学习模型用于异常检测，最后与运营团队合作在运营平台上设计和评估解决方案。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [169] [Swap Bounded Envy](https://arxiv.org/abs/2508.09290)
*Federico Echenique,Sumit Goel,SangMok Lee*

Main category: econ.TH

TL;DR: 研究离散物品分配中的公平性，提出在一定偏好限制下实现“交换有界嫉妒”分配的算法。


<details>
  <summary>Details</summary>
Motivation: 精确公平（无嫉妒）的离散物品分配无法实现，因此探讨近似公平的概念。

Method: 提出一种算法，在对代理偏好有一定限制的条件下进行分配。

Result: 在给定偏好限制下，能实现“交换有界嫉妒”的分配。

Conclusion: 提出的算法可在一定条件下解决离散物品近似公平分配问题。

Abstract: We study fairness in the allocation of discrete goods. Exactly fair
(envy-free) allocations are impossible, so we discuss notions of approximate
fairness. In particular, we focus on allocations in which the swap of two items
serves to eliminate any envy, either for the allocated bundles or with respect
to a reference bundle. We propose an algorithm that, under some restrictions on
agents' preferences, achieves an allocation with ``swap bounded envy.''

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [170] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: 现有框架未能整合社交媒体数据监管领域，本文引入PETLP合规框架，通过Reddit分析揭示相关差异和问题，助研究者应对监管复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有框架未整合社交媒体数据在GDPR、版权法和平台条款下的监管领域，研究者缺乏统一指导。

Method: 引入PETLP合规框架，将法律保障嵌入扩展ETL管道，把数据保护影响评估作为动态文件；进行系统性Reddit分析。

Result: 揭示了不同主体提取权差异，指出社交媒体数据无法真正匿名化，以及数据集创建和模型分发间的法律差距。

Conclusion: PETLP框架将合规决策结构化，简化机构数据管理计划，能让研究者自信应对监管复杂性，弥合法律要求与研究实践的差距。

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


### [171] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: 本文介绍AI Blob!系统，利用语义编目和大语言模型探索档案电视镜头检索与重新语境化，展示语义技术在档案利用新途径的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索语义编目和大语言模型在档案电视镜头检索与重新语境化的潜力。

Method: 受意大利电视节目启发，集成自动语音识别、语义嵌入和检索增强生成技术，处理1547个意大利电视视频数据集，通过转录、分段、嵌入向量数据库，根据用户提示生成相关查询以检索和重组视听片段。

Result: 系统能生成模拟编辑手法的蒙太奇，实现动态、内容感知的检索。

Conclusion: 语义技术可促进档案利用新途径，实现自动化叙事构建和文化分析，为媒体史学和人工智能档案研究提供框架和数据集。

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [172] [Distributed Diamond Formation of Sliding Squares](https://arxiv.org/abs/2508.09638)
*Irina Kostitsyna,David Liedtke,Christian Scheideler*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The sliding square model is a widely used abstraction for studying
self-reconfigurable robotic systems, where modules are square-shaped robots
that move by sliding or rotating over one another. In this paper, we propose a
novel distributed algorithm that allows a group of modules to reconfigure into
a diamond shape, starting from an arbitrary side-connected configuration. It is
connectivity-preserving and operates under minimal assumptions: one leader
module, common chirality, constant memory per module, and visibility and
communication restricted to immediate neighbors. Unlike prior work, which
relaxes the original sliding square move-set, our approach uses the unmodified
move-set, addressing the additional challenge of handling locked
configurations. Our algorithm is sequential in nature and operates with a
worst-case time complexity of $\mathcal{O}(n^2)$ rounds, which is optimal for
sequential algorithms. To improve runtime, we introduce two parallel variants
of the algorithm. Both rely on a spanning tree data structure, allowing modules
to make decisions based on local connectivity. Our experimental results show a
significant speedup for the first variant, and linear average runtime for the
second variant, which is worst-case optimal for parallel algorithms.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [173] [DeepWKB: Learning WKB Expansions of Invariant Distributions for Stochastic Systems](https://arxiv.org/abs/2508.09529)
*Yao Li,Yicheng Liu,Shirou Wang*

Main category: math.DS

TL;DR: 提出名为DeepWKB的深度学习方法估计随机扰动系统的不变分布，适用于小噪声强度和高维随机系统，为计算准势提供新选择。


<details>
  <summary>Details</summary>
Motivation: 解决大多数现有方法在小噪声强度下估计随机扰动系统不变分布的难题，以及适用于高维随机系统。

Method: 利用蒙特卡罗数据和V与Z_ε满足的偏微分方程，分别计算V和Z_ε来近似不变分布。

Result: 能在小噪声强度的奇异区域近似不变分布，适用于高维随机系统。

Conclusion: DeepWKB方法是计算准势的可扩展且灵活的替代方案，对复杂系统分析有重要作用。

Abstract: This paper introduces a novel deep learning method, called DeepWKB, for
estimating the invariant distribution of randomly perturbed systems via its
Wentzel-Kramers-Brillouin (WKB) approximation $u_\epsilon(x) = Q(\epsilon)^{-1}
Z_\epsilon(x) \exp\{-V(x)/\epsilon\}$, where $V$ is known as the
quasi-potential, $\epsilon$ denotes the noise strength, and $Q(\epsilon)$ is
the normalization factor. By utilizing both Monte Carlo data and the partial
differential equations satisfied by $V$ and $Z_\epsilon$, the DeepWKB method
computes $V$ and $Z_\epsilon$ separately. This enables an approximation of the
invariant distribution in the singular regime where $\epsilon$ is sufficiently
small, which remains a significant challenge for most existing methods.
Moreover, the DeepWKB method is applicable to higher-dimensional stochastic
systems whose deterministic counterparts admit non-trivial attractors. In
particular, it provides a scalable and flexible alternative for computing the
quasi-potential, which plays a key role in the analysis of rare events,
metastability, and the stochastic stability of complex systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [174] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 本文综述生成式AI在医学影像领域的进展、作用、评估框架、部署障碍及与基础模型融合前景，以指导未来研究和跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正快速改变医学影像，需全面总结其进展并评估作用，推动其临床应用。

Method: 系统研究生成式AI在影像工作流程各阶段的贡献，提出三层评估框架。

Result: 明确生成式AI作用，指出部署障碍，探索与基础模型融合可能。

Conclusion: 通过梳理技术进展和转化路径，可指导未来研究，促进跨学科合作。

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [175] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: 提出混合模型解决结肠息肉分割难题，性能优于现有方法，分割精度和抗伪影能力有提升。


<details>
  <summary>Details</summary>
Motivation: 结肠息肉大小、形状等特征多变，边界不清晰，现有方法精确分割有挑战，需更优解决方案。

Method: 引入混合（Transformer + CNN）模型，利用边界感知注意力机制。

Result: 定量评估显示，与现有方法相比，召回率提高1.76%至0.9555，准确率提高0.07%至0.9849，抗伪影能力增强。

Conclusion: 混合模型在解决息肉分割关键挑战上表现优越，提升了分割精度和抗伪影能力。

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [176] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: 提出 impuTMAE 模型用于胶质瘤生存预测，处理缺失模态数据，性能达最优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 医疗数据复杂、不完整且有缺失模态，有效处理对训练多模态模型至关重要。

Method: 引入基于 Transformer 的端到端方法 impuTMAE，采用高效多模态预训练策略，通过重建掩码块学习模态间和模态内交互并填补缺失模态。

Result: 模型在 TCGA - GBM/LGG 和 BraTS 数据集上经预训练和微调，在胶质瘤患者生存预测中超越先前多模态方法，达到最优性能。

Conclusion: impuTMAE 能处理缺失数据，高效利用资源，在胶质瘤生存预测中表现出色。

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [177] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 本文提出一种新的联邦学习方法，利用模型不确定性进行聚合、预测不确定性进行推理，实现跨腹部CT数据集的通用分割，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 不同CT分割数据集来源和标注不同，有效利用这些异构数据并保护患者隐私存在挑战。

Method: 利用随机小批量梯度下降中的固有噪声估计模型权重分布，在客户端提供模型参数的不确定性；在服务器使用贝叶斯逆方差聚合方案结合不确定性信息进行参数聚合；通过传播模型权重的不确定性量化预测不确定性，并在推理阶段利用预测不确定性提高性能。

Result: 与现有基线相比，该方法在提高联邦聚合质量和不确定性加权推理方面有效。

Conclusion: 所提出的联邦学习方法能有效实现跨腹部CT数据集的通用分割。

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [178] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: 研究零样本自监督学习重建减少磁共振胆胰管造影（MRCP）屏气时间的可行性，结果表明该方法可提高图像质量并减少屏气时间，浅训练是实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究应用零样本自监督学习重建减少MRCP屏气时间的可行性。

Method: 从11名健康志愿者获取屏气MRCP，评估零样本重建与并行成像和压缩传感重建效果，用预训练网络减少反向传播深度解决训练时间长问题。

Result: 零样本学习重建图像质量显著优于压缩传感重建，浅训练11分钟性能与传统零样本训练271分钟相当。

Conclusion: 零样本学习可实现高保真MRCP重建并减少屏气时间，浅训练适用于临床工作流程。

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [179] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: 提出用于计算病理学中解释分类器的人机-VLM交互系统，可测试和区分解释，推动从可解释AI到已解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型解释对医学图像分析系统临床应用至关重要，现有技术如GradCAM不能形成解释。

Method: 提出人机-VLM交互系统，包括AI集成幻灯片查看器进行滑动窗口实验，用通用视觉语言模型量化解释的预测性。

Result: 能定性测试解释的主张，可量化区分不同解释。

Conclusion: 为数字病理学及其他领域从可解释AI到已解释AI提供了实际途径。

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [180] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: 本文提出首个端到端自动乳腺摄影报告生成框架AMRG，在DMID数据集上训练评估，性能良好，为放射报告生成提供基础。


<details>
  <summary>Details</summary>
Motivation: 乳腺摄影报告生成任务具挑战性且研究不足，需建立可复现基准。

Method: 基于MedGemma - 4B - it - a，采用LoRA进行参数高效微调，在DMID数据集训练评估，探索LoRA超参数并对比多VLM骨干。

Result: 框架在语言生成和临床指标表现好，ROUGE - L 0.5691、METEOR 0.6152、CIDEr 0.5818、BI - RADS准确率0.5582，诊断一致性提升、幻觉减少。

Conclusion: AMRG为放射报告生成提供可扩展、适应性强基础，为多模态医学AI研究铺路。

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


### [181] [A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.09271)
*Reihaneh Hassanzadeh,Anees Abrol,Hamid Reza Hassanzadeh,Vince D. Calhoun*

Main category: eess.IV

TL;DR: 本文提出用生成对抗网络方法重构缺失模态数据，在阿尔茨海默病分类上比传统方法准确率提升9%。


<details>
  <summary>Details</summary>
Motivation: 神经影像领域多模态数据存在不完整问题，传统方法有局限性，需要有效补全数据的策略。

Method: 提出一种生成对抗网络方法，利用T1加权结构磁共振成像和功能网络连接两种模态，从现有模态重构缺失模态并保留疾病模式。

Result: 使用生成插补方法在阿尔茨海默病与认知正常组的分类准确率比传统方法提高了9%。

Conclusion: 提出的生成对抗网络方法在处理多模态不完整数据时比传统方法更有效，能提高分类准确率。

Abstract: Multimodal data analysis can lead to more accurate diagnoses of brain
disorders due to the complementary information that each modality adds.
However, a major challenge of using multimodal datasets in the neuroimaging
field is incomplete data, where some of the modalities are missing for certain
subjects. Hence, effective strategies are needed for completing the data.
Traditional methods, such as subsampling or zero-filling, may reduce the
accuracy of predictions or introduce unintended biases. In contrast, advanced
methods such as generative models have emerged as promising solutions without
these limitations. In this study, we proposed a generative adversarial network
method designed to reconstruct missing modalities from existing ones while
preserving the disease patterns. We used T1-weighted structural magnetic
resonance imaging and functional network connectivity as two modalities. Our
findings showed a 9% improvement in the classification accuracy for Alzheimer's
disease versus cognitive normal groups when using our generative imputation
method compared to the traditional approaches.

</details>


### [182] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: 提出T - CACE框架从非对比MRI合成多期对比增强MRI，实验显示其在多方面表现优于现有方法，提高肝脏病变评估的安全性、效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI存在造影剂使用风险、手动评估耗时和标注数据集有限等问题，需新方法解决。

Method: 提出T - CACE框架，包含条件令牌编码机制、动态时间感知注意力掩码和时间分类一致性约束。

Result: 在两个独立肝脏MRI数据集上实验表明，T - CACE在图像合成、分割和病变分类方面优于现有方法。

Conclusion: T - CACE框架是传统对比增强成像的有效替代方案，能改善肝脏病变评估。

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of
liver cancer, significantly improving the classification of the lesion and
patient outcomes. However, traditional MRI faces challenges including risks
from contrast agent (CA) administration, time-consuming manual assessment, and
limited annotated datasets. To address these limitations, we propose a
Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for
synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from
non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a
conditional token encoding (CTE) mechanism that unifies anatomical priors and
temporal phase information into latent representations; and a dynamic
time-aware attention mask (DTAM) that adaptively modulates inter-phase
information flow using a Gaussian-decayed attention mechanism, ensuring smooth
and physiologically plausible transitions across phases. Furthermore, a
constraint for temporal classification consistency (TCC) aligns the lesion
classification output with the evolution of the physiological signal, further
enhancing diagnostic reliability. Extensive experiments on two independent
liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods
in image synthesis, segmentation, and lesion classification. This framework
offers a clinically relevant and efficient alternative to traditional
contrast-enhanced imaging, improving safety, diagnostic efficiency, and
reliability for the assessment of liver lesion. The implementation of T-CACE is
publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [183] [Marketron Through the Looking Glass: From Equity Dynamics to Option Pricing in Incomplete Markets](https://arxiv.org/abs/2508.09863)
*Igor Halperin,Andrey Itkin*

Main category: q-fin.PR

TL;DR: 本文将Marketron模型扩展到期权市场，开发基于效用的定价方法，解决HJB方程并进行校准，还探讨模型能否重现标的资产对数收益率统计特性。


<details>
  <summary>Details</summary>
Motivation: 原Marketron模型校准于标普500时间序列数据，本文要将其扩展到期权市场，应对市场不完备性带来的挑战，同时解决量化金融中构建统一框架的长期难题。

Method: 开发基于效用的定价方法，通过最优投资问题的对偶解构建风险调整测度，用新方法求解HJB方程以实现高效校准。

Result: 成功将模型扩展到期权市场，在标准笔记本硬件上完成校准。

Conclusion: 探讨了模型校准到市场期权价格后能否同时重现标的资产对数收益率统计特性，对构建统一框架有一定推进。

Abstract: The Marketron model, introduced by [Halperin, Itkin, 2025], describes price
formation in inelastic markets as the nonlinear diffusion of a quasiparticle
(the marketron) in a multidimensional space comprising the log-price $x$, a
memory variable $y$ encoding past money flows, and unobservable return
predictors $z$. While the original work calibrated the model to S\&P 500 time
series data, this paper extends the framework to option markets - a
fundamentally distinct challenge due to market incompleteness stemming from
non-tradable state variables. We develop a utility-based pricing approach that
constructs a risk-adjusted measure via the dual solution of an optimal
investment problem. The resulting Hamilton-Jacobi-Bellman (HJB) equation,
though computationally formidable, is solved using a novel methodology enabling
efficient calibration even on standard laptop hardware. Having done that, we
look at the additional question to answer: whether the Marketron model,
calibrated to market option prices, can simultaneously reproduce the
statistical properties of the underlying asset's log-returns. We discuss our
results in view of the long-standing challenge in quantitative finance of
developing an unified framework capable of jointly capturing equity returns,
option smile dynamics, and potentially volatility index behavior.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [184] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 提出用户意图驱动的语义通信系统，结合多模态大模型、掩码引导注意力模块和信道状态感知模块，实验表明该系统能实现深度意图理解且性能优于DeepJSCC。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统无法深入理解和泛化用户真实意图。

Method: 集成多模态大模型生成用户意图先验，提出掩码引导注意力模块突出关键语义区域，设计信道状态感知模块确保在不同信道条件下自适应、鲁棒传输。

Result: 系统实现深度意图理解，在瑞利信道SNR为5 dB时，PSNR、SSIM和LPIPS分别提升8%、6%和19%，优于DeepJSCC。

Conclusion: 所提用户意图驱动的语义通信系统有效可行，能更好地理解用户意图并提升性能。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [185] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: 提出OWASP多智能体系统威胁建模指南扩展，解决大语言模型驱动多智能体架构安全挑战。


<details>
  <summary>Details</summary>
Motivation: OWASP现有分类法在建模失败方面存在差距，需解决大语言模型驱动多智能体架构独特挑战。

Method: 引入基于实际部署的额外威胁类别和场景，概述评估策略。

Result: 识别出OWASP现有分类法的建模失败差距，补充了威胁类别和场景及评估策略。

Conclusion: 该工作扩展了OWASP框架适用性，有助于提升现实部署中多智能体系统安全和恢复能力。

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>


### [186] [Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.09230)
*Yutong Wu,Jie Zhang,Yiming Li,Chao Zhang,Qing Guo,Nils Lukas,Tianwei Zhang*

Main category: cs.MA

TL;DR: 提出新防御方法Cowpox增强多智能体系统鲁棒性，经验证明有效并有理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统设计缺乏鲁棒性考虑，单个智能体被攻击会影响整个系统。

Method: 提出Cowpox方法，采用分布式机制，生成并分发特殊治愈样本免疫和恢复智能体。

Result: 通过实验证明Cowpox有效。

Conclusion: Cowpox能可证明地增强多智能体系统的鲁棒性。

Abstract: Vision Language Model (VLM)-based agents are stateful, autonomous entities
capable of perceiving and interacting with their environments through vision
and language. Multi-agent systems comprise specialized agents who collaborate
to solve a (complex) task. A core security property is robustness, stating that
the system should maintain its integrity under adversarial attacks. However,
the design of existing multi-agent systems lacks the robustness consideration,
as a successful exploit against one agent can spread and infect other agents to
undermine the entire system's assurance. To address this, we propose a new
defense approach, Cowpox, to provably enhance the robustness of multi-agent
systems. It incorporates a distributed mechanism, which improves the recovery
rate of agents by limiting the expected number of infections to other agents.
The core idea is to generate and distribute a special cure sample that
immunizes an agent against the attack before exposure and helps recover the
already infected agents. We demonstrate the effectiveness of Cowpox empirically
and provide theoretical robustness guarantees.

</details>


### [187] [Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective](https://arxiv.org/abs/2508.09541)
*Gang Chen,Guoxin Wang,Anton van Beek,Zhenjun Ming,Yan Yan*

Main category: cs.MA

TL;DR: 本文用多智能体强化学习研究多智能体自组织系统任务执行中依赖层级的出现，发现层级动态出现且受任务环境等影响。


<details>
  <summary>Details</summary>
Motivation: 多智能体自组织系统自组织特性使涌现行为有不可预测性，聚焦任务执行中依赖层级的出现、演变及影响因素。

Method: 采用多智能体强化学习训练系统执行协作推箱子任务，计算智能体动作梯度量化依赖关系并分析层级涌现。

Result: 层级随智能体协作动态出现，随任务要求演变，自然涌现而非预配置，受任务环境和网络初始化条件影响，由智能体“天赋”“努力”和“环境”相互作用产生。

Conclusion: 多智能体自组织系统中依赖层级动态涌现且受多种因素影响，“天赋”“努力”和“环境”共同作用促成层级形成。

Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [188] [A Limits Study of Memory-side Tiering Telemetry](https://arxiv.org/abs/2508.09351)
*Vinicius Petrucci,Felippe Zacarias,David Roberts*

Main category: cs.OS

TL;DR: 本文介绍CXL实验内存请求记录器，用于揭示内存访问模式，结合多种方法的HMU可改善内存分层，在DLRM上有性能提升，强调可编程设备级遥测是未来内存系统解决方案。


<details>
  <summary>Details</summary>
Motivation: 不断增加的工作负载需求和新兴技术要求在计算系统中使用多种内存和存储层级，现有分层策略存在覆盖和准确性方面的局限。

Method: 使用CXL实验内存请求记录器揭示运行时内存访问模式，用于软件模拟未来内存遥测硬件；结合基于数据地址监控的反应式放置、主动数据移动和编译器提示，利用内存模块中的HMU改善内存分层。

Result: 在DLRM上，基于分析的页面放置比Linux NUMA平衡分层有1.94倍的潜在加速，与主机DRAM分配相比仅慢3%，同时将超过90%的页面卸载到CXL内存。

Conclusion: 现有分层策略有局限性，可编程的设备级遥测是未来内存系统可扩展且高效的解决方案。

Abstract: Increasing workload demands and emerging technologies necessitate the use of
various memory and storage tiers in computing systems. This paper presents
results from a CXL-based Experimental Memory Request Logger that reveals
precise memory access patterns at runtime without interfering with the running
workloads. We use it for software emulation of future memory telemetry
hardware. By combining reactive placement based on data address monitoring,
proactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)
within memory modules can greatly improve memory tiering solutions. Analysis of
page placement using profiled access counts on a Deep Learning Recommendation
Model (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing
tiering, and only a 3% slowdown compared to Host-DRAM allocation while
offloading over 90% of pages to CXL memory. The study underscores the
limitations of existing tiering strategies in terms of coverage and accuracy,
and makes a strong case for programmable, device-level telemetry as a scalable
and efficient solution for future memory systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [189] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)
*Shiyang Lai,Junsol Kim,Nadav Kunievsky,Yujin Potter,James Evans*

Main category: cs.HC

TL;DR: 研究测试文化偏见AI对人类决策的影响，发现党派AI助手有积极效果但有信任惩罚，展示不同偏见AI可缩小认知与表现差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统追求意识形态中立可能导致自动化偏见，研究文化偏见AI是否能提升人类决策。

Method: 对2500名参与者进行随机试验，让他们与政治观点多样的GPT - 4o变体在信息评估任务中互动。

Result: 党派AI助手提升人类表现、增加参与度、减少评估偏见，尤其在面对相反观点时；但存在信任惩罚，展示两种不同偏见AI可缩小认知与表现差距。

Conclusion: 传统AI中立观点需重新审视，战略性融入多元文化偏见能促进更好且有韧性的人类决策。

Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet
this may introduce automation bias by suppressing cognitive engagement in human
decision-making. We conducted randomized trials with 2,500 participants to test
whether culturally biased AI enhances human decision-making. Participants
interacted with politically diverse GPT-4o variants on information evaluation
tasks. Partisan AI assistants enhanced human performance, increased engagement,
and reduced evaluative bias compared to non-biased counterparts, with amplified
benefits when participants encountered opposing views. These gains carried a
trust penalty: participants underappreciated biased AI and overcredited neutral
systems. Exposing participants to two AIs whose biases flanked human
perspectives closed the perception-performance gap. These findings complicate
conventional wisdom about AI neutrality, suggesting that strategic integration
of diverse cultural biases may foster improved and resilient human
decision-making.

</details>


### [190] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)
*Xi Long,Christy Boscardin,Lauren A. Maggio,Joseph A. Costello,Ralph Gonzales,Rasmyah Hammoudeh,Ki Lai,Yoon Soo Park,Brian C. Gin*

Main category: cs.HC

TL;DR: 本文探讨AI辅助知识综合数据提取，发现AI准确性更取决于可解释性而非幻觉，AI可成知识综合伙伴但需保留人类见解。


<details>
  <summary>Details</summary>
Motivation: 知识综合数据提取劳动密集，AI辅助虽有效率但准确性存疑，需区分AI“幻觉”与合法解释差异。

Method: 开发基于大语言模型的提取平台自动提取数据，比较187篇出版物和17个提取问题上AI与人类的回答，用评分者信度和主题相似度评分衡量一致性，对比提取回答与源出版物识别错误。

Result: AI在具体明确问题上与人类高度一致，在需主观解释或文本缺失问题上一致性较低；人类间一致性不高于AI - 人类；不一致主要源于解释差异，AI不准确情况少见，人类不准确概率近AI三倍。

Conclusion: AI准确性更依赖可解释性，重复AI提取可识别解释复杂性，AI可成为知识综合的透明、可信伙伴，但要保留人类关键见解。

Abstract: Knowledge syntheses (literature reviews) are essential to health professions
education (HPE), consolidating findings to advance theory and practice.
However, they are labor-intensive, especially during data extraction.
Artificial Intelligence (AI)-assisted extraction promises efficiency but raises
concerns about accuracy, making it critical to distinguish AI 'hallucinations'
(fabricated content) from legitimate interpretive differences. We developed an
extraction platform using large language models (LLMs) to automate data
extraction and compared AI to human responses across 187 publications and 17
extraction questions from a published scoping review. AI-human, human-human,
and AI-AI consistencies were measured using interrater reliability
(categorical) and thematic similarity ratings (open-ended). Errors were
identified by comparing extracted responses to source publications. AI was
highly consistent with humans for concrete, explicitly stated questions (e.g.,
title, aims) and lower for questions requiring subjective interpretation or
absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human
consistency was not higher than AI-human and showed the same question-dependent
variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due
to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while
humans were nearly three times more likely to state inaccuracies (4.37%).
Findings suggest AI accuracy depends more on interpretability than
hallucination. Repeating AI extraction can identify interpretive complexity or
ambiguity, refining processes before human review. AI can be a transparent,
trustworthy partner in knowledge synthesis, though caution is needed to
preserve critical human insights.

</details>


### [191] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究分析ChatGPT生成的伦理话题议论文本的特征及对读者的说服效果，发现其说服效果受限，为相关研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 探究ChatGPT生成的伦理话题议论文本的修辞和语言特征，以及对人类读者的说服影响。

Method: 通过62名参与者的用户研究和前后交互调查，对生成文本进行语言和修辞分析。

Result: ChatGPT构建的议论文本结构连贯，但说服效果受限，伦理问题上读者伦理担忧持续或加剧，效果因话题而异。

Conclusion: 研究为伦理敏感领域的AI说服研究提供新见解和未来研究基础。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [192] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 研究ChatGPT、Gemini和Claude生成故事中的性别叙事偏差，揭示偏差并强调多层面评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究ChatGPT、Gemini和Claude生成故事中的性别叙事偏差。

Method: 利用Propp角色分类和Freytag叙事结构进行提示设计，采用细读方法分析故事。

Result: 生成的故事中存在尤其是隐性的偏差。

Conclusion: 使用解释性方法在多个层面评估偏差很重要。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [193] [Classifying Cool Dwarfs: Comprehensive Spectral Typing of Field and Peculiar Dwarfs Using Machine Learning](https://arxiv.org/abs/2508.09370)
*Tianxing Zhou,Christopher A. Theissen,S. Jean Feeser,William M. J. Best,Adam J. Burgasser,Kelle L. Cruz,Lexu Zhao*

Main category: astro-ph.SR

TL;DR: 研究机器学习在低分辨率近红外光谱的M0 - T9矮星光谱类型分类中的应用，对比不同模型，KNN模型表现最佳，还分析了信噪比和特征重要性。


<details>
  <summary>Details</summary>
Motivation: 当前低质量恒星和棕矮星光谱分类依赖人工，随着大规模光谱调查产生大量数据，机器学习自动光谱分类方法愈发重要，本文旨在用其对晚期矮星进行重力和金属性子类分类。

Method: 以分箱通量为输入特征，比较随机森林（RF）、支持向量机（SVM）和K近邻（KNN）模型构建的光谱类型估计器的效果，测试不同归一化的影响并分析不同光谱区域的相对重要性。

Result: 最佳的KNN模型能以95.5 ± 0.6%的准确率将源分类到±1光谱类型内，以89.5 ± 0.9%的准确率分配表面重力和金属性子类；信噪比≳60的源分类准确率≳95%；RF模型中zy波段作用最突出，FeH和TiO特征重要性最高。

Conclusion: 机器学习方法可有效应用于低分辨率近红外光谱的M0 - T9矮星光谱类型分类，KNN模型表现良好，且信噪比和特定光谱特征对分类有重要影响。

Abstract: Low-mass stars and brown dwarfs -- spectral types (SpTs) M0 and later -- play
a significant role in studying stellar and substellar processes and
demographics, reaching down to planetary-mass objects. Currently, the
classification of these sources remains heavily reliant on visual inspection of
spectral features, equivalent width measurements, or narrow-/wide-band spectral
indices. Recent advances in machine learning (ML) methods offer automated
approaches for spectral typing, which are becoming increasingly important as
large spectroscopic surveys such as Gaia, SDSS, and SPHEREx generate datasets
containing millions of spectra. We investigate the application of ML in
spectral type classification on low-resolution (R $\sim$ 120) near-infrared
spectra of M0--T9 dwarfs obtained with the SpeX instrument on the NASA Infrared
Telescope Facility. We specifically aim to classify the gravity- and
metallicity-dependent subclasses for late-type dwarfs. We used binned fluxes as
input features and compared the efficacy of spectral type estimators built
using Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor
(KNN) models. We tested the influence of different normalizations and analyzed
the relative importance of different spectral regions for surface gravity and
metallicity subclass classification. Our best-performing model (using KNN)
classifies 95.5 $\pm$ 0.6% of sources to within $\pm$1 SpT, and assigns surface
gravity and metallicity subclasses with 89.5 $\pm$ 0.9% accuracy. We test the
dependence of signal-to-noise ratio on classification accuracy and find sources
with SNR $\gtrsim$ 60 have $\gtrsim$ 95% accuracy. We also find that zy-band
plays the most prominent role in the RF model, with FeH and TiO having the
highest feature importance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [194] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文探索通过神经形态计算范式实现节能的鲁棒拟合，在英特尔Loihi 2硬件上设计了新的脉冲神经网络，能耗仅为标准CPU上成熟算法的15%。


<details>
  <summary>Details</summary>
Motivation: 当前鲁棒拟合研究中能效方面受关注少，而高能耗是AI应用的重要问题，因此探索节能的鲁棒拟合。

Method: 在英特尔Loihi 2真实神经形态硬件上设计新的脉冲神经网络，采用新的事件驱动模型估计公式并使用算法策略缓解硬件精度和指令集限制。

Result: 神经形态鲁棒拟合在达到与标准CPU上成熟算法同等精度时，能耗仅为其15%。

Conclusion: 通过神经形态计算范式可实现节能的鲁棒拟合。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [195] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: 提出新框架TRACE，仅从动态多视图视频中对复杂动态3D场景的运动物理建模，实验显示其在未来帧外推任务中表现出色，还能轻松分割对象。


<details>
  <summary>Details</summary>
Motivation: 现有利用物理信息损失或集成简单物理模型的方法难以学习复杂运动物理，或需要额外标签，因此要提出新方法。

Method: 提出TRACE框架，将每个3D点视为有大小和方向的刚性粒子，直接学习每个粒子的平移旋转动力学系统，明确估计一组完整物理参数来控制粒子随时间的运动。

Result: 在三个现有动态数据集和一个新创建的具有挑战性的合成数据集上的实验表明，该方法在未来帧外推任务中优于基线方法。

Conclusion: 该框架能有效对复杂动态3D场景的运动物理建模，还可通过聚类学习到的物理参数轻松分割多个对象或部分。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [196] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: 提出软件工具ARI3D用于交互式分析三维X射线CT图像区域，实现对微观结构的分类和量化。


<details>
  <summary>Details</summary>
Motivation: 传统三维X射线CT图像微观结构定量分析受成像伪影挑战，需用户做大量决策，因此需要工具辅助。

Method: 开发软件工具ARI3D，辅助用户完成对三维图像中对象分类和量化协议的各步骤。

Result: 未提及具体结果。

Conclusion: ARI3D可改善相识别、考虑部分体积效应、提高对象量化的检测极限和准确性，并统一不同科学领域的定量三维分析。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [197] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出双架构框架应对连续手语识别挑战，实验取得新基准成绩，验证特定任务网络可提升性能。


<details>
  <summary>Details</summary>
Motivation: 连续手语识别存在跨手语者差异大、对新句子结构泛化能力差的问题，传统方法难以有效解决。

Method: 提出Signer - Invariant Conformer解决手语者无关挑战，设计Multi - Scale Fusion Transformer处理未见句子任务。

Result: 在Isharah - 1000数据集实验取得新基准，在特定任务上降低字错误率，在SignEval 2025挑战赛获好名次。

Conclusion: 开发针对连续手语识别特定挑战的任务专用网络可显著提升性能，为后续研究建立新基线。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [198] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出用于检测厌女和性别歧视内容的多模态框架，在两个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上大量冒犯女性内容，通用冒犯内容检测方法难以检测厌女内容，需针对性解决方案。

Method: 提出包含MANM、GFRM、CFLM三个模块的多模态框架，使用自适应门控多模态上下文感知注意力，利用图细化特征，学习特定特征，创建厌女词汇表计算得分，应用测试时特征空间增强。

Result: 在MAMI和MMHS150K数据集上，宏观F1值比现有方法分别平均提高10.17%和8.88%。

Conclusion: 所提多模态框架在检测厌女和性别歧视内容方面有效，优于现有方法。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [199] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: 提出IAD - R1通用后训练框架提升VLMs工业异常检测能力，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中缺陷样本少限制传统方法，VLMs泛化能力虽强但在该领域表现有限。

Method: 提出IAD - R1框架，采用两阶段训练策略，包括PA - SFT阶段用Expert - AD数据集训练和SC - GRPO阶段用奖励函数优化。

Result: IAD - R1在7个VLMs上有显著提升，在6个基准数据集上平均准确率最高提升43.3%，0.5B参数模型零样本表现超商业模型。

Conclusion: IAD - R1有效且优越，代码等将公开。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [200] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: 提出CADAR用于AR认知攻击检测，融合多模态输入并进行统计推理，实验显示精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有AR认知攻击检测方法存在局限，如缺乏语义推理能力、解释性不足。

Method: 提出CADAR方法，融合多模态视觉 - 语言输入获得符号感知图表示，利用基于粒子滤波的统计推理检测攻击。

Result: 在扩展AR认知攻击数据集实验中，在具有挑战性场景下比强基线精度最多提高10.7%。

Conclusion: 神经符号方法对有效且可解释的认知攻击检测有前景。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [201] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 提出RL - MoE框架将敏感视觉数据转为隐私保护文本描述，实验显示其隐私保护优且文本内容丰富，为隐私敏感领域提供方案。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中AI相机对视觉数据需求与隐私保护冲突，现有隐私保护机制不足。

Method: 提出RL - MoE框架，结合专家混合（MoE）架构进行场景分解，用强化学习（RL）代理优化生成文本以兼顾语义准确性和隐私保护。

Result: 在CFP - FP数据集上，将重放攻击成功率降至9.4%，生成的文本内容比基线方法更丰富。

Conclusion: 为隐私敏感领域构建可信AI系统提供实用且可扩展的解决方案，为智能城市和自动驾驶网络安全发展铺路。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [202] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 提出数据高效框架Δ - AttnMask用于视觉指令微调的数据选择，用少量数据达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉指令微调对视觉 - 语言模型很重要，但数据选择研究不足，且有更严格的数据选择挑战。

Method: 通过注意力引导对模型隐藏状态进行掩码，计算原始状态和高注意力区域掩码状态之间的损失差异来量化样本质量。

Result: 在多个视觉 - 语言模型和数据集上，用20%的数据达到SOTA性能，训练加速5倍，整体准确率超全量数据集基线10.1%。

Conclusion: Δ - AttnMask模型和数据无关，在多模态和架构中有广泛适用性。

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [203] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 本文针对源数据不可用且目标数据仅含中性表情的场景，提出用于源自由域适应（SFDA）的个性化特征转换（PFT）方法，避免面部表情生成的复杂性和噪声，提高效率。


<details>
  <summary>Details</summary>
Motivation: 深度面部表情识别（FER）模型在处理微妙表情和高主体间差异时表现受限，现有SFDA方法难以处理目标数据仅来自单一类别的情况，且图像生成不稳定、计算量大。

Method: 提出PFT方法，先在源域数据上预训练转换器，将源主体的特定风格特征进行转换，通过优化表情一致性和风格感知目标来保留表情信息，再在中性目标数据上进行适应，在潜空间进行转换。

Result: PFT避免了面部表情生成的复杂性和噪声，产生了用于分类优化的判别性嵌入。

Conclusion: PFT无需图像合成，减少计算开销，仅调整部分模型，相比基于图像的翻译方法更高效。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [204] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出MME - Emotion基准评估多模态大语言模型（MLLMs）情感理解和推理能力，评估20个先进MLLMs并揭示其优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有情感基准有限，不知MLLMs在不同场景泛化能力和识别情绪触发因素的推理能力。

Method: 构建MME - Emotion基准，含6000多个视频片段及问答对，涵盖八种情感任务，采用多智能体系统框架和混合指标评估。

Result: 当前MLLMs情商表现不佳，最佳模型识别得分39.3%，思维链得分56.0%；通用和专业模型获取情商方式不同。

Conclusion: MME - Emotion可作为未来提升MLLMs情商的基础。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [205] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 文章指出当前多模态大语言模型越狱攻击评估标准可能高估攻击效果，提出四轴评估框架识别有效越狱，开发BSD策略，经测试提升了攻击成功率和输出有害性，揭示现有安全系统弱点。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型易受对抗性提示攻击，当前评估标准可能高估攻击效果，需改进评估和攻击方法。

Method: 引入四轴评估框架，开发Balanced Structural Decomposition (BSD)递归重写策略。

Result: BSD在13个模型上测试，攻击成功率提高67%，输出有害性提高21%，拒绝率降低。

Conclusion: 当前多模态安全系统存在被低估的弱点，BSD策略更有效。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [206] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 提出结合有限手写公式与大规模LaTeX渲染公式的方法，构建Tex80M数据集，训练TexTeller模型达SOTA性能并开源。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）领域因数据稀缺发展受阻，手动标注困难且成本高。

Method: 开发可扩展数据引擎生成LaTeX序列，结合有限手写公式与大规模LaTeX渲染公式构建Tex80M数据集，使用Tex80M和小手写数学表达式数据集混合训练TexTeller模型。

Result: TexTeller模型在几乎所有基准测试中达到了最先进（SOTA）性能。

Conclusion: 公开模型、数据集和代码库，推动HMER领域进一步研究。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [207] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: 提出Gradient - Direction - Aware Gaussian Splatting (GDAGS)解决3DGS在复杂场景的局限，评估显示其有更好渲染质量、减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在复杂场景存在过度重建和高斯过密化问题，增加内存开销，需改进。

Method: 提出梯度相干比（GCR）区分梯度方向一致和冲突的高斯，用非线性动态加权机制实现梯度方向感知的密度控制。

Result: 在不同真实世界基准测试中，GDAGS实现了更好的渲染质量，减少过度重建和过密化，优化高斯使用使内存消耗降低50%。

Conclusion: GDAGS能有效解决3DGS在复杂场景的问题，提升渲染效果并降低内存需求。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [208] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: 提出SegDAC方法，在视觉强化学习的视觉泛化和样本效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习需从高维输入和嘈杂奖励中学习感知和动作，现有大感知模型与RL有效集成以实现视觉泛化和提高样本效率的方法尚不明确。

Method: 提出Segmentation - Driven Actor - Critic方法SegDAC，用Segment Anything进行以对象为中心的分解，用YOLO - World通过文本提示进行语义定位，采用基于transformer的新架构，用在线RL学习关注哪些片段且无需人工标签。

Result: 在Maniskill3的视觉泛化基准测试中，SegDAC实现了显著更好的视觉泛化，在最难设置下使先前性能翻倍，在所有评估任务的样本效率上匹配或超越先前方法。

Conclusion: SegDAC在视觉强化学习的视觉泛化和样本效率方面表现出色。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [209] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出FusionEnsemble - Net用于医疗保健通信中手语识别，结合视觉和运动数据，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗保健通信中准确识别手语存在挑战，需要能准确解读复杂多模态手势的框架。

Method: 提出FusionEnsemble - Net，通过四个不同时空网络同步处理RGB视频和距离多普勒图雷达模态，用基于注意力的融合模块融合特征，最后在集成分类头组合输出。

Result: 在大规模MultiMeDaLIS意大利手语数据集上测试准确率达99.44%，优于现有方法。

Conclusion: 基于注意力融合的多样化时空网络集成，能为复杂多模态孤立手势识别任务提供鲁棒准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [210] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 本文构建最大多标注者皮肤病变分割数据集IMA++，研究标注者、恶性程度等因素导致的变异性，发现标注者间一致性与皮肤病变恶性程度有关，可从皮肤镜图像预测一致性，利用该关联提升多模型架构平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割存在标注者内和标注者间的变异性，边界模糊的病变易产生分歧且常与恶性肿瘤相关，需深入研究变异性。

Method: 构建数据集IMA++，使用Dice测量标注者间一致性（IAA），从皮肤镜图像预测IAA，在多任务学习目标中利用IAA作为临床特征。

Result: 发现IAA与皮肤病变恶性程度有显著关联（p<0.001），预测IAA平均绝对误差为0.108，在多数据集和多模型架构上平衡准确率提升4.2%。

Conclusion: 利用标注者间一致性与病变恶性程度的关联，可作为临床特征提升皮肤病变分类的平衡准确率。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [211] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: 提出X - UniMotion用于全身人体运动的统一隐式潜在表示，能跨身份进行高保真运动迁移，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有运动迁移方法依赖显式骨骼姿势和启发式跨身份调整，本文旨在实现高保真、详细的跨身份运动迁移。

Method: 引入自监督端到端框架，联合学习运动编码器和潜在表示，结合基于DiT的视频生成模型，通过2D空间和颜色增强以及合成3D渲染强制运动 - 身份解耦，用辅助解码器引导运动令牌学习。

Result: X - UniMotion在实验中优于现有方法，能生成具有卓越运动保真度和身份保留的高表现力动画。

Conclusion: X - UniMotion是一种有效的全身人体运动统一隐式潜在表示方法，可实现高质量跨身份运动迁移。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [212] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 本文提出RampNet两阶段管道，生成大规模高质量路缘坡道检测数据集并训练模型，性能超之前工作。


<details>
  <summary>Details</summary>
Motivation: 路缘坡道检测因缺乏大规模高质量数据集仍是难题，之前数据在质量或规模上不足。

Method: 提出两阶段RampNet管道，第一阶段将政府提供的路缘坡道位置数据自动转换为谷歌街景图像像素坐标生成超210000个标注全景图数据集，第二阶段用生成数据集训练改进的ConvNeXt V2检测模型。

Result: 生成数据集精度94.0%、召回率92.5%，检测模型AP达0.9236，远超之前工作。

Conclusion: 贡献了首个大规模、高质量路缘坡道检测数据集、基准和模型。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [213] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究动漫角色草图到全彩绘图的图像翻译模型，发现C - GAN最有效。


<details>
  <summary>Details</summary>
Motivation: 解决漫画和动漫行业中从草图生成全彩绘图成本高的瓶颈问题。

Method: 研究包括Neural Style Transfer、C - GAN和CycleGAN在内的多种图像到图像翻译模型，并进行定性和定量评估。

Result: C - GAN能生成接近人类创作的高质量、高分辨率图像。

Conclusion: C - GAN是最有效的用于动漫角色草图到全彩绘图转换的模型。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [214] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 提出新视觉任务和PaIR - Net框架，创建PaIR数据集，实验显示性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉动作语义和场景中的空间上下文信息，为全面理解不同视觉上下文的动作，需考虑动作内容和发生位置。

Method: 引入同时预测高级动作语义和细粒度身体部位接触区域的新视觉任务，提出PaIR - Net框架，包含CPAM、PGCS和IIM组件，创建PaIR数据集。

Result: PaIR - Net显著优于基线方法，消融实验证实各组件有效。

Conclusion: 所提新任务、框架和数据集有良好效果，代码和数据集将发布。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [215] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出RelayFormer用于图像和视频视觉操作定位，实验达SOTA


<details>
  <summary>Details</summary>
Motivation: 现有视觉操作定位方法缺乏跨模态泛化能力，难以高效处理高分辨率或长时长输入

Method: 提出RelayFormer架构，利用灵活局部单元和GLoRA机制，通过轻量级适配模块集成现有Transformer骨干网络，设计轻量级基于查询的掩码解码器

Result: 在多个基准测试中取得了最先进的定位性能

Conclusion: RelayFormer为可扩展和模态无关的视觉操作定位设定了新基线

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [216] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出新颖输入自适应导航方法提高视觉语言导航（VLN）模型效率，在七个VLN基准测试中实现计算量超2倍减少。


<details>
  <summary>Details</summary>
Motivation: 现有历史感知多模态Transformer模型虽提升VLN性能，但在计算资源有限场景下模型规模成瓶颈。

Method: 引入三种自适应算法，分别在空间、模型内和时间层面提升效率，包括选择性处理全景视图、基于重要性的自适应阈值早期退出、缓存机制。

Result: 在七个VLN基准测试中，三种现成代理在标准和连续环境下计算量减少超2倍。

Conclusion: 所提输入自适应导航方法能有效提高VLN模型效率，代码已公开。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [217] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: 提出GEN - AFFECT框架用于生成富有表现力且身份一致的个性化2D头像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有2D头像生成方法难以捕捉细粒度面部表情且难以跨表情保持身份一致性。

Method: 提出在提取的身份 - 表情表示上对多模态扩散变压器进行条件设置，推理时采用一致注意力机制进行信息共享。

Result: 在生成表情准确性、身份保留和跨细粒度面部表情的目标身份一致性方面表现优于先前的先进方法。

Conclusion: GEN - AFFECT框架在个性化头像生成中有效，能生成富有表现力且身份一致的头像。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [218] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: 现有视频大模型处理长视频存在局限，文章提出无训练框架Video - EM，建模关键帧为情节事件，利用CoT思维，评估显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有关键帧检索方法将问题简化为静态文本图像匹配，忽略时空关系，可能产生冗余关键帧，影响视频问答准确性，需改进。

Method: 引入无训练框架Video - EM，将关键帧建模为时间有序的情节事件，利用CoT思维迭代识别信息丰富的情节记忆子集。

Result: 在多个基准测试中，Video - EM表现优于基线，性能提升4 - 9%，且使用更少帧。

Conclusion: Video - EM框架在长视频理解和问答上具有优越性。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [219] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 本文提出结合ProGAN和SAGAN的GAN变体生成手语图像，在IS和FID指标上优于传统ProGAN，并发布大型印度手语数据集。


<details>
  <summary>Details</summary>
Motivation: 手语识别有进展，但手语生成仍待探索，且需平衡分辨率和细节。

Method: 开发结合ProGAN和SAGAN的GAN变体，使用改进的基于注意力的模型。

Result: 生成高质量印度手语字母、数字和单词图像，IS提升3.2，FID提升30.12。

Conclusion: 改进模型在生成印度手语图像上表现更好，且发布的数据集有价值。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [220] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 提出HyperKD知识蒸馏框架解决高光谱遥感中基础模型应用难题，实验证明其能提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 基础模型直接用于高光谱遥感因光谱差异和观测数据稀缺而有挑战，需有效方法转移知识。

Method: 提出HyperKD框架，基于自编码器，从Prithvi模型向EnMAP图像学生模型蒸馏知识，用特征策略解决逆领域适应问题。

Result: HyperKD显著提升自编码器表征学习，提高重建保真度，下游任务表现更稳健。

Conclusion: 知识蒸馏框架在高光谱遥感分析中有潜力。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [221] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: 提出基于局部曲率特征的加权快速准确对接方法CWFBind，实验显示其在多基准测试中表现出色，兼顾准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的对接方法忽视关键几何信息，导致口袋定位不准确和结合构象不真实。

Method: 在特征提取阶段整合局部曲率描述符；在消息传递过程中嵌入度感知加权机制；采用配体感知动态半径策略和增强损失函数解决口袋预测中的类别不平衡问题。

Result: CWFBind在多个对接基准测试中取得有竞争力的性能。

Conclusion: CWFBind实现了准确性和效率的平衡。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [222] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: 提出COXNet框架用于RGBT微小目标检测，有三项核心创新，在RGBTDronePerson数据集上比现有方法有mAP₅₀ 3.32%的提升。


<details>
  <summary>Details</summary>
Motivation: 检测多模态RGBT图像中的微小目标是计算机视觉中的关键挑战，无人机场景使挑战加剧，现有方法难以有效利用可见和热模态的互补信息。

Method: 提出COXNet框架，包含跨层融合模块、动态对齐和尺度细化模块以及使用GeoShape相似度度量的优化标签分配策略。

Result: COXNet在RGBTDronePerson数据集上比现有方法实现了3.32%的mAP₅₀提升。

Conclusion: COXNet在复杂环境中进行鲁棒检测是有效的。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [223] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: 介绍GoViG新任务，仅依赖原始第一人称视觉数据生成导航指令，将任务分解为两个子任务，采用两种推理策略，用R2R - Goal数据集评估，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构化输入，在未知和非结构化环境适应性差，提出GoViG任务以提高适应性。

Method: 将任务分解为视觉预测和指令生成两个子任务，集成到自回归多模态大语言模型中，采用单遍和交错推理两种策略。

Result: 在R2R - Goal数据集上评估，BLEU - 4和CIDEr分数优于现有方法，有强大的跨领域泛化能力。

Conclusion: GoViG方法在生成导航指令方面有效，能显著提升性能和泛化能力。

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [224] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: 提出Hi - SMGNN框架用于预测胶质瘤IDH突变状态，实验显示其优于基线和最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有功能MRI预测胶质瘤IDH突变状态的方法可用性低且有噪声，现有结构和形态连接组方法忽略大脑层次组织和多尺度交互。

Method: 提出Hi - SMGNN框架，集成区域到模块级别的结构和形态连接组，有带孪生网络和跨模态注意力的多模态交互模块、减少冗余的多尺度特征融合机制和增强个体特异性与可解释性的个性化模块划分策略。

Result: 在UCSF - PDGM数据集上实验表明Hi - SMGNN优于基线和最先进模型，在IDH突变预测中展现出更好的鲁棒性和有效性。

Conclusion: Hi - SMGNN是一种有效的IDH突变预测方法。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [225] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: 提出MInDI - 3D用于去除CBCT伪影，减少辐射暴露，评估显示其有效、可扩展、泛化能力好且获临床认可。


<details>
  <summary>Details</summary>
Motivation: 减少现实世界中稀疏视图锥形束计算机断层扫描（CBCT）的成像辐射暴露。

Method: 将InDI概念从2D扩展到3D，对CBCT体积直接进行迭代去噪处理，用CT - RATE公共数据集生成大量伪CBCT数据集训练模型。

Result: MInDI - 3D有效，PSNR增益大，可减少8倍辐射暴露，性能随训练数据增加而提升，与3D U - Net性能相当，能泛化到新扫描器几何结构，获临床认可。

Conclusion: MInDI - 3D是一种有效的3D条件扩散模型，可用于CBCT伪影去除和减少辐射暴露。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [226] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: 提出NEURAL框架，用语义引导数据压缩解决多模态医学影像数据存储和传输挑战，在肺炎检测中表现良好。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像数据快速增长，在资源受限临床环境中存在存储和传输挑战。

Method: 利用微调的生成式视觉 - 语言模型中图像与放射报告的交叉注意力分数对胸部X光进行结构剪枝，将图像转换为图表示，融合剪枝视觉图与临床报告知识图。

Result: 在MIMIC - CXR和CheXpert Plus数据集上，实现93.4 - 97.7%的图像数据大小缩减，AUC达0.88 - 0.95，优于使用未压缩数据的基线模型。

Conclusion: NEURAL解决了数据大小和临床实用性的权衡问题，能实现高效工作流程和远程放射学，不牺牲性能。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [227] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 本文提出首个论文转视频的智能系统Preacher以解决现有模型局限，能跨五个研究领域生成高质量视频摘要，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在论文转视频任务中存在上下文窗口有限、视频时长约束死板、风格多样性不足和无法呈现特定领域知识等局限。

Method: 采用自上而下方法对论文进行分解、总结和重构，再自下而上生成视频；定义关键场景并引入渐进式思维链（P-CoT）进行细致迭代规划。

Result: Preacher成功在五个研究领域生成高质量视频摘要。

Conclusion: Preacher在论文转视频任务中表现出超越当前视频生成模型的能力。

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [228] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 提出基于层理论框架融合MRI和组织病理学数据，性能超基线且在数据缺失时稳健，代码开源。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤分子亚型分类需侵入性组织提取，现有多模态方法有局限，图模型难保留特征，结构重建机制研究不足。

Method: 提出基于层理论的框架进行MRI和组织病理学数据的结构感知和一致性融合。

Result: 模型性能优于基线方法，在数据缺失或不完整场景中表现稳健。

Conclusion: 研究有助于开发快速诊断的虚拟活检工具。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [229] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出基于NeRF架构的测试时优化（TTO）方法用于长期3D点跟踪，在2D和3D跟踪上有出色表现。


<details>
  <summary>Details</summary>
Motivation: 当前点跟踪方法难以获得一致运动或局限于2D运动，需要新方法进行长期3D点跟踪。

Method: 提出用新的可逆神经辐射场（InvNeRF）架构参数化函数，结合渲染方法优势，采用多尺度HexPlanes和新算法。

Result: 在STIR和SCARE数据集上测试，2D点跟踪平均精度超TTO先进方法近50%，3D点跟踪是首个TTO方法且超越前馈方法。

Conclusion: 所提方法在2D和3D点跟踪上表现优异，结合了基于可变形NeRF重建的优点。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [230] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出RayletDF方法用于3D表面重建，在多数据集上表现优且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于坐标的方法在渲染显式表面时计算量大，需更高效方法。

Method: 引入射线距离场技术，构建包含射线特征提取器、射线距离场预测器和多射线混合器的管道。

Result: 在多个公共真实世界数据集上评估，表现优于现有方法。

Conclusion: 该方法有出色泛化能力，单次前向传播可在未见数据集上重建3D表面。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [231] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 本文提出Region - to - Region变换及R2R模型，设计Clear - VAE和Harmony Controller，提出Random Poisson Blending构建新数据集RPHarmony，实验证明方法优越。


<details>
  <summary>Details</summary>
Motivation: 当前基于LDM的图像协调方法在细节保留和协调能力上有挑战，现有合成数据集缺乏局部变化和复杂光照条件，需提升图像协调能力。

Method: 提出Region - to - Region变换；设计Clear - VAE保留高频细节并消除不协调元素；引入带MACA的Harmony Controller动态调整前景；提出Random Poisson Blending构建新数据集RPHarmony。

Result: 实验表明该方法在定量指标和视觉协调度上优于其他方法，新数据集能让模型在真实例子中生成更逼真图像。

Conclusion: 提出的方法和构建的数据集有效，代码、数据集和模型权重已开放。

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [232] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 本文展示了现成的Stable Diffusion模型可用于视觉上下文学习，无需额外微调就能适应六项任务，还能通过集成多提示提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉上下文学习方法涉及专门训练和额外数据，过程复杂且通用性受限，因此探索用现成模型进行视觉上下文学习。

Method: 在Stable Diffusion架构的自注意力层中进行注意力重新计算，明确纳入查询和示例提示之间的上下文。

Result: 该模型能适应六项不同任务，如在Pascal - 5i数据集的前景分割任务中，mIoU比Visual Prompting和IMProv分别提高8.9%和3.2%，还能通过集成多提示提升性能。

Conclusion: 现成的Stable Diffusion模型可有效用于视觉上下文学习，无需额外微调，具有良好性能和通用性。

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [233] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 本文提出组合匹配新方法用于几何形状组装，能减少匹配歧义，实验效果超现有技术。


<details>
  <summary>Details</summary>
Motivation: 以往几何组装方法依赖传统形状匹配与配准，需改进。

Method: 明确建模互锁形状的'相同表面形状'和'相反体积占用'特性，用等变神经网络估计形状方向以对齐区域。

Result: 在几何组装基准测试中，所提方法显著减少匹配局部歧义，效果超现有技术。

Conclusion: 所提组合匹配方法在几何形状组装中有效且稳健。

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [234] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: 提出免训练框架Story2Board用于自然语言生成故事板，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注主体身份，忽略视觉叙事关键方面，如空间构图、背景演变和叙事节奏。

Method: 引入由潜在面板锚定和互注意力值混合组成的轻量级一致性框架，用现成语言模型转换故事为面板级提示，提出丰富故事板基准和场景多样性指标。

Result: 定性和定量结果及用户研究表明，Story2Board生成的故事板更具动态性、连贯性和叙事吸引力。

Conclusion: Story2Board能使扩散模型生成视觉多样且一致的故事板，优于现有基线。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


### [235] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出深度学习模型自动分割脑组织照片，模型性能接近人工，工具公开。


<details>
  <summary>Details</summary>
Motivation: 现有脑切片照片组织分割需昂贵人工干预，需自动化方法。

Method: 采用U - Net架构，用1414张手动分割图像和2000张合成图像训练。

Result: 模型在未参与训练照片子集上，中位Dice分数超0.98，平均表面距离低于0.4mm，95%豪斯多夫距离低于1.60mm。

Conclusion: 模型性能接近人工，开发的工具公开可用。

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [236] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 传统单数据集训练面对新数据分布常失败，提出通用协作混合模型COME，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在超声图像分析中因数据有限、噪声等问题失败，需构建多异构超声数据集通用框架，解决跨数据集干扰和保留特定特征问题。

Method: 提出COME模型，建立双结构 - 语义共享专家创建通用表示空间，与特定源专家协作提取特征。

Result: 在三种评估模式下实验，COME显著提升平均AP，优于现有方法。

Conclusion: COME模型能利用跨数据集经验分布，为小批量或未见数据提供通用先验，具有强大泛化能力。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [237] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 本文针对自动营养分析缺乏标准评估方法和高质量数据集的问题，引入JFB数据集、综合基准框架并给出基线结果，验证了专业模型性能更优，为该领域提供了新评估数据集和框架。


<details>
  <summary>Details</summary>
Motivation: 自动营养分析领域缺乏标准化评估方法和高质量的真实世界基准数据集，阻碍了该领域的发展。

Method: 1. 推出包含1000张有人工验证注释的食物图像的JFB数据集；2. 详细介绍综合基准框架，含鲁棒指标和面向应用的整体评分；3. 给出通用视觉语言模型和专业模型january/food - vision - v1的基线结果。

Result: 专业模型january/food - vision - v1的整体得分为86.2，比表现最好的通用配置高12.1分。

Conclusion: 本研究为研究界提供了有价值的新评估数据集和严格框架，以指导和评估自动营养分析的未来发展。

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [238] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 文章指出合成图像优势，引入GPT - 4o生成的Echo - 4o - Image数据集微调模型得到Echo - 4o，还提出新评估基准，Echo - 4o表现好且数据集迁移性强。


<details>
  <summary>Details</summary>
Motivation: 探讨相比真实图像数据集，使用GPT - 4o生成的合成数据的原因及优势。

Method: 引入180K规模的Echo - 4o - Image数据集微调Bagel得到Echo - 4o，提出GenEval++和Imagine - Bench两个新评估基准。

Result: Echo - 4o在标准基准测试中表现出色，Echo - 4o - Image应用于其他基础模型有性能提升。

Conclusion: 合成图像有补充罕见场景、提供干净可控监督的优势，Echo - 4o - Image数据集有较强迁移性。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [Objective Soups: Multilingual Multi-Task Modeling for Speech Processing](https://arxiv.org/abs/2508.09228)
*A F M Saif,Lisha Chen,Xiaodong Cui,Songtao Lu,Brian Kingsbury,Tianyi Chen*

Main category: eess.AS

TL;DR: 本文研究多语言、多任务语音处理（MSP），提出三种多目标MSP公式及轻量级层选择机制，实验表明分层多目标优化更有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 训练单模型进行MSP时，不同任务目标冲突，多目标优化随任务数增加效果变差，需探讨冲突目标应联合优化还是分层优化。

Method: 研究三种多目标MSP公式（目标汤配方），在不同优化级别应用多目标优化；引入轻量级层选择机制，仅用问题层计算避冲突梯度。

Result: 在CoVoST v2、LibriSpeech和AISHELL - 1上实验，分离识别和翻译任务的双层配方始终优于标准平面优化。

Conclusion: 分层多目标优化是构建先进MSP模型更有效且可扩展的方法。

Abstract: Training a single model for multilingual, multi-task speech processing (MSP)
is severely hampered by conflicting objectives between tasks like speech
recognition and translation. While multi-objective optimization (MOO) aims to
align gradient updates, its effectiveness diminishes as the number of tasks
grows, making it difficult to find a common descent direction. This raises a
fundamental question: should highly conflicting objectives be optimized jointly
or separated into a hierarchical structure? To address this question, this
paper investigates three multi-objective MSP formulations, which we refer to as
\textbf{objective soup recipes}. These formulations apply multi-objective
optimization at different optimization levels to mitigate potential conflicts
among all objectives. To ensure efficiency, we introduce a lightweight
layer-selection mechanism that computes the conflict-avoiding gradient using
only the most problematic layers, minimizing computational and memory overhead.
Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a
bi-level recipe separating recognition and translation tasks consistently
outperforms standard flat optimization. Our work demonstrates that hierarchical
MOO is a more effective and scalable approach for building state-of-the-art MSP
models. Our code has been released at
https://github.com/afmsaif/Objective_Soups.

</details>


### [240] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 研究用双向Mamba检测合成语音，提出Fake - Mamba方案，评估显示其效果优于SOTA模型且可实时推理。


<details>
  <summary>Details</summary>
Motivation: 语音合成发展加剧安全威胁，推动实时深度伪造检测研究，探究双向Mamba用于检测合成语音的可行性。

Method: 提出Fake - Mamba方案，集成XLSR前端与双向Mamba，引入TransBiMamba、ConBiMamba和PN - BiMamba三个高效编码器。

Result: 在ASVspoof 21 LA、21 DF和In - The - Wild基准测试中，Fake - Mamba的EER分别为0.97%、1.74%和5.85%，优于SOTA模型XLSR - Conformer和XLSR - Mamba。

Conclusion: Fake - Mamba框架能实时推理，有强泛化能力和实际可行性。

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [241] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 提出文本到韵律特征映射的独立模型，经训练与对比实验证明在韵律建模任务有潜力。


<details>
  <summary>Details</summary>
Motivation: 利用韵律中丰富的情感、语义信息及个人特质，构建模型用于下游任务如TTS。

Method: ProMode编码器输入部分掩码的声学特征和文本内容获取固定长度潜在韵律嵌入，解码器用编码的韵律输入和未掩码文本内容预测掩码区域声学特征。

Result: 在F0和能量预测上不同粒度均有改进，集成到TTS系统的感知测试显示比基线有更高韵律偏好。

Conclusion: 模型在韵律建模重要的任务中有潜力。

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


### [242] [Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning](https://arxiv.org/abs/2508.09803)
*Carlos Franzreb,Arnab Das,Tim Polzehl,Sebastian Möller*

Main category: eess.AS

TL;DR: 当前说话人匿名隐私评估对同性别目标选择算法存在高估，提出添加目标分类器并结合对抗学习改进评估，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前说话人匿名隐私评估在使用同性别目标选择算法时高估隐私，未考虑匿名语音包含源和目标说话人信息。

Method: 添加目标分类器测量目标说话人信息影响，并通过对抗学习移除，用于评估。

Result: 该方法对多个匿名器有效，尤其在使用同性别目标选择算法时。

Conclusion: 该方法可使说话人匿名隐私评估更可靠。

Abstract: The current privacy evaluation for speaker anonymization often overestimates
privacy when a same-gender target selection algorithm (TSA) is used, although
this TSA leaks the speaker's gender and should hence be more vulnerable. We
hypothesize that this occurs because the evaluation does not account for the
fact that anonymized speech contains information from both the source and
target speakers. To address this, we propose to add a target classifier that
measures the influence of target speaker information in the evaluation, which
can also be removed with adversarial learning. Experiments demonstrate that
this approach is effective for multiple anonymizers, particularly when using a
same-gender TSA, leading to a more reliable assessment.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [243] [Bayesian-Driven Graph Reasoning for Active Radio Map Construction](https://arxiv.org/abs/2508.09142)
*Wenlihan Lu,Shijian Gao,Miaowen Wen,Yuxuan Liang,Chan-Byoung Chae,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出不确定性感知的无线电地图（URAM）重建框架，集成深度学习组件，实验表明比现有基线提高重建精度达34%。


<details>
  <summary>Details</summary>
Motivation: 低空经济下无线电地图对保障空中平台无线连接至关重要，但自主空中代理数据收集受电池容量限制，影响覆盖范围和效率。

Method: 提出URAM重建框架，集成贝叶斯神经网络实时估计空间不确定性和基于注意力的强化学习策略进行全局推理，基于图推理进行轨迹规划。

Result: URAM比现有基线提高重建精度达34%。

Conclusion: 所提出的URAM框架能有效提高无线电地图重建精度。

Abstract: With the emergence of the low-altitude economy, radio maps have become
essential for ensuring reliable wireless connectivity to aerial platforms.
Autonomous aerial agents are commonly deployed for data collection using
waypoint-based navigation; however, their limited battery capacity
significantly constrains coverage and efficiency. To address this, we propose
an uncertainty-aware radio map (URAM) reconstruction framework that explicitly
leverages graph-based reasoning tailored for waypoint navigation. Our approach
integrates two key deep learning components: (1) a Bayesian neural network that
estimates spatial uncertainty in real time, and (2) an attention-based
reinforcement learning policy that performs global reasoning over a
probabilistic roadmap, using uncertainty estimates to plan informative and
energy-efficient trajectories. This graph-based reasoning enables intelligent,
non-myopic trajectory planning, guiding agents toward the most informative
regions while satisfying safety constraints. Experimental results show that
URAM improves reconstruction accuracy by up to 34% over existing baselines.

</details>


### [244] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: 提出用于无线电地图（RM）构建的RadioMamba架构，兼顾准确性与效率，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的RM构建方法存在准确性 - 效率权衡问题，需要解决该问题。

Method: 引入RadioMamba混合Mamba - UNet架构，采用Mamba - 卷积块，Mamba分支捕捉全局依赖，卷积分支提取局部特征。

Result: RadioMamba比现有方法准确性更高，运行速度快近20倍，仅使用2.9%的模型参数。

Conclusion: RadioMamba提高了准确性和效率，是下一代无线系统实时智能优化的可行方法。

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [245] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: 提出基于深度学习的RT - HAD框架用于轴外数字全息显微镜，解决血细胞聚集体分析难题，实现快速处理大数据和准确检测。


<details>
  <summary>Details</summary>
Motivation: 分析稀有血细胞聚集体具挑战性但能推动无标记功能诊断，传统方法有局限，需改进诊断方法。

Method: 提出RT - HAD框架，结合物理一致的全息重建与检测，用图表示血细胞识别聚集体。

Result: RT - HAD能即时处理超30GB图像数据，血小板聚集体检测误差率8.9%，符合实验室可接受误差率。

Conclusion: RT - HAD解决了即时诊断中的大数据挑战。

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


### [246] [Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications](https://arxiv.org/abs/2508.09242)
*Gaojie Zhou,Junhua Li*

Main category: q-bio.QM

TL;DR: 提出轻量级统一解码模型用于跨脑机接口范式分类，在三种范式上表现优于对比模型，为跨范式分类提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口分类模型多针对单一范式，应用于新范式需重新开发，且实际应用和便携设备部署需要更简单模型。

Method: 模型先进行时空卷积，接着用多尺度局部特征选择模块提取跨范式局部特征并生成加权特征，最后用多维全局特征提取模块提取多维全局特征并与加权特征融合。

Result: 在三种经典脑机接口范式上，模型准确率、宏精度、宏召回率和宏F1分数分别达88.39%、82.36%、80.01%和0.8092，显著优于对比模型。

Conclusion: 本研究为跨脑机接口范式分类提供可行方案，为新一代统一解码系统奠定技术基础，推动低成本通用实际应用。

Abstract: Classification models used in brain-computer interface (BCI) are usually
designed for a single BCI paradigm. This requires the redevelopment of the
model when applying it to a new BCI paradigm, resulting in repeated costs and
effort. Moreover, less complex deep learning models are desired for practical
usage, as well as for deployment on portable devices. In or-der to fill the
above gaps, we, in this study, proposed a light-weight and unified decoding
model for cross-BCI-paradigm classification. The proposed model starts with a
tempo-spatial convolution. It is followed by a multi-scale local feature
selec-tion module, aiming to extract local features shared across BCI paradigms
and generate weighted features. Finally, a mul-ti-dimensional global feature
extraction module is designed, in which multi-dimensional global features are
extracted from the weighted features and fused with the weighted features to
form high-level feature representations associated with BCI para-digms. The
results, evaluated on a mixture of three classical BCI paradigms (i.e., MI,
SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,
80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and
macro-F1-score, respectively, significantly out-performing the compared models.
This study pro-vides a feasible solution for cross-BCI-paradigm
classifica-tion. It lays a technological foundation for de-veloping a new
generation of unified decoding systems, paving the way for low-cost and
universal practical applications.

</details>


### [247] [Exploring Molecular Odor Taxonomies for Structure-based Odor Predictions using Machine Learning](https://arxiv.org/abs/2508.09217)
*Akshay Sajan,Stijn Sluis,Reza Haydarlou,Sanne Abeln,Pasquale Lisena,Raphael Troncy,Caro Verbeek,Inger Leemans,Halima Mouhib*

Main category: q-bio.QM

TL;DR: 利用专家和数据驱动的气味分类法可提升基于结构的气味预测机器学习模型性能，还对分类法质量评估并公开数据。


<details>
  <summary>Details</summary>
Motivation: 解决从分子结构预测气味时对气味空间理解有限和结构 - 气味关系复杂的问题。

Method: 使用基于语义和感知相似性的专家分类法以及基于聚类气味描述符共现模式的数据驱动分类法来改进机器学习模型预测。

Result: 两种分类法都提升了不同机器学习模型的预测效果，优于随机分组，通过不同气味类别的预测性能评估了分类法质量。

Conclusion: 数据驱动分类法有助于评估专家分类法和理解分子气味空间，公开数据为未来研究提供基础。

Abstract: One of the key challenges to predict odor from molecular structure is
unarguably our limited understanding of the odor space and the complexity of
the underlying structure-odor relationships. Here, we show that the predictive
performance of machine learning models for structure-based odor predictions can
be improved using both, an expert and a data-driven odor taxonomy. The expert
taxonomy is based on semantic and perceptual similarities, while the
data-driven taxonomy is based on clustering co-occurrence patterns of odor
descriptors directly from the prepared dataset. Both taxonomies improve the
predictions of different machine learning models and outperform random
groupings of descriptors that do not reflect existing relations between odor
descriptors. We assess the quality of both taxonomies through their predictive
performance across different odor classes and perform an in-depth error
analysis highlighting the complexity of odor-structure relationships and
identifying potential inconsistencies within the taxonomies by showcasing pear
odorants used in perfumery. The data-driven taxonomy allows us to critically
evaluate our expert taxonomy and better understand the molecular odor space.
Both taxonomies as well as a full dataset are made available to the community,
providing a stepping stone for a future community-driven exploration of the
molecular basis of smell. In addition, we provide a detailed multi-layer expert
taxonomy including a total of 777 different descriptors from the Pyrfume
repository.

</details>


### [248] [NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical RemodelliNg](https://arxiv.org/abs/2508.09757)
*Nashira Baena,Mariana da Silva,Irina Grigorescu,Aakash Saboo,Saga Masui,Jaques-Donald Tournier,Emma C. Robinson*

Main category: q-bio.QM

TL;DR: 提出新框架学习个体大脑皮层发育轨迹，优于现有方法，为脑发育预测和畸形早期识别带来新可能。


<details>
  <summary>Details</summary>
Motivation: 当前规范建模框架难以捕捉精细解剖细节，需新方法理解个体皮层发育以识别神经发育障碍相关偏差。

Method: 通过分层网络架构，从生物力学约束的纵向微分同胚图像配准中学习个体生长轨迹，用新生儿MRI数据训练。

Result: 相比现有基线方法，提高了变形的生物合理性，生成的生长轨迹更符合群体趋势，变形更平滑，负雅可比行列式更少。

Conclusion: 该框架为脑成熟预测建模和皮层发育畸形早期识别开辟了新途径。

Abstract: Understanding individual cortical development is essential for identifying
deviations linked to neurodevelopmental disorders. However, current normative
modelling frameworks struggle to capture fine-scale anatomical details due to
their reliance on modelling data within a population-average reference space.
Here, we present a novel framework for learning individual growth trajectories
from biomechanically constrained, longitudinal, diffeomorphic image
registration, implemented via a hierarchical network architecture. Trained on
neonatal MRI data from the Developing Human Connectome Project, the method
improves the biological plausibility of warps, generating growth trajectories
that better follow population-level trends while generating smoother warps,
with fewer negative Jacobians, relative to state-of-the-art baselines. The
resulting subject-specific deformations provide interpretable, biologically
grounded mappings of development. This framework opens new possibilities for
predictive modeling of brain maturation and early identification of
malformations of cortical development.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [249] [Counting Short Trajectories in Elementary Cellular Automata using the Transfer Matrix Method](https://arxiv.org/abs/2508.09768)
*Cédric Koller,Barbora Hudcová*

Main category: nlin.CG

TL;DR: 研究初等元胞自动机（ECAs）全局动力学，用转移矩阵法计算有限时间内短吸引子对应初始配置数量及熵，建立熵与Wolfram分类的定量联系，方法有计算成本限制。


<details>
  <summary>Details</summary>
Motivation: 为理解ECAs行为提供定量基础。

Method: 采用转移矩阵法（TMM），计算给定参数下收敛到特定吸引子的初始配置的熵。

Result: 不同Wolfram分类规则有不同熵变化特征，如Class 1规则随时间增加快速收敛到最大熵等。

Conclusion: 该方法为量化轨迹统计提供精确框架，但因计算成本高，实际分析限于短轨迹。

Abstract: Elementary Cellular Automata (ECAs) exhibit diverse behaviours often
categorized by Wolfram's qualitative classification. To provide a quantitative
basis for understanding these behaviours, we investigate the global dynamics of
such automata and we describe a method that allows us to compute the number of
all configurations leading to short attractors in a limited number of time
steps. This computation yields exact results in the thermodynamic limit (as the
CA grid size grows to infinity), and is based on the Transfer Matrix Method
(TMM) that we adapt for our purposes. Specifically, given two parameters $(p,
c)$ we are able to compute the entropy of all initial configurations converging
to an attractor of size $c$ after $p$ time-steps. By calculating such
statistics for various ECA rules, we establish a quantitative connection
between the entropy and the qualitative Wolfram classification scheme. Class 1
rules rapidly converge to maximal entropy for stationary states ($c=1$) as $p$
increases. Class 2 rules also approach maximal entropy quickly for appropriate
cycle lengths $c$, potentially requiring consideration of translations. Class 3
rules exhibit zero or low finite entropy that saturates after a short
transient. Class 4 rules show finite positive entropy, similar to some Class 3
rules. This method provides a precise framework for quantifying trajectory
statistics, although its exponential computational cost in $p+c$ restricts
practical analysis to short trajectories.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [250] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: 本文从特征选择角度增强机器学习算法，将关键词特征与传统特征结合，实验表明该方法有效，能降低分类误差，不依赖第三方服务，最佳准确率达99.68%。


<details>
  <summary>Details</summary>
Motivation: 互联网钓鱼攻击增多，需早期检测钓鱼网站URL，前人已提出机器学习算法，本文想从特征选择角度增强这些算法。

Method: 提出将关键词特征与传统特征结合的新方法，并应用于多个传统机器学习算法。

Result: 该方法有用且有效，在大数据集上平均降低30%分类误差，在小数据集上增强更显著，最佳准确率达99.68%。

Conclusion: 所提方法能有效增强机器学习算法对钓鱼URL的分类效果，且不依赖第三方信息。

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [251] [On the Consistency and Performance of the Iterative Bayesian Update](https://arxiv.org/abs/2508.09980)
*Ehab ElSalamouny,Catuscia Palamidessi*

Main category: cs.CR

TL;DR: 文章证明IBU一致性，实验表明其在多种机制下表现好，还给出处理无限敏感数据字母表的技术。


<details>
  <summary>Details</summary>
Motivation: 现有文献对IBU一致性要么忽略要么证明有误，需证明其一致性。

Method: 利用IBU是最大似然估计器的事实证明一致性，通过真实数据集实验对比。

Result: 证明IBU一致，在几何、拉普拉斯和指数机制下显著优于其他方法，在k - RR和RAPPOR机制下相当，给出处理无限敏感数据字母表的技术。

Conclusion: IBU具有一致性，在多种情况下表现良好且能处理无限敏感数据字母表。

Abstract: For many social, scientific, and commercial purposes, it is often important
to estimate the distribution of the users' data regarding a sensitive
attribute, e.g., their ages, locations, etc. To allow this estimation while
protecting the users' privacy, every user applies a local privacy protection
mechanism that releases a noisy (sanitized) version of their original datum to
the data collector; then the original distribution is estimated using one of
the known methods, such as the matrix inversion (INV), RAPPOR's estimator, and
the iterative Bayesian update (IBU). Unlike the other estimators, the
consistency of IBU, i.e., the convergence of its estimate to the real
distribution as the amount of noisy data grows, has been either ignored or
incorrectly proved in the literature. In this article, we use the fact that IBU
is a maximum likelihood estimator to prove that IBU is consistent. We also
show, through experiments on real datasets, that IBU significantly outperforms
the other methods when the users' data are sanitized by geometric, Laplace, and
exponential mechanisms, whereas it is comparable to the other methods in the
case of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the
alphabet of the sensitive data is infinite, and we show a technique that allows
IBU to operate in this case too.

</details>


### [252] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 提出LoD框架用于LVLMs越狱攻击检测，在多模型和基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs易受越狱攻击且检测方法多依赖启发式规则，性能欠佳。

Method: 提出LoD框架，将越狱检测视为异常检测，引入MSCAV和安全模式自编码器。

Result: 在三个LVLMs和五个基准上实验，平均AUROC达0.9951，最低AUROC较最强基线提升38.89%。

Conclusion: LoD框架能准确统一检测越狱攻击，取得了优异性能。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [253] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 提出Contextual Integrity Verification (CIV)安全架构，可保护大语言模型免受提示注入攻击，在基准测试中表现良好且可即插即用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受提示注入和越狱攻击，现有启发式护栏常被绕过，需要新的安全架构。

Method: 提出CIV，在推理时为每个令牌附加加密签名来源标签，并通过预softmax硬注意力掩码在变压器内执行源信任格。

Result: 在基准测试中攻击成功率为0%，保留93.1%的令牌级相似度，良性任务模型困惑度无下降，有一定延迟开销。

Conclusion: CIV是轻量级补丁，无需微调，能为Llama - 3 - 8B和Mistral - 7B提供即插即用保护，还发布相关资源支持可重复研究。

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [254] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: 本文分析KV缓存隐私风险，设计三种攻击向量，提出KV - Cloak防御机制，实验证明其有效且对模型影响小。


<details>
  <summary>Details</summary>
Motivation: KV缓存虽加速大语言模型推理，但存在未充分研究的隐私风险，需进行分析并提出防御方案。

Method: 设计直接反演攻击、碰撞攻击和注入攻击三种攻击向量；提出基于可逆矩阵混淆方案和算子融合的KV - Cloak防御机制。

Result: KV - Cloak能有效抵御所有攻击，将重建质量降至随机噪声水平，几乎不降低模型准确率且性能开销极小。

Conclusion: KV - Cloak为可信大语言模型部署提供了实用解决方案。

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [255] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: 本文提出社交传感器云服务提供商身份克隆检测新方法，实验证明其可行性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有社交传感器云身份克隆检测方法性能不佳、缺乏重复账户检测方案及大规模真实数据集评估。

Method: 提出的技术包括相似身份检测方法（使用弱监督深度森林模型，利用非隐私敏感用户特征）和基于密码学的认证协议。

Result: 在大型真实数据集上的实验表明，该技术相比现有方法具有可行性和优越性能。

Conclusion: 所提身份克隆检测技术可行且性能优越。

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [256] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: 研究将基于签名的检测纳入训练管道对模型训练的影响，对比不同训练方式，结果显示增强了鲁棒性但有假阳性下限，最后讨论局限并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有AI恶意软件检测系统中基于签名的检测和机器学习组件通常孤立开发和组合，错过降低数据复杂性和增强对抗恶意程序防御的机会。

Method: 对比在综合数据集上训练的模型和仅在未被签名标记的样本上训练机器学习组件的AI系统。

Result: 提高了对对抗性恶意程序和时间数据漂移的鲁棒性，但存在由次优规则选择导致的固定假阳性下限。

Conclusion: 讨论了研究的局限性，并指出未来研究可将动态分析纳入基于AI的恶意软件检测以增强系统弹性。

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [257] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: 提出基于图的恶意软件检测与解释堆叠集成框架，实验表明该框架提升分类性能并能解释恶意软件行为


<details>
  <summary>Details</summary>
Motivation: 现代计算环境中恶意软件检测需准确、可解释且抗规避技术的模型，单模型方法存在泛化性和可解释性问题

Method: 动态从PE文件提取CFG，通过两步嵌入策略编码基本块，用不同消息传递机制的GNN作为基础学习器，用基于注意力的多层感知机作为元学习器聚合输出，引入集成感知的事后解释技术

Result: 框架提升了分类性能

Conclusion: 框架能提升分类性能并对恶意软件行为给出有价值解释

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [258] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: 提出感知现实转换器框架模拟八种神经感知状况，评估显示Vision Transformer架构性能最优，建立相关基准并提供评估指标，有多方面应用。


<details>
  <summary>Details</summary>
Motivation: 解决神经视觉感知状况导致患者与他人体验差异大的问题。

Method: 采用六种不同神经架构构建感知现实转换器框架，学习自然图像到特定感知状态的映射。

Result: Vision Transformer架构在ImageNet和CIFAR - 10数据集评估中表现最优，优于传统CNN和生成方法。

Conclusion: 建立首个神经感知模拟基准，提供新的扰动函数和评估指标，有医学教育等多方面应用，推动对神经网络模拟非典型人类感知的理解。

Abstract: Neurological conditions affecting visual perception create profound
experiential divides between affected individuals and their caregivers,
families, and medical professionals. We present the Perceptual Reality
Transformer, a comprehensive framework employing six distinct neural
architectures to simulate eight neurological perception conditions with
scientifically-grounded visual transformations. Our system learns mappings from
natural images to condition-specific perceptual states, enabling others to
experience approximations of simultanagnosia, prosopagnosia, ADHD attention
deficits, visual agnosia, depression-related changes, anxiety tunnel vision,
and Alzheimer's memory effects. Through systematic evaluation across ImageNet
and CIFAR-10 datasets, we demonstrate that Vision Transformer architectures
achieve optimal performance, outperforming traditional CNN and generative
approaches. Our work establishes the first systematic benchmark for
neurological perception simulation, contributes novel condition-specific
perturbation functions grounded in clinical literature, and provides
quantitative metrics for evaluating simulation fidelity. The framework has
immediate applications in medical education, empathy training, and assistive
technology development, while advancing our fundamental understanding of how
neural networks can model atypical human perception.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [259] [Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data](https://arxiv.org/abs/2508.09243)
*Sebastian Kot*

Main category: econ.GN

TL;DR: 本文研究现代重商主义，用PCA分析新闻文章语义嵌入以追踪其动态。


<details>
  <summary>Details</summary>
Motivation: 研究现代重商主义作为从1945年后全球化范式的颠覆性转变。

Method: 对768维SBERT生成的新闻文章语义嵌入应用主成分分析（PCA），提取正交潜在因素。

Result: 分析主成分载荷确定驱动分类性能的关键语义特征，提高可解释性和预测准确性。

Conclusion: 该方法提供了通过高维文本分析定量追踪新兴重商主义动态的可扩展、数据驱动框架。

Abstract: This paper examines Modern Mercantilism, characterized by rising economic
nationalism, strategic technological decoupling, and geopolitical
fragmentation, as a disruptive shift from the post-1945 globalization paradigm.
It applies Principal Component Analysis (PCA) to 768-dimensional
SBERT-generated semantic embeddings of curated news articles to extract
orthogonal latent factors that discriminate binary event outcomes linked to
protectionism, technological sovereignty, and bloc realignments. Analysis of
principal component loadings identifies key semantic features driving
classification performance, enhancing interpretability and predictive accuracy.
This methodology provides a scalable, data-driven framework for quantitatively
tracking emergent mercantilist dynamics through high-dimensional text analytics

</details>


### [260] [The Market Effects of Algorithms](https://arxiv.org/abs/2508.09513)
*Lindsey Raymond*

Main category: econ.GN

TL;DR: 研究美国单户住房市场中算法预测可用性对市场的影响，发现数字化使算法投资者进入，人类投资者转移投资对象，算法投资者多购买少数族裔住房，减少了房价种族差异。


<details>
  <summary>Details</summary>
Motivation: 了解算法优化个人决策对市场的影响，以美国单户住房市场为研究对象。

Method: 通过识别市场层面的自然实验，即住房记录从物理到数字的数字化转变，研究其对使用算法评估房屋成本的影响。

Result: 数字化使算法投资者进入市场，人类投资者转向难以用算法预测的房屋；算法投资者多购买少数族裔住房，使少数族裔住房平均售价提高5%，减少45%的房价种族差异。

Conclusion: 算法在市场层面有减轻人类偏见的潜力。

Abstract: While there is excitement about the potential for algorithms to optimize
individual decision-making, changes in individual behavior will, almost
inevitably, impact markets. Yet little is known about such effects. In this
paper, I study how the availability of algorithmic prediction changes entry,
allocation, and prices in the US single-family housing market, a key driver of
household wealth. I identify a market-level natural experiment that generates
variation in the cost of using algorithms to value houses: digitization, the
transition from physical to digital housing records. I show that digitization
leads to entry by investors using algorithms, but does not push out investors
using human judgment. Instead, human investors shift toward houses that are
difficult to predict algorithmically. Algorithmic investors predominantly
purchase minority-owned homes, a segment of the market where humans may be
biased. Digitization increases the average sale price of minority-owned homes
by 5% and reduces racial disparities in home prices by 45%. Algorithmic
investors, via competition, affect the prices paid by owner-occupiers and human
investors for minority homes; such changes drive the majority of the reduction
in racial disparities. The decrease in racial inequality underscores the
potential for algorithms to mitigate human biases at the market level.

</details>


### [261] [Artificial Intelligence, Domain AI Readiness, and Firm Productivity](https://arxiv.org/abs/2508.09634)
*Sipeng Zeng,Xiaoning Wang,Tianshu Sun*

Main category: econ.GN

TL;DR: 研究中国上市企业，发现企业AI能力与领域AI就绪度有强互补性，就绪度高的领域AI应用收益更大，且就绪度提升源于学术进步。


<details>
  <summary>Details</summary>
Motivation: 探究为何有些企业和行业能成功应用AI，而有些不能，聚焦领域AI就绪度。

Method: 使用2016 - 2022年中国上市企业面板数据，从专利数据创建新指标，分析IPC4代码共现情况衡量领域AI就绪度，还使用本地AI政策作为工具变量。

Result: 企业AI能力与领域AI就绪度有强互补性，就绪度高的领域AI应用带来更高生产力和创新收益，结果稳健。

Conclusion: 这种互补性由领域 - AI集成的外部进步驱动，领域AI就绪度提升主要源于特定领域AI的学术进步。

Abstract: Although Artificial Intelligence (AI) holds great promise for enhancing
innovation and productivity, many firms struggle to realize its benefits. We
investigate why some firms and industries succeed with AI while others do not,
focusing on the degree to which an industrial domain is technologically
integrated with AI, which we term "domain AI readiness". Using panel data on
Chinese listed firms from 2016 to 2022, we examine how the interaction between
firm-level AI capabilities and domain AI readiness affects firm performance. We
create novel constructs from patent data and measure the domain AI readiness of
a specific domain by analyzing the co-occurrence of four-digit International
Patent Classification (IPC4) codes related to AI with the specific domain
across all patents in that domain. Our findings reveal a strong
complementarity: AI capabilities yield greater productivity and innovation
gains when deployed in domains with higher AI readiness, whereas benefits are
limited in domains that are technologically unprepared or already obsolete.
These results remain robust when using local AI policy initiatives as
instrumental variables. Further analysis shows that this complementarity is
driven by external advances in domain-AI integration, rather than firms' own
strategic pivots. Time-series analysis of IPC4 co-occurrence patterns further
suggests that improvements in domain AI readiness stem primarily from the
academic advancements of AI in specific domains.

</details>


### [262] [A Characterization Framework for Stable Sets and Their Variants](https://arxiv.org/abs/2508.09798)
*Athanasios Andrikopoulos,Nikolaos Sampanis*

Main category: econ.GN

TL;DR: 本文在无限替代集的非自反二元关系框架下扩展稳定集及其主要变体，并给出一般解存在的拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 社会选择理论中偏好循环时最大元素集可能为空，需解决有效选择的定义问题。

Method: 在无限替代集的非自反二元关系框架下扩展稳定集及其主要变体。

Result: 对稳定集及其主要变体进行了扩展，并给出了一般解存在的拓扑特征。

Conclusion: 在无限替代集框架下对稳定集相关概念的扩展及拓扑特征的给出，有助于解决偏好循环时的选择问题。

Abstract: The theory of optimal choice sets offers a well-established solution
framework in social choice and game theory. In social choice theory,
decision-making is typically modeled as a maximization problem. However, when
preferences are cyclic -- as can occur in economic processes -- the set of
maximal elements may be empty, raising the key question of what should be
considered a valid choice. To address this issue, several approaches --
collectively known as general solution theories -- have been proposed for
constructing non-empty choice sets. Among the most prominent in the context of
a finite set of alternatives are the Stable Set (also known as the Von
Neumann-Morgenstern set) and its extensions, such as the Extended Stable Set,
the socially stable set, and the $m$-, and $w$-stable sets. In this paper, we
extend the classical concept of the stable set and its major variants -
specifically, the extended stable set, the socially stable set, and the $m$-
and $w$-stable sets - within the framework of irreflexive binary relations over
infinite sets of alternatives. Additionally, we provide a topological
characterization for the existence of such general solutions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [263] [Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery](https://arxiv.org/abs/2508.09183)
*Farzan Moosavi,Bilal Farooq*

Main category: quant-ph

TL;DR: 研究用量子计算解决大规模带时间窗的容量拾取与交付问题，提出带PQC的RL框架，对比实验显示其优势。


<details>
  <summary>Details</summary>
Motivation: 经典方法在解决大规模优化问题时变得难以处理，量子计算为解决NP难组合问题提供了有前景的替代方案，因此研究用量子计算解决大规模CPDPTW。

Method: 设计了一个用参数化量子电路（PQC）增强的强化学习（RL）框架，提出一种新的特定问题编码量子电路，还设计近端策略优化（PPO）和量子奇异值变换（QSVT）用于对比。

Result: 通过数值实验突出了所提出方法在解决方案规模和训练复杂性方面的优越性。

Conclusion: 所提出的方法能在纳入现实约束的情况下，有效解决大规模CPDPTW，在规模和训练复杂度上表现更优。

Abstract: Quantum computation has demonstrated a promising alternative to solving the
NP-hard combinatorial problems. Specifically, when it comes to optimization,
classical approaches become intractable to account for large-scale solutions.
Specifically, we investigate quantum computing to solve the large-scale
Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this
regard, a Reinforcement Learning (RL) framework augmented with a Parametrized
Quantum Circuit (PQC) is designed to minimize the travel time in a realistic
last-mile on-demand delivery. A novel problem-specific encoding quantum circuit
with an entangling and variational layer is proposed. Moreover, Proximal Policy
Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are
designed for comparison through numerical experiments, highlighting the
superiority of the proposed method in terms of the scale of the solution and
training complexity while incorporating the real-world constraints.

</details>


### [264] [Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks](https://arxiv.org/abs/2508.09209)
*Kun Ming Goh*

Main category: quant-ph

TL;DR: 研究混合量子 - 经典GAN（HQCGANs），用Qiskit模拟器评估不同量子比特数的HQCGANs，结果显示经典GAN得分最佳，7 - 量子比特HQCGAN有竞争力，验证了噪声量子电路作为GAN潜在先验的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统GAN性能受限于经典噪声分布采样的潜在表示质量，研究HQCGANs以探索新的潜在向量生成方式。

Method: 使用Qiskit的AerSimulator和现实噪声模型模拟近期量子设备，用二元MNIST数据集，训练150个周期，用FID和KID评估模型。

Result: 经典GAN得分最佳，7 - 量子比特HQCGAN有竞争力，后期缩小差距，3 - 量子比特模型有收敛限制，训练时间增加适中。

Conclusion: 验证了噪声量子电路作为GAN潜在先验的可行性，在NISQ时代有增强生成建模的潜力。

Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm
for producing high-fidelity data samples, yet their performance is constrained
by the quality of latent representations, typically sampled from classical
noise distributions. This study investigates hybrid quantum-classical GANs
(HQCGANs) in which a quantum generator, implemented via parameterised quantum
circuits, produces latent vectors for a classical discriminator. We evaluate a
classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using
Qiskit's AerSimulator with realistic noise models to emulate near-term quantum
devices. The binary MNIST dataset (digits 0 and 1) is used to align with the
low-dimensional latent spaces imposed by current quantum hardware. Models are
trained for 150 epochs and assessed with Frechet Inception Distance (FID) and
Kernel Inception Distance (KID). Results show that while the classical GAN
achieved the best scores, the 7-qubit HQCGAN produced competitive performance,
narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier
convergence limitations. Efficiency analysis indicates only moderate training
time increases despite quantum sampling overhead. These findings validate the
feasibility of noisy quantum circuits as latent priors in GAN architectures,
highlighting their potential to enhance generative modelling within the
constraints of the noisy intermediate-scale quantum (NISQ) era.

</details>


### [265] [On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators](https://arxiv.org/abs/2508.09844)
*Jasmin Frkatovic,Akash Malemath,Ivan Kankeu,Yannick Werner,Matthias Tschöpe,Vitor Fortes Rey,Sungho Suh,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 研究量子生成对抗网络（QGANs）图像生成能力，发现其泛化有挑战


<details>
  <summary>Details</summary>
Motivation: 探究QGANs在图像生成任务中的能力

Method: 对主流架构进行大量数值测试，在生成器输出为纯态时解析推导判别器质量下界

Result: QGANs难以跨数据集泛化，只能收敛到训练数据的平均表示；给出判别器质量下界

Conclusion: 现有量子生成模型泛化能力存在根本挑战，结果对相关模型有广泛启示

Abstract: We investigate the capabilities of Quantum Generative Adversarial Networks
(QGANs) in image generations tasks. Our analysis centers on fully quantum
implementations of both the generator and discriminator. Through extensive
numerical testing of current main architectures, we find that QGANs struggle to
generalize across datasets, converging on merely the average representation of
the training data. When the output of the generator is a pure-state, we
analytically derive a lower bound for the discriminator quality given by the
fidelity between the pure-state output of the generator and the target data
distribution, thereby providing a theoretical explanation for the limitations
observed in current models. Our findings reveal fundamental challenges in the
generalization capabilities of existing quantum generative models. While our
analysis focuses on QGANs, the results carry broader implications for the
performance of related quantum generative models.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [266] [Deep Generative Models for Discrete Genotype Simulation](https://arxiv.org/abs/2508.09212)
*Sihan Xie,Thierry Tribout,Didier Boichard,Blaise Hanczar,Julien Chiquet,Eric Barrey*

Main category: q-bio.GN

TL;DR: 研究探索生成基因型数据，开发评估常用生成模型并提出适配方法，实验表明模型有效，给出比较和实用指南，代码公开。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注基因表达或单倍型数据生成，基因型数据因离散性更具挑战，需探索其生成。

Method: 开发评估常用生成模型（VAEs、Diffusion Models、GANs），提出适配离散基因型数据的方法，在大规模数据集上实验，用既定指标评估。

Result: 模型能有效捕捉遗传模式，保留基因型 - 表型关联。

Conclusion: 对模型进行全面比较，为未来基因型模拟研究提供实用指南。

Abstract: Deep generative models open new avenues for simulating realistic genomic data
while preserving privacy and addressing data accessibility constraints. While
previous studies have primarily focused on generating gene expression or
haplotype data, this study explores generating genotype data in both
unconditioned and phenotype-conditioned settings, which is inherently more
challenging due to the discrete nature of genotype data. In this work, we
developed and evaluated commonly used generative models, including Variational
Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks
(GANs), and proposed adaptation tailored to discrete genotype data. We
conducted extensive experiments on large-scale datasets, including all
chromosomes from cow and multiple chromosomes from human. Model performance was
assessed using a well-established set of metrics drawn from both deep learning
and quantitative genetics literature. Our results show that these models can
effectively capture genetic patterns and preserve genotype-phenotype
association. Our findings provide a comprehensive comparison of these models
and offer practical guidelines for future research in genotype simulation. We
have made our code publicly available at
https://github.com/SihanXXX/DiscreteGenoGen.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [267] [QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds](https://arxiv.org/abs/2504.19716)
*Navin Sriram Ravie,Keerthi Vasan M,Asokan Thondiyath,Bijo Sebastian*

Main category: cs.RO

TL;DR: 本文提出轻量级机器人抓取规划分析方法，对比现有方法进行模拟和真实环境测试。


<details>
  <summary>Details</summary>
Motivation: 现有抓取规划方法在现实场景泛化性差、规划时间长且缺乏重复性，限制实际应用。

Method: 将抓取规划算法设为优化问题以估计物体表面抓取点，采用软区域生长算法进行平面分割，用基于优化的质量指标评估抓取点。

Result: 在多个模拟物体上与GPD对比，也在真实环境用图像和点云数据评估。

Conclusion: 所提方法在抓取规划上有一定有效性。

Abstract: Grasping has been a long-standing challenge in facilitating the final
interface between a robot and the environment. As environments and tasks become
complicated, the need to embed higher intelligence to infer from the
surroundings and act on them has become necessary. Although most methods
utilize techniques to estimate grasp pose by treating the problem via pure
sampling-based approaches in the six-degree-of-freedom space or as a learning
problem, they usually fail in real-life settings owing to poor generalization
across domains. In addition, the time taken to generate the grasp plan and the
lack of repeatability, owing to sampling inefficiency and the probabilistic
nature of existing grasp planning approaches, severely limits their application
in real-world tasks. This paper presents a lightweight analytical approach
towards robotic grasp planning, particularly antipodal grasps, with little to
no sampling in the six-degree-of-freedom space. The proposed grasp planning
algorithm is formulated as an optimization problem towards estimating grasp
points on the object surface instead of directly estimating the end-effector
pose. To this extent, a soft-region-growing algorithm is presented for
effective plane segmentation, even in the case of curved surfaces. An
optimization-based quality metric is then used for the evaluation of grasp
points to ensure indirect force closure. The proposed grasp framework is
compared with the existing state-of-the-art grasp planning approach, Grasp pose
detection (GPD), as a baseline over multiple simulated objects. The
effectiveness of the proposed approach in comparison to GPD is also evaluated
in a real-world setting using image and point-cloud data, with the planned
grasps being executed using a ROBOTIQ gripper and UR5 manipulator.

</details>


### [268] [SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents](https://arxiv.org/abs/2508.09508)
*Reema Raval,Shalabh Gupta*

Main category: cs.RO

TL;DR: 针对典型海洋环境给无人水面艇（USV）导航带来的挑战，提出SMART - OC算法，经仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 典型海洋环境复杂，有动态障碍物和时变洋流，USV需实时调整路径以避障并利用洋流，该文旨在解决此导航难题。

Method: 提出Self - Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents (SMART - OC)算法，将路径上的障碍物风险与到达目标的时间成本结合以寻找时间 - 风险最优路径。

Result: 通过仿真实验验证了SMART - OC算法的有效性，USV能快速重新规划路径，避障并利用洋流成功到达目标。

Conclusion: SMART - OC算法可在动态环境中实现实时时间 - 风险最优重新规划，有助于USV在复杂海洋环境中安全高效导航。

Abstract: Typical marine environments are highly complex with spatio-temporally varying
currents and dynamic obstacles, presenting significant challenges to Unmanned
Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need
to continuously adapt their paths with real-time information to avoid
collisions and follow the path of least resistance to the goal via exploiting
ocean currents. In this regard, we introduce a novel algorithm, called
Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents
(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic
environments. SMART-OC integrates the obstacle risks along a path with the time
cost to reach the goal to find the time-risk optimal path. The effectiveness of
SMART-OC is validated by simulation experiments, which demonstrate that the USV
performs fast replannings to avoid dynamic obstacles and exploit ocean currents
to successfully reach the goal.

</details>


### [269] [CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail](https://arxiv.org/abs/2508.09558)
*Jiahui Zuo,Boyang Zhang,Fumin Zhang*

Main category: cs.RO

TL;DR: 本文设计了受鹰启发的指甲结构用于电缆操作，提出单抓端到端3D电缆布线框架，实验效果优于传统方法，为3D空间电缆布线提供参考。


<details>
  <summary>Details</summary>
Motivation: 解决普通平行双指夹爪在电缆抓取和引导时存在过挤压和过拉伸的风险，以及电缆布线作为复杂多阶段机器人操作场景对自动化的挑战。

Method: 设计受鹰启发的指甲并安装在夹爪手指上，提出单抓端到端3D电缆布线框架，通过基于视觉的任务配置状态估计和基于运动原语的离线轨迹规划实现连续控制。

Result: 通过对各种电缆和通道槽的评估，该框架在等效感知条件下显著优于传统的拾取 - 放置操作过程。

Conclusion: 可重构的任务设置和所提出的框架为未来3D空间中的电缆布线操作提供了参考。

Abstract: The manipulation of deformable linear flexures has a wide range of
applications in industry, such as cable routing in automotive manufacturing and
textile production. Cable routing, as a complex multi-stage robot manipulation
scenario, is a challenging task for robot automation. Common parallel
two-finger grippers have the risk of over-squeezing and over-tension when
grasping and guiding cables. In this paper, a novel eagle-inspired fingernail
is designed and mounted on the gripper fingers, which helps with cable grasping
on planar surfaces and in-hand cable guiding operations. Then we present a
single-grasp end-to-end 3D cable routing framework utilizing the proposed
fingernails, instead of the common pick-and-place strategy. Continuous control
is achieved to efficiently manipulate cables through vision-based state
estimation of task configurations and offline trajectory planning based on
motion primitives. We evaluate the effectiveness of the proposed framework with
a variety of cables and channel slots, significantly outperforming the
pick-and-place manipulation process under equivalent perceptual conditions. Our
reconfigurable task setting and the proposed framework provide a reference for
future cable routing manipulations in 3D space.

</details>


### [270] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 提出结合大语言模型与行为树的人机交互框架，实验表明其在现实场景实用，准确率约94%，代码开源。


<details>
  <summary>Details</summary>
Motivation: 智能机器人融入人类环境，传统控制方法限制动态非结构化环境下的可用性，需直观可靠的人机交互接口。

Method: 结合大语言模型与行为树，通过激活特定领域插件将自然语言指令转化为可执行动作，支持可扩展和模块化集成。

Result: 在不同环境进行实验，平均认知到执行准确率约94%。

Conclusion: 该方法对人机交互系统和机器人有重要贡献，框架代码已开源。

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [271] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: 本文提出广义行为克隆（GBC）框架解决不同形态类人机器人数据处理和学习算法不通用问题，经多机器人验证有良好性能和迁移性。


<details>
  <summary>Details</summary>
Motivation: 不同形态类人机器人的数据处理和学习算法缺乏通用性，阻碍类人机器人创建。

Method: 通过自适应数据管道、DAgger - MMPPO算法及MMTransformer架构，构建从人类运动到机器人动作的完整路径，并基于Isaac Lab搭建开源平台。

Result: 在多个异构类人机器人上训练策略，展现出良好性能和对新动作的迁移能力。

Conclusion: 建立了创建真正通用类人机器人控制器的首个实用统一途径。

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


### [272] [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)
*Zihan Wang,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 提出解决无人机视觉驱动自主河道跟踪问题的方法，含MGAE、SDM、CADE，仿真效果好。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在GPS信号不可靠的密集河道环境中自主河道跟踪的问题，应用于救援、监视和环境监测等领域。

Method: 将河道跟踪形式化为覆盖控制问题，提出MGAE、SDM和CADE架构构建基于模型的SafeRL框架。

Result: MGAE收敛更快、性能更优；SDM短期状态预测更准确；CADE有效集成安全监管。

Conclusion: CADE能将安全监管集成到基于模型的强化学习中，拉格朗日方法训练时平衡奖励和安全，安全层推理时提升性能。

Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is
critical for applications such as rescue, surveillance, and environmental
monitoring, particularly in dense riverine environments where GPS signals are
unreliable. We formalize river following as a coverage control problem in which
the reward function is submodular, yielding diminishing returns as more unique
river segments are visited, thereby framing the task as a Submodular Markov
Decision Process. First, we introduce Marginal Gain Advantage Estimation, which
refines the reward advantage function by using a sliding window baseline
computed from historical episodic returns, thus aligning the advantage
estimation with the agent's evolving recognition of action value in
non-Markovian settings. Second, we develop a Semantic Dynamics Model based on
patchified water semantic masks that provides more interpretable and
data-efficient short-term prediction of future observations compared to latent
vision dynamics models. Third, we present the Constrained Actor Dynamics
Estimator architecture, which integrates the actor, the cost estimator, and SDM
for cost advantage estimation to form a model-based SafeRL framework capable of
solving partially observable Constrained Submodular Markov Decision Processes.
Simulation results demonstrate that MGAE achieves faster convergence and
superior performance over traditional critic-based methods like Generalized
Advantage Estimation. SDM provides more accurate short-term state predictions
that enable the cost estimator to better predict potential violations. Overall,
CADE effectively integrates safety regulation into model-based RL, with the
Lagrangian approach achieving the soft balance of reward and safety during
training, while the safety layer enhances performance during inference by hard
action overlay.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [273] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 文章探讨商业通信数字化下欺骗性语言检测，受控环境检测准确率超99%，多语言环境有问题，需强大自动文本识别系统。


<details>
  <summary>Details</summary>
Motivation: 商业通信数字化使欺骗性语言出现，需研究系统检测方法。

Method: 综合古典修辞学、传播心理学、语言理论和实证研究，用计算文本分析和个性化Transformer模型。

Result: 在受控环境检测准确率超99%，多语言环境因数据和处理基础设施问题难以复制该表现。

Conclusion: 通信理论表征和实证近似间差距增大，需强大自动文本识别系统。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [274] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 本文提出新数据集、评估指标和基于大语言模型的解决方案Columbo，其性能优于当前最先进的NameGuess，并已投入生产使用。


<details>
  <summary>Details</summary>
Motivation: 扩展表格缩写列名对下游数据任务至关重要，现有工作使用的合成数据和评估指标存在局限性。

Method: 引入4个企业/科学领域的新数据集；提出新的同义词感知评估指标；开发基于大语言模型的Columbo，利用上下文、规则、思维链推理和标记级分析。

Result: Columbo在5个数据集上比NameGuess性能提升4 - 29%，并已在环境科学主要数据门户EDI投入生产使用。

Conclusion: 新数据集、评估指标和Columbo的提出显著推进了表格缩写列名扩展问题的研究现状。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [275] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 提出ParallelSearch框架解决现有推理增强搜索代理顺序处理查询的瓶颈，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强搜索代理按顺序处理查询，存在计算效率瓶颈，需解决该问题。

Method: 提出ParallelSearch框架，让大语言模型识别并行查询结构并同时执行搜索操作，引入专用奖励函数。

Result: 在七个问答基准上平均性能比现有基线高2.9%，处理可并行问题时性能提升12.7%，且只需69.6%的大语言模型调用。

Conclusion: ParallelSearch框架能有效解决现有方法的顺序处理瓶颈，提升计算效率和性能。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [276] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 评估GPT - 4o在低资源设置下对罕见病命名实体识别（NER）的能力，采用多种基于提示的策略，实验表明其性能良好，提示优化的大语言模型可作为传统监督模型的有效替代。


<details>
  <summary>Details</summary>
Motivation: 罕见病领域的NER因数据标注有限、实体类型语义模糊和长尾分布等面临独特挑战，需评估GPT - 4o在低资源设置下的能力。

Method: 使用零样本提示、少样本上下文学习、检索增强生成（RAG）和任务级微调等基于提示的策略，设计结构化提示框架，引入两种语义引导的少样本示例选择方法。

Result: 在RareDis语料库实验中，GPT - 4o性能优于BioClinicalBERT，任务级微调取得新的最优结果；少样本提示在低令牌预算下回报高，RAG额外收益有限；错误分类揭示常见失败模式。

Conclusion: 提示优化的大语言模型可作为生物医学NER中传统监督模型的有效、可扩展替代方案，尤其适用于标注数据稀缺的罕见病应用。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [277] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: 提出神经符号方法TEN从半结构化文本提取表格数据，实验表明其性能优于纯神经基线，用户研究也证实其表格更准确易处理。


<details>
  <summary>Details</summary>
Motivation: 纯神经方法在从无一致分隔符的半结构化文本中提取表格数据时，因幻觉和无法执行硬约束而表现不佳。

Method: TEN在大语言模型上使用结构分解提示生成初始表格，用符号检查器评估并检测问题，通过批判LLM生成修复指导，在自调试循环中修正表格。

Result: TEN在多个数据集和指标上显著优于纯神经基线，精确匹配准确率更高，幻觉率大幅降低；用户研究显示其表格评分更高，超60%情况受用户青睐。

Conclusion: TEN是一种有效的从半结构化文本提取表格数据的方法，在性能和用户体验上均有优势。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [278] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 本文介绍并评估了Human - AI Hybrid Delphi (HAH - Delphi)框架以增强专家共识开发，经三个阶段测试，证明该框架灵活、可扩展，能生成高质量、情境敏感的共识。


<details>
  <summary>Details</summary>
Motivation: 传统专家共识方法存在高负担、简化解读、缺乏条件细节等局限，且信息过载等问题加剧了这些挑战，因此需要新方法。

Method: 引入HAH - Delphi框架，结合生成式AI模型（Gemini 2.5 Pro）、高级人类专家小组和结构化引导，并分三个阶段进行测试。

Result: AI在第一阶段复制了95%已发表的专家共识结论，第二阶段与高级人类专家有95%的方向一致性，第三阶段六名高级专家小组达成>90%的共识覆盖并提前达到主题饱和，AI提供支持。

Conclusion: HAH - Delphi框架灵活、可扩展，能生成高质量、情境敏感的共识，方法稳健，可用于大规模生成条件性、个性化指导和共识框架。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [279] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: 提出APIO用于语法错误纠正和文本简化任务，无需手动指定种子提示，达到新的SOTA性能并开源数据等。


<details>
  <summary>Details</summary>
Motivation: 已有自动提示优化方法依赖手动指定种子提示，为进一步推动研究，在语法错误纠正和文本简化任务上探索更好方法。

Method: 提出APIO，一种简单有效的提示归纳和优化方法，且不依赖手动指定种子提示。

Result: APIO在语法错误纠正和文本简化任务上，对于纯大语言模型提示方法达到了新的最优性能。

Conclusion: APIO方法有效，推动了相关任务研究，开源成果利于后续研究。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [280] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 提出Active Reading框架训练模型学习知识，实验显示该框架效果优于普通微调，还可用于预训练构建更具事实性的模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从参数记忆中学习和回忆事实不可靠，从业者缺乏确保模型可靠一致学习知识的工具。

Method: 提出Active Reading框架，让模型用自生成学习策略学习给定材料。

Result: 在专家领域，用Active Reading训练的模型比普通微调吸收更多知识，如在SimpleQA和FinanceBench上表现大幅提升；发布Meta WikiExpert - 8B，在事实问答上优于数百亿参数模型。

Conclusion: Active Reading框架能有效让模型学习知识，可用于预训练构建更具事实性的模型。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [281] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 提出动态段落选择器（DPS）解决RAG系统重排序模块瓶颈，评估显示其性能优于现有方法，提升复杂场景推理能力。


<details>
  <summary>Details</summary>
Motivation: RAG系统重排序模块在处理复杂多跳查询时存在问题，传统方法难以兼顾信息完整性和噪声控制。

Method: 引入DPS，将段落选择视为监督学习问题，微调以捕捉段落间依赖关系，作为即插即用模块融入RAG管道。

Result: 在五个基准测试中，DPS始终优于现有重排序器和微调方法，在MuSiQue数据集上显著提高F1分数。

Conclusion: DPS通过自适应证据选择，大幅提升复杂RAG场景的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [282] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出可控共情推理方法，构建数据集，采用强化学习等策略提升模型情感支持能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型缺乏基于心理学原理的深度共情推理，为促进情感健康，需改进模型。

Method: 提出可控共情推理，结合自然语言推理与结构化心理步骤；构建细粒度数据集；采用强化学习与统一过程 - 结果奖励模型；引入基于个性的对话重写和冗余感知奖励重加权策略。

Result: 显著提高了模型的情感支持能力。

Conclusion: 该方法推动了富有共情、类人的支持系统的发展。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [283] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 研究语言模型解码温度调整，提出用Precision - Recall框架重新思考损失函数，实现更好的Precision和Recall权衡。


<details>
  <summary>Details</summary>
Motivation: 解决提高语言模型多样性的问题，探究调整解码温度方法的效果及问题。

Method: 通过简单常见案例分析温度调整对模型的影响，提出用Precision - Recall框架重新思考语言模型的损失函数。

Result: 该方法比单纯结合负对数似然训练和温度缩放能实现更好的Precision和Recall权衡。

Conclusion: 研究结果为更通用和鲁棒的语言建模技术提供了途径。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [284] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 论文提出GFPO方法抑制大语言模型长度膨胀问题，在多个基准测试中削减长度同时保持准确率，还提出Adaptive Difficulty GFPO，证明训练时计算量增加可减少测试时计算量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在使用可验证奖励的强化学习训练时存在用长度换准确率的问题，产生大量填充文本。

Method: 引入GFPO，训练时按问题采样更大组并基于响应长度和标记效率两个指标过滤响应；提出Adaptive Difficulty GFPO，根据实时难度估计动态分配训练资源。

Result: 在Phi - 4 - reasoning模型上，GFPO削减GRPO长度膨胀46 - 71%，优化标记奖励比可使长度膨胀削减至71 - 85%。

Conclusion: GFPO表明增加训练时计算量能直接减少测试时计算量，是有效提升推理效率的权衡策略。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [285] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 本文实证评估大语言模型在印度法律场景下执行关键法律任务的表现，发现其在起草和问题发现上表现出色，但在专业法律研究方面存在问题，强调人类专业知识仍不可或缺。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能融入法律行业时，大语言模型执行关键法律任务的能力。

Method: 通过调查实验，比较大语言模型与初级律师的输出，并让高级法律专业学生对工作的有用性、准确性和全面性进行评分。

Result: 大语言模型在起草和问题发现方面表现出色，常与人类工作相当或更优，但在专业法律研究方面存在困难，常产生幻觉、事实错误或编造的输出。

Conclusion: 虽然大语言模型可增强某些法律任务，但人类专业知识对于细致推理和准确应用法律仍然至关重要。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [286] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 随着大语言模型应用增多，确保输出符合人类价值观和安全标准很重要，但缺乏统一评估框架。本文提出多维评估框架，经实验展示其识别模型优缺点及指导研究方向的作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入现实应用，需确保输出符合人类价值观和安全标准，但现有对齐方法缺乏统一评估框架，难以系统比较和指导部署决策。

Method: 引入多维评估框架，从对齐检测、对齐质量、计算效率和鲁棒性四个关键维度评估大语言模型的对齐技术。

Result: 通过对不同基础模型和对齐策略的实验，该框架能识别当前最先进模型的优缺点。

Conclusion: 此评估框架为未来研究方向提供了有价值的见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [287] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 研究对比不同分词器在放射学报告总结任务中的表现，发现特定领域分词器效果更好且有实用优势。


<details>
  <summary>Details</summary>
Motivation: 语言模型的词汇表对文本生成质量有重要作用，但在放射学领域其影响研究不足，需进行系统比较。

Method: 系统比较通用、医学和特定领域分词器在三种成像方式的放射学报告总结任务中的表现，还研究了有无在PubMed摘要上预训练的情况。

Result: 从头训练时，医学和特定领域词汇表表现优于常用自然语言词汇表；预训练可部分缓解分词器间性能差异，特定领域分词器效果最佳且能减少内存需求。

Conclusion: 调整语言模型的词汇表以适应临床领域有实际益处，能提高性能、降低计算需求，使模型在研究和实际医疗场景更易使用和有效。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [288] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 本文聚焦多LLM序列选择问题，提出基于神经上下文老虎机的算法，实验证明其比其他LLM选择算法更有效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM流行，需低成本预测LLM能否成功输出答案，复杂任务需多LLM协作，现有算法不适用序列选择。

Method: 提出基于神经上下文老虎机的算法，在线训练神经网络为每个子任务建模LLM成功率，指导LLM选择。

Result: 在电信问答和医疗诊断预测数据集上的实验表明，该方法比其他LLM选择算法更有效。

Conclusion: 所提算法能解决多LLM序列选择问题，即使缺乏历史性能数据也能有效指导选择。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


### [289] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出自动框架用大语言模型生成文本解释，评估质量并研究其对模型性能影响，自动化解释在提升模型性能上与人工标注相当。


<details>
  <summary>Details</summary>
Motivation: 传统文本解释依赖人工标注，成本高、耗人力且扩展性差，需自动化方法。

Method: 使用多个先进大语言模型构建自动框架生成文本解释，用自然语言生成指标评估质量，在两个基准数据集上研究其对预训练语言模型和大语言模型性能的影响。

Result: 自动化解释在提升模型性能上与人工标注解释具有高度竞争力。

Conclusion: 基于大语言模型的自动化文本解释生成是扩展自然语言处理数据集和提升模型性能的有前景途径。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [290] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 文章探讨可解释NLP实践应用，通过访谈分析从业者和研究者观点，发现概念差距、低满意度和评估挑战，强调明确定义与以用户为中心框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 可解释NLP受关注，但从业者对其实际应用和有效性的观点研究不足，本文旨在填补该研究空白。

Method: 通过对行业从业者进行基于定性访谈的研究，并与学术研究者进行补充访谈，系统分析和比较他们的观点。

Result: 发现概念差距、对当前可解释性方法满意度低，以及评估挑战。

Conclusion: 强调需要明确定义与以用户为中心的框架，以更好地在实践中应用可解释NLP。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [291] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文对用于训练AI临床助手的临床心理健康数据集进行全面调查，指出现存问题并给出建议。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康障碍增多，但专业临床医生不足，AI辅助心理健康诊疗有前景，然而开发有效、可靠且符合伦理的AI依赖高质量临床训练数据集，现存数据集存在分散、记录不足和难以获取等问题。

Method: 对相关临床心理健康数据集按精神障碍类型、数据模态、任务类型、可访问性和社会文化背景进行分类，并研究合成临床心理健康数据集。

Result: 发现存在缺乏纵向数据、文化和语言代表性有限、收集和标注标准不一致以及合成数据模态不足等关键差距。

Conclusion: 概述了未来数据集整理和标准化的关键挑战，并给出促进开发更强大、可推广和公平的心理健康AI系统的可行建议。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [292] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文对创新的大语言模型架构进行系统研究，涵盖多种高效建模方法及应用，为高效大语言模型架构绘制蓝图，以推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构计算量大，对大规模训练和实际部署造成障碍，需研究创新架构提升效率。

Method: 从语言建模出发，系统研究线性和稀疏序列建模方法、高效全注意力变体、稀疏专家混合、混合模型架构和新兴扩散大语言模型，并探讨其在其他模态的应用。

Result: 将近期研究分类，呈现现代高效大语言模型架构蓝图。

Conclusion: 希望能激励未来朝着更高效、多功能的人工智能系统开展研究。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [293] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 介绍了评估长上下文理解的基准PRELUDE，实验显示现有方法在该任务上落后人类，凸显长上下文理解和推理有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准对长上下文理解和深度推理要求不足，提出新基准以更好评估长上下文理解能力。

Method: 创建PRELUDE基准，通过判断角色前传故事与原著规范叙事是否一致的任务来评估。

Result: 实验结果表明，上下文学习、RAG、使用最先进大语言模型的领域内训练和商业DeepResearch服务比人类落后超15%；模型推理准确率与人类差距超30%。

Conclusion: 长上下文理解和推理能力有很大的改进空间。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [294] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 论文介绍了Memory Decoder，可在不改变原模型参数下实现高效领域适配，实验表明能有效适配多种模型到特定领域并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型适配特定领域有挑战，现有方法如DAPT训练成本高、会灾难性遗忘，RAG推理延迟大。

Method: 引入Memory Decoder，用小型transformer解码器模仿外部非参数检索器行为，训练后可与共享相同分词器的预训练语言模型无缝集成。

Result: Memory Decoder能使多种Qwen和Llama模型有效适配生物医学、金融和法律三个专业领域，平均降低困惑度6.17点。

Conclusion: Memory Decoder以预训练记忆组件为核心的新范式可即插即用，持续提升目标领域内多模型性能。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [295] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 介绍VisCodex框架，提升多模态大语言模型代码生成能力，引入MCD数据集和InfiBench - V基准，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型从多模态输入生成代码的能力有限，需要改进。

Method: 采用基于任务向量的模型融合技术，将先进的编码大语言模型集成到强大的视觉 - 语言主干中；引入MCD数据集支持训练和评估；提出InfiBench - V基准进行评估。

Result: VisCodex在开源多模态大语言模型中达到了最先进的性能，接近GPT - 4o等专有模型。

Conclusion: 模型融合策略和新数据集是有效的。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>
